[
["index.html", "Testing one book at a time Prerequisites", " Testing one book at a time Yihui Xie 2019-09-17 Prerequisites print(assets_dir) #&gt; [1] &quot;/home/datascience/repos/machine-learning-rsuite/import/assets&quot; image_file &lt;- file.path(assets_dir, &quot;linear_regression.jpg&quot;) file.exists(image_file) #&gt; [1] TRUE knitr::include_graphics(image_file) This is a sample book written in Markdown. You can use anything that Pandoc’s Markdown supports, e.g., a math equation \\(a^2 + b^2 = c^2\\). The bookdown package can be installed from CRAN or Github: install.packages(&quot;bookdown&quot;) # or the development version # devtools::install_github(&quot;rstudio/bookdown&quot;) Remember each Rmd file contains one and only one chapter, and a chapter is defined by the first-level heading #. To compile this example to PDF, you need XeLaTeX. You are recommended to install TinyTeX (which includes XeLaTeX): https://yihui.name/tinytex/. #&gt; Error in readChar(con, 5L, useBytes = TRUE): cannot open the connection "],
["predicting-sunspot-frequency-with-keras.html", "Chapter 1 Predicting Sunspot Frequency with Keras 1.1 Forecasting sunspots with deep learning 1.2 Recurrent neural networks 1.3 Data 1.4 Exploratory data analysis 1.5 Backtesting: time series cross validation 1.6 The LSTM model 1.7 Data setup 1.8 Preprocessing with recipes 1.9 Reshaping the data 1.10 Building the LSTM model 1.11 Backtesting the model on all splits", " Chapter 1 Predicting Sunspot Frequency with Keras Source: https://blogs.rstudio.com/tensorflow/posts/2018-06-25-sunspots-lstm/ 1.1 Forecasting sunspots with deep learning In this post we will examine making time series predictions using the sunspots dataset that ships with base R. Sunspots are dark spots on the sun, associated with lower temperature. Our post will focus on both how to apply deep learning to time series forecasting, and how to properly apply cross validation in this domain. We’re using the monthly version of the dataset, sunspots.month (there is a yearly version, too). It contains 265 years worth of data (from 1749 through 2013) on the number of sunspots per month. # Core Tidyverse library(tidyverse) library(glue) library(forcats) # Time Series library(timetk) library(tidyquant) library(tibbletime) # Visualization library(cowplot) # Preprocessing library(recipes) # Sampling / Accuracy library(rsample) library(yardstick) # Modeling library(keras) library(tfruns) If you have not previously run Keras in R, you will need to install Keras using the install_keras() function. # Install Keras if you have not installed before install_keras() Forecasting this dataset is challenging because of high short term variability as well as long-term irregularities evident in the cycles. For example, maximum amplitudes reached by the low frequency cycle differ a lot, as does the number of high frequency cycle steps needed to reach that maximum low frequency cycle height. Our post will focus on two dominant aspects: how to apply deep learning to time series forecasting, and how to properly apply cross validation in this domain. For the latter, we will use the rsample package that allows to do resampling on time series data. As to the former, our goal is not to reach utmost performance but to show the general course of action when using recurrent neural networks to model this kind of data. 1.2 Recurrent neural networks When our data has a sequential structure, it is recurrent neural networks (RNNs) we use to model it. As of today, among RNNs, the best established architectures are the GRU (Gated Recurrent Unit) and the LSTM (Long Short Term Memory). For today, let’s not zoom in on what makes them special, but on what they have in common with the most stripped-down RNN: the basic recurrence structure. In contrast to the prototype of a neural network, often called Multilayer Perceptron (MLP), the RNN has a state that is carried on over time. This is nicely seen in this diagram from Goodfellow et al., a.k.a. the “bible of deep learning”: At each time, the state is a combination of the current input and the previous hidden state. This is reminiscent of autoregressive models, but with neural networks, there has to be some point where we halt the dependence. That’s because in order to determine the weights, we keep calculating how our loss changes as the input changes. Now if the input we have to consider, at an arbitrary timestep, ranges back indefinitely - then we will not be able to calculate all those gradients. In practice, then, our hidden state will, at every iteration, be carried forward through a fixed number of steps. We’ll come back to that as soon as we’ve loaded and pre-processed the data. 1.3 Data sunspot.month is a ts class (not tidy), so we’ll convert to a tidy data set using the tk_tbl() function from timetk. We use this instead of as.tibble() from tibble to automatically preserve the time series index as a zoo yearmon index. Last, we’ll convert the zoo index to date using lubridate::as_date() (loaded with tidyquant) and then change to a tbl_time object to make time series operations easier. # make tidy sun_spots &lt;- datasets::sunspot.month %&gt;% tk_tbl() %&gt;% mutate(index = as_date(index)) %&gt;% as_tbl_time(index = index) sun_spots #&gt; # A time tibble: 3,177 x 2 #&gt; # Index: index #&gt; index value #&gt; &lt;date&gt; &lt;dbl&gt; #&gt; 1 1749-01-01 58 #&gt; 2 1749-02-01 62.6 #&gt; 3 1749-03-01 70 #&gt; 4 1749-04-01 55.7 #&gt; 5 1749-05-01 85 #&gt; 6 1749-06-01 83.5 #&gt; # … with 3,171 more rows 1.4 Exploratory data analysis The time series is long (265 years!). We can visualize the time series both in full, and zoomed in on the first 10 years to get a feel for the series. 1.4.1 VISUALIZING SUNSPOT DATA WITH COWPLOT We’ll make two ggplots and combine them using cowplot::plot_grid(). Note that for the zoomed in plot, we make use of tibbletime::time_filter(), which is an easy way to perform time-based filtering. # show 10 years of data p1 &lt;- sun_spots %&gt;% ggplot(aes(index, value)) + geom_point(color = palette_light()[[1]], alpha = 0.5) + theme_tq() + labs( title = &quot;From 1749 to 2013 (Full Data Set)&quot; ) p2 &lt;- sun_spots %&gt;% filter_time(&quot;start&quot; ~ &quot;1800&quot;) %&gt;% ggplot(aes(index, value)) + geom_line(color = palette_light()[[1]], alpha = 0.5) + geom_point(color = palette_light()[[1]]) + geom_smooth(method = &quot;loess&quot;, span = 0.2, se = FALSE) + theme_tq() + labs( title = &quot;1749 to 1759 (Zoomed In To Show Changes over the Year)&quot;, caption = &quot;datasets::sunspot.month&quot; ) p_title &lt;- ggdraw() + draw_label(&quot;Sunspots&quot;, size = 18, fontface = &quot;bold&quot;, colour = palette_light()[[1]]) plot_grid(p_title, p1, p2, ncol = 1, rel_heights = c(0.1, 1, 1)) 1.5 Backtesting: time series cross validation When doing cross validation on sequential data, the time dependencies on preceding samples must be preserved. We can create a cross validation sampling plan by offsetting the window used to select sequential sub-samples. In essence, we’re creatively dealing with the fact that there’s no future test data available by creating multiple synthetic “futures” - a process often, esp. in finance, called “backtesting”. As mentioned in the introduction, the rsample package includes facitlities for backtesting on time series. The vignette, “Time Series Analysis Example”, describes a procedure that uses the rolling_origin() function to create samples designed for time series cross validation. We’ll use this approach. 1.5.1 DEVELOPING A BACKTESTING STRATEGY The sampling plan we create uses 100 years (initial = 12 x 100 samples) for the training set and 50 years (assess = 12 x 50) for the testing (validation) set. We select a skip span of about 22 years (skip = 12 x 22 - 1) to approximately evenly distribute the samples into 6 sets that span the entire 265 years of sunspots history. Last, we select cumulative = FALSE to allow the origin to shift which ensures that models on more recent data are not given an unfair advantage (more observations) over those operating on less recent data. The tibble return contains the rolling_origin_resamples. # backtesting periods_train &lt;- 12 * 100 periods_test &lt;- 12 * 50 skip_span &lt;- 12 * 22 - 1 rolling_origin_resamples &lt;- rolling_origin( sun_spots, initial = periods_train, assess = periods_test, cumulative = FALSE, skip = skip_span ) rolling_origin_resamples #&gt; # Rolling origin forecast resampling #&gt; # A tibble: 6 x 2 #&gt; splits id #&gt; &lt;list&gt; &lt;chr&gt; #&gt; 1 &lt;split [1.2K/600]&gt; Slice1 #&gt; 2 &lt;split [1.2K/600]&gt; Slice2 #&gt; 3 &lt;split [1.2K/600]&gt; Slice3 #&gt; 4 &lt;split [1.2K/600]&gt; Slice4 #&gt; 5 &lt;split [1.2K/600]&gt; Slice5 #&gt; 6 &lt;split [1.2K/600]&gt; Slice6 1.5.2 VISUALIZING THE BACKTESTING STRATEGY We can visualize the resamples with two custom functions. The first, plot_split(), plots one of the resampling splits using ggplot2. Note that an expand_y_axis argument is added to expand the date range to the full sun_spots dataset date range. This will become useful when we visualize all plots together. # Plotting function for a single split plot_split &lt;- function(split, expand_y_axis = TRUE, alpha = 1, size = 1, base_size = 14) { # Manipulate data train_tbl &lt;- training(split) %&gt;% add_column(key = &quot;training&quot;) test_tbl &lt;- testing(split) %&gt;% add_column(key = &quot;testing&quot;) data_manipulated &lt;- bind_rows(train_tbl, test_tbl) %&gt;% as_tbl_time(index = index) %&gt;% mutate(key = fct_relevel(key, &quot;training&quot;, &quot;testing&quot;)) # Collect attributes train_time_summary &lt;- train_tbl %&gt;% tk_index() %&gt;% tk_get_timeseries_summary() test_time_summary &lt;- test_tbl %&gt;% tk_index() %&gt;% tk_get_timeseries_summary() # Visualize g &lt;- data_manipulated %&gt;% ggplot(aes(x = index, y = value, color = key)) + geom_line(size = size, alpha = alpha) + theme_tq(base_size = base_size) + scale_color_tq() + labs( title = glue(&quot;Split: {split$id}&quot;), subtitle = glue(&quot;{train_time_summary$start} to &quot;, &quot;{test_time_summary$end}&quot;), y = &quot;&quot;, x = &quot;&quot; ) + theme(legend.position = &quot;none&quot;) if (expand_y_axis) { sun_spots_time_summary &lt;- sun_spots %&gt;% tk_index() %&gt;% tk_get_timeseries_summary() g &lt;- g + scale_x_date(limits = c(sun_spots_time_summary$start, sun_spots_time_summary$end)) } g } The plot_split() function takes one split (in this case Slice01), and returns a visual of the sampling strategy. We expand the axis to the range for the full dataset using `expand_y_axis = TRUE. rolling_origin_resamples$splits[[1]] %&gt;% plot_split(expand_y_axis = TRUE) + theme(legend.position = &quot;bottom&quot;) The second function, plot_sampling_plan(), scales the plot_split() function to all of the samples using purrr and cowplot. # Plotting function that scales to all splits plot_sampling_plan &lt;- function(sampling_tbl, expand_y_axis = TRUE, ncol = 3, alpha = 1, size = 1, base_size = 14, title = &quot;Sampling Plan&quot;) { # Map plot_split() to sampling_tbl sampling_tbl_with_plots &lt;- sampling_tbl %&gt;% mutate(gg_plots = map(splits, plot_split, expand_y_axis = expand_y_axis, alpha = alpha, base_size = base_size)) # Make plots with cowplot plot_list &lt;- sampling_tbl_with_plots$gg_plots p_temp &lt;- plot_list[[1]] + theme(legend.position = &quot;bottom&quot;) legend &lt;- get_legend(p_temp) p_body &lt;- plot_grid(plotlist = plot_list, ncol = ncol) p_title &lt;- ggdraw() + draw_label(title, size = 14, fontface = &quot;bold&quot;, colour = palette_light()[[1]]) g &lt;- plot_grid(p_title, p_body, legend, ncol = 1, rel_heights = c(0.05, 1, 0.05)) g } We can now visualize the entire backtesting strategy with plot_sampling_plan(). We can see how the sampling plan shifts the sampling window with each progressive slice of the train/test splits. rolling_origin_resamples %&gt;% plot_sampling_plan(expand_y_axis = T, ncol = 3, alpha = 1, size = 1, base_size = 10, title = &quot;Backtesting Strategy: Rolling Origin Sampling Plan&quot;) And, we can set expand_y_axis = FALSE to zoom in on the samples. rolling_origin_resamples %&gt;% plot_sampling_plan(expand_y_axis = F, ncol = 3, alpha = 1, size = 1, base_size = 10, title = &quot;Backtesting Strategy: Zoomed In&quot;) We’ll use this backtesting strategy (6 samples from one time series each with 50/10 split in years and a ~20 year offset) when testing the veracity of the LSTM model on the sunspots dataset. 1.6 The LSTM model To begin, we’ll develop an LSTM model on a single sample from the backtesting strategy, namely, the most recent slice. We’ll then apply the model to all samples to investigate modeling performance. example_split &lt;- rolling_origin_resamples$splits[[6]] example_split_id &lt;- rolling_origin_resamples$id[[6]] We can reuse the plot_split() function to visualize the split. Set expand_y_axis = FALSE to zoom in on the subsample. plot_split(example_split, expand_y_axis = FALSE, size = 0.5) + theme(legend.position = &quot;bottom&quot;) + ggtitle(glue(&quot;Split: {example_split_id}&quot;)) 1.7 Data setup To aid hyperparameter tuning, besides the training set we also need a validation set. For example, we will use a callback, callback_early_stopping, that stops training when no significant performance is seen on the validation set (what’s considered significant is up to you). We will dedicate 2 thirds of the analysis set to training, and 1 third to validation. df_trn &lt;- analysis(example_split)[1:800, , drop = FALSE] df_val &lt;- analysis(example_split)[801:1200, , drop = FALSE] df_tst &lt;- assessment(example_split) First, let’s combine the training and testing data sets into a single data set with a column key that specifies where they came from (either “training” or “testing)”. Note that the tbl_time object will need to have the index respecified during the bind_rows() step, but this issue should be corrected in dplyr soon. df &lt;- bind_rows( df_trn %&gt;% add_column(key = &quot;training&quot;), df_val %&gt;% add_column(key = &quot;validation&quot;), df_tst %&gt;% add_column(key = &quot;testing&quot;) ) %&gt;% as_tbl_time(index = index) df #&gt; # A time tibble: 1,800 x 3 #&gt; # Index: index #&gt; index value key #&gt; &lt;date&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 1859-01-01 83.7 training #&gt; 2 1859-02-01 87.6 training #&gt; 3 1859-03-01 90.3 training #&gt; 4 1859-04-01 85.7 training #&gt; 5 1859-05-01 91 training #&gt; 6 1859-06-01 87.1 training #&gt; # … with 1,794 more rows 1.8 Preprocessing with recipes The LSTM algorithm will usually work better if the input data has been centered and scaled. We can conveniently accomplish this using the recipes package. In addition to step_center and step_scale, we’re using step_sqrt to reduce variance and remov outliers. The actual transformations are executed when we bake the data according to the recipe: rec_obj &lt;- recipe(value ~ ., df) %&gt;% step_sqrt(value) %&gt;% step_center(value) %&gt;% step_scale(value) %&gt;% prep() df_processed_tbl &lt;- bake(rec_obj, df) df_processed_tbl #&gt; # A tibble: 1,800 x 3 #&gt; index value key #&gt; &lt;date&gt; &lt;dbl&gt; &lt;fct&gt; #&gt; 1 1859-01-01 0.728 training #&gt; 2 1859-02-01 0.792 training #&gt; 3 1859-03-01 0.835 training #&gt; 4 1859-04-01 0.761 training #&gt; 5 1859-05-01 0.847 training #&gt; 6 1859-06-01 0.784 training #&gt; # … with 1,794 more rows Next, let’s capture the original center and scale so we can invert the steps after modeling. The square root step can then simply be undone by squaring the back-transformed data. center_history &lt;- rec_obj$steps[[2]]$means[&quot;value&quot;] scale_history &lt;- rec_obj$steps[[3]]$sds[&quot;value&quot;] c(&quot;center&quot; = center_history, &quot;scale&quot; = scale_history) #&gt; center.value scale.value #&gt; 6.76 3.29 1.9 Reshaping the data Keras LSTM expects the input as well as the target data to be in a specific shape. The input has to be a 3-d array of size num_samples, num_timesteps, num_features. Here, num_samples is the number of observations in the set. This will get fed to the model in portions of batch_size. The second dimension, num_timesteps, is the length of the hidden state we were talking about above. Finally, the third dimension is the number of predictors we’re using. For univariate time series, this is 1. How long should we choose the hidden state to be? This generally depends on the dataset and our goal. If we did one-step-ahead forecasts - thus, forecasting the following month only - our main concern would be choosing a state length that allows to learn any patterns present in the data. Now say we wanted to forecast 12 months instead, as does SILSO, the World Data Center for the production, preservation and dissemination of the international sunspot number. The way we can do this, with Keras, is by wiring the LSTM hidden states to sets of consecutive outputs of the same length. Thus, if we want to produce predictions for 12 months, our LSTM should have a hidden state length of 12. These 12 time steps will then get wired to 12 linear predictor units using a time_distributed() wrapper. That wrapper’s task is to apply the same calculation (i.e., the same weight matrix) to every state input it receives. Now, what’s the target array’s format supposed to be? As we’re forecasting several timesteps here, the target data again needs to be 3-dimensional. Dimension 1 again is the batch dimension, dimension 2 again corresponds to the number of timesteps (the forecasted ones), and dimension 3 is the size of the wrapped layer. In our case, the wrapped layer is a layer_dense() of a single unit, as we want exactly one prediction per point in time. So, let’s reshape the data. The main action here is creating the sliding windows of 12 steps of input, followed by 12 steps of output each. This is easiest to understand with a shorter and simpler example. Say our input were the numbers from 1 to 10, and our chosen sequence length (state size) were 4. Tthis is how we would want our training input to look: 1,2,3,4 2,3,4,5 3,4,5,6 And our target data, correspondingly: 5,6,7,8 6,7,8,9 7,8,9,10 We’ll define a short function that does this reshaping on a given dataset. Then finally, we add the third axis that is formally needed (even though that axis is of size 1 in our case). # these variables are being defined just because of the order in which # we present things in this post (first the data, then the model) # they will be superseded by FLAGS$n_timesteps, FLAGS$batch_size and n_predictions # in the following snippet n_timesteps &lt;- 12 n_predictions &lt;- n_timesteps batch_size &lt;- 10 # functions used build_matrix &lt;- function(tseries, overall_timesteps) { t(sapply(1:(length(tseries) - overall_timesteps + 1), function(x) tseries[x:(x + overall_timesteps - 1)])) } reshape_X_3d &lt;- function(X) { dim(X) &lt;- c(dim(X)[1], dim(X)[2], 1) X } # extract values from data frame train_vals &lt;- df_processed_tbl %&gt;% filter(key == &quot;training&quot;) %&gt;% select(value) %&gt;% pull() valid_vals &lt;- df_processed_tbl %&gt;% filter(key == &quot;validation&quot;) %&gt;% select(value) %&gt;% pull() test_vals &lt;- df_processed_tbl %&gt;% filter(key == &quot;testing&quot;) %&gt;% select(value) %&gt;% pull() # build the windowed matrices train_matrix &lt;- build_matrix(train_vals, n_timesteps + n_predictions) valid_matrix &lt;- build_matrix(valid_vals, n_timesteps + n_predictions) test_matrix &lt;- build_matrix(test_vals, n_timesteps + n_predictions) # separate matrices into training and testing parts # also, discard last batch if there are fewer than batch_size samples # (a purely technical requirement) X_train &lt;- train_matrix[, 1:n_timesteps] y_train &lt;- train_matrix[, (n_timesteps + 1):(n_timesteps * 2)] X_train &lt;- X_train[1:(nrow(X_train) %/% batch_size * batch_size), ] y_train &lt;- y_train[1:(nrow(y_train) %/% batch_size * batch_size), ] X_valid &lt;- valid_matrix[, 1:n_timesteps] y_valid &lt;- valid_matrix[, (n_timesteps + 1):(n_timesteps * 2)] X_valid &lt;- X_valid[1:(nrow(X_valid) %/% batch_size * batch_size), ] y_valid &lt;- y_valid[1:(nrow(y_valid) %/% batch_size * batch_size), ] X_test &lt;- test_matrix[, 1:n_timesteps] y_test &lt;- test_matrix[, (n_timesteps + 1):(n_timesteps * 2)] X_test &lt;- X_test[1:(nrow(X_test) %/% batch_size * batch_size), ] y_test &lt;- y_test[1:(nrow(y_test) %/% batch_size * batch_size), ] # add on the required third axis X_train &lt;- reshape_X_3d(X_train) X_valid &lt;- reshape_X_3d(X_valid) X_test &lt;- reshape_X_3d(X_test) y_train &lt;- reshape_X_3d(y_train) y_valid &lt;- reshape_X_3d(y_valid) y_test &lt;- reshape_X_3d(y_test) 1.10 Building the LSTM model Now that we have our data in the required form, let’s finally build the model. As always in deep learning, an important, and often time-consuming, part of the job is tuning hyperparameters. To keep this post self-contained, and considering this is primarily a tutorial on how to use LSTM in R, let’s assume the following settings were found after extensive experimentation (in reality experimentation did take place, but not to a degree that performance couldn’t possibly be improved). Instead of hard coding the hyperparameters, we’ll use tfruns to set up an environment where we could easily perform grid search. We’ll quickly comment on what these parameters do but mainly leave those topics to further posts. FLAGS &lt;- flags( # There is a so-called &quot;stateful LSTM&quot; in Keras. While LSTM is stateful # per se, this adds a further tweak where the hidden states get # initialized with values from the item at same position in the previous # batch. This is helpful just under specific circumstances, or if you want # to create an &quot;infinite stream&quot; of states, in which case you&#39;d use 1 as # the batch size. Below, we show how the code would have to be changed to # use this, but it won&#39;t be further discussed here. flag_boolean(&quot;stateful&quot;, FALSE), # Should we use several layers of LSTM? # Again, just included for completeness, it did not yield any superior # performance on this task. # This will actually stack exactly one additional layer of LSTM units. flag_boolean(&quot;stack_layers&quot;, FALSE), # number of samples fed to the model in one go flag_integer(&quot;batch_size&quot;, 10), # size of the hidden state, equals size of predictions flag_integer(&quot;n_timesteps&quot;, 12), # how many epochs to train for flag_integer(&quot;n_epochs&quot;, 100), # fraction of the units to drop for the linear transformation of the inputs flag_numeric(&quot;dropout&quot;, 0.2), # fraction of the units to drop for the linear transformation of the # recurrent state flag_numeric(&quot;recurrent_dropout&quot;, 0.2), # loss function. Found to work better for this specific case than mean # squared error flag_string(&quot;loss&quot;, &quot;logcosh&quot;), # optimizer = stochastic gradient descent. Seemed to work better than adam # or rmsprop here (as indicated by limited testing) flag_string(&quot;optimizer_type&quot;, &quot;sgd&quot;), # size of the LSTM layer flag_integer(&quot;n_units&quot;, 128), # learning rate flag_numeric(&quot;lr&quot;, 0.003), # momentum, an additional parameter to the SGD optimizer flag_numeric(&quot;momentum&quot;, 0.9), # parameter to the early stopping callback flag_integer(&quot;patience&quot;, 10) ) # the number of predictions we&#39;ll make equals the length of the hidden state n_predictions &lt;- FLAGS$n_timesteps # how many features = predictors we have n_features &lt;- 1 # just in case we wanted to try different optimizers, we could add here optimizer &lt;- switch(FLAGS$optimizer_type, sgd = optimizer_sgd(lr = FLAGS$lr, momentum = FLAGS$momentum) ) # callbacks to be passed to the fit() function # We just use one here: we may stop before n_epochs if the loss on the # validation set does not decrease (by a configurable amount, over a # configurable time) callbacks &lt;- list( callback_early_stopping(patience = FLAGS$patience) ) After all these preparations, the code for constructing and training the model is rather short! Let’s first quickly view the “long version”, that would allow you to test stacking several LSTMs or use a stateful LSTM, then go through the final short version (that does neither) and comment on it. This, just for reference, is the complete code. model &lt;- keras_model_sequential() model %&gt;% layer_lstm( units = FLAGS$n_units, batch_input_shape = c(FLAGS$batch_size, FLAGS$n_timesteps, n_features), dropout = FLAGS$dropout, recurrent_dropout = FLAGS$recurrent_dropout, return_sequences = TRUE, stateful = FLAGS$stateful ) if (FLAGS$stack_layers) { model %&gt;% layer_lstm( units = FLAGS$n_units, dropout = FLAGS$dropout, recurrent_dropout = FLAGS$recurrent_dropout, return_sequences = TRUE, stateful = FLAGS$stateful ) } model %&gt;% time_distributed(layer_dense(units = 1)) model %&gt;% compile( loss = FLAGS$loss, optimizer = optimizer, metrics = list(&quot;mean_squared_error&quot;) ) if (!FLAGS$stateful) { model %&gt;% fit( x = X_train, y = y_train, validation_data = list(X_valid, y_valid), batch_size = FLAGS$batch_size, epochs = FLAGS$n_epochs, callbacks = callbacks ) } else { for (i in 1:FLAGS$n_epochs) { model %&gt;% fit( x = X_train, y = y_train, validation_data = list(X_valid, y_valid), callbacks = callbacks, batch_size = FLAGS$batch_size, epochs = 1, shuffle = FALSE ) model %&gt;% reset_states() } } if (FLAGS$stateful) model %&gt;% reset_states() Now let’s step through the simpler, yet better (or equally) performing configuration below. # create the model model &lt;- keras_model_sequential() # add layers # we have just two, the LSTM and the time_distributed model %&gt;% layer_lstm( units = FLAGS$n_units, # the first layer in a model needs to know the shape of the input data batch_input_shape = c(FLAGS$batch_size, FLAGS$n_timesteps, n_features), dropout = FLAGS$dropout, recurrent_dropout = FLAGS$recurrent_dropout, # by default, an LSTM just returns the final state return_sequences = TRUE ) %&gt;% time_distributed(layer_dense(units = 1)) model %&gt;% compile( loss = FLAGS$loss, optimizer = optimizer, # in addition to the loss, Keras will inform us about current # MSE while training metrics = list(&quot;mean_squared_error&quot;) ) history &lt;- model %&gt;% fit( x = X_train, y = y_train, validation_data = list(X_valid, y_valid), batch_size = FLAGS$batch_size, epochs = FLAGS$n_epochs, callbacks = callbacks ) As we see, training was stopped after ~55 epochs as validation loss did not decrease any more. We also see that performance on the validation set is way worse than performance on the training set - normally indicating overfitting. This topic too, we’ll leave to a separate discussion another time, but interestingly regularization using higher values of dropout and recurrent_dropout (combined with increasing model capacity) did not yield better generalization performance. This is probably related to the characteristics of this specific time series we mentioned in the introduction. Now let’s see how well the model was able to capture the characteristics of the training set. pred_train &lt;- model %&gt;% predict(X_train, batch_size = FLAGS$batch_size) %&gt;% .[, , 1] # Retransform values to original scale pred_train &lt;- (pred_train * scale_history + center_history) ^2 compare_train &lt;- df %&gt;% filter(key == &quot;training&quot;) # build a dataframe that has both actual and predicted values for (i in 1:nrow(pred_train)) { varname &lt;- paste0(&quot;pred_train&quot;, i) compare_train &lt;- mutate(compare_train,!!varname := c( rep(NA, FLAGS$n_timesteps + i - 1), pred_train[i,], rep(NA, nrow(compare_train) - FLAGS$n_timesteps * 2 - i + 1) )) } We compute the average RSME over all sequences of predictions. coln &lt;- colnames(compare_train)[4:ncol(compare_train)] cols &lt;- map(coln, quo(sym(.))) #&gt; Error in is_symbol(x): object &#39;.&#39; not found rsme_train &lt;- map_dbl(cols, function(col) rmse( compare_train, truth = value, estimate = !!col, na.rm = TRUE )) %&gt;% mean() #&gt; `.x` must be a vector, not a function rsme_train #&gt; Error in eval(expr, envir, enclos): object &#39;rsme_train&#39; not found How do these predictions really look? As a visualization of all predicted sequences would look pretty crowded, we arbitrarily pick start points at regular intervals. ggplot(compare_train, aes(x = index, y = value)) + geom_line() + geom_line(aes(y = pred_train1), color = &quot;cyan&quot;) + geom_line(aes(y = pred_train50), color = &quot;red&quot;) + geom_line(aes(y = pred_train100), color = &quot;green&quot;) + geom_line(aes(y = pred_train150), color = &quot;violet&quot;) + geom_line(aes(y = pred_train200), color = &quot;cyan&quot;) + geom_line(aes(y = pred_train250), color = &quot;red&quot;) + geom_line(aes(y = pred_train300), color = &quot;red&quot;) + geom_line(aes(y = pred_train350), color = &quot;green&quot;) + geom_line(aes(y = pred_train400), color = &quot;cyan&quot;) + geom_line(aes(y = pred_train450), color = &quot;red&quot;) + geom_line(aes(y = pred_train500), color = &quot;green&quot;) + geom_line(aes(y = pred_train550), color = &quot;violet&quot;) + geom_line(aes(y = pred_train600), color = &quot;cyan&quot;) + geom_line(aes(y = pred_train650), color = &quot;red&quot;) + geom_line(aes(y = pred_train700), color = &quot;red&quot;) + geom_line(aes(y = pred_train750), color = &quot;green&quot;) + ggtitle(&quot;Predictions on the training set&quot;) This looks pretty good. From the validation loss, we don’t quite expect the same from the test set, though. Let’s see. model_path &lt;- data_out_dir pred_test &lt;- model %&gt;% predict(X_test, batch_size = FLAGS$batch_size) %&gt;% .[, , 1] # Retransform values to original scale pred_test &lt;- (pred_test * scale_history + center_history) ^2 pred_test[1:10, 1:5] %&gt;% print() #&gt; [,1] [,2] [,3] [,4] [,5] #&gt; [1,] 82.0 95.1 111.8 111.7 111.9 #&gt; [2,] 66.4 97.8 107.7 112.9 112.2 #&gt; [3,] 75.9 97.4 109.6 112.5 106.6 #&gt; [4,] 71.1 97.1 108.7 107.2 116.9 #&gt; [5,] 73.0 97.3 103.7 117.5 107.6 #&gt; [6,] 72.3 92.0 113.8 108.1 93.9 #&gt; [7,] 68.0 101.8 104.6 94.1 91.2 #&gt; [8,] 78.6 94.0 90.6 91.2 89.8 #&gt; [9,] 66.9 78.1 87.1 90.4 96.5 #&gt; [10,] 58.3 77.2 87.1 97.0 87.7 compare_test &lt;- df %&gt;% filter(key == &quot;testing&quot;) # build a dataframe that has both actual and predicted values for (i in 1:nrow(pred_test)) { varname &lt;- paste0(&quot;pred_test&quot;, i) compare_test &lt;- mutate(compare_test,!!varname := c( rep(NA, FLAGS$n_timesteps + i - 1), pred_test[i,], rep(NA, nrow(compare_test) - FLAGS$n_timesteps * 2 - i + 1) )) } compare_test %&gt;% write_csv(str_replace(model_path, &quot;.hdf5&quot;, &quot;.test.csv&quot;)) #&gt; Error in open.connection(path, &quot;wb&quot;): cannot open the connection compare_test[FLAGS$n_timesteps:(FLAGS$n_timesteps + 10), c(2, 4:8)] %&gt;% print() #&gt; # A tibble: 11 x 6 #&gt; value pred_test1 pred_test2 pred_test3 pred_test4 pred_test5 #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 125 NA NA NA NA NA #&gt; 2 146. 82.0 NA NA NA NA #&gt; 3 106 95.1 66.4 NA NA NA #&gt; 4 102. 112. 97.8 75.9 NA NA #&gt; 5 122 112. 108. 97.4 71.1 NA #&gt; 6 120. 112. 113. 110. 97.1 73.0 #&gt; # … with 5 more rows coln &lt;- colnames(compare_test)[4:ncol(compare_test)] cols &lt;- map(coln, quo(sym(.))) #&gt; Error in is_symbol(x): object &#39;.&#39; not found rsme_test &lt;- map_dbl(cols, function(col) rmse( compare_test, truth = value, estimate = !!col, na.rm = TRUE )) %&gt;% mean() #&gt; `.x` must be a vector, not a function rsme_test #&gt; Error in eval(expr, envir, enclos): object &#39;rsme_test&#39; not found ggplot(compare_test, aes(x = index, y = value)) + geom_line() + geom_line(aes(y = pred_test1), color = &quot;cyan&quot;) + geom_line(aes(y = pred_test50), color = &quot;red&quot;) + geom_line(aes(y = pred_test100), color = &quot;green&quot;) + geom_line(aes(y = pred_test150), color = &quot;violet&quot;) + geom_line(aes(y = pred_test200), color = &quot;cyan&quot;) + geom_line(aes(y = pred_test250), color = &quot;red&quot;) + geom_line(aes(y = pred_test300), color = &quot;green&quot;) + geom_line(aes(y = pred_test350), color = &quot;cyan&quot;) + geom_line(aes(y = pred_test400), color = &quot;red&quot;) + geom_line(aes(y = pred_test450), color = &quot;green&quot;) + geom_line(aes(y = pred_test500), color = &quot;cyan&quot;) + geom_line(aes(y = pred_test550), color = &quot;violet&quot;) + ggtitle(&quot;Predictions on test set&quot;) That’s not as good as on the training set, but not bad either, given this time series is quite challenging. Having defined and run our model on a manually chosen example split, let’s now revert to our overall re-sampling frame. 1.11 Backtesting the model on all splits To obtain predictions on all splits, we move the above code into a function and apply it to all splits. First, here’s the function. It returns a list of two dataframes, one for the training and test sets each, that contain the model’s predictions together with the actual values. obtain_predictions &lt;- function(split) { df_trn &lt;- analysis(split)[1:800, , drop = FALSE] df_val &lt;- analysis(split)[801:1200, , drop = FALSE] df_tst &lt;- assessment(split) df &lt;- bind_rows( df_trn %&gt;% add_column(key = &quot;training&quot;), df_val %&gt;% add_column(key = &quot;validation&quot;), df_tst %&gt;% add_column(key = &quot;testing&quot;) ) %&gt;% as_tbl_time(index = index) rec_obj &lt;- recipe(value ~ ., df) %&gt;% step_sqrt(value) %&gt;% step_center(value) %&gt;% step_scale(value) %&gt;% prep() df_processed_tbl &lt;- bake(rec_obj, df) center_history &lt;- rec_obj$steps[[2]]$means[&quot;value&quot;] scale_history &lt;- rec_obj$steps[[3]]$sds[&quot;value&quot;] FLAGS &lt;- flags( flag_boolean(&quot;stateful&quot;, FALSE), flag_boolean(&quot;stack_layers&quot;, FALSE), flag_integer(&quot;batch_size&quot;, 10), flag_integer(&quot;n_timesteps&quot;, 12), flag_integer(&quot;n_epochs&quot;, 100), flag_numeric(&quot;dropout&quot;, 0.2), flag_numeric(&quot;recurrent_dropout&quot;, 0.2), flag_string(&quot;loss&quot;, &quot;logcosh&quot;), flag_string(&quot;optimizer_type&quot;, &quot;sgd&quot;), flag_integer(&quot;n_units&quot;, 128), flag_numeric(&quot;lr&quot;, 0.003), flag_numeric(&quot;momentum&quot;, 0.9), flag_integer(&quot;patience&quot;, 10) ) n_predictions &lt;- FLAGS$n_timesteps n_features &lt;- 1 optimizer &lt;- switch(FLAGS$optimizer_type, sgd = optimizer_sgd(lr = FLAGS$lr, momentum = FLAGS$momentum)) callbacks &lt;- list( callback_early_stopping(patience = FLAGS$patience) ) train_vals &lt;- df_processed_tbl %&gt;% filter(key == &quot;training&quot;) %&gt;% select(value) %&gt;% pull() valid_vals &lt;- df_processed_tbl %&gt;% filter(key == &quot;validation&quot;) %&gt;% select(value) %&gt;% pull() test_vals &lt;- df_processed_tbl %&gt;% filter(key == &quot;testing&quot;) %&gt;% select(value) %&gt;% pull() train_matrix &lt;- build_matrix(train_vals, FLAGS$n_timesteps + n_predictions) valid_matrix &lt;- build_matrix(valid_vals, FLAGS$n_timesteps + n_predictions) test_matrix &lt;- build_matrix(test_vals, FLAGS$n_timesteps + n_predictions) X_train &lt;- train_matrix[, 1:FLAGS$n_timesteps] y_train &lt;- train_matrix[, (FLAGS$n_timesteps + 1):(FLAGS$n_timesteps * 2)] X_train &lt;- X_train[1:(nrow(X_train) %/% FLAGS$batch_size * FLAGS$batch_size),] y_train &lt;- y_train[1:(nrow(y_train) %/% FLAGS$batch_size * FLAGS$batch_size),] X_valid &lt;- valid_matrix[, 1:FLAGS$n_timesteps] y_valid &lt;- valid_matrix[, (FLAGS$n_timesteps + 1):(FLAGS$n_timesteps * 2)] X_valid &lt;- X_valid[1:(nrow(X_valid) %/% FLAGS$batch_size * FLAGS$batch_size),] y_valid &lt;- y_valid[1:(nrow(y_valid) %/% FLAGS$batch_size * FLAGS$batch_size),] X_test &lt;- test_matrix[, 1:FLAGS$n_timesteps] y_test &lt;- test_matrix[, (FLAGS$n_timesteps + 1):(FLAGS$n_timesteps * 2)] X_test &lt;- X_test[1:(nrow(X_test) %/% FLAGS$batch_size * FLAGS$batch_size),] y_test &lt;- y_test[1:(nrow(y_test) %/% FLAGS$batch_size * FLAGS$batch_size),] X_train &lt;- reshape_X_3d(X_train) X_valid &lt;- reshape_X_3d(X_valid) X_test &lt;- reshape_X_3d(X_test) y_train &lt;- reshape_X_3d(y_train) y_valid &lt;- reshape_X_3d(y_valid) y_test &lt;- reshape_X_3d(y_test) model &lt;- keras_model_sequential() model %&gt;% layer_lstm( units = FLAGS$n_units, batch_input_shape = c(FLAGS$batch_size, FLAGS$n_timesteps, n_features), dropout = FLAGS$dropout, recurrent_dropout = FLAGS$recurrent_dropout, return_sequences = TRUE ) %&gt;% time_distributed(layer_dense(units = 1)) model %&gt;% compile( loss = FLAGS$loss, optimizer = optimizer, metrics = list(&quot;mean_squared_error&quot;) ) model %&gt;% fit( x = X_train, y = y_train, validation_data = list(X_valid, y_valid), batch_size = FLAGS$batch_size, epochs = FLAGS$n_epochs, callbacks = callbacks ) pred_train &lt;- model %&gt;% predict(X_train, batch_size = FLAGS$batch_size) %&gt;% .[, , 1] # Retransform values pred_train &lt;- (pred_train * scale_history + center_history) ^ 2 compare_train &lt;- df %&gt;% filter(key == &quot;training&quot;) for (i in 1:nrow(pred_train)) { varname &lt;- paste0(&quot;pred_train&quot;, i) compare_train &lt;- mutate(compare_train, !!varname := c( rep(NA, FLAGS$n_timesteps + i - 1), pred_train[i, ], rep(NA, nrow(compare_train) - FLAGS$n_timesteps * 2 - i + 1) )) } pred_test &lt;- model %&gt;% predict(X_test, batch_size = FLAGS$batch_size) %&gt;% .[, , 1] # Retransform values pred_test &lt;- (pred_test * scale_history + center_history) ^ 2 compare_test &lt;- df %&gt;% filter(key == &quot;testing&quot;) for (i in 1:nrow(pred_test)) { varname &lt;- paste0(&quot;pred_test&quot;, i) compare_test &lt;- mutate(compare_test, !!varname := c( rep(NA, FLAGS$n_timesteps + i - 1), pred_test[i, ], rep(NA, nrow(compare_test) - FLAGS$n_timesteps * 2 - i + 1) )) } list(train = compare_train, test = compare_test) } Mapping the function over all splits yields a list of predictions. all_split_preds &lt;- rolling_origin_resamples %&gt;% mutate(predict = map(splits, obtain_predictions)) calc_rmse &lt;- function(df) { coln &lt;- colnames(df)[4:ncol(df)] cols &lt;- map(coln, quo(sym(.))) map_dbl(cols, function(col) rmse( df, truth = value, estimate = !!col, na.rm = TRUE )) %&gt;% mean() } all_split_preds &lt;- all_split_preds %&gt;% unnest(predict) all_split_preds_train &lt;- all_split_preds[seq(1, 11, by = 2), ] all_split_preds_test &lt;- all_split_preds[seq(2, 12, by = 2), ] all_split_rmses_train &lt;- all_split_preds_train %&gt;% mutate(rmse = map_dbl(predict, calc_rmse)) %&gt;% select(id, rmse) #&gt; Error in is_symbol(x): object &#39;.&#39; not found all_split_rmses_test &lt;- all_split_preds_test %&gt;% mutate(rmse = map_dbl(predict, calc_rmse)) %&gt;% select(id, rmse) #&gt; Error in is_symbol(x): object &#39;.&#39; not found How does it look? Here’s RMSE on the training set for the 6 splits. all_split_rmses_train #&gt; Error in eval(expr, envir, enclos): object &#39;all_split_rmses_train&#39; not found all_split_rmses_test #&gt; Error in eval(expr, envir, enclos): object &#39;all_split_rmses_test&#39; not found Looking at these numbers, we see something interesting: Generalization performance is much better for the first three slices of the time series than for the latter ones. This confirms our impression, stated above, that there seems to be some hidden development going on, rendering forecasting more difficult. And here are visualizations of the predictions on the respective training and test sets. First, the training sets: plot_train &lt;- function(slice, name) { ggplot(slice, aes(x = index, y = value)) + geom_line() + geom_line(aes(y = pred_train1), color = &quot;cyan&quot;) + geom_line(aes(y = pred_train50), color = &quot;red&quot;) + geom_line(aes(y = pred_train100), color = &quot;green&quot;) + geom_line(aes(y = pred_train150), color = &quot;violet&quot;) + geom_line(aes(y = pred_train200), color = &quot;cyan&quot;) + geom_line(aes(y = pred_train250), color = &quot;red&quot;) + geom_line(aes(y = pred_train300), color = &quot;red&quot;) + geom_line(aes(y = pred_train350), color = &quot;green&quot;) + geom_line(aes(y = pred_train400), color = &quot;cyan&quot;) + geom_line(aes(y = pred_train450), color = &quot;red&quot;) + geom_line(aes(y = pred_train500), color = &quot;green&quot;) + geom_line(aes(y = pred_train550), color = &quot;violet&quot;) + geom_line(aes(y = pred_train600), color = &quot;cyan&quot;) + geom_line(aes(y = pred_train650), color = &quot;red&quot;) + geom_line(aes(y = pred_train700), color = &quot;red&quot;) + geom_line(aes(y = pred_train750), color = &quot;green&quot;) + ggtitle(name) } train_plots &lt;- map2(all_split_preds_train$predict, all_split_preds_train$id, plot_train) p_body_train &lt;- plot_grid(plotlist = train_plots, ncol = 3) p_title_train &lt;- ggdraw() + draw_label(&quot;Backtested Predictions: Training Sets&quot;, size = 18, fontface = &quot;bold&quot;) plot_grid(p_title_train, p_body_train, ncol = 1, rel_heights = c(0.05, 1, 0.05)) And the test sets: plot_test &lt;- function(slice, name) { ggplot(slice, aes(x = index, y = value)) + geom_line() + geom_line(aes(y = pred_test1), color = &quot;cyan&quot;) + geom_line(aes(y = pred_test50), color = &quot;red&quot;) + geom_line(aes(y = pred_test100), color = &quot;green&quot;) + geom_line(aes(y = pred_test150), color = &quot;violet&quot;) + geom_line(aes(y = pred_test200), color = &quot;cyan&quot;) + geom_line(aes(y = pred_test250), color = &quot;red&quot;) + geom_line(aes(y = pred_test300), color = &quot;green&quot;) + geom_line(aes(y = pred_test350), color = &quot;cyan&quot;) + geom_line(aes(y = pred_test400), color = &quot;red&quot;) + geom_line(aes(y = pred_test450), color = &quot;green&quot;) + geom_line(aes(y = pred_test500), color = &quot;cyan&quot;) + geom_line(aes(y = pred_test550), color = &quot;violet&quot;) + ggtitle(name) } test_plots &lt;- map2(all_split_preds_test$predict, all_split_preds_test$id, plot_test) p_body_test &lt;- plot_grid(plotlist = test_plots, ncol = 3) p_title_test &lt;- ggdraw() + draw_label(&quot;Backtested Predictions: Test Sets&quot;, size = 18, fontface = &quot;bold&quot;) plot_grid(p_title_test, p_body_test, ncol = 1, rel_heights = c(0.05, 1, 0.05)) This has been a long post, and necessarily will have left a lot of questions open, first and foremost: How do we obtain good settings for the hyperparameters (learning rate, number of epochs, dropout)? How do we choose the length of the hidden state? Or even, can we have an intuition how well LSTM will perform on a given dataset (with its specific characteristics)? We will tackle questions like the above in upcoming posts. "]
]
