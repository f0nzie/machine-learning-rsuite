\documentclass[]{book}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={Neural Networks},
            pdfauthor={Alfonso R. Reyes},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{natbib}
\bibliographystyle{apalike}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\providecommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}

  \title{Neural Networks}
    \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
    \author{Alfonso R. Reyes}
    \preauthor{\centering\large\emph}
  \postauthor{\par}
      \predate{\centering\large\emph}
  \postdate{\par}
    \date{2019-09-18}

\usepackage{booktabs}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{prerequisites}{%
\chapter*{Prerequisites}\label{prerequisites}}
\addcontentsline{toc}{chapter}{Prerequisites}

This is a \emph{sample} book written in \textbf{Markdown}. You can use anything that Pandoc's Markdown supports, e.g., a math equation \(a^2 + b^2 = c^2\).

The \textbf{bookdown} package can be installed from CRAN or Github:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"bookdown"}\NormalTok{)}
\CommentTok{# or the development version}
\CommentTok{# devtools::install_github("rstudio/bookdown")}
\end{Highlighting}
\end{Shaded}

\hypertarget{building-deep-neural-nets-with-h2o-that-predict-arrhythmia-of-the-heart}{%
\chapter{Building deep neural nets with h2o that predict arrhythmia of the heart}\label{building-deep-neural-nets-with-h2o-that-predict-arrhythmia-of-the-heart}}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

27 February 2017

This week, I am showing how to build feed-forward deep neural networks or multilayer perceptrons. The models in this example are built to classify ECG data into being either from healthy hearts or from someone suffering from arrhythmia. I will show how to prepare a dataset for modeling, setting weights and other modeling parameters, and finally, how to evaluate model performance with the \texttt{h2o} package.

\hypertarget{deep-learning-with-neural-networks}{%
\subsection{Deep learning with neural networks}\label{deep-learning-with-neural-networks}}

Deep learning with neural networks is arguably one of the most rapidly growing applications of machine learning and AI today. They allow building complex models that consist of multiple hidden layers within artificial networks and are able to find non-linear patterns in unstructured data. Deep neural networks are usually feed-forward, which means that each layer feeds its output to subsequent layers, but recurrent or feed-back neural networks can also be built. Feed-forward neural networks are also called multilayer perceptrons (MLPs).

\hypertarget{h2o}{%
\subsection{H2O}\label{h2o}}

The R package h2o provides a convenient interface to \textbf{H2O}, which is an open-source machine learning and deep learning platform. H2O distributes a wide range of common machine learning algorithms for classification, regression and deep learning.

\hypertarget{preparing-the-r-session}{%
\subsection{Preparing the R session}\label{preparing-the-r-session}}

First, we need to load the packages.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(dplyr)}
\KeywordTok{library}\NormalTok{(h2o)}
\KeywordTok{library}\NormalTok{(ggplot2)}
\KeywordTok{library}\NormalTok{(ggrepel)}
\KeywordTok{library}\NormalTok{(h2o)}

\KeywordTok{h2o.init}\NormalTok{()}
\CommentTok{#> }
\CommentTok{#> H2O is not running yet, starting it now...}
\CommentTok{#> }
\CommentTok{#> Note:  In case of errors look at the following log files:}
\CommentTok{#>     /tmp/RtmpxG0YLu/h2o_datascience_started_from_r.out}
\CommentTok{#>     /tmp/RtmpxG0YLu/h2o_datascience_started_from_r.err}
\CommentTok{#> }
\CommentTok{#> }
\CommentTok{#> Starting H2O JVM and connecting: . Connection successful!}
\CommentTok{#> }
\CommentTok{#> R is connected to the H2O cluster: }
\CommentTok{#>     H2O cluster uptime:         1 seconds 141 milliseconds }
\CommentTok{#>     H2O cluster timezone:       America/Chicago }
\CommentTok{#>     H2O data parsing timezone:  UTC }
\CommentTok{#>     H2O cluster version:        3.22.1.1 }
\CommentTok{#>     H2O cluster version age:    8 months and 21 days !!! }
\CommentTok{#>     H2O cluster name:           H2O_started_from_R_datascience_mwl453 }
\CommentTok{#>     H2O cluster total nodes:    1 }
\CommentTok{#>     H2O cluster total memory:   6.96 GB }
\CommentTok{#>     H2O cluster total cores:    8 }
\CommentTok{#>     H2O cluster allowed cores:  8 }
\CommentTok{#>     H2O cluster healthy:        TRUE }
\CommentTok{#>     H2O Connection ip:          localhost }
\CommentTok{#>     H2O Connection port:        54321 }
\CommentTok{#>     H2O Connection proxy:       NA }
\CommentTok{#>     H2O Internal Security:      FALSE }
\CommentTok{#>     H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4 }
\CommentTok{#>     R Version:                  R version 3.6.0 (2019-04-26)}
\CommentTok{#> Warning in h2o.clusterInfo(): }
\CommentTok{#> Your H2O cluster version is too old (8 months and 21 days)!}
\CommentTok{#> Please download and install the latest version from http://h2o.ai/download/}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my_theme <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(}\DataTypeTok{base_size =} \DecValTok{12}\NormalTok{, }\DataTypeTok{base_family =} \StringTok{"sans"}\NormalTok{)\{}
  \KeywordTok{theme_minimal}\NormalTok{(}\DataTypeTok{base_size =}\NormalTok{ base_size, }\DataTypeTok{base_family =}\NormalTok{ base_family) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}
    \DataTypeTok{axis.text =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{size =} \DecValTok{12}\NormalTok{),}
    \DataTypeTok{axis.title =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{size =} \DecValTok{14}\NormalTok{),}
    \DataTypeTok{panel.grid.major =} \KeywordTok{element_line}\NormalTok{(}\DataTypeTok{color =} \StringTok{"grey"}\NormalTok{),}
    \DataTypeTok{panel.grid.minor =} \KeywordTok{element_blank}\NormalTok{(),}
    \DataTypeTok{panel.background =} \KeywordTok{element_rect}\NormalTok{(}\DataTypeTok{fill =} \StringTok{"aliceblue"}\NormalTok{),}
    \DataTypeTok{strip.background =} \KeywordTok{element_rect}\NormalTok{(}\DataTypeTok{fill =} \StringTok{"darkgrey"}\NormalTok{, }\DataTypeTok{color =} \StringTok{"grey"}\NormalTok{, }\DataTypeTok{size =} \DecValTok{1}\NormalTok{),}
    \DataTypeTok{strip.text =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{face =} \StringTok{"bold"}\NormalTok{, }\DataTypeTok{size =} \DecValTok{12}\NormalTok{, }\DataTypeTok{color =} \StringTok{"white"}\NormalTok{),}
    \DataTypeTok{legend.position =} \StringTok{"right"}\NormalTok{,}
    \DataTypeTok{legend.justification =} \StringTok{"top"}\NormalTok{, }
    \DataTypeTok{panel.border =} \KeywordTok{element_rect}\NormalTok{(}\DataTypeTok{color =} \StringTok{"grey"}\NormalTok{, }\DataTypeTok{fill =} \OtherTok{NA}\NormalTok{, }\DataTypeTok{size =} \FloatTok{0.5}\NormalTok{)}
\NormalTok{  )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\hypertarget{arrhythmia-data}{%
\section{Arrhythmia data}\label{arrhythmia-data}}

The data I am using to demonstrate the building of neural nets is the arrhythmia dataset from \href{https://archive.ics.uci.edu/ml/datasets/Arrhythmia}{UC Irvine's machine learning database}. It contains 279 features from ECG heart rhythm diagnostics and one output column. I am not going to rename the feature columns because they are too many and the descriptions are too complex. Also, we don't need to know specifically which features we are looking at for building the models.

\begin{quote}
For a description of each feature, see \url{https://archive.ics.uci.edu/ml/machine-learning-databases/arrhythmia/arrhythmia.names}.
\end{quote}

The output column defines 16 classes: \emph{class 1} samples are from healthy ECGs, the remaining classes belong to different types of arrhythmia, with \emph{class 16} being all remaining arrhythmia cases that didn't fit into distinct classes.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{arrhythmia <-}\StringTok{ }\KeywordTok{read.table}\NormalTok{(}\KeywordTok{file.path}\NormalTok{(data_raw_dir, }\StringTok{"arrhythmia.data.txt"}\NormalTok{), }\DataTypeTok{sep =} \StringTok{","}\NormalTok{)}
\NormalTok{arrhythmia[arrhythmia }\OperatorTok{==}\StringTok{ "?"}\NormalTok{] <-}\StringTok{ }\OtherTok{NA}

\CommentTok{# making sure, that all feature columns are numeric}
\NormalTok{arrhythmia[}\OperatorTok{-}\DecValTok{280}\NormalTok{] <-}\StringTok{ }\KeywordTok{lapply}\NormalTok{(arrhythmia[}\OperatorTok{-}\DecValTok{280}\NormalTok{], as.character)}
\NormalTok{arrhythmia[}\OperatorTok{-}\DecValTok{280}\NormalTok{] <-}\StringTok{ }\KeywordTok{lapply}\NormalTok{(arrhythmia[}\OperatorTok{-}\DecValTok{280}\NormalTok{], as.numeric)}

\CommentTok{#  renaming output column and converting to factor}
\KeywordTok{colnames}\NormalTok{(arrhythmia)[}\DecValTok{280}\NormalTok{] <-}\StringTok{ "class"}
\NormalTok{arrhythmia}\OperatorTok{$}\NormalTok{class <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(arrhythmia}\OperatorTok{$}\NormalTok{class)}
\end{Highlighting}
\end{Shaded}

As usual, I want to get acquainted with the data and explore it's properties before I am building any model. So, I am first going to look at the distribution of classes and of healthy and arrhythmia samples.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p1 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(arrhythmia, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ class)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{fill =} \StringTok{"navy"}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{0.7}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{my_theme}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

Because I am interested in distinguishing healthy from arrhythmia ECGs, I am converting the output to binary format by combining all arrhythmia cases into one class.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#  all arrhythmia cases into one class}
\NormalTok{arrhythmia}\OperatorTok{$}\NormalTok{diagnosis <-}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(arrhythmia}\OperatorTok{$}\NormalTok{class }\OperatorTok{==}\StringTok{ }\DecValTok{1}\NormalTok{, }\StringTok{"healthy"}\NormalTok{, }\StringTok{"arrhythmia"}\NormalTok{)}
\NormalTok{arrhythmia}\OperatorTok{$}\NormalTok{diagnosis <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(arrhythmia}\OperatorTok{$}\NormalTok{diagnosis)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p2 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(arrhythmia, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ diagnosis)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{fill =} \StringTok{"navy"}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{0.7}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{my_theme}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(gridExtra)}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'gridExtra'}
\CommentTok{#> The following object is masked from 'package:dplyr':}
\CommentTok{#> }
\CommentTok{#>     combine}
\KeywordTok{library}\NormalTok{(grid)}

\KeywordTok{grid.arrange}\NormalTok{(p1, p2, }\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_239b-deeplearning_h2o_arrhythmia-sglander_files/figure-latex/plot-cases-1} \end{center}

With binary classification, we have almost the same numbers of healthy and arrhythmia cases in our dataset.

I am also interested in how much the normal and arrhythmia cases cluster in a Principal Component Analysis (PCA). I am first preparing the PCA plotting function and then run it on the feature data.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(pcaGoPromoter)}
\CommentTok{#> Warning: replacing previous import 'BiocGenerics::boxplot' by}
\CommentTok{#> 'graphics::boxplot' when loading 'Biostrings'}
\CommentTok{#> Warning: replacing previous import 'BiocGenerics::image' by}
\CommentTok{#> 'graphics::image' when loading 'Biostrings'}
\CommentTok{#> Warning: replacing previous import 'S4Vectors::na.exclude' by}
\CommentTok{#> 'stats::na.exclude' when loading 'Biostrings'}
\CommentTok{#> Warning: replacing previous import 'IRanges::smoothEnds' by}
\CommentTok{#> 'stats::smoothEnds' when loading 'Biostrings'}
\CommentTok{#> Warning: replacing previous import 'BiocGenerics::density' by}
\CommentTok{#> 'stats::density' when loading 'Biostrings'}
\CommentTok{#> Warning: replacing previous import 'IRanges::mad' by 'stats::mad' when}
\CommentTok{#> loading 'Biostrings'}
\CommentTok{#> Warning: replacing previous import 'S4Vectors::na.omit' by 'stats::na.omit'}
\CommentTok{#> when loading 'Biostrings'}
\CommentTok{#> Warning: replacing previous import 'S4Vectors::complete.cases' by}
\CommentTok{#> 'stats::complete.cases' when loading 'Biostrings'}
\CommentTok{#> Warning: replacing previous import 'IRanges::runmed' by 'stats::runmed'}
\CommentTok{#> when loading 'Biostrings'}
\CommentTok{#> Warning: replacing previous import 'IRanges::start' by 'stats::start' when}
\CommentTok{#> loading 'Biostrings'}
\CommentTok{#> Warning: replacing previous import 'IRanges::window<-' by 'stats::window<-'}
\CommentTok{#> when loading 'Biostrings'}
\CommentTok{#> Warning: replacing previous import 'S4Vectors::window' by 'stats::window'}
\CommentTok{#> when loading 'Biostrings'}
\CommentTok{#> Warning: replacing previous import 'S4Vectors::aggregate' by}
\CommentTok{#> 'stats::aggregate' when loading 'Biostrings'}
\CommentTok{#> Warning: replacing previous import 'BiocGenerics::weights' by}
\CommentTok{#> 'stats::weights' when loading 'Biostrings'}
\CommentTok{#> Warning: replacing previous import 'IRanges::cor' by 'stats::cor' when}
\CommentTok{#> loading 'Biostrings'}
\CommentTok{#> Warning: replacing previous import 'IRanges::cov' by 'stats::cov' when}
\CommentTok{#> loading 'Biostrings'}
\CommentTok{#> Warning: replacing previous import 'IRanges::quantile' by 'stats::quantile'}
\CommentTok{#> when loading 'Biostrings'}
\CommentTok{#> Warning: replacing previous import 'IRanges::end' by 'stats::end' when}
\CommentTok{#> loading 'Biostrings'}
\CommentTok{#> Warning: replacing previous import 'BiocGenerics::residuals' by}
\CommentTok{#> 'stats::residuals' when loading 'Biostrings'}
\CommentTok{#> Warning: replacing previous import 'IRanges::median' by 'stats::median'}
\CommentTok{#> when loading 'Biostrings'}
\CommentTok{#> Warning: replacing previous import 'IRanges::sd' by 'stats::sd' when}
\CommentTok{#> loading 'Biostrings'}
\CommentTok{#> Warning: replacing previous import 'IRanges::var' by 'stats::var' when}
\CommentTok{#> loading 'Biostrings'}
\CommentTok{#> Warning: replacing previous import 'S4Vectors::xtabs' by 'stats::xtabs'}
\CommentTok{#> when loading 'Biostrings'}
\CommentTok{#> Warning: replacing previous import 'IRanges::IQR' by 'stats::IQR' when}
\CommentTok{#> loading 'Biostrings'}
\CommentTok{#> Warning: replacing previous import 'S4Vectors::tail' by 'utils::tail' when}
\CommentTok{#> loading 'Biostrings'}
\CommentTok{#> Warning: replacing previous import 'IRanges::stack' by 'utils::stack' when}
\CommentTok{#> loading 'Biostrings'}
\CommentTok{#> Warning: replacing previous import 'XVector::relist' by 'utils::relist'}
\CommentTok{#> when loading 'Biostrings'}
\CommentTok{#> Warning: replacing previous import 'S4Vectors::head' by 'utils::head' when}
\CommentTok{#> loading 'Biostrings'}

\NormalTok{pca_func <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(pcaOutput2, group_name)\{}
\NormalTok{    centroids <-}\StringTok{ }\KeywordTok{aggregate}\NormalTok{(}\KeywordTok{cbind}\NormalTok{(PC1, PC2) }\OperatorTok{~}\StringTok{ }\NormalTok{groups, pcaOutput2, mean)}
\NormalTok{    conf.rgn  <-}\StringTok{ }\KeywordTok{do.call}\NormalTok{(rbind, }\KeywordTok{lapply}\NormalTok{(}\KeywordTok{unique}\NormalTok{(pcaOutput2}\OperatorTok{$}\NormalTok{groups), }\ControlFlowTok{function}\NormalTok{(t)}
          \KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{groups =} \KeywordTok{as.character}\NormalTok{(t),}
                     \KeywordTok{ellipse}\NormalTok{(}\KeywordTok{cov}\NormalTok{(pcaOutput2[pcaOutput2}\OperatorTok{$}\NormalTok{groups }\OperatorTok{==}\StringTok{ }\NormalTok{t, }\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{]),}
                           \DataTypeTok{centre =} \KeywordTok{as.matrix}\NormalTok{(centroids[centroids}\OperatorTok{$}\NormalTok{groups }\OperatorTok{==}\StringTok{ }\NormalTok{t, }\DecValTok{2}\OperatorTok{:}\DecValTok{3}\NormalTok{]),}
                           \DataTypeTok{level =} \FloatTok{0.95}\NormalTok{),}
                     \DataTypeTok{stringsAsFactors =} \OtherTok{FALSE}\NormalTok{)))}
        
\NormalTok{    plot <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ pcaOutput2, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ PC1, }\DataTypeTok{y =}\NormalTok{ PC2, }\DataTypeTok{group =}\NormalTok{ groups, }
                                          \DataTypeTok{color =}\NormalTok{ groups)) }\OperatorTok{+}\StringTok{ }
\StringTok{      }\KeywordTok{geom_polygon}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ conf.rgn, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{fill =}\NormalTok{ groups), }\DataTypeTok{alpha =} \FloatTok{0.2}\NormalTok{) }\OperatorTok{+}
\StringTok{      }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{size =} \DecValTok{2}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{0.5}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{      }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{color =} \KeywordTok{paste}\NormalTok{(group_name),}
           \DataTypeTok{fill =} \KeywordTok{paste}\NormalTok{(group_name),}
           \DataTypeTok{x =} \KeywordTok{paste0}\NormalTok{(}\StringTok{"PC1: "}\NormalTok{, }\KeywordTok{round}\NormalTok{(pcaOutput}\OperatorTok{$}\NormalTok{pov[}\DecValTok{1}\NormalTok{], }\DataTypeTok{digits =} \DecValTok{2}\NormalTok{) }\OperatorTok{*}\StringTok{ }\DecValTok{100}\NormalTok{, }\StringTok{"% variance"}\NormalTok{),}
           \DataTypeTok{y =} \KeywordTok{paste0}\NormalTok{(}\StringTok{"PC2: "}\NormalTok{, }\KeywordTok{round}\NormalTok{(pcaOutput}\OperatorTok{$}\NormalTok{pov[}\DecValTok{2}\NormalTok{], }\DataTypeTok{digits =} \DecValTok{2}\NormalTok{) }\OperatorTok{*}\StringTok{ }\DecValTok{100}\NormalTok{, }\StringTok{"% variance"}\NormalTok{)) }\OperatorTok{+}
\StringTok{      }\KeywordTok{my_theme}\NormalTok{()}
    
    \KeywordTok{return}\NormalTok{(plot)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Find what columns have NAs and the quantity}
\ControlFlowTok{for}\NormalTok{ (col }\ControlFlowTok{in} \KeywordTok{names}\NormalTok{(arrhythmia)) \{}
\NormalTok{    n_nas <-}\StringTok{ }\KeywordTok{length}\NormalTok{(}\KeywordTok{which}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(arrhythmia[, col])))}
    \ControlFlowTok{if}\NormalTok{ (n_nas }\OperatorTok{>}\StringTok{ }\DecValTok{0}\NormalTok{) }\KeywordTok{cat}\NormalTok{(col, n_nas, }\StringTok{"}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\NormalTok{\}}
\CommentTok{#> V11 8 }
\CommentTok{#> V12 22 }
\CommentTok{#> V13 1 }
\CommentTok{#> V14 376 }
\CommentTok{#> V15 1}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Replace NAs with zeros}
\NormalTok{arrhythmia[}\KeywordTok{is.na}\NormalTok{(arrhythmia)] <-}\StringTok{ }\DecValTok{0}
\end{Highlighting}
\end{Shaded}

Find and plot the PCAs.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pcaOutput <-}\StringTok{ }\KeywordTok{pca}\NormalTok{(}\KeywordTok{t}\NormalTok{(arrhythmia[}\OperatorTok{-}\KeywordTok{c}\NormalTok{(}\DecValTok{280}\NormalTok{, }\DecValTok{281}\NormalTok{)]), }\DataTypeTok{printDropped=}\OtherTok{FALSE}\NormalTok{, }
                 \DataTypeTok{scale=}\OtherTok{TRUE}\NormalTok{, }
                 \DataTypeTok{center =} \OtherTok{TRUE}\NormalTok{)}

\NormalTok{pcaOutput2 <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(pcaOutput}\OperatorTok{$}\NormalTok{scores)}

\NormalTok{pcaOutput2}\OperatorTok{$}\NormalTok{groups <-}\StringTok{ }\NormalTok{arrhythmia}\OperatorTok{$}\NormalTok{class}
\NormalTok{p1 <-}\StringTok{ }\KeywordTok{pca_func}\NormalTok{(pcaOutput2, }\DataTypeTok{group_name =} \StringTok{"class"}\NormalTok{)}

\NormalTok{pcaOutput2}\OperatorTok{$}\NormalTok{groups <-}\StringTok{ }\NormalTok{arrhythmia}\OperatorTok{$}\NormalTok{diagnosis}
\NormalTok{p2 <-}\StringTok{ }\KeywordTok{pca_func}\NormalTok{(pcaOutput2, }\DataTypeTok{group_name =} \StringTok{"diagnosis"}\NormalTok{)}

\KeywordTok{grid.arrange}\NormalTok{(p1, p2, }\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_239b-deeplearning_h2o_arrhythmia-sglander_files/figure-latex/plot-pca-1} \end{center}

The PCA shows that there is a big overlap between healthy and arrhythmia samples, i.e.~there does not seem to be major global differences in all features. The class that is most distinct from all others seems to be class 9.

I want to give the arrhythmia cases that are very different from the rest a stronger weight in the neural network, so I define a \textbf{weight column} where every sample outside the central PCA cluster will get a ``2'', they will in effect be used twice in the model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{weights <-}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(pcaOutput2}\OperatorTok{$}\NormalTok{PC1 }\OperatorTok{<}\StringTok{ }\DecValTok{-5} \OperatorTok{&}\StringTok{ }\KeywordTok{abs}\NormalTok{(pcaOutput2}\OperatorTok{$}\NormalTok{PC2) }\OperatorTok{>}\StringTok{ }\DecValTok{10}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

I also want to know what the variance is within features.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(matrixStats)}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'matrixStats'}
\CommentTok{#> The following object is masked from 'package:dplyr':}
\CommentTok{#> }
\CommentTok{#>     count}

\NormalTok{colvars <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{feature =} \KeywordTok{colnames}\NormalTok{(arrhythmia[}\OperatorTok{-}\KeywordTok{c}\NormalTok{(}\DecValTok{280}\NormalTok{, }\DecValTok{281}\NormalTok{)]),}
                      \DataTypeTok{variance =} \KeywordTok{colVars}\NormalTok{(}\KeywordTok{as.matrix}\NormalTok{(arrhythmia[}\OperatorTok{-}\KeywordTok{c}\NormalTok{(}\DecValTok{280}\NormalTok{, }\DecValTok{281}\NormalTok{)])))}

\KeywordTok{subset}\NormalTok{(colvars, variance }\OperatorTok{>}\StringTok{ }\DecValTok{50}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{feature =} \KeywordTok{factor}\NormalTok{(feature, }\DataTypeTok{levels =} \KeywordTok{colnames}\NormalTok{(arrhythmia[}\OperatorTok{-}\KeywordTok{c}\NormalTok{(}\DecValTok{280}\NormalTok{, }\DecValTok{281}\NormalTok{)]))) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ feature, }\DataTypeTok{y =}\NormalTok{ variance)) }\OperatorTok{+}
\StringTok{    }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{stat =} \StringTok{"identity"}\NormalTok{, }\DataTypeTok{fill =} \StringTok{"navy"}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{0.7}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{my_theme}\NormalTok{() }\OperatorTok{+}
\StringTok{    }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{axis.text.x =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{angle =} \DecValTok{90}\NormalTok{, }\DataTypeTok{vjust =} \FloatTok{0.5}\NormalTok{, }\DataTypeTok{hjust =} \DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_239b-deeplearning_h2o_arrhythmia-sglander_files/figure-latex/plot-feature-variance-1} \end{center}

Features with low variance are less likely to strongly contribute to a differentiation between healthy and arrhythmia cases, so I am going to remove them. I am also concatenating the weights column:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{arrhythmia_subset <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(weights, }
\NormalTok{                           arrhythmia[, }\KeywordTok{c}\NormalTok{(}\DecValTok{281}\NormalTok{, }\DecValTok{280}\NormalTok{, }\KeywordTok{which}\NormalTok{(colvars}\OperatorTok{$}\NormalTok{variance }\OperatorTok{>}\StringTok{ }\DecValTok{50}\NormalTok{))])}
\end{Highlighting}
\end{Shaded}

\hypertarget{converting-the-dataframe-to-a-h2o-object}{%
\section{\texorpdfstring{Converting the dataframe to a \texttt{h2o} object}{Converting the dataframe to a h2o object}}\label{converting-the-dataframe-to-a-h2o-object}}

Now that I have my final data frame for modeling, for working with h2o functions, the data needs to be converted from a \textbf{DataFrame} to an \textbf{H2O Frame}. This is done with the \texttt{as\_h2o\_frame()} function.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#as_h2o_frame(arrhythmia_subset)}
\NormalTok{arrhythmia_hf <-}\StringTok{ }\KeywordTok{as.h2o}\NormalTok{(arrhythmia_subset, }\DataTypeTok{key=}\StringTok{"arrhtythmia.hex"}\NormalTok{)}
\CommentTok{#> }
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|}\StringTok{                                                                 }\ErrorTok{|}\StringTok{   }\DecValTok{0}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=================================================================|}\StringTok{ }\DecValTok{100}\NormalTok{%}
\end{Highlighting}
\end{Shaded}

We can now access all functions from the h2o package that are built to work on \texttt{h2o} Frames. A useful such function is \texttt{h2o.describe()}. It is similar to base R's \texttt{summary()} function but outputs many more descriptive measures for our data. To get a good overview about these measures, I am going to plot them.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyr) }\CommentTok{# for gathering}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'tidyr'}
\CommentTok{#> The following object is masked from 'package:S4Vectors':}
\CommentTok{#> }
\CommentTok{#>     expand}
\KeywordTok{h2o.describe}\NormalTok{(arrhythmia_hf[, }\DecValTok{-1}\NormalTok{]) }\OperatorTok{%>%}\StringTok{ }\CommentTok{# excluding the weights column}
\StringTok{  }\KeywordTok{gather}\NormalTok{(x, y, Zeros}\OperatorTok{:}\NormalTok{Sigma) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{group =} \KeywordTok{ifelse}\NormalTok{(}
\NormalTok{    x }\OperatorTok{%in%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Min"}\NormalTok{, }\StringTok{"Max"}\NormalTok{, }\StringTok{"Mean"}\NormalTok{), }\StringTok{"min, mean, max"}\NormalTok{, }
    \KeywordTok{ifelse}\NormalTok{(x }\OperatorTok{%in%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"NegInf"}\NormalTok{, }\StringTok{"PosInf"}\NormalTok{), }\StringTok{"Inf"}\NormalTok{, }\StringTok{"sigma, zeros"}\NormalTok{))) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\CommentTok{# separating them into facets makes them easier to see}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{Label =} \KeywordTok{factor}\NormalTok{(Label, }\DataTypeTok{levels =} \KeywordTok{colnames}\NormalTok{(arrhythmia_hf[, }\DecValTok{-1}\NormalTok{]))) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Label, }\DataTypeTok{y =} \KeywordTok{as.numeric}\NormalTok{(y), }\DataTypeTok{color =}\NormalTok{ x)) }\OperatorTok{+}
\StringTok{    }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{size =} \DecValTok{4}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{0.6}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{scale_color_brewer}\NormalTok{(}\DataTypeTok{palette =} \StringTok{"Set1"}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{my_theme}\NormalTok{() }\OperatorTok{+}
\StringTok{    }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{axis.text.x =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{angle =} \DecValTok{90}\NormalTok{, }\DataTypeTok{vjust =} \FloatTok{0.5}\NormalTok{, }\DataTypeTok{hjust =} \DecValTok{1}\NormalTok{)) }\OperatorTok{+}
\StringTok{    }\KeywordTok{facet_grid}\NormalTok{(group }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{scales =} \StringTok{"free"}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"Feature"}\NormalTok{,}
         \DataTypeTok{y =} \StringTok{"Value"}\NormalTok{,}
         \DataTypeTok{color =} \StringTok{""}\NormalTok{)}
\CommentTok{#> Warning: Removed 2 rows containing missing values (geom_point).}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_239b-deeplearning_h2o_arrhythmia-sglander_files/figure-latex/plot-h2o-describe-1} \end{center}

I am also interested in the correlation between features and the output. We can use the \texttt{h2o.cor()} function to calculate the correlation matrix. It is again much easier to understand the data when we visualize it, so I am going to create another plot.\\

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(reshape2) }\CommentTok{# for melting}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'reshape2'}
\CommentTok{#> The following object is masked from 'package:tidyr':}
\CommentTok{#> }
\CommentTok{#>     smiths}

\CommentTok{# diagnosis is now a characer column and we need to convert it again}
\NormalTok{arrhythmia_hf[, }\DecValTok{2}\NormalTok{] <-}\StringTok{ }\KeywordTok{h2o.asfactor}\NormalTok{(arrhythmia_hf[, }\DecValTok{2}\NormalTok{]) }
\NormalTok{arrhythmia_hf[, }\DecValTok{3}\NormalTok{] <-}\StringTok{ }\KeywordTok{h2o.asfactor}\NormalTok{(arrhythmia_hf[, }\DecValTok{3}\NormalTok{]) }\CommentTok{# same for class}

\NormalTok{cor <-}\StringTok{ }\KeywordTok{h2o.cor}\NormalTok{(arrhythmia_hf[, }\OperatorTok{-}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{)])}
\KeywordTok{rownames}\NormalTok{(cor) <-}\StringTok{ }\KeywordTok{colnames}\NormalTok{(cor)}

\KeywordTok{melt}\NormalTok{(cor) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{Var2 =} \KeywordTok{rep}\NormalTok{(}\KeywordTok{rownames}\NormalTok{(cor), }\KeywordTok{nrow}\NormalTok{(cor))) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{Var2 =} \KeywordTok{factor}\NormalTok{(Var2, }\DataTypeTok{levels =} \KeywordTok{colnames}\NormalTok{(cor))) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{variable =} \KeywordTok{factor}\NormalTok{(variable, }\DataTypeTok{levels =} \KeywordTok{colnames}\NormalTok{(cor))) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ variable, }\DataTypeTok{y =}\NormalTok{ Var2, }\DataTypeTok{fill =}\NormalTok{ value)) }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{geom_tile}\NormalTok{(}\DataTypeTok{width =} \FloatTok{0.9}\NormalTok{, }\DataTypeTok{height =} \FloatTok{0.9}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{scale_fill_gradient2}\NormalTok{(}\DataTypeTok{low =} \StringTok{"white"}\NormalTok{, }\DataTypeTok{high =} \StringTok{"red"}\NormalTok{, }\DataTypeTok{name =} \StringTok{"Cor."}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{my_theme}\NormalTok{() }\OperatorTok{+}
\StringTok{    }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{axis.text.x =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{angle =} \DecValTok{90}\NormalTok{, }\DataTypeTok{vjust =} \FloatTok{0.5}\NormalTok{, }\DataTypeTok{hjust =} \DecValTok{1}\NormalTok{)) }\OperatorTok{+}
\StringTok{    }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{""}\NormalTok{, }
         \DataTypeTok{y =} \StringTok{""}\NormalTok{)}
\CommentTok{#> No id variables; using all as measure variables}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_239b-deeplearning_h2o_arrhythmia-sglander_files/figure-latex/plot-correlation-1} \end{center}

\hypertarget{training-test-and-validation-data}{%
\section{Training, test and validation data}\label{training-test-and-validation-data}}

Now we can use the \texttt{h2o.splitFrame()} function to split the data into training, validation and test data.

Here, I am using 70\% for training and 15\% each for validation and testing. We could also just split the data into two sections, a training and test set but when we have sufficient samples, it is a good idea to evaluate model performance on an independent test set on top of training with a validation set. Because we can easily overfit a model, we want to get an idea about how generalizable it is - this we can only assess by looking at how well it works on previously unknown data.

I am also defining \texttt{response}, \texttt{features} and \texttt{weights} column names now.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{splits <-}\StringTok{ }\KeywordTok{h2o.splitFrame}\NormalTok{(arrhythmia_hf, }
                         \DataTypeTok{ratios =} \KeywordTok{c}\NormalTok{(}\FloatTok{0.7}\NormalTok{, }\FloatTok{0.15}\NormalTok{), }
                         \DataTypeTok{seed =} \DecValTok{1}\NormalTok{)}

\NormalTok{train <-}\StringTok{ }\NormalTok{splits[[}\DecValTok{1}\NormalTok{]]}
\NormalTok{valid <-}\StringTok{ }\NormalTok{splits[[}\DecValTok{2}\NormalTok{]]}
\NormalTok{test <-}\StringTok{ }\NormalTok{splits[[}\DecValTok{3}\NormalTok{]]}

\NormalTok{response <-}\StringTok{ "diagnosis"}
\NormalTok{weights <-}\StringTok{ "weights"}
\NormalTok{features <-}\StringTok{ }\KeywordTok{setdiff}\NormalTok{(}\KeywordTok{colnames}\NormalTok{(train), }\KeywordTok{c}\NormalTok{(response, weights, }\StringTok{"class"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(train}\OperatorTok{$}\NormalTok{diagnosis, }\DataTypeTok{exact_quantiles =} \OtherTok{TRUE}\NormalTok{)}
\CommentTok{#>  diagnosis      }
\CommentTok{#>  healthy   :163 }
\CommentTok{#>  arrhythmia:155}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(valid}\OperatorTok{$}\NormalTok{diagnosis, }\DataTypeTok{exact_quantiles =} \OtherTok{TRUE}\NormalTok{)}
\CommentTok{#>  diagnosis     }
\CommentTok{#>  healthy   :43 }
\CommentTok{#>  arrhythmia:25}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(test}\OperatorTok{$}\NormalTok{diagnosis, }\DataTypeTok{exact_quantiles =} \OtherTok{TRUE}\NormalTok{)}
\CommentTok{#>  diagnosis     }
\CommentTok{#>  healthy   :39 }
\CommentTok{#>  arrhythmia:27}
\end{Highlighting}
\end{Shaded}

If we had more categorical features, we could use the \texttt{h2o.interaction()} function to define interaction terms, but since we only have numeric features here, we don't need this.

We can also run a PCA on the training data, using the \texttt{h2o.prcomp()} function to calculate the singular value decomposition of the Gram matrix with the power method.\\

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pca <-}\StringTok{ }\KeywordTok{h2o.prcomp}\NormalTok{(}\DataTypeTok{training_frame =}\NormalTok{ train,}
           \DataTypeTok{x =}\NormalTok{ features,}
           \DataTypeTok{validation_frame =}\NormalTok{ valid,}
           \DataTypeTok{transform =} \StringTok{"NORMALIZE"}\NormalTok{,}
           \DataTypeTok{k =} \DecValTok{3}\NormalTok{,}
           \DataTypeTok{seed =} \DecValTok{42}\NormalTok{)}
\CommentTok{#> }
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|}\StringTok{                                                                 }\ErrorTok{|}\StringTok{   }\DecValTok{0}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=================================================================|}\StringTok{ }\DecValTok{100}\NormalTok{%}
\CommentTok{#> Warning in doTryCatch(return(expr), name, parentenv, handler): _train:}
\CommentTok{#> Dataset used may contain fewer number of rows due to removal of rows with}
\CommentTok{#> NA/missing values. If this is not desirable, set impute_missing argument in}
\CommentTok{#> pca call to TRUE/True/true/... depending on the client language.}
\NormalTok{pca}
\CommentTok{#> Model Details:}
\CommentTok{#> ==============}
\CommentTok{#> }
\CommentTok{#> H2ODimReductionModel: pca}
\CommentTok{#> Model ID:  PCA_model_R_1568840446217_1 }
\CommentTok{#> Importance of components: }
\CommentTok{#>                             pc1      pc2      pc3}
\CommentTok{#> Standard deviation     0.582620 0.507796 0.421869}
\CommentTok{#> Proportion of Variance 0.164697 0.125110 0.086351}
\CommentTok{#> Cumulative Proportion  0.164697 0.289808 0.376159}
\CommentTok{#> }
\CommentTok{#> }
\CommentTok{#> H2ODimReductionMetrics: pca}
\CommentTok{#> }
\CommentTok{#> No model metrics available for PCA}
\CommentTok{#> H2ODimReductionMetrics: pca}
\CommentTok{#> }
\CommentTok{#> No model metrics available for PCA}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{eigenvec <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(pca}\OperatorTok{@}\NormalTok{model}\OperatorTok{$}\NormalTok{eigenvectors)}
\NormalTok{eigenvec}\OperatorTok{$}\NormalTok{label <-}\StringTok{ }\NormalTok{features}

\KeywordTok{ggplot}\NormalTok{(eigenvec, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ pc1, }\DataTypeTok{y =}\NormalTok{ pc2, }\DataTypeTok{label =}\NormalTok{ label)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{color =} \StringTok{"navy"}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{0.7}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_text_repel}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{my_theme}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_239b-deeplearning_h2o_arrhythmia-sglander_files/figure-latex/plot-eigenvectors-1} \end{center}

\hypertarget{modeling}{%
\section{Modeling}\label{modeling}}

Now, we can build a deep neural network model. We can specify quite a few parameters, like

\begin{itemize}
\item
  \textbf{Cross-validation}: Cross validation can tell us the training and validation errors for each model. The final model will be overwritten with the best model, if we don't specify otherwise.
\item
  \textbf{Adaptive learning rate}: For deep learning with h2o, we by default use stochastic gradient descent optimization with an an adaptive learning rate. The two corresponding parameters rho and epsilon help us find global (or near enough) optima.
\item
  \textbf{Activation function}: The activation function defines the node output relative to a given set of inputs. We want our activation function to be non-linear and continuously differentiable.
\item
  \textbf{Hidden nodes}: Defines the number of hidden layers and the number of nodes per layer.
\item
  \textbf{Epochs}: Increasing the number of epochs (one full training cycle on all training samples) can increase model performance, but we also run the risk of overfitting. To determine the optimal number of epochs, we need to use early stopping.
\item
  \textbf{Early stopping}: By default, early stopping is enabled. This means that training will be stopped when we reach a certain validation error to prevent overfitting.
\end{itemize}

Of course, you need quite a bit of experience and intuition to hit on a good combination of parameters. That's why it usually makes sense to do a grid search for hyper-parameter tuning. Here, I want to focus on building and evaluating deep learning models, though. I will cover grid search in next week's post.\\

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# this will take some time and all CPUs}
\NormalTok{dl_model <-}\StringTok{ }\KeywordTok{h2o.deeplearning}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ features,}
                             \DataTypeTok{y =}\NormalTok{ response,}
                             \DataTypeTok{weights_column =}\NormalTok{ weights,}
                             \DataTypeTok{model_id =} \StringTok{"dl_model"}\NormalTok{,}
                             \DataTypeTok{training_frame =}\NormalTok{ train,}
                             \DataTypeTok{validation_frame =}\NormalTok{ valid,}
                             \DataTypeTok{nfolds =} \DecValTok{15}\NormalTok{,                                   }\CommentTok{# 10x cross validation}
                             \DataTypeTok{keep_cross_validation_fold_assignment =} \OtherTok{TRUE}\NormalTok{,}
                             \DataTypeTok{fold_assignment =} \StringTok{"Stratified"}\NormalTok{,}
                             \DataTypeTok{activation =} \StringTok{"RectifierWithDropout"}\NormalTok{,}
                             \DataTypeTok{score_each_iteration =} \OtherTok{TRUE}\NormalTok{,}
                             \DataTypeTok{hidden =} \KeywordTok{c}\NormalTok{(}\DecValTok{200}\NormalTok{, }\DecValTok{200}\NormalTok{, }\DecValTok{200}\NormalTok{, }\DecValTok{200}\NormalTok{, }\DecValTok{200}\NormalTok{),           }\CommentTok{# 5 hidden layers, each of 200 neurons}
                             \DataTypeTok{epochs =} \DecValTok{100}\NormalTok{,}
                             \DataTypeTok{variable_importances =} \OtherTok{TRUE}\NormalTok{,}
                             \DataTypeTok{export_weights_and_biases =} \OtherTok{TRUE}\NormalTok{,}
                             \DataTypeTok{seed =} \DecValTok{42}\NormalTok{)}
\CommentTok{#> }
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|}\StringTok{                                                                 }\ErrorTok{|}\StringTok{   }\DecValTok{0}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==}\StringTok{                                                               }\ErrorTok{|}\StringTok{   }\DecValTok{3}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|======}\StringTok{                                                           }\ErrorTok{|}\StringTok{  }\DecValTok{10}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=======}\StringTok{                                                          }\ErrorTok{|}\StringTok{  }\DecValTok{11}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|========}\StringTok{                                                         }\ErrorTok{|}\StringTok{  }\DecValTok{12}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=========}\StringTok{                                                        }\ErrorTok{|}\StringTok{  }\DecValTok{13}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==========}\StringTok{                                                       }\ErrorTok{|}\StringTok{  }\DecValTok{15}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=============}\StringTok{                                                    }\ErrorTok{|}\StringTok{  }\DecValTok{19}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==============}\StringTok{                                                   }\ErrorTok{|}\StringTok{  }\DecValTok{21}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|===============}\StringTok{                                                  }\ErrorTok{|}\StringTok{  }\DecValTok{23}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|================}\StringTok{                                                 }\ErrorTok{|}\StringTok{  }\DecValTok{25}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=================}\StringTok{                                                }\ErrorTok{|}\StringTok{  }\DecValTok{27}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==================}\StringTok{                                               }\ErrorTok{|}\StringTok{  }\DecValTok{28}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|===================}\StringTok{                                              }\ErrorTok{|}\StringTok{  }\DecValTok{29}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|====================}\StringTok{                                             }\ErrorTok{|}\StringTok{  }\DecValTok{31}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=====================}\StringTok{                                            }\ErrorTok{|}\StringTok{  }\DecValTok{32}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|======================}\StringTok{                                           }\ErrorTok{|}\StringTok{  }\DecValTok{33}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|========================}\StringTok{                                         }\ErrorTok{|}\StringTok{  }\DecValTok{37}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=========================}\StringTok{                                        }\ErrorTok{|}\StringTok{  }\DecValTok{38}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==========================}\StringTok{                                       }\ErrorTok{|}\StringTok{  }\DecValTok{39}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==========================}\StringTok{                                       }\ErrorTok{|}\StringTok{  }\DecValTok{41}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|===========================}\StringTok{                                      }\ErrorTok{|}\StringTok{  }\DecValTok{41}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|============================}\StringTok{                                     }\ErrorTok{|}\StringTok{  }\DecValTok{42}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=============================}\StringTok{                                    }\ErrorTok{|}\StringTok{  }\DecValTok{44}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==============================}\StringTok{                                   }\ErrorTok{|}\StringTok{  }\DecValTok{46}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==============================}\StringTok{                                   }\ErrorTok{|}\StringTok{  }\DecValTok{47}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|===============================}\StringTok{                                  }\ErrorTok{|}\StringTok{  }\DecValTok{48}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|================================}\StringTok{                                 }\ErrorTok{|}\StringTok{  }\DecValTok{49}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==================================}\StringTok{                               }\ErrorTok{|}\StringTok{  }\DecValTok{52}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|===================================}\StringTok{                              }\ErrorTok{|}\StringTok{  }\DecValTok{53}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|===================================}\StringTok{                              }\ErrorTok{|}\StringTok{  }\DecValTok{54}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|====================================}\StringTok{                             }\ErrorTok{|}\StringTok{  }\DecValTok{55}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|====================================}\StringTok{                             }\ErrorTok{|}\StringTok{  }\DecValTok{56}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=====================================}\StringTok{                            }\ErrorTok{|}\StringTok{  }\DecValTok{57}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=======================================}\StringTok{                          }\ErrorTok{|}\StringTok{  }\DecValTok{61}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|========================================}\StringTok{                         }\ErrorTok{|}\StringTok{  }\DecValTok{61}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=========================================}\StringTok{                        }\ErrorTok{|}\StringTok{  }\DecValTok{63}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==========================================}\StringTok{                       }\ErrorTok{|}\StringTok{  }\DecValTok{64}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|===========================================}\StringTok{                      }\ErrorTok{|}\StringTok{  }\DecValTok{66}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|============================================}\StringTok{                     }\ErrorTok{|}\StringTok{  }\DecValTok{68}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=============================================}\StringTok{                    }\ErrorTok{|}\StringTok{  }\DecValTok{69}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==============================================}\StringTok{                   }\ErrorTok{|}\StringTok{  }\DecValTok{71}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|===============================================}\StringTok{                  }\ErrorTok{|}\StringTok{  }\DecValTok{72}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|================================================}\StringTok{                 }\ErrorTok{|}\StringTok{  }\DecValTok{74}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|===============================================================}\StringTok{  }\ErrorTok{|}\StringTok{  }\DecValTok{96}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|===============================================================}\StringTok{  }\ErrorTok{|}\StringTok{  }\DecValTok{97}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|================================================================}\StringTok{ }\ErrorTok{|}\StringTok{  }\DecValTok{99}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=================================================================|}\StringTok{ }\DecValTok{100}\NormalTok{%}
\end{Highlighting}
\end{Shaded}

Because training can take a while, depending on how many samples, features, nodes and hidden layers you are training on, it is a good idea to save your model.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# if file exists, overwrite it}
\KeywordTok{h2o.saveModel}\NormalTok{(dl_model, }\DataTypeTok{path =} \KeywordTok{file.path}\NormalTok{(data_out_dir, }\StringTok{"dl_model"}\NormalTok{), }\DataTypeTok{force =} \OtherTok{TRUE}\NormalTok{)}
\CommentTok{#> [1] "/home/datascience/repos/machine-learning-rsuite/export/dl_model/dl_model"}
\end{Highlighting}
\end{Shaded}

We can then re-load the model again any time to check the model quality and make predictions on new data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dl_model <-}\StringTok{ }\KeywordTok{h2o.loadModel}\NormalTok{(}\KeywordTok{file.path}\NormalTok{(data_out_dir, }\StringTok{"dl_model/dl_model"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\hypertarget{model-performance}{%
\section{Model performance}\label{model-performance}}

We now want to know how our model performed on the validation data. The summary() function will give us a detailed overview of our model. I am not showing the output here, because it is quite extensive.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sum_model <-}\StringTok{ }\KeywordTok{summary}\NormalTok{(dl_model)}
\CommentTok{#> Model Details:}
\CommentTok{#> ==============}
\CommentTok{#> }
\CommentTok{#> H2OBinomialModel: deeplearning}
\CommentTok{#> Model Key:  dl_model }
\CommentTok{#> Status of Neuron Layers: predicting diagnosis, 2-class classification, bernoulli distribution, CrossEntropy loss, 179,402 weights/biases, 2.1 MB, 34,090 training samples, mini-batch size 1}
\CommentTok{#>   layer units             type dropout       l1       l2 mean_rate}
\CommentTok{#> 1     1    90            Input  0.00 %       NA       NA        NA}
\CommentTok{#> 2     2   200 RectifierDropout 50.00 % 0.000000 0.000000  0.004555}
\CommentTok{#> 3     3   200 RectifierDropout 50.00 % 0.000000 0.000000  0.006484}
\CommentTok{#> 4     4   200 RectifierDropout 50.00 % 0.000000 0.000000  0.009338}
\CommentTok{#> 5     5   200 RectifierDropout 50.00 % 0.000000 0.000000  0.008749}
\CommentTok{#> 6     6   200 RectifierDropout 50.00 % 0.000000 0.000000  0.020938}
\CommentTok{#> 7     7     2          Softmax      NA 0.000000 0.000000  0.002158}
\CommentTok{#>   rate_rms momentum mean_weight weight_rms mean_bias bias_rms}
\CommentTok{#> 1       NA       NA          NA         NA        NA       NA}
\CommentTok{#> 2 0.003825 0.000000    0.002715   0.096436  0.430206 0.056454}
\CommentTok{#> 3 0.003681 0.000000   -0.008004   0.074799  0.948645 0.052865}
\CommentTok{#> 4 0.004960 0.000000   -0.007273   0.072414  0.967009 0.028673}
\CommentTok{#> 5 0.004542 0.000000   -0.005750   0.071128  0.974126 0.033538}
\CommentTok{#> 6 0.048796 0.000000   -0.010381   0.070846  0.952112 0.033043}
\CommentTok{#> 7 0.001022 0.000000   -0.041102   0.378216  0.000671 0.124920}
\CommentTok{#> }
\CommentTok{#> H2OBinomialMetrics: deeplearning}
\CommentTok{#> ** Reported on training data. **}
\CommentTok{#> ** Metrics reported on full training frame **}
\CommentTok{#> }
\CommentTok{#> MSE:  0.0212}
\CommentTok{#> RMSE:  0.146}
\CommentTok{#> LogLoss:  0.0884}
\CommentTok{#> Mean Per-Class Error:  0.0244}
\CommentTok{#> AUC:  0.985}
\CommentTok{#> pr_auc:  0.964}
\CommentTok{#> Gini:  0.969}
\CommentTok{#> }
\CommentTok{#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:}
\CommentTok{#>            arrhythmia healthy    Error    Rate}
\CommentTok{#> arrhythmia        156       8 0.048780  =8/164}
\CommentTok{#> healthy             0     163 0.000000  =0/163}
\CommentTok{#> Totals            156     171 0.024465  =8/327}
\CommentTok{#> }
\CommentTok{#> Maximum Metrics: Maximum metrics at their respective thresholds}
\CommentTok{#>                         metric threshold    value idx}
\CommentTok{#> 1                       max f1  0.422204 0.976048 170}
\CommentTok{#> 2                       max f2  0.422204 0.990279 170}
\CommentTok{#> 3                 max f0point5  0.886807 0.972046 155}
\CommentTok{#> 4                 max accuracy  0.628002 0.975535 166}
\CommentTok{#> 5                max precision  0.990334 1.000000   0}
\CommentTok{#> 6                   max recall  0.422204 1.000000 170}
\CommentTok{#> 7              max specificity  0.990334 1.000000   0}
\CommentTok{#> 8             max absolute_mcc  0.422204 0.952217 170}
\CommentTok{#> 9   max min_per_class_accuracy  0.730555 0.969512 163}
\CommentTok{#> 10 max mean_per_class_accuracy  0.422204 0.975610 170}
\CommentTok{#> }
\CommentTok{#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`}
\CommentTok{#> H2OBinomialMetrics: deeplearning}
\CommentTok{#> ** Reported on validation data. **}
\CommentTok{#> ** Metrics reported on full validation frame **}
\CommentTok{#> }
\CommentTok{#> MSE:  0.185}
\CommentTok{#> RMSE:  0.431}
\CommentTok{#> LogLoss:  1}
\CommentTok{#> Mean Per-Class Error:  0.195}
\CommentTok{#> AUC:  0.87}
\CommentTok{#> pr_auc:  0.889}
\CommentTok{#> Gini:  0.74}
\CommentTok{#> }
\CommentTok{#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:}
\CommentTok{#>            arrhythmia healthy    Error    Rate}
\CommentTok{#> arrhythmia         17       8 0.320000   =8/25}
\CommentTok{#> healthy             3      40 0.069767   =3/43}
\CommentTok{#> Totals             20      48 0.161765  =11/68}
\CommentTok{#> }
\CommentTok{#> Maximum Metrics: Maximum metrics at their respective thresholds}
\CommentTok{#>                         metric threshold    value idx}
\CommentTok{#> 1                       max f1  0.005650 0.879121  47}
\CommentTok{#> 2                       max f2  0.000018 0.929204  53}
\CommentTok{#> 3                 max f0point5  0.503477 0.862069  39}
\CommentTok{#> 4                 max accuracy  0.005650 0.838235  47}
\CommentTok{#> 5                max precision  0.990942 1.000000   0}
\CommentTok{#> 6                   max recall  0.000004 1.000000  59}
\CommentTok{#> 7              max specificity  0.990942 1.000000   0}
\CommentTok{#> 8             max absolute_mcc  0.005650 0.645749  47}
\CommentTok{#> 9   max min_per_class_accuracy  0.503477 0.800000  39}
\CommentTok{#> 10 max mean_per_class_accuracy  0.503477 0.806977  39}
\CommentTok{#> }
\CommentTok{#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`}
\CommentTok{#> H2OBinomialMetrics: deeplearning}
\CommentTok{#> ** Reported on cross-validation data. **}
\CommentTok{#> ** 15-fold cross-validation on training data (Metrics computed for combined holdout predictions) **}
\CommentTok{#> }
\CommentTok{#> MSE:  0.176}
\CommentTok{#> RMSE:  0.419}
\CommentTok{#> LogLoss:  0.624}
\CommentTok{#> Mean Per-Class Error:  0.208}
\CommentTok{#> AUC:  0.843}
\CommentTok{#> pr_auc:  0.787}
\CommentTok{#> Gini:  0.685}
\CommentTok{#> }
\CommentTok{#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:}
\CommentTok{#>            arrhythmia healthy    Error     Rate}
\CommentTok{#> arrhythmia        130      34 0.207317  =34/164}
\CommentTok{#> healthy            34     129 0.208589  =34/163}
\CommentTok{#> Totals            164     163 0.207951  =68/327}
\CommentTok{#> }
\CommentTok{#> Maximum Metrics: Maximum metrics at their respective thresholds}
\CommentTok{#>                         metric threshold    value idx}
\CommentTok{#> 1                       max f1  0.635968 0.791411 162}
\CommentTok{#> 2                       max f2  0.001835 0.881393 266}
\CommentTok{#> 3                 max f0point5  0.758633 0.797665 151}
\CommentTok{#> 4                 max accuracy  0.635968 0.792049 162}
\CommentTok{#> 5                max precision  0.988261 1.000000   0}
\CommentTok{#> 6                   max recall  0.001134 1.000000 273}
\CommentTok{#> 7              max specificity  0.988261 1.000000   0}
\CommentTok{#> 8             max absolute_mcc  0.635968 0.584094 162}
\CommentTok{#> 9   max min_per_class_accuracy  0.635968 0.791411 162}
\CommentTok{#> 10 max mean_per_class_accuracy  0.635968 0.792047 162}
\CommentTok{#> }
\CommentTok{#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`}
\CommentTok{#> Cross-Validation Metrics Summary: }
\CommentTok{#>                               mean          sd  cv_1_valid  cv_2_valid}
\CommentTok{#> accuracy                 0.8611481 0.053447507  0.71428573   0.8235294}
\CommentTok{#> auc                      0.8550837  0.07367064   0.6631016  0.71666664}
\CommentTok{#> err                      0.1388519 0.053447507   0.2857143   0.1764706}
\CommentTok{#> err_count                3.3333333   1.6261748         8.0         3.0}
\CommentTok{#> f0point5                0.83738595    0.069643   0.6321839   0.6896552}
\CommentTok{#> f1                       0.8651681  0.05386143  0.73333335  0.72727275}
\CommentTok{#> f2                       0.8994148 0.046931576   0.8730159   0.7692308}
\CommentTok{#> lift_top_group           1.4293057   0.6512436         0.0         0.0}
\CommentTok{#> logloss                  0.6092432  0.18783744   0.9387797   1.1304744}
\CommentTok{#> max_per_class_error     0.23747644  0.10214973  0.47058824         0.2}
\CommentTok{#> mcc                      0.7298193  0.09693578   0.5536258  0.60385966}
\CommentTok{#> mean_per_class_accuracy  0.8558342 0.054132007   0.7647059  0.81666666}
\CommentTok{#> mean_per_class_error    0.14416581 0.054132007  0.23529412  0.18333334}
\CommentTok{#> mse                      0.1686008 0.049769267  0.29961145  0.25869125}
\CommentTok{#> precision                0.8216217   0.0810019  0.57894737   0.6666667}
\CommentTok{#> r2                      0.28901088  0.21953593 -0.25612497 -0.24602953}
\CommentTok{#> recall                   0.9271715  0.05560393         1.0         0.8}
\CommentTok{#> rmse                    0.40071017  0.06337255  0.54736775    0.508617}
\CommentTok{#> specificity             0.78449684  0.11391768   0.5294118   0.8333333}
\CommentTok{#>                         cv_3_valid cv_4_valid  cv_5_valid cv_6_valid}
\CommentTok{#> accuracy                       1.0 0.82608694   0.9444444     0.9375}
\CommentTok{#> auc                            1.0 0.84126985  0.96103895   0.984375}
\CommentTok{#> err                            0.0 0.17391305 0.055555556     0.0625}
\CommentTok{#> err_count                      0.0        4.0         1.0        1.0}
\CommentTok{#> f0point5                       1.0 0.81395346  0.98039216 0.90909094}
\CommentTok{#> f1                             1.0      0.875  0.95238096  0.9411765}
\CommentTok{#> f2                             1.0  0.9459459   0.9259259  0.9756098}
\CommentTok{#> lift_top_group           1.6666666  1.6428572   1.6363636        2.0}
\CommentTok{#> logloss                  0.1915051 0.56637216   0.4381006 0.19899572}
\CommentTok{#> max_per_class_error            0.0 0.44444445  0.09090909      0.125}
\CommentTok{#> mcc                            1.0  0.6573422   0.8918826  0.8819171}
\CommentTok{#> mean_per_class_accuracy        1.0  0.7777778  0.95454544     0.9375}
\CommentTok{#> mean_per_class_error           0.0 0.22222222 0.045454547     0.0625}
\CommentTok{#> mse                     0.05087649 0.16059141   0.1431109 0.06466938}
\CommentTok{#> precision                      1.0  0.7777778         1.0  0.8888889}
\CommentTok{#> r2                      0.78801465   0.325771   0.3978191 0.74132246}
\CommentTok{#> recall                         1.0        1.0  0.90909094        1.0}
\CommentTok{#> rmse                    0.22555818 0.40073857      0.3783 0.25430176}
\CommentTok{#> specificity                    1.0  0.5555556         1.0      0.875}
\CommentTok{#>                          cv_7_valid cv_8_valid cv_9_valid cv_10_valid}
\CommentTok{#> accuracy                 0.90909094 0.90909094  0.8076923   0.8666667}
\CommentTok{#> auc                      0.91071427  0.9285714  0.7647059  0.87946427}
\CommentTok{#> err                      0.09090909 0.09090909  0.1923077  0.13333334}
\CommentTok{#> err_count                       2.0        1.0        5.0         4.0}
\CommentTok{#> f0point5                      0.875  0.8974359 0.82474226  0.88709676}
\CommentTok{#> f1                            0.875 0.93333334  0.8648649  0.84615386}
\CommentTok{#> f2                            0.875  0.9722222 0.90909094   0.8088235}
\CommentTok{#> lift_top_group                 2.75  1.5714285  1.5294118    2.142857}
\CommentTok{#> logloss                   0.5307357  0.8580031  0.6874807   0.6276062}
\CommentTok{#> max_per_class_error           0.125       0.25 0.44444445  0.21428572}
\CommentTok{#> mcc                       0.8035714 0.81009257  0.5608894  0.73648536}
\CommentTok{#> mean_per_class_accuracy  0.90178573      0.875   0.748366  0.86160713}
\CommentTok{#> mean_per_class_error     0.09821428      0.125   0.251634  0.13839285}
\CommentTok{#> mse                     0.112374716 0.19855355 0.20534797   0.1689381}
\CommentTok{#> precision                     0.875      0.875        0.8   0.9166667}
\CommentTok{#> r2                        0.5143807 0.14196502 0.09271092  0.32123086}
\CommentTok{#> recall                        0.875        1.0  0.9411765  0.78571427}
\CommentTok{#> rmse                     0.33522338 0.44559348 0.45315337   0.4110208}
\CommentTok{#> specificity               0.9285714       0.75  0.5555556      0.9375}
\CommentTok{#>                         cv_11_valid  cv_12_valid cv_13_valid cv_14_valid}
\CommentTok{#> accuracy                  0.8636364    0.7619048     0.78125  0.83870965}
\CommentTok{#> auc                        0.892562    0.6818182    0.796875  0.87916666}
\CommentTok{#> err                      0.13636364   0.23809524     0.21875  0.16129032}
\CommentTok{#> err_count                       3.0          5.0         7.0         5.0}
\CommentTok{#> f0point5                 0.84745765   0.71428573  0.77380955   0.8333333}
\CommentTok{#> f1                        0.8695652          0.8   0.7878788   0.8484849}
\CommentTok{#> f2                       0.89285713   0.90909094  0.80246913  0.86419755}
\CommentTok{#> lift_top_group                  2.0          0.0         2.0         0.0}
\CommentTok{#> logloss                  0.39886966    0.9508356   0.6924645   0.5638011}
\CommentTok{#> max_per_class_error      0.18181819   0.45454547        0.25         0.2}
\CommentTok{#> mcc                      0.73029673    0.6030227  0.56360185   0.6778302}
\CommentTok{#> mean_per_class_accuracy   0.8636364   0.77272725     0.78125      0.8375}
\CommentTok{#> mean_per_class_error     0.13636364   0.22727273     0.21875      0.1625}
\CommentTok{#> mse                      0.12954943    0.2684783  0.21720445  0.14379291}
\CommentTok{#> precision                 0.8333333    0.6666667   0.7647059   0.8235294}
\CommentTok{#> r2                       0.48180228 -0.076353945  0.13118221   0.4242292}
\CommentTok{#> recall                   0.90909094          1.0      0.8125       0.875}
\CommentTok{#> rmse                     0.35992974    0.5181489  0.46605197  0.37920037}
\CommentTok{#> specificity               0.8181818   0.54545456        0.75         0.8}
\CommentTok{#>                         cv_15_valid}
\CommentTok{#> accuracy                 0.93333334}
\CommentTok{#> auc                       0.9259259}
\CommentTok{#> err                      0.06666667}
\CommentTok{#> err_count                       1.0}
\CommentTok{#> f0point5                 0.88235295}
\CommentTok{#> f1                        0.9230769}
\CommentTok{#> f2                        0.9677419}
\CommentTok{#> lift_top_group                  2.5}
\CommentTok{#> logloss                  0.36462396}
\CommentTok{#> max_per_class_error      0.11111111}
\CommentTok{#> mcc                       0.8728716}
\CommentTok{#> mean_per_class_accuracy   0.9444444}
\CommentTok{#> mean_per_class_error    0.055555556}
\CommentTok{#> mse                      0.10722163}
\CommentTok{#> precision                0.85714287}
\CommentTok{#> r2                        0.5532432}
\CommentTok{#> recall                          1.0}
\CommentTok{#> rmse                     0.32744715}
\CommentTok{#> specificity               0.8888889}
\CommentTok{#> }
\CommentTok{#> Scoring History: }
\CommentTok{#>              timestamp          duration training_speed    epochs}
\CommentTok{#> 1  2019-09-18 16:03:00         0.000 sec             NA   0.00000}
\CommentTok{#> 2  2019-09-18 16:03:01  1 min 59.189 sec   4177 obs/sec  10.72013}
\CommentTok{#> 3  2019-09-18 16:03:02  2 min  0.244 sec   3836 obs/sec  21.44025}
\CommentTok{#> 4  2019-09-18 16:03:03  2 min  1.069 sec   4020 obs/sec  32.16038}
\CommentTok{#> 5  2019-09-18 16:03:03  2 min  1.962 sec   4023 obs/sec  42.88050}
\CommentTok{#> 6  2019-09-18 16:03:04  2 min  2.760 sec   4124 obs/sec  53.60063}
\CommentTok{#> 7  2019-09-18 16:03:05  2 min  3.686 sec   4085 obs/sec  64.32075}
\CommentTok{#> 8  2019-09-18 16:03:06  2 min  4.490 sec   4141 obs/sec  75.04088}
\CommentTok{#> 9  2019-09-18 16:03:07  2 min  5.281 sec   4202 obs/sec  85.76101}
\CommentTok{#> 10 2019-09-18 16:03:07  2 min  5.955 sec   4309 obs/sec  96.48113}
\CommentTok{#> 11 2019-09-18 16:03:08  2 min  6.661 sec   4384 obs/sec 107.20126}
\CommentTok{#>    iterations      samples training_rmse training_logloss training_r2}
\CommentTok{#> 1           0     0.000000            NA               NA          NA}
\CommentTok{#> 2           1  3409.000000       0.38306          0.53514     0.41304}
\CommentTok{#> 3           2  6818.000000       0.34810          0.38507     0.51531}
\CommentTok{#> 4           3 10227.000000       0.30740          0.33144     0.62202}
\CommentTok{#> 5           4 13636.000000       0.28218          0.27393     0.68149}
\CommentTok{#> 6           5 17045.000000       0.28487          0.27781     0.67538}
\CommentTok{#> 7           6 20454.000000       0.23736          0.20509     0.77464}
\CommentTok{#> 8           7 23863.000000       0.20890          0.16462     0.82544}
\CommentTok{#> 9           8 27272.000000       0.23711          0.18927     0.77511}
\CommentTok{#> 10          9 30681.000000       0.19374          0.13858     0.84986}
\CommentTok{#> 11         10 34090.000000       0.14574          0.08839     0.91504}
\CommentTok{#>    training_auc training_pr_auc training_lift}
\CommentTok{#> 1            NA              NA            NA}
\CommentTok{#> 2       0.88710         0.85340       2.00613}
\CommentTok{#> 3       0.92690         0.88764       2.00613}
\CommentTok{#> 4       0.93618         0.89868       2.00613}
\CommentTok{#> 5       0.94796         0.90958       2.00613}
\CommentTok{#> 6       0.95537         0.92408       2.00613}
\CommentTok{#> 7       0.96431         0.93000       2.00613}
\CommentTok{#> 8       0.97265         0.94703       2.00613}
\CommentTok{#> 9       0.97640         0.95408       2.00613}
\CommentTok{#> 10      0.98002         0.95901       2.00613}
\CommentTok{#> 11      0.98470         0.96384       2.00613}
\CommentTok{#>    training_classification_error validation_rmse validation_logloss}
\CommentTok{#> 1                             NA              NA                 NA}
\CommentTok{#> 2                        0.17737         0.40981            0.65225}
\CommentTok{#> 3                        0.13150         0.37642            0.46161}
\CommentTok{#> 4                        0.11621         0.41046            0.70123}
\CommentTok{#> 5                        0.09786         0.40806            0.60142}
\CommentTok{#> 6                        0.08257         0.41888            0.73485}
\CommentTok{#> 7                        0.06422         0.42726            0.83088}
\CommentTok{#> 8                        0.04893         0.43291            0.84339}
\CommentTok{#> 9                        0.04893         0.38827            0.67725}
\CommentTok{#> 10                       0.03976         0.43238            1.02128}
\CommentTok{#> 11                       0.02446         0.43052            1.00304}
\CommentTok{#>    validation_r2 validation_auc validation_pr_auc validation_lift}
\CommentTok{#> 1             NA             NA                NA              NA}
\CommentTok{#> 2        0.27759        0.85581           0.86142         1.58140}
\CommentTok{#> 3        0.39051        0.86233           0.87462         1.58140}
\CommentTok{#> 4        0.27531        0.86047           0.87686         1.58140}
\CommentTok{#> 5        0.28377        0.88000           0.89082         1.58140}
\CommentTok{#> 6        0.24529        0.87256           0.88604         1.58140}
\CommentTok{#> 7        0.21476        0.84651           0.87261         1.58140}
\CommentTok{#> 8        0.19386        0.83070           0.85766         1.58140}
\CommentTok{#> 9        0.35154        0.87814           0.89373         1.58140}
\CommentTok{#> 10       0.19585        0.85395           0.87313         1.58140}
\CommentTok{#> 11       0.20276        0.86977           0.88944         1.58140}
\CommentTok{#>    validation_classification_error}
\CommentTok{#> 1                               NA}
\CommentTok{#> 2                          0.16176}
\CommentTok{#> 3                          0.17647}
\CommentTok{#> 4                          0.17647}
\CommentTok{#> 5                          0.17647}
\CommentTok{#> 6                          0.19118}
\CommentTok{#> 7                          0.17647}
\CommentTok{#> 8                          0.16176}
\CommentTok{#> 9                          0.16176}
\CommentTok{#> 10                         0.17647}
\CommentTok{#> 11                         0.16176}
\CommentTok{#> }
\CommentTok{#> Variable Importances: (Extract with `h2o.varimp`) }
\CommentTok{#> =================================================}
\CommentTok{#> }
\CommentTok{#> Variable Importances: }
\CommentTok{#>   variable relative_importance scaled_importance percentage}
\CommentTok{#> 1     V169            1.000000          1.000000   0.015382}
\CommentTok{#> 2     V103            0.874618          0.874618   0.013453}
\CommentTok{#> 3     V136            0.870775          0.870775   0.013394}
\CommentTok{#> 4     V239            0.843140          0.843140   0.012969}
\CommentTok{#> 5     V112            0.836208          0.836208   0.012862}
\CommentTok{#> }
\CommentTok{#> ---}
\CommentTok{#>    variable relative_importance scaled_importance percentage}
\CommentTok{#> 85      V88            0.643007          0.643007   0.009891}
\CommentTok{#> 86     V179            0.642139          0.642139   0.009877}
\CommentTok{#> 87     V137            0.640863          0.640863   0.009858}
\CommentTok{#> 88     V168            0.638585          0.638585   0.009823}
\CommentTok{#> 89     V219            0.632628          0.632628   0.009731}
\CommentTok{#> 90      V33            0.626277          0.626277   0.009633}
\end{Highlighting}
\end{Shaded}

One performance metric we are usually interested in is the mean per class error for training and validation data.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{h2o.mean_per_class_error}\NormalTok{(dl_model, }\DataTypeTok{train =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{valid =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{xval =} \OtherTok{TRUE}\NormalTok{)}
\CommentTok{#>  train  valid   xval }
\CommentTok{#> 0.0244 0.1949 0.2080}
\end{Highlighting}
\end{Shaded}

The confusion matrix tells us, how many classes have been predicted correctly and how many predictions were accurate. Here, we see the errors in predictions on validation data.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{h2o.confusionMatrix}\NormalTok{(dl_model, }\DataTypeTok{valid =} \OtherTok{TRUE}\NormalTok{)}
\CommentTok{#> Confusion Matrix (vertical: actual; across: predicted)  for max f1 @ threshold = 0.00565012071574118:}
\CommentTok{#>            arrhythmia healthy    Error    Rate}
\CommentTok{#> arrhythmia         17       8 0.320000   =8/25}
\CommentTok{#> healthy             3      40 0.069767   =3/43}
\CommentTok{#> Totals             20      48 0.161765  =11/68}
\end{Highlighting}
\end{Shaded}

We can also plot the classification error over all epochs or samples.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(dl_model,}
     \DataTypeTok{timestep =} \StringTok{"epochs"}\NormalTok{,}
     \DataTypeTok{metric =} \StringTok{"classification_error"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_239b-deeplearning_h2o_arrhythmia-sglander_files/figure-latex/unnamed-chunk-19-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(dl_model,}
     \DataTypeTok{timestep =} \StringTok{"samples"}\NormalTok{,}
     \DataTypeTok{metric =} \StringTok{"classification_error"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_239b-deeplearning_h2o_arrhythmia-sglander_files/figure-latex/unnamed-chunk-20-1} \end{center}

Next to the classification error, we are usually interested in the logistic loss (negative log-likelihood or log loss). It describes the sum of errors for each sample in the training or validation data or the negative logarithm of the likelihood of error for a given prediction/ classification. Simply put, the lower the loss, the better the model (if we ignore potential overfitting).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(dl_model,}
     \DataTypeTok{timestep =} \StringTok{"epochs"}\NormalTok{,}
     \DataTypeTok{metric =} \StringTok{"logloss"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_239b-deeplearning_h2o_arrhythmia-sglander_files/figure-latex/unnamed-chunk-21-1} \end{center}

We can also plot the mean squared error (MSE). The \textbf{MSE} tells us the average of the prediction errors squared, i.e.~the estimator's variance and bias. The closer to zero, the better a model.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(dl_model,}
     \DataTypeTok{timestep =} \StringTok{"epochs"}\NormalTok{,}
     \DataTypeTok{metric =} \StringTok{"rmse"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_239b-deeplearning_h2o_arrhythmia-sglander_files/figure-latex/unnamed-chunk-22-1} \end{center}

Next, we want to know the area under the curve (AUC). \textbf{AUC} is an important metric for measuring binary classification model performances. It gives the area under the curve, i.e.~the integral, of true positive vs false positive rates. The closer to 1, the better a model.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{h2o.auc}\NormalTok{(dl_model, }\DataTypeTok{train =} \OtherTok{TRUE}\NormalTok{)}
\CommentTok{#> [1] 0.985}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{h2o.auc}\NormalTok{(dl_model, }\DataTypeTok{valid =} \OtherTok{TRUE}\NormalTok{)}
\CommentTok{#> [1] 0.87}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{h2o.auc}\NormalTok{(dl_model, }\DataTypeTok{xval =} \OtherTok{TRUE}\NormalTok{)}
\CommentTok{#> [1] 0.843}
\end{Highlighting}
\end{Shaded}

The weights for connecting two adjacent layers and per-neuron biases that we specified the model to save, can be accessed with:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{w <-}\StringTok{ }\KeywordTok{h2o.weights}\NormalTok{(dl_model, }\DataTypeTok{matrix_id =} \DecValTok{1}\NormalTok{)}
\NormalTok{b <-}\StringTok{ }\KeywordTok{h2o.biases}\NormalTok{(dl_model, }\DataTypeTok{vector_id =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Variable importance can be extracted as well (but keep in mind, that variable importance in deep neural networks is difficult to assess and should be considered only as rough estimates).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{h2o.varimp}\NormalTok{(dl_model)}
\CommentTok{#> Variable Importances: }
\CommentTok{#>   variable relative_importance scaled_importance percentage}
\CommentTok{#> 1     V169            1.000000          1.000000   0.015382}
\CommentTok{#> 2     V103            0.874618          0.874618   0.013453}
\CommentTok{#> 3     V136            0.870775          0.870775   0.013394}
\CommentTok{#> 4     V239            0.843140          0.843140   0.012969}
\CommentTok{#> 5     V112            0.836208          0.836208   0.012862}
\CommentTok{#> }
\CommentTok{#> ---}
\CommentTok{#>    variable relative_importance scaled_importance percentage}
\CommentTok{#> 85      V88            0.643007          0.643007   0.009891}
\CommentTok{#> 86     V179            0.642139          0.642139   0.009877}
\CommentTok{#> 87     V137            0.640863          0.640863   0.009858}
\CommentTok{#> 88     V168            0.638585          0.638585   0.009823}
\CommentTok{#> 89     V219            0.632628          0.632628   0.009731}
\CommentTok{#> 90      V33            0.626277          0.626277   0.009633}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{h2o.varimp_plot}\NormalTok{(dl_model)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_239b-deeplearning_h2o_arrhythmia-sglander_files/figure-latex/unnamed-chunk-28-1} \end{center}

\hypertarget{test-data}{%
\section{Test data}\label{test-data}}

Now that we have a good idea about model performance on validation data, we want to know how it performed on unseen test data. A good model should find an optimal balance between accuracy on training and test data. A model that has 0\% error on the training data but 40\% error on the test data is in effect useless. It overfit on the training data and is thus not able to generalize to unknown data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{perf <-}\StringTok{ }\KeywordTok{h2o.performance}\NormalTok{(dl_model, test)}
\NormalTok{perf}
\CommentTok{#> H2OBinomialMetrics: deeplearning}
\CommentTok{#> }
\CommentTok{#> MSE:  0.256}
\CommentTok{#> RMSE:  0.506}
\CommentTok{#> LogLoss:  1.63}
\CommentTok{#> Mean Per-Class Error:  0.303}
\CommentTok{#> AUC:  0.786}
\CommentTok{#> pr_auc:  0.788}
\CommentTok{#> Gini:  0.573}
\CommentTok{#> }
\CommentTok{#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:}
\CommentTok{#>            arrhythmia healthy    Error    Rate}
\CommentTok{#> arrhythmia         12      15 0.555556  =15/27}
\CommentTok{#> healthy             2      37 0.051282   =2/39}
\CommentTok{#> Totals             14      52 0.257576  =17/66}
\CommentTok{#> }
\CommentTok{#> Maximum Metrics: Maximum metrics at their respective thresholds}
\CommentTok{#>                         metric threshold    value idx}
\CommentTok{#> 1                       max f1  0.000015 0.813187  51}
\CommentTok{#> 2                       max f2  0.000003 0.911215  57}
\CommentTok{#> 3                 max f0point5  0.376976 0.792350  35}
\CommentTok{#> 4                 max accuracy  0.376976 0.742424  35}
\CommentTok{#> 5                max precision  0.990135 1.000000   0}
\CommentTok{#> 6                   max recall  0.000003 1.000000  57}
\CommentTok{#> 7              max specificity  0.990135 1.000000   0}
\CommentTok{#> 8             max absolute_mcc  0.376976 0.478238  35}
\CommentTok{#> 9   max min_per_class_accuracy  0.376976 0.740741  35}
\CommentTok{#> 10 max mean_per_class_accuracy  0.376976 0.742165  35}
\CommentTok{#> }
\CommentTok{#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`}
\end{Highlighting}
\end{Shaded}

Plotting the test performance's AUC plot shows us approximately how good the predictions are.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(perf)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_239b-deeplearning_h2o_arrhythmia-sglander_files/figure-latex/unnamed-chunk-30-1} \end{center}

We also want to know the log loss, MSE and AUC values, as well as other model metrics for the test data:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{h2o.logloss}\NormalTok{(perf)}
\CommentTok{#> [1] 1.63}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{h2o.mse}\NormalTok{(perf)}
\CommentTok{#> [1] 0.256}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{h2o.auc}\NormalTok{(perf)}
\CommentTok{#> [1] 0.786}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(}\KeywordTok{h2o.metric}\NormalTok{(perf))}
\CommentTok{#> Metrics for Thresholds: Binomial metrics as a function of classification thresholds}
\CommentTok{#>   threshold       f1       f2 f0point5 accuracy precision   recall}
\CommentTok{#> 1  0.990135 0.050000 0.031847 0.116279 0.424242  1.000000 0.025641}
\CommentTok{#> 2  0.988445 0.097561 0.063291 0.212766 0.439394  1.000000 0.051282}
\CommentTok{#> 3  0.988408 0.142857 0.094340 0.294118 0.454545  1.000000 0.076923}
\CommentTok{#> 4  0.986825 0.186047 0.125000 0.363636 0.469697  1.000000 0.102564}
\CommentTok{#> 5  0.986705 0.227273 0.155280 0.423729 0.484848  1.000000 0.128205}
\CommentTok{#> 6  0.986509 0.222222 0.154321 0.396825 0.469697  0.833333 0.128205}
\CommentTok{#>   specificity absolute_mcc min_per_class_accuracy mean_per_class_accuracy}
\CommentTok{#> 1    1.000000     0.103203               0.025641                0.512821}
\CommentTok{#> 2    1.000000     0.147087               0.051282                0.525641}
\CommentTok{#> 3    1.000000     0.181568               0.076923                0.538462}
\CommentTok{#> 4    1.000000     0.211341               0.102564                0.551282}
\CommentTok{#> 5    1.000000     0.238215               0.128205                0.564103}
\CommentTok{#> 6    0.962963     0.155921               0.128205                0.545584}
\CommentTok{#>   tns fns fps tps      tnr      fnr      fpr      tpr idx}
\CommentTok{#> 1  27  38   0   1 1.000000 0.974359 0.000000 0.025641   0}
\CommentTok{#> 2  27  37   0   2 1.000000 0.948718 0.000000 0.051282   1}
\CommentTok{#> 3  27  36   0   3 1.000000 0.923077 0.000000 0.076923   2}
\CommentTok{#> 4  27  35   0   4 1.000000 0.897436 0.000000 0.102564   3}
\CommentTok{#> 5  27  34   0   5 1.000000 0.871795 0.000000 0.128205   4}
\CommentTok{#> 6  26  34   1   5 0.962963 0.871795 0.037037 0.128205   5}
\end{Highlighting}
\end{Shaded}

The confusion matrix alone can be seen with the \texttt{h2o.confusionMatrix()} function, but is is also part of the performance summary.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{h2o.confusionMatrix}\NormalTok{(dl_model, test)}
\CommentTok{#> Confusion Matrix (vertical: actual; across: predicted)  for max f1 @ threshold = 1.49810039577815e-05:}
\CommentTok{#>            arrhythmia healthy    Error    Rate}
\CommentTok{#> arrhythmia         12      15 0.555556  =15/27}
\CommentTok{#> healthy             2      37 0.051282   =2/39}
\CommentTok{#> Totals             14      52 0.257576  =17/66}
\end{Highlighting}
\end{Shaded}

The final predictions with probabilities can be extracted with the \texttt{h2o.predict()} function. Beware though, that the number of correct and wrong classifications can be slightly different from the confusion matrix above.

Here, I combine the predictions with the actual test diagnoses and classes into a data frame. For plotting I also want to have a column, that tells me whether the predictions were correct. By default, a prediction probability above 0.5 will get scored as a prediction for the respective category. I find it often makes sense to be more stringent with this, though and set a higher threshold. Therefore, I am creating another column with stringent predictions, where I only count predictions that were made with more than 80\% probability. Everything that does not fall within this range gets scored as ``uncertain''. For these stringent predictions, I am also creating a column that tells me whether they were accurate.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{finalRf_predictions <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{class =} \KeywordTok{as.vector}\NormalTok{(test}\OperatorTok{$}\NormalTok{class), }
                                  \DataTypeTok{actual =} \KeywordTok{as.vector}\NormalTok{(test}\OperatorTok{$}\NormalTok{diagnosis), }
                                  \KeywordTok{as.data.frame}\NormalTok{(}\KeywordTok{h2o.predict}\NormalTok{(}\DataTypeTok{object =}\NormalTok{ dl_model, }
                                                            \DataTypeTok{newdata =}\NormalTok{ test)))}
\CommentTok{#> }
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|}\StringTok{                                                                 }\ErrorTok{|}\StringTok{   }\DecValTok{0}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=================================================================|}\StringTok{ }\DecValTok{100}\NormalTok{%}

\NormalTok{finalRf_predictions}\OperatorTok{$}\NormalTok{accurate <-}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(}
\NormalTok{  finalRf_predictions}\OperatorTok{$}\NormalTok{actual }\OperatorTok{==}\StringTok{ }\NormalTok{finalRf_predictions}\OperatorTok{$}\NormalTok{predict, }\StringTok{"yes"}\NormalTok{, }\StringTok{"no"}\NormalTok{)}

\NormalTok{finalRf_predictions}\OperatorTok{$}\NormalTok{predict_stringent <-}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(}
\NormalTok{  finalRf_predictions}\OperatorTok{$}\NormalTok{arrhythmia }\OperatorTok{>}\StringTok{ }\FloatTok{0.8}\NormalTok{, }\StringTok{"arrhythmia"}\NormalTok{,           }
  \KeywordTok{ifelse}\NormalTok{(finalRf_predictions}\OperatorTok{$}\NormalTok{healthy }\OperatorTok{>}\StringTok{ }\FloatTok{0.8}\NormalTok{, }\StringTok{"healthy"}\NormalTok{, }\StringTok{"uncertain"}\NormalTok{))}

\NormalTok{finalRf_predictions}\OperatorTok{$}\NormalTok{accurate_stringent <-}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(}
\NormalTok{  finalRf_predictions}\OperatorTok{$}\NormalTok{actual }\OperatorTok{==}\StringTok{ }\NormalTok{finalRf_predictions}\OperatorTok{$}\NormalTok{predict_stringent, }\StringTok{"yes"}\NormalTok{, }
  \KeywordTok{ifelse}\NormalTok{(finalRf_predictions}\OperatorTok{$}\NormalTok{predict_stringent }\OperatorTok{==}\StringTok{ "uncertain"}\NormalTok{, }\StringTok{"na"}\NormalTok{, }\StringTok{"no"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{finalRf_predictions }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(actual, predict) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{n =} \KeywordTok{n}\NormalTok{())}
\CommentTok{#> # A tibble: 4 x 3}
\CommentTok{#> # Groups:   actual [2]}
\CommentTok{#>   actual     predict        n}
\CommentTok{#>   <fct>      <fct>      <int>}
\CommentTok{#> 1 arrhythmia arrhythmia    16}
\CommentTok{#> 2 arrhythmia healthy       11}
\CommentTok{#> 3 healthy    arrhythmia     7}
\CommentTok{#> 4 healthy    healthy       32}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{finalRf_predictions }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(actual, predict_stringent) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{n =} \KeywordTok{n}\NormalTok{())}
\CommentTok{#> # A tibble: 6 x 3}
\CommentTok{#> # Groups:   actual [2]}
\CommentTok{#>   actual     predict_stringent     n}
\CommentTok{#>   <fct>      <chr>             <int>}
\CommentTok{#> 1 arrhythmia arrhythmia           18}
\CommentTok{#> 2 arrhythmia healthy               7}
\CommentTok{#> 3 arrhythmia uncertain             2}
\CommentTok{#> 4 healthy    arrhythmia            9}
\CommentTok{#> 5 healthy    healthy              27}
\CommentTok{#> 6 healthy    uncertain             3}
\end{Highlighting}
\end{Shaded}

To get a better overview, I am going to plot the predictions (default and stringent):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p1 <-}\StringTok{ }\NormalTok{finalRf_predictions }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ actual, }\DataTypeTok{fill =}\NormalTok{ accurate)) }\OperatorTok{+}
\StringTok{    }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{position =} \StringTok{"dodge"}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{scale_fill_brewer}\NormalTok{(}\DataTypeTok{palette =} \StringTok{"Set1"}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{my_theme}\NormalTok{() }\OperatorTok{+}
\StringTok{    }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{fill =} \StringTok{"Were}\CharTok{\textbackslash{}n}\StringTok{predictions}\CharTok{\textbackslash{}n}\StringTok{accurate?"}\NormalTok{,}
         \DataTypeTok{title =} \StringTok{"Default predictions"}\NormalTok{)}

\NormalTok{p2 <-}\StringTok{ }\NormalTok{finalRf_predictions }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{subset}\NormalTok{(accurate_stringent }\OperatorTok{!=}\StringTok{ "na"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ actual, }\DataTypeTok{fill =}\NormalTok{ accurate_stringent)) }\OperatorTok{+}
\StringTok{    }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{position =} \StringTok{"dodge"}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{scale_fill_brewer}\NormalTok{(}\DataTypeTok{palette =} \StringTok{"Set1"}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{my_theme}\NormalTok{() }\OperatorTok{+}
\StringTok{    }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{fill =} \StringTok{"Were}\CharTok{\textbackslash{}n}\StringTok{predictions}\CharTok{\textbackslash{}n}\StringTok{accurate?"}\NormalTok{,}
         \DataTypeTok{title =} \StringTok{"Stringent predictions"}\NormalTok{)}

\KeywordTok{grid.arrange}\NormalTok{(p1, p2, }\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_239b-deeplearning_h2o_arrhythmia-sglander_files/figure-latex/unnamed-chunk-39-1} \end{center}

Being more stringent with the prediction threshold slightly reduced the number of errors but not by much.

I also want to know whether there are certain classes of arrhythmia that are especially prone to being misclassified:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p1 <-}\StringTok{ }\KeywordTok{subset}\NormalTok{(finalRf_predictions, actual }\OperatorTok{==}\StringTok{ "arrhythmia"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ predict, }\DataTypeTok{fill =}\NormalTok{ class)) }\OperatorTok{+}
\StringTok{    }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{position =} \StringTok{"dodge"}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{my_theme}\NormalTok{() }\OperatorTok{+}
\StringTok{    }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{title =} \StringTok{"Prediction accuracy of arrhythmia cases"}\NormalTok{,}
         \DataTypeTok{subtitle =} \StringTok{"Default predictions"}\NormalTok{,}
         \DataTypeTok{x =} \StringTok{"predicted to be"}\NormalTok{)}

\NormalTok{p2 <-}\StringTok{ }\KeywordTok{subset}\NormalTok{(finalRf_predictions, actual }\OperatorTok{==}\StringTok{ "arrhythmia"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ predict_stringent, }\DataTypeTok{fill =}\NormalTok{ class)) }\OperatorTok{+}
\StringTok{    }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{position =} \StringTok{"dodge"}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{my_theme}\NormalTok{() }\OperatorTok{+}
\StringTok{    }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{title =} \StringTok{"Prediction accuracy of arrhythmia cases"}\NormalTok{,}
         \DataTypeTok{subtitle =} \StringTok{"Stringent predictions"}\NormalTok{,}
         \DataTypeTok{x =} \StringTok{"predicted to be"}\NormalTok{)}

\KeywordTok{grid.arrange}\NormalTok{(p1, p2, }\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_239b-deeplearning_h2o_arrhythmia-sglander_files/figure-latex/unnamed-chunk-40-1} \end{center}

There are no obvious biases towards some classes but with the small number of samples for most classes, this is difficult to assess.

\hypertarget{final-conclusions-how-useful-is-the-model}{%
\section{Final conclusions: How useful is the model?}\label{final-conclusions-how-useful-is-the-model}}

Most samples were classified correctly, but the total error was not particularly good. Moreover, when evaluating the usefulness of a specific model, we need to keep in mind what we want to achieve with it and which questions we want to answer. If we wanted to deploy this model in a clinical setting, it should assist with diagnosing patients. So, we need to think about what the consequences of wrong classifications would be. Would it be better to optimize for high sensitivity, in this example as many arrhythmia cases as possible get detected - with the drawback that we probably also diagnose a few healthy people? Or do we want to maximize precision, meaning that we could be confident that a patient who got predicted to have arrhythmia does indeed have it, while accepting that a few arrhythmia cases would remain undiagnosed? When we consider stringent predictions, this model correctly classified 19 out of 27 arrhythmia cases, but 6 were misdiagnosed. This would mean that some patients who were actually sick, wouldn't have gotten the correct treatment (if decided solely based on this model). For real-life application, this is obviously not sufficient!

Next week, I'll be trying to improve the model by doing a grid search for hyper-parameter tuning.

So, stay tuned\ldots{} (sorry, couldn't resist ;-))

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sessionInfo}\NormalTok{()}
\CommentTok{#> R version 3.6.0 (2019-04-26)}
\CommentTok{#> Platform: x86_64-pc-linux-gnu (64-bit)}
\CommentTok{#> Running under: Ubuntu 18.04.3 LTS}
\CommentTok{#> }
\CommentTok{#> Matrix products: default}
\CommentTok{#> BLAS/LAPACK: /usr/lib/x86_64-linux-gnu/libopenblasp-r0.2.20.so}
\CommentTok{#> }
\CommentTok{#> locale:}
\CommentTok{#>  [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              }
\CommentTok{#>  [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    }
\CommentTok{#>  [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   }
\CommentTok{#>  [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                 }
\CommentTok{#>  [9] LC_ADDRESS=C               LC_TELEPHONE=C            }
\CommentTok{#> [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       }
\CommentTok{#> }
\CommentTok{#> attached base packages:}
\CommentTok{#>  [1] stats4    parallel  grid      stats     graphics  grDevices utils    }
\CommentTok{#>  [8] datasets  methods   base     }
\CommentTok{#> }
\CommentTok{#> other attached packages:}
\CommentTok{#>  [1] reshape2_1.4.3       tidyr_0.8.3          matrixStats_0.54.0  }
\CommentTok{#>  [4] pcaGoPromoter_1.28.0 Biostrings_2.52.0    XVector_0.24.0      }
\CommentTok{#>  [7] IRanges_2.18.0       S4Vectors_0.22.0     BiocGenerics_0.30.0 }
\CommentTok{#> [10] ellipse_0.4.1        gridExtra_2.3        ggrepel_0.8.1       }
\CommentTok{#> [13] ggplot2_3.1.1        h2o_3.22.1.1         dplyr_0.8.0.1       }
\CommentTok{#> [16] logging_0.9-107     }
\CommentTok{#> }
\CommentTok{#> loaded via a namespace (and not attached):}
\CommentTok{#>  [1] Rcpp_1.0.1           assertthat_0.2.1     zeallot_0.1.0       }
\CommentTok{#>  [4] rprojroot_1.3-2      digest_0.6.18        utf8_1.1.4          }
\CommentTok{#>  [7] R6_2.4.0             plyr_1.8.4           backports_1.1.4     }
\CommentTok{#> [10] RSQLite_2.1.1        evaluate_0.13        pillar_1.4.0        }
\CommentTok{#> [13] zlibbioc_1.30.0      rlang_0.3.4          lazyeval_0.2.2      }
\CommentTok{#> [16] rstudioapi_0.10      blob_1.1.1           rmarkdown_1.12      }
\CommentTok{#> [19] labeling_0.3         stringr_1.4.0        RCurl_1.95-4.12     }
\CommentTok{#> [22] bit_1.1-14           munsell_0.5.0        compiler_3.6.0      }
\CommentTok{#> [25] xfun_0.6             pkgconfig_2.0.2      htmltools_0.3.6     }
\CommentTok{#> [28] tidyselect_0.2.5     tibble_2.1.1         bookdown_0.10       }
\CommentTok{#> [31] fansi_0.4.0          crayon_1.3.4         withr_2.1.2         }
\CommentTok{#> [34] bitops_1.0-6         jsonlite_1.6         gtable_0.3.0        }
\CommentTok{#> [37] DBI_1.0.0            magrittr_1.5         scales_1.0.0        }
\CommentTok{#> [40] cli_1.1.0            stringi_1.4.3        vctrs_0.1.0         }
\CommentTok{#> [43] RColorBrewer_1.1-2   tools_3.6.0          bit64_0.9-7         }
\CommentTok{#> [46] Biobase_2.44.0       glue_1.3.1           purrr_0.3.2         }
\CommentTok{#> [49] yaml_2.2.0           AnnotationDbi_1.46.0 colorspace_1.4-1    }
\CommentTok{#> [52] memoise_1.1.0        knitr_1.22}
\end{Highlighting}
\end{Shaded}

\hypertarget{credit-scoring}{%
\chapter{Credit Scoring}\label{credit-scoring}}

\hypertarget{introduction-1}{%
\section{Introduction}\label{introduction-1}}

Source: \url{https://www.r-bloggers.com/using-neural-networks-for-credit-scoring-a-simple-example/}

\hypertarget{motivation}{%
\section{Motivation}\label{motivation}}

Credit scoring is the practice of analysing a persons background and credit application in order to assess the creditworthiness of the person. One can take numerous approaches on analysing this creditworthiness. In the end it basically comes down to first selecting the correct independent variables (e.g.~income, age, gender) that lead to a given level of creditworthiness.

In other words: \texttt{creditworthiness\ =\ f(income,\ age,\ gender,\ \ldots{})}.

A creditscoring system can be represented by linear regression, logistic regression, machine learning or a combination of these. Neural networks are situated in the domain of machine learning. The following is an strongly simplified example. The actual procedure of building a credit scoring system is much more complex and the resulting model will most likely not consist of solely or even a neural network.

If you're unsure on what a neural network exactly is, I find this a good place to start.

For this example the R package \texttt{neuralnet} is used, for a more in-depth view on the exact workings of the package see neuralnet: \texttt{Training\ of\ Neural\ Networks} by F. Gnther and S. Fritsch.

\hypertarget{load-the-data}{%
\section{load the data}\label{load-the-data}}

Dataset downloaded: \url{https://gist.github.com/Bart6114/8675941\#file-creditset-csv}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1234567890}\NormalTok{)}

\KeywordTok{library}\NormalTok{(neuralnet)}

\NormalTok{dataset <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\KeywordTok{file.path}\NormalTok{(data_raw_dir, }\StringTok{"creditset.csv"}\NormalTok{))}
\KeywordTok{head}\NormalTok{(dataset)}
\CommentTok{#>   clientid income  age   loan      LTI default10yr}
\CommentTok{#> 1        1  66156 59.0 8106.5 0.122537           0}
\CommentTok{#> 2        2  34415 48.1 6564.7 0.190752           0}
\CommentTok{#> 3        3  57317 63.1 8021.0 0.139940           0}
\CommentTok{#> 4        4  42710 45.8 6103.6 0.142911           0}
\CommentTok{#> 5        5  66953 18.6 8770.1 0.130989           1}
\CommentTok{#> 6        6  24904 57.5   15.5 0.000622           0}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{names}\NormalTok{(dataset)}
\CommentTok{#> [1] "clientid"    "income"      "age"         "loan"        "LTI"        }
\CommentTok{#> [6] "default10yr"}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(dataset)}
\CommentTok{#>     clientid        income           age            loan      }
\CommentTok{#>  Min.   :   1   Min.   :20014   Min.   :18.1   Min.   :    1  }
\CommentTok{#>  1st Qu.: 501   1st Qu.:32796   1st Qu.:29.1   1st Qu.: 1940  }
\CommentTok{#>  Median :1000   Median :45789   Median :41.4   Median : 3975  }
\CommentTok{#>  Mean   :1000   Mean   :45332   Mean   :40.9   Mean   : 4444  }
\CommentTok{#>  3rd Qu.:1500   3rd Qu.:57791   3rd Qu.:52.6   3rd Qu.: 6432  }
\CommentTok{#>  Max.   :2000   Max.   :69996   Max.   :64.0   Max.   :13766  }
\CommentTok{#>       LTI          default10yr   }
\CommentTok{#>  Min.   :0.0000   Min.   :0.000  }
\CommentTok{#>  1st Qu.:0.0479   1st Qu.:0.000  }
\CommentTok{#>  Median :0.0994   Median :0.000  }
\CommentTok{#>  Mean   :0.0984   Mean   :0.142  }
\CommentTok{#>  3rd Qu.:0.1476   3rd Qu.:0.000  }
\CommentTok{#>  Max.   :0.1999   Max.   :1.000}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# distribution of defaults}
\KeywordTok{table}\NormalTok{(dataset}\OperatorTok{$}\NormalTok{default10yr)}
\CommentTok{#> }
\CommentTok{#>    0    1 }
\CommentTok{#> 1717  283}
\KeywordTok{min}\NormalTok{(dataset}\OperatorTok{$}\NormalTok{LTI)}
\CommentTok{#> [1] 4.91e-05}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{jitter}\NormalTok{(dataset}\OperatorTok{$}\NormalTok{default10yr, }\DecValTok{1}\NormalTok{) }\OperatorTok{~}\StringTok{ }\KeywordTok{jitter}\NormalTok{(dataset}\OperatorTok{$}\NormalTok{LTI, }\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_900-credit_neuralnet_files/figure-latex/unnamed-chunk-5-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# convert LTI continuous variable to categorical}
\NormalTok{dataset}\OperatorTok{$}\NormalTok{LTIrng <-}\StringTok{ }\KeywordTok{cut}\NormalTok{(dataset}\OperatorTok{$}\NormalTok{LTI, }\DataTypeTok{breaks =} \DecValTok{10}\NormalTok{)}
\KeywordTok{unique}\NormalTok{(dataset}\OperatorTok{$}\NormalTok{LTIrng)}
\CommentTok{#>  [1] (0.12,0.14]      (0.18,0.2]       (0.14,0.16]      (-0.000151,0.02]}
\CommentTok{#>  [5] (0.1,0.12]       (0.04,0.06]      (0.06,0.08]      (0.08,0.1]      }
\CommentTok{#>  [9] (0.16,0.18]      (0.02,0.04]     }
\CommentTok{#> 10 Levels: (-0.000151,0.02] (0.02,0.04] (0.04,0.06] ... (0.18,0.2]}
\KeywordTok{plot}\NormalTok{(dataset}\OperatorTok{$}\NormalTok{LTIrng, dataset}\OperatorTok{$}\NormalTok{default10yr)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_900-credit_neuralnet_files/figure-latex/unnamed-chunk-6-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# what age and LTI is more likely to default}
\KeywordTok{library}\NormalTok{(ggplot2)}
\CommentTok{#> Registered S3 methods overwritten by 'ggplot2':}
\CommentTok{#>   method         from }
\CommentTok{#>   [.quosures     rlang}
\CommentTok{#>   c.quosures     rlang}
\CommentTok{#>   print.quosures rlang}

\KeywordTok{ggplot}\NormalTok{(dataset, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ age, }\DataTypeTok{y =}\NormalTok{ LTI, }\DataTypeTok{col =}\NormalTok{ default10yr)) }\OperatorTok{+}
\StringTok{    }\KeywordTok{geom_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_900-credit_neuralnet_files/figure-latex/unnamed-chunk-7-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# what age and loan size is more likely to default}
\KeywordTok{library}\NormalTok{(ggplot2)}

\KeywordTok{ggplot}\NormalTok{(dataset, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ age, }\DataTypeTok{y =}\NormalTok{ loan, }\DataTypeTok{col =}\NormalTok{ default10yr)) }\OperatorTok{+}
\StringTok{    }\KeywordTok{geom_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_900-credit_neuralnet_files/figure-latex/unnamed-chunk-8-1} \end{center}

\hypertarget{objective}{%
\section{Objective}\label{objective}}

The dataset contains information on different clients who received a loan at least 10 years ago. The variables income (yearly), age, loan (size in euros) and LTI (the loan to yearly income ratio) are available. Our goal is to devise a model which predicts, based on the input variables LTI and age, whether or not a default will occur within 10 years.

\hypertarget{steps}{%
\section{Steps}\label{steps}}

The dataset will be split up in a subset used for training the neural network and another set used for testing. As the ordering of the dataset is completely random, we do not have to extract random rows and can just take the first x rows.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## extract a set to train the NN}
\NormalTok{trainset <-}\StringTok{ }\NormalTok{dataset[}\DecValTok{1}\OperatorTok{:}\DecValTok{800}\NormalTok{, ]}

\CommentTok{## select the test set}
\NormalTok{testset <-}\StringTok{ }\NormalTok{dataset[}\DecValTok{801}\OperatorTok{:}\DecValTok{2000}\NormalTok{, ]}
\end{Highlighting}
\end{Shaded}

\hypertarget{build-the-neural-network}{%
\subsection{Build the neural network}\label{build-the-neural-network}}

Now we'll build a neural network with 4 hidden nodes (a neural network is comprised of an input, hidden and output nodes). The number of nodes is chosen here without a clear method, however there are some rules of thumb. The \texttt{lifesign} option refers to the verbosity. The \texttt{ouput} is not linear and we will use a \texttt{threshold} value of 10\%. The \texttt{neuralnet} package uses resilient backpropagation with weight backtracking as its standard algorithm.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## build the neural network (NN)}
\NormalTok{creditnet <-}\StringTok{ }\KeywordTok{neuralnet}\NormalTok{(default10yr }\OperatorTok{~}\StringTok{ }\NormalTok{LTI }\OperatorTok{+}\StringTok{ }\NormalTok{age, trainset, }
                       \DataTypeTok{hidden =} \DecValTok{4}\NormalTok{, }
                       \DataTypeTok{lifesign =} \StringTok{"minimal"}\NormalTok{, }
                       \DataTypeTok{linear.output =} \OtherTok{FALSE}\NormalTok{, }
                       \DataTypeTok{threshold =} \FloatTok{0.1}\NormalTok{)}
\CommentTok{#> hidden: 4    thresh: 0.1    rep: 1/1    steps:   44487   error: 0.20554  time: 10.05 secs}
\end{Highlighting}
\end{Shaded}

The neuralnet package also has the possibility to visualize the generated model and show the found weights.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## plot the NN}
\KeywordTok{plot}\NormalTok{(creditnet, }\DataTypeTok{rep =} \StringTok{"best"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_900-credit_neuralnet_files/figure-latex/unnamed-chunk-11-1} \end{center}

\hypertarget{test-the-neural-network}{%
\section{Test the neural network}\label{test-the-neural-network}}

Once we've trained the neural network we are ready to test it. We use the testset subset for this. The \texttt{compute} function is applied for computing the outputs based on the LTI and age inputs from the testset.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## test the resulting output}
\NormalTok{temp_test <-}\StringTok{ }\KeywordTok{subset}\NormalTok{(testset, }\DataTypeTok{select =} \KeywordTok{c}\NormalTok{(}\StringTok{"LTI"}\NormalTok{, }\StringTok{"age"}\NormalTok{))}

\NormalTok{creditnet.results <-}\StringTok{ }\KeywordTok{compute}\NormalTok{(creditnet, temp_test)}
\end{Highlighting}
\end{Shaded}

The temp dataset contains only the columns LTI and age of the train set. Only these variables are used for input. The set looks as follows:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(temp_test)}
\CommentTok{#>        LTI  age}
\CommentTok{#> 801 0.0231 25.9}
\CommentTok{#> 802 0.1373 40.8}
\CommentTok{#> 803 0.1046 32.5}
\CommentTok{#> 804 0.1599 53.2}
\CommentTok{#> 805 0.1116 46.5}
\CommentTok{#> 806 0.1149 47.1}
\end{Highlighting}
\end{Shaded}

Let's have a look at what the neural network produced:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{actual =}\NormalTok{ testset}\OperatorTok{$}\NormalTok{default10yr, }\DataTypeTok{prediction =}\NormalTok{ creditnet.results}\OperatorTok{$}\NormalTok{net.result)}
\NormalTok{results[}\DecValTok{100}\OperatorTok{:}\DecValTok{115}\NormalTok{, ]}
\CommentTok{#>     actual prediction}
\CommentTok{#> 900      0   7.29e-32}
\CommentTok{#> 901      0   8.17e-11}
\CommentTok{#> 902      0   4.33e-45}
\CommentTok{#> 903      1   1.00e+00}
\CommentTok{#> 904      0   8.06e-04}
\CommentTok{#> 905      0   3.54e-40}
\CommentTok{#> 906      0   1.48e-24}
\CommentTok{#> 907      1   1.00e+00}
\CommentTok{#> 908      0   1.11e-02}
\CommentTok{#> 909      0   8.05e-44}
\CommentTok{#> 910      0   6.72e-07}
\CommentTok{#> 911      1   1.00e+00}
\CommentTok{#> 912      0   9.97e-59}
\CommentTok{#> 913      1   1.00e+00}
\CommentTok{#> 914      0   3.39e-37}
\CommentTok{#> 915      0   1.18e-07}
\end{Highlighting}
\end{Shaded}

We can round to the nearest integer to improve readability:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results}\OperatorTok{$}\NormalTok{prediction <-}\StringTok{ }\KeywordTok{round}\NormalTok{(results}\OperatorTok{$}\NormalTok{prediction)}
\NormalTok{results[}\DecValTok{100}\OperatorTok{:}\DecValTok{115}\NormalTok{, ]}
\CommentTok{#>     actual prediction}
\CommentTok{#> 900      0          0}
\CommentTok{#> 901      0          0}
\CommentTok{#> 902      0          0}
\CommentTok{#> 903      1          1}
\CommentTok{#> 904      0          0}
\CommentTok{#> 905      0          0}
\CommentTok{#> 906      0          0}
\CommentTok{#> 907      1          1}
\CommentTok{#> 908      0          0}
\CommentTok{#> 909      0          0}
\CommentTok{#> 910      0          0}
\CommentTok{#> 911      1          1}
\CommentTok{#> 912      0          0}
\CommentTok{#> 913      1          1}
\CommentTok{#> 914      0          0}
\CommentTok{#> 915      0          0}
\end{Highlighting}
\end{Shaded}

As you can see it is pretty close! As already stated, this is a strongly simplified example. But it might serve as a basis for you to play around with your first neural network.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# how many predictions were wrong}
\NormalTok{indices <-}\StringTok{ }\KeywordTok{which}\NormalTok{(results}\OperatorTok{$}\NormalTok{actual }\OperatorTok{!=}\StringTok{ }\NormalTok{results}\OperatorTok{$}\NormalTok{prediction)}
\NormalTok{indices}
\CommentTok{#> [1]  330 1008}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# what are the predictions that failed}
\NormalTok{results[indices,]}
\CommentTok{#>      actual prediction}
\CommentTok{#> 1130      0          1}
\CommentTok{#> 1808      1          0}
\end{Highlighting}
\end{Shaded}

\hypertarget{wine-with-neuralnet}{%
\chapter{Wine with neuralnet}\label{wine-with-neuralnet}}

Source: \url{https://www.r-bloggers.com/multilabel-classification-with-neuralnet-package/}

The neuralnet package is perhaps not the best option in R for using neural networks. If you ask why, for starters it does not recognize the typical formula y\textasciitilde{}., it does not support factors, it does not provide a lot of models other than a standard MLP, and it has great competitors in the nnet package that seems to be better integrated in R and can be used with the caret package, and in the MXnet package that is a high level deep learning library which provides a wide variety of neural networks.

But still, I think there is some value in the ease of use of the neuralnet package, especially for a beginner, therefore I'll be using it.

I'm going to be using both the neuralnet and, curiously enough, the nnet package. Let's load them:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load libs}
\KeywordTok{require}\NormalTok{(neuralnet)}
\CommentTok{#> Loading required package: neuralnet}
\KeywordTok{require}\NormalTok{(nnet)}
\CommentTok{#> Loading required package: nnet}
\KeywordTok{require}\NormalTok{(ggplot2)}
\CommentTok{#> Loading required package: ggplot2}
\CommentTok{#> Registered S3 methods overwritten by 'ggplot2':}
\CommentTok{#>   method         from }
\CommentTok{#>   [.quosures     rlang}
\CommentTok{#>   c.quosures     rlang}
\CommentTok{#>   print.quosures rlang}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{the-dataset}{%
\section{The dataset}\label{the-dataset}}

I looked in the UCI Machine Learning Repository1 and found the wine dataset.

This dataset contains the results of a chemical analysis on 3 different kind of wines. The target variable is the label of the wine which is a factor with 3 (unordered) levels. The predictors are all continuous and represent 13 variables obtained as a result of chemical measurements.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# get the data file from the package location}
\NormalTok{wine_dataset_path <-}\StringTok{ }\KeywordTok{file.path}\NormalTok{(data_raw_dir, }\StringTok{"wine.data"}\NormalTok{)}
\NormalTok{wine_dataset_path}
\CommentTok{#> [1] "/home/datascience/repos/machine-learning-rsuite/import/wine.data"}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{wines <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(wine_dataset_path)}
\NormalTok{wines}
\CommentTok{#>     X1 X14.23 X1.71 X2.43 X15.6 X127 X2.8 X3.06 X.28 X2.29 X5.64 X1.04}
\CommentTok{#> 1    1   13.2  1.78  2.14  11.2  100 2.65  2.76 0.26  1.28  4.38 1.050}
\CommentTok{#> 2    1   13.2  2.36  2.67  18.6  101 2.80  3.24 0.30  2.81  5.68 1.030}
\CommentTok{#> 3    1   14.4  1.95  2.50  16.8  113 3.85  3.49 0.24  2.18  7.80 0.860}
\CommentTok{#> 4    1   13.2  2.59  2.87  21.0  118 2.80  2.69 0.39  1.82  4.32 1.040}
\CommentTok{#> 5    1   14.2  1.76  2.45  15.2  112 3.27  3.39 0.34  1.97  6.75 1.050}
\CommentTok{#> 6    1   14.4  1.87  2.45  14.6   96 2.50  2.52 0.30  1.98  5.25 1.020}
\CommentTok{#> 7    1   14.1  2.15  2.61  17.6  121 2.60  2.51 0.31  1.25  5.05 1.060}
\CommentTok{#> 8    1   14.8  1.64  2.17  14.0   97 2.80  2.98 0.29  1.98  5.20 1.080}
\CommentTok{#> 9    1   13.9  1.35  2.27  16.0   98 2.98  3.15 0.22  1.85  7.22 1.010}
\CommentTok{#> 10   1   14.1  2.16  2.30  18.0  105 2.95  3.32 0.22  2.38  5.75 1.250}
\CommentTok{#> 11   1   14.1  1.48  2.32  16.8   95 2.20  2.43 0.26  1.57  5.00 1.170}
\CommentTok{#> 12   1   13.8  1.73  2.41  16.0   89 2.60  2.76 0.29  1.81  5.60 1.150}
\CommentTok{#> 13   1   14.8  1.73  2.39  11.4   91 3.10  3.69 0.43  2.81  5.40 1.250}
\CommentTok{#> 14   1   14.4  1.87  2.38  12.0  102 3.30  3.64 0.29  2.96  7.50 1.200}
\CommentTok{#> 15   1   13.6  1.81  2.70  17.2  112 2.85  2.91 0.30  1.46  7.30 1.280}
\CommentTok{#> 16   1   14.3  1.92  2.72  20.0  120 2.80  3.14 0.33  1.97  6.20 1.070}
\CommentTok{#> 17   1   13.8  1.57  2.62  20.0  115 2.95  3.40 0.40  1.72  6.60 1.130}
\CommentTok{#> 18   1   14.2  1.59  2.48  16.5  108 3.30  3.93 0.32  1.86  8.70 1.230}
\CommentTok{#> 19   1   13.6  3.10  2.56  15.2  116 2.70  3.03 0.17  1.66  5.10 0.960}
\CommentTok{#> 20   1   14.1  1.63  2.28  16.0  126 3.00  3.17 0.24  2.10  5.65 1.090}
\CommentTok{#> 21   1   12.9  3.80  2.65  18.6  102 2.41  2.41 0.25  1.98  4.50 1.030}
\CommentTok{#> 22   1   13.7  1.86  2.36  16.6  101 2.61  2.88 0.27  1.69  3.80 1.110}
\CommentTok{#> 23   1   12.8  1.60  2.52  17.8   95 2.48  2.37 0.26  1.46  3.93 1.090}
\CommentTok{#> 24   1   13.5  1.81  2.61  20.0   96 2.53  2.61 0.28  1.66  3.52 1.120}
\CommentTok{#> 25   1   13.1  2.05  3.22  25.0  124 2.63  2.68 0.47  1.92  3.58 1.130}
\CommentTok{#> 26   1   13.4  1.77  2.62  16.1   93 2.85  2.94 0.34  1.45  4.80 0.920}
\CommentTok{#> 27   1   13.3  1.72  2.14  17.0   94 2.40  2.19 0.27  1.35  3.95 1.020}
\CommentTok{#> 28   1   13.9  1.90  2.80  19.4  107 2.95  2.97 0.37  1.76  4.50 1.250}
\CommentTok{#> 29   1   14.0  1.68  2.21  16.0   96 2.65  2.33 0.26  1.98  4.70 1.040}
\CommentTok{#> 30   1   13.7  1.50  2.70  22.5  101 3.00  3.25 0.29  2.38  5.70 1.190}
\CommentTok{#> 31   1   13.6  1.66  2.36  19.1  106 2.86  3.19 0.22  1.95  6.90 1.090}
\CommentTok{#> 32   1   13.7  1.83  2.36  17.2  104 2.42  2.69 0.42  1.97  3.84 1.230}
\CommentTok{#> 33   1   13.8  1.53  2.70  19.5  132 2.95  2.74 0.50  1.35  5.40 1.250}
\CommentTok{#> 34   1   13.5  1.80  2.65  19.0  110 2.35  2.53 0.29  1.54  4.20 1.100}
\CommentTok{#> 35   1   13.5  1.81  2.41  20.5  100 2.70  2.98 0.26  1.86  5.10 1.040}
\CommentTok{#> 36   1   13.3  1.64  2.84  15.5  110 2.60  2.68 0.34  1.36  4.60 1.090}
\CommentTok{#> 37   1   13.1  1.65  2.55  18.0   98 2.45  2.43 0.29  1.44  4.25 1.120}
\CommentTok{#> 38   1   13.1  1.50  2.10  15.5   98 2.40  2.64 0.28  1.37  3.70 1.180}
\CommentTok{#> 39   1   14.2  3.99  2.51  13.2  128 3.00  3.04 0.20  2.08  5.10 0.890}
\CommentTok{#> 40   1   13.6  1.71  2.31  16.2  117 3.15  3.29 0.34  2.34  6.13 0.950}
\CommentTok{#> 41   1   13.4  3.84  2.12  18.8   90 2.45  2.68 0.27  1.48  4.28 0.910}
\CommentTok{#> 42   1   13.9  1.89  2.59  15.0  101 3.25  3.56 0.17  1.70  5.43 0.880}
\CommentTok{#> 43   1   13.2  3.98  2.29  17.5  103 2.64  2.63 0.32  1.66  4.36 0.820}
\CommentTok{#> 44   1   13.1  1.77  2.10  17.0  107 3.00  3.00 0.28  2.03  5.04 0.880}
\CommentTok{#> 45   1   14.2  4.04  2.44  18.9  111 2.85  2.65 0.30  1.25  5.24 0.870}
\CommentTok{#> 46   1   14.4  3.59  2.28  16.0  102 3.25  3.17 0.27  2.19  4.90 1.040}
\CommentTok{#> 47   1   13.9  1.68  2.12  16.0  101 3.10  3.39 0.21  2.14  6.10 0.910}
\CommentTok{#> 48   1   14.1  2.02  2.40  18.8  103 2.75  2.92 0.32  2.38  6.20 1.070}
\CommentTok{#> 49   1   13.9  1.73  2.27  17.4  108 2.88  3.54 0.32  2.08  8.90 1.120}
\CommentTok{#> 50   1   13.1  1.73  2.04  12.4   92 2.72  3.27 0.17  2.91  7.20 1.120}
\CommentTok{#> 51   1   13.8  1.65  2.60  17.2   94 2.45  2.99 0.22  2.29  5.60 1.240}
\CommentTok{#> 52   1   13.8  1.75  2.42  14.0  111 3.88  3.74 0.32  1.87  7.05 1.010}
\CommentTok{#> 53   1   13.8  1.90  2.68  17.1  115 3.00  2.79 0.39  1.68  6.30 1.130}
\CommentTok{#> 54   1   13.7  1.67  2.25  16.4  118 2.60  2.90 0.21  1.62  5.85 0.920}
\CommentTok{#> 55   1   13.6  1.73  2.46  20.5  116 2.96  2.78 0.20  2.45  6.25 0.980}
\CommentTok{#> 56   1   14.2  1.70  2.30  16.3  118 3.20  3.00 0.26  2.03  6.38 0.940}
\CommentTok{#> 57   1   13.3  1.97  2.68  16.8  102 3.00  3.23 0.31  1.66  6.00 1.070}
\CommentTok{#> 58   1   13.7  1.43  2.50  16.7  108 3.40  3.67 0.19  2.04  6.80 0.890}
\CommentTok{#> 59   2   12.4  0.94  1.36  10.6   88 1.98  0.57 0.28  0.42  1.95 1.050}
\CommentTok{#> 60   2   12.3  1.10  2.28  16.0  101 2.05  1.09 0.63  0.41  3.27 1.250}
\CommentTok{#> 61   2   12.6  1.36  2.02  16.8  100 2.02  1.41 0.53  0.62  5.75 0.980}
\CommentTok{#> 62   2   13.7  1.25  1.92  18.0   94 2.10  1.79 0.32  0.73  3.80 1.230}
\CommentTok{#> 63   2   12.4  1.13  2.16  19.0   87 3.50  3.10 0.19  1.87  4.45 1.220}
\CommentTok{#> 64   2   12.2  1.45  2.53  19.0  104 1.89  1.75 0.45  1.03  2.95 1.450}
\CommentTok{#> 65   2   12.4  1.21  2.56  18.1   98 2.42  2.65 0.37  2.08  4.60 1.190}
\CommentTok{#> 66   2   13.1  1.01  1.70  15.0   78 2.98  3.18 0.26  2.28  5.30 1.120}
\CommentTok{#> 67   2   12.4  1.17  1.92  19.6   78 2.11  2.00 0.27  1.04  4.68 1.120}
\CommentTok{#> 68   2   13.3  0.94  2.36  17.0  110 2.53  1.30 0.55  0.42  3.17 1.020}
\CommentTok{#> 69   2   12.2  1.19  1.75  16.8  151 1.85  1.28 0.14  2.50  2.85 1.280}
\CommentTok{#> 70   2   12.3  1.61  2.21  20.4  103 1.10  1.02 0.37  1.46  3.05 0.906}
\CommentTok{#> 71   2   13.9  1.51  2.67  25.0   86 2.95  2.86 0.21  1.87  3.38 1.360}
\CommentTok{#> 72   2   13.5  1.66  2.24  24.0   87 1.88  1.84 0.27  1.03  3.74 0.980}
\CommentTok{#> 73   2   13.0  1.67  2.60  30.0  139 3.30  2.89 0.21  1.96  3.35 1.310}
\CommentTok{#> 74   2   12.0  1.09  2.30  21.0  101 3.38  2.14 0.13  1.65  3.21 0.990}
\CommentTok{#> 75   2   11.7  1.88  1.92  16.0   97 1.61  1.57 0.34  1.15  3.80 1.230}
\CommentTok{#> 76   2   13.0  0.90  1.71  16.0   86 1.95  2.03 0.24  1.46  4.60 1.190}
\CommentTok{#> 77   2   11.8  2.89  2.23  18.0  112 1.72  1.32 0.43  0.95  2.65 0.960}
\CommentTok{#> 78   2   12.3  0.99  1.95  14.8  136 1.90  1.85 0.35  2.76  3.40 1.060}
\CommentTok{#> 79   2   12.7  3.87  2.40  23.0  101 2.83  2.55 0.43  1.95  2.57 1.190}
\CommentTok{#> 80   2   12.0  0.92  2.00  19.0   86 2.42  2.26 0.30  1.43  2.50 1.380}
\CommentTok{#> 81   2   12.7  1.81  2.20  18.8   86 2.20  2.53 0.26  1.77  3.90 1.160}
\CommentTok{#> 82   2   12.1  1.13  2.51  24.0   78 2.00  1.58 0.40  1.40  2.20 1.310}
\CommentTok{#> 83   2   13.1  3.86  2.32  22.5   85 1.65  1.59 0.61  1.62  4.80 0.840}
\CommentTok{#> 84   2   11.8  0.89  2.58  18.0   94 2.20  2.21 0.22  2.35  3.05 0.790}
\CommentTok{#> 85   2   12.7  0.98  2.24  18.0   99 2.20  1.94 0.30  1.46  2.62 1.230}
\CommentTok{#> 86   2   12.2  1.61  2.31  22.8   90 1.78  1.69 0.43  1.56  2.45 1.330}
\CommentTok{#> 87   2   11.7  1.67  2.62  26.0   88 1.92  1.61 0.40  1.34  2.60 1.360}
\CommentTok{#> 88   2   11.6  2.06  2.46  21.6   84 1.95  1.69 0.48  1.35  2.80 1.000}
\CommentTok{#> 89   2   12.1  1.33  2.30  23.6   70 2.20  1.59 0.42  1.38  1.74 1.070}
\CommentTok{#> 90   2   12.1  1.83  2.32  18.5   81 1.60  1.50 0.52  1.64  2.40 1.080}
\CommentTok{#> 91   2   12.0  1.51  2.42  22.0   86 1.45  1.25 0.50  1.63  3.60 1.050}
\CommentTok{#> 92   2   12.7  1.53  2.26  20.7   80 1.38  1.46 0.58  1.62  3.05 0.960}
\CommentTok{#> 93   2   12.3  2.83  2.22  18.0   88 2.45  2.25 0.25  1.99  2.15 1.150}
\CommentTok{#> 94   2   11.6  1.99  2.28  18.0   98 3.02  2.26 0.17  1.35  3.25 1.160}
\CommentTok{#> 95   2   12.5  1.52  2.20  19.0  162 2.50  2.27 0.32  3.28  2.60 1.160}
\CommentTok{#> 96   2   11.8  2.12  2.74  21.5  134 1.60  0.99 0.14  1.56  2.50 0.950}
\CommentTok{#> 97   2   12.3  1.41  1.98  16.0   85 2.55  2.50 0.29  1.77  2.90 1.230}
\CommentTok{#> 98   2   12.4  1.07  2.10  18.5   88 3.52  3.75 0.24  1.95  4.50 1.040}
\CommentTok{#> 99   2   12.3  3.17  2.21  18.0   88 2.85  2.99 0.45  2.81  2.30 1.420}
\CommentTok{#> 100  2   12.1  2.08  1.70  17.5   97 2.23  2.17 0.26  1.40  3.30 1.270}
\CommentTok{#> 101  2   12.6  1.34  1.90  18.5   88 1.45  1.36 0.29  1.35  2.45 1.040}
\CommentTok{#> 102  2   12.3  2.45  2.46  21.0   98 2.56  2.11 0.34  1.31  2.80 0.800}
\CommentTok{#> 103  2   11.8  1.72  1.88  19.5   86 2.50  1.64 0.37  1.42  2.06 0.940}
\CommentTok{#> 104  2   12.5  1.73  1.98  20.5   85 2.20  1.92 0.32  1.48  2.94 1.040}
\CommentTok{#> 105  2   12.4  2.55  2.27  22.0   90 1.68  1.84 0.66  1.42  2.70 0.860}
\CommentTok{#> 106  2   12.2  1.73  2.12  19.0   80 1.65  2.03 0.37  1.63  3.40 1.000}
\CommentTok{#> 107  2   12.7  1.75  2.28  22.5   84 1.38  1.76 0.48  1.63  3.30 0.880}
\CommentTok{#> 108  2   12.2  1.29  1.94  19.0   92 2.36  2.04 0.39  2.08  2.70 0.860}
\CommentTok{#> 109  2   11.6  1.35  2.70  20.0   94 2.74  2.92 0.29  2.49  2.65 0.960}
\CommentTok{#> 110  2   11.5  3.74  1.82  19.5  107 3.18  2.58 0.24  3.58  2.90 0.750}
\CommentTok{#> 111  2   12.5  2.43  2.17  21.0   88 2.55  2.27 0.26  1.22  2.00 0.900}
\CommentTok{#> 112  2   11.8  2.68  2.92  20.0  103 1.75  2.03 0.60  1.05  3.80 1.230}
\CommentTok{#> 113  2   11.4  0.74  2.50  21.0   88 2.48  2.01 0.42  1.44  3.08 1.100}
\CommentTok{#> 114  2   12.1  1.39  2.50  22.5   84 2.56  2.29 0.43  1.04  2.90 0.930}
\CommentTok{#> 115  2   11.0  1.51  2.20  21.5   85 2.46  2.17 0.52  2.01  1.90 1.710}
\CommentTok{#> 116  2   11.8  1.47  1.99  20.8   86 1.98  1.60 0.30  1.53  1.95 0.950}
\CommentTok{#> 117  2   12.4  1.61  2.19  22.5  108 2.00  2.09 0.34  1.61  2.06 1.060}
\CommentTok{#> 118  2   12.8  3.43  1.98  16.0   80 1.63  1.25 0.43  0.83  3.40 0.700}
\CommentTok{#> 119  2   12.0  3.43  2.00  19.0   87 2.00  1.64 0.37  1.87  1.28 0.930}
\CommentTok{#> 120  2   11.4  2.40  2.42  20.0   96 2.90  2.79 0.32  1.83  3.25 0.800}
\CommentTok{#> 121  2   11.6  2.05  3.23  28.5  119 3.18  5.08 0.47  1.87  6.00 0.930}
\CommentTok{#> 122  2   12.4  4.43  2.73  26.5  102 2.20  2.13 0.43  1.71  2.08 0.920}
\CommentTok{#> 123  2   13.1  5.80  2.13  21.5   86 2.62  2.65 0.30  2.01  2.60 0.730}
\CommentTok{#> 124  2   11.9  4.31  2.39  21.0   82 2.86  3.03 0.21  2.91  2.80 0.750}
\CommentTok{#> 125  2   12.1  2.16  2.17  21.0   85 2.60  2.65 0.37  1.35  2.76 0.860}
\CommentTok{#> 126  2   12.4  1.53  2.29  21.5   86 2.74  3.15 0.39  1.77  3.94 0.690}
\CommentTok{#> 127  2   11.8  2.13  2.78  28.5   92 2.13  2.24 0.58  1.76  3.00 0.970}
\CommentTok{#> 128  2   12.4  1.63  2.30  24.5   88 2.22  2.45 0.40  1.90  2.12 0.890}
\CommentTok{#> 129  2   12.0  4.30  2.38  22.0   80 2.10  1.75 0.42  1.35  2.60 0.790}
\CommentTok{#> 130  3   12.9  1.35  2.32  18.0  122 1.51  1.25 0.21  0.94  4.10 0.760}
\CommentTok{#> 131  3   12.9  2.99  2.40  20.0  104 1.30  1.22 0.24  0.83  5.40 0.740}
\CommentTok{#> 132  3   12.8  2.31  2.40  24.0   98 1.15  1.09 0.27  0.83  5.70 0.660}
\CommentTok{#> 133  3   12.7  3.55  2.36  21.5  106 1.70  1.20 0.17  0.84  5.00 0.780}
\CommentTok{#> 134  3   12.5  1.24  2.25  17.5   85 2.00  0.58 0.60  1.25  5.45 0.750}
\CommentTok{#> 135  3   12.6  2.46  2.20  18.5   94 1.62  0.66 0.63  0.94  7.10 0.730}
\CommentTok{#> 136  3   12.2  4.72  2.54  21.0   89 1.38  0.47 0.53  0.80  3.85 0.750}
\CommentTok{#> 137  3   12.5  5.51  2.64  25.0   96 1.79  0.60 0.63  1.10  5.00 0.820}
\CommentTok{#> 138  3   13.5  3.59  2.19  19.5   88 1.62  0.48 0.58  0.88  5.70 0.810}
\CommentTok{#> 139  3   12.8  2.96  2.61  24.0  101 2.32  0.60 0.53  0.81  4.92 0.890}
\CommentTok{#> 140  3   12.9  2.81  2.70  21.0   96 1.54  0.50 0.53  0.75  4.60 0.770}
\CommentTok{#> 141  3   13.4  2.56  2.35  20.0   89 1.40  0.50 0.37  0.64  5.60 0.700}
\CommentTok{#> 142  3   13.5  3.17  2.72  23.5   97 1.55  0.52 0.50  0.55  4.35 0.890}
\CommentTok{#> 143  3   13.6  4.95  2.35  20.0   92 2.00  0.80 0.47  1.02  4.40 0.910}
\CommentTok{#> 144  3   12.2  3.88  2.20  18.5  112 1.38  0.78 0.29  1.14  8.21 0.650}
\CommentTok{#> 145  3   13.2  3.57  2.15  21.0  102 1.50  0.55 0.43  1.30  4.00 0.600}
\CommentTok{#> 146  3   13.9  5.04  2.23  20.0   80 0.98  0.34 0.40  0.68  4.90 0.580}
\CommentTok{#> 147  3   12.9  4.61  2.48  21.5   86 1.70  0.65 0.47  0.86  7.65 0.540}
\CommentTok{#> 148  3   13.3  3.24  2.38  21.5   92 1.93  0.76 0.45  1.25  8.42 0.550}
\CommentTok{#> 149  3   13.1  3.90  2.36  21.5  113 1.41  1.39 0.34  1.14  9.40 0.570}
\CommentTok{#> 150  3   13.5  3.12  2.62  24.0  123 1.40  1.57 0.22  1.25  8.60 0.590}
\CommentTok{#> 151  3   12.8  2.67  2.48  22.0  112 1.48  1.36 0.24  1.26 10.80 0.480}
\CommentTok{#> 152  3   13.1  1.90  2.75  25.5  116 2.20  1.28 0.26  1.56  7.10 0.610}
\CommentTok{#> 153  3   13.2  3.30  2.28  18.5   98 1.80  0.83 0.61  1.87 10.52 0.560}
\CommentTok{#> 154  3   12.6  1.29  2.10  20.0  103 1.48  0.58 0.53  1.40  7.60 0.580}
\CommentTok{#> 155  3   13.2  5.19  2.32  22.0   93 1.74  0.63 0.61  1.55  7.90 0.600}
\CommentTok{#> 156  3   13.8  4.12  2.38  19.5   89 1.80  0.83 0.48  1.56  9.01 0.570}
\CommentTok{#> 157  3   12.4  3.03  2.64  27.0   97 1.90  0.58 0.63  1.14  7.50 0.670}
\CommentTok{#> 158  3   14.3  1.68  2.70  25.0   98 2.80  1.31 0.53  2.70 13.00 0.570}
\CommentTok{#> 159  3   13.5  1.67  2.64  22.5   89 2.60  1.10 0.52  2.29 11.75 0.570}
\CommentTok{#> 160  3   12.4  3.83  2.38  21.0   88 2.30  0.92 0.50  1.04  7.65 0.560}
\CommentTok{#> 161  3   13.7  3.26  2.54  20.0  107 1.83  0.56 0.50  0.80  5.88 0.960}
\CommentTok{#> 162  3   12.8  3.27  2.58  22.0  106 1.65  0.60 0.60  0.96  5.58 0.870}
\CommentTok{#> 163  3   13.0  3.45  2.35  18.5  106 1.39  0.70 0.40  0.94  5.28 0.680}
\CommentTok{#> 164  3   13.8  2.76  2.30  22.0   90 1.35  0.68 0.41  1.03  9.58 0.700}
\CommentTok{#> 165  3   13.7  4.36  2.26  22.5   88 1.28  0.47 0.52  1.15  6.62 0.780}
\CommentTok{#> 166  3   13.4  3.70  2.60  23.0  111 1.70  0.92 0.43  1.46 10.68 0.850}
\CommentTok{#> 167  3   12.8  3.37  2.30  19.5   88 1.48  0.66 0.40  0.97 10.26 0.720}
\CommentTok{#> 168  3   13.6  2.58  2.69  24.5  105 1.55  0.84 0.39  1.54  8.66 0.740}
\CommentTok{#> 169  3   13.4  4.60  2.86  25.0  112 1.98  0.96 0.27  1.11  8.50 0.670}
\CommentTok{#> 170  3   12.2  3.03  2.32  19.0   96 1.25  0.49 0.40  0.73  5.50 0.660}
\CommentTok{#> 171  3   12.8  2.39  2.28  19.5   86 1.39  0.51 0.48  0.64  9.90 0.570}
\CommentTok{#> 172  3   14.2  2.51  2.48  20.0   91 1.68  0.70 0.44  1.24  9.70 0.620}
\CommentTok{#> 173  3   13.7  5.65  2.45  20.5   95 1.68  0.61 0.52  1.06  7.70 0.640}
\CommentTok{#> 174  3   13.4  3.91  2.48  23.0  102 1.80  0.75 0.43  1.41  7.30 0.700}
\CommentTok{#> 175  3   13.3  4.28  2.26  20.0  120 1.59  0.69 0.43  1.35 10.20 0.590}
\CommentTok{#> 176  3   13.2  2.59  2.37  20.0  120 1.65  0.68 0.53  1.46  9.30 0.600}
\CommentTok{#> 177  3   14.1  4.10  2.74  24.5   96 2.05  0.76 0.56  1.35  9.20 0.610}
\CommentTok{#>     X3.92 X1065}
\CommentTok{#> 1    3.40  1050}
\CommentTok{#> 2    3.17  1185}
\CommentTok{#> 3    3.45  1480}
\CommentTok{#> 4    2.93   735}
\CommentTok{#> 5    2.85  1450}
\CommentTok{#> 6    3.58  1290}
\CommentTok{#> 7    3.58  1295}
\CommentTok{#> 8    2.85  1045}
\CommentTok{#> 9    3.55  1045}
\CommentTok{#> 10   3.17  1510}
\CommentTok{#> 11   2.82  1280}
\CommentTok{#> 12   2.90  1320}
\CommentTok{#> 13   2.73  1150}
\CommentTok{#> 14   3.00  1547}
\CommentTok{#> 15   2.88  1310}
\CommentTok{#> 16   2.65  1280}
\CommentTok{#> 17   2.57  1130}
\CommentTok{#> 18   2.82  1680}
\CommentTok{#> 19   3.36   845}
\CommentTok{#> 20   3.71   780}
\CommentTok{#> 21   3.52   770}
\CommentTok{#> 22   4.00  1035}
\CommentTok{#> 23   3.63  1015}
\CommentTok{#> 24   3.82   845}
\CommentTok{#> 25   3.20   830}
\CommentTok{#> 26   3.22  1195}
\CommentTok{#> 27   2.77  1285}
\CommentTok{#> 28   3.40   915}
\CommentTok{#> 29   3.59  1035}
\CommentTok{#> 30   2.71  1285}
\CommentTok{#> 31   2.88  1515}
\CommentTok{#> 32   2.87   990}
\CommentTok{#> 33   3.00  1235}
\CommentTok{#> 34   2.87  1095}
\CommentTok{#> 35   3.47   920}
\CommentTok{#> 36   2.78   880}
\CommentTok{#> 37   2.51  1105}
\CommentTok{#> 38   2.69  1020}
\CommentTok{#> 39   3.53   760}
\CommentTok{#> 40   3.38   795}
\CommentTok{#> 41   3.00  1035}
\CommentTok{#> 42   3.56  1095}
\CommentTok{#> 43   3.00   680}
\CommentTok{#> 44   3.35   885}
\CommentTok{#> 45   3.33  1080}
\CommentTok{#> 46   3.44  1065}
\CommentTok{#> 47   3.33   985}
\CommentTok{#> 48   2.75  1060}
\CommentTok{#> 49   3.10  1260}
\CommentTok{#> 50   2.91  1150}
\CommentTok{#> 51   3.37  1265}
\CommentTok{#> 52   3.26  1190}
\CommentTok{#> 53   2.93  1375}
\CommentTok{#> 54   3.20  1060}
\CommentTok{#> 55   3.03  1120}
\CommentTok{#> 56   3.31   970}
\CommentTok{#> 57   2.84  1270}
\CommentTok{#> 58   2.87  1285}
\CommentTok{#> 59   1.82   520}
\CommentTok{#> 60   1.67   680}
\CommentTok{#> 61   1.59   450}
\CommentTok{#> 62   2.46   630}
\CommentTok{#> 63   2.87   420}
\CommentTok{#> 64   2.23   355}
\CommentTok{#> 65   2.30   678}
\CommentTok{#> 66   3.18   502}
\CommentTok{#> 67   3.48   510}
\CommentTok{#> 68   1.93   750}
\CommentTok{#> 69   3.07   718}
\CommentTok{#> 70   1.82   870}
\CommentTok{#> 71   3.16   410}
\CommentTok{#> 72   2.78   472}
\CommentTok{#> 73   3.50   985}
\CommentTok{#> 74   3.13   886}
\CommentTok{#> 75   2.14   428}
\CommentTok{#> 76   2.48   392}
\CommentTok{#> 77   2.52   500}
\CommentTok{#> 78   2.31   750}
\CommentTok{#> 79   3.13   463}
\CommentTok{#> 80   3.12   278}
\CommentTok{#> 81   3.14   714}
\CommentTok{#> 82   2.72   630}
\CommentTok{#> 83   2.01   515}
\CommentTok{#> 84   3.08   520}
\CommentTok{#> 85   3.16   450}
\CommentTok{#> 86   2.26   495}
\CommentTok{#> 87   3.21   562}
\CommentTok{#> 88   2.75   680}
\CommentTok{#> 89   3.21   625}
\CommentTok{#> 90   2.27   480}
\CommentTok{#> 91   2.65   450}
\CommentTok{#> 92   2.06   495}
\CommentTok{#> 93   3.30   290}
\CommentTok{#> 94   2.96   345}
\CommentTok{#> 95   2.63   937}
\CommentTok{#> 96   2.26   625}
\CommentTok{#> 97   2.74   428}
\CommentTok{#> 98   2.77   660}
\CommentTok{#> 99   2.83   406}
\CommentTok{#> 100  2.96   710}
\CommentTok{#> 101  2.77   562}
\CommentTok{#> 102  3.38   438}
\CommentTok{#> 103  2.44   415}
\CommentTok{#> 104  3.57   672}
\CommentTok{#> 105  3.30   315}
\CommentTok{#> 106  3.17   510}
\CommentTok{#> 107  2.42   488}
\CommentTok{#> 108  3.02   312}
\CommentTok{#> 109  3.26   680}
\CommentTok{#> 110  2.81   562}
\CommentTok{#> 111  2.78   325}
\CommentTok{#> 112  2.50   607}
\CommentTok{#> 113  2.31   434}
\CommentTok{#> 114  3.19   385}
\CommentTok{#> 115  2.87   407}
\CommentTok{#> 116  3.33   495}
\CommentTok{#> 117  2.96   345}
\CommentTok{#> 118  2.12   372}
\CommentTok{#> 119  3.05   564}
\CommentTok{#> 120  3.39   625}
\CommentTok{#> 121  3.69   465}
\CommentTok{#> 122  3.12   365}
\CommentTok{#> 123  3.10   380}
\CommentTok{#> 124  3.64   380}
\CommentTok{#> 125  3.28   378}
\CommentTok{#> 126  2.84   352}
\CommentTok{#> 127  2.44   466}
\CommentTok{#> 128  2.78   342}
\CommentTok{#> 129  2.57   580}
\CommentTok{#> 130  1.29   630}
\CommentTok{#> 131  1.42   530}
\CommentTok{#> 132  1.36   560}
\CommentTok{#> 133  1.29   600}
\CommentTok{#> 134  1.51   650}
\CommentTok{#> 135  1.58   695}
\CommentTok{#> 136  1.27   720}
\CommentTok{#> 137  1.69   515}
\CommentTok{#> 138  1.82   580}
\CommentTok{#> 139  2.15   590}
\CommentTok{#> 140  2.31   600}
\CommentTok{#> 141  2.47   780}
\CommentTok{#> 142  2.06   520}
\CommentTok{#> 143  2.05   550}
\CommentTok{#> 144  2.00   855}
\CommentTok{#> 145  1.68   830}
\CommentTok{#> 146  1.33   415}
\CommentTok{#> 147  1.86   625}
\CommentTok{#> 148  1.62   650}
\CommentTok{#> 149  1.33   550}
\CommentTok{#> 150  1.30   500}
\CommentTok{#> 151  1.47   480}
\CommentTok{#> 152  1.33   425}
\CommentTok{#> 153  1.51   675}
\CommentTok{#> 154  1.55   640}
\CommentTok{#> 155  1.48   725}
\CommentTok{#> 156  1.64   480}
\CommentTok{#> 157  1.73   880}
\CommentTok{#> 158  1.96   660}
\CommentTok{#> 159  1.78   620}
\CommentTok{#> 160  1.58   520}
\CommentTok{#> 161  1.82   680}
\CommentTok{#> 162  2.11   570}
\CommentTok{#> 163  1.75   675}
\CommentTok{#> 164  1.68   615}
\CommentTok{#> 165  1.75   520}
\CommentTok{#> 166  1.56   695}
\CommentTok{#> 167  1.75   685}
\CommentTok{#> 168  1.80   750}
\CommentTok{#> 169  1.92   630}
\CommentTok{#> 170  1.83   510}
\CommentTok{#> 171  1.63   470}
\CommentTok{#> 172  1.71   660}
\CommentTok{#> 173  1.74   740}
\CommentTok{#> 174  1.56   750}
\CommentTok{#> 175  1.56   835}
\CommentTok{#> 176  1.62   840}
\CommentTok{#> 177  1.60   560}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{names}\NormalTok{(wines) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"label"}\NormalTok{,}
                  \StringTok{"Alcohol"}\NormalTok{,}
                  \StringTok{"Malic_acid"}\NormalTok{,}
                  \StringTok{"Ash"}\NormalTok{,}
                  \StringTok{"Alcalinity_of_ash"}\NormalTok{,}
                  \StringTok{"Magnesium"}\NormalTok{,}
                  \StringTok{"Total_phenols"}\NormalTok{,}
                  \StringTok{"Flavanoids"}\NormalTok{,}
                  \StringTok{"Nonflavanoid_phenols"}\NormalTok{,}
                  \StringTok{"Proanthocyanins"}\NormalTok{,}
                  \StringTok{"Color_intensity"}\NormalTok{,}
                  \StringTok{"Hue"}\NormalTok{,}
                  \StringTok{"OD280_OD315_of_diluted_wines"}\NormalTok{,}
                  \StringTok{"Proline"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(wines)}
\CommentTok{#>   label Alcohol Malic_acid  Ash Alcalinity_of_ash Magnesium Total_phenols}
\CommentTok{#> 1     1    13.2       1.78 2.14              11.2       100          2.65}
\CommentTok{#> 2     1    13.2       2.36 2.67              18.6       101          2.80}
\CommentTok{#> 3     1    14.4       1.95 2.50              16.8       113          3.85}
\CommentTok{#> 4     1    13.2       2.59 2.87              21.0       118          2.80}
\CommentTok{#> 5     1    14.2       1.76 2.45              15.2       112          3.27}
\CommentTok{#> 6     1    14.4       1.87 2.45              14.6        96          2.50}
\CommentTok{#>   Flavanoids Nonflavanoid_phenols Proanthocyanins Color_intensity  Hue}
\CommentTok{#> 1       2.76                 0.26            1.28            4.38 1.05}
\CommentTok{#> 2       3.24                 0.30            2.81            5.68 1.03}
\CommentTok{#> 3       3.49                 0.24            2.18            7.80 0.86}
\CommentTok{#> 4       2.69                 0.39            1.82            4.32 1.04}
\CommentTok{#> 5       3.39                 0.34            1.97            6.75 1.05}
\CommentTok{#> 6       2.52                 0.30            1.98            5.25 1.02}
\CommentTok{#>   OD280_OD315_of_diluted_wines Proline}
\CommentTok{#> 1                         3.40    1050}
\CommentTok{#> 2                         3.17    1185}
\CommentTok{#> 3                         3.45    1480}
\CommentTok{#> 4                         2.93     735}
\CommentTok{#> 5                         2.85    1450}
\CommentTok{#> 6                         3.58    1290}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plt1 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(wines, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Alcohol, }\DataTypeTok{y =}\NormalTok{ Magnesium, }\DataTypeTok{colour =} \KeywordTok{as.factor}\NormalTok{(label))) }\OperatorTok{+}
\StringTok{    }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{size=}\DecValTok{3}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"Wines"}\NormalTok{)}

\NormalTok{plt1}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_904-wine_selection_nn_files/figure-latex/unnamed-chunk-7-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plt2 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(wines, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Alcohol, }\DataTypeTok{y =}\NormalTok{ Proline, }\DataTypeTok{colour =} \KeywordTok{as.factor}\NormalTok{(label))) }\OperatorTok{+}
\StringTok{    }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{size=}\DecValTok{3}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"Wines"}\NormalTok{)}
\NormalTok{plt2}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_904-wine_selection_nn_files/figure-latex/unnamed-chunk-8-1} \end{center}

\hypertarget{preprocessing}{%
\section{Preprocessing}\label{preprocessing}}

During the preprocessing phase, I have to do at least the following two things:

Encode the categorical variables.
Standardize the predictors.
First of all, let's encode our target variable. The encoding of the categorical variables is needed when using neuralnet since it does not like factors at all. It will shout at you if you try to feed in a factor (I am told nnet likes factors though).

In the wine dataset the variable label contains three different labels: 1,2 and 3.

The usual practice, as far as I know, is to encode categorical variables as a ``one hot'' vector. For instance, if I had three classes, like in this case, I'd need to replace the label variable with three variables like these:

\begin{verbatim}
#   l1,l2,l3
#   1,0,0
#   0,0,1
#   ...
\end{verbatim}

In this case the first observation would be labelled as a 1, the second would be labelled as a 2, and so on. Ironically, the \texttt{nnet} package provides a function to perform this encoding in a painless way:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Encode as a one hot vector multilabel data}
\NormalTok{train <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(wines[, }\DecValTok{2}\OperatorTok{:}\DecValTok{14}\NormalTok{], }\KeywordTok{class.ind}\NormalTok{(}\KeywordTok{as.factor}\NormalTok{(wines}\OperatorTok{$}\NormalTok{label)))}

\CommentTok{# Set labels name}
\KeywordTok{names}\NormalTok{(train) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\KeywordTok{names}\NormalTok{(wines)[}\DecValTok{2}\OperatorTok{:}\DecValTok{14}\NormalTok{],}\StringTok{"l1"}\NormalTok{,}\StringTok{"l2"}\NormalTok{,}\StringTok{"l3"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

By the way, since the predictors are all continuous, you do not need to encode any of them, however, in case you needed to, you could apply the same strategy applied above to all the categorical predictors. Unless of course you'd like to try some other kind of custom encoding.

Now let's standardize the predictors in the {[}01{]}"\textgreater{}{[}01{]} interval by leveraging the lapply function:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Scale data}
\NormalTok{scl <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x) \{ (x }\OperatorTok{-}\StringTok{ }\KeywordTok{min}\NormalTok{(x))}\OperatorTok{/}\NormalTok{(}\KeywordTok{max}\NormalTok{(x) }\OperatorTok{-}\StringTok{ }\KeywordTok{min}\NormalTok{(x)) \}}
\NormalTok{train[, }\DecValTok{1}\OperatorTok{:}\DecValTok{13}\NormalTok{] <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\KeywordTok{lapply}\NormalTok{(train[, }\DecValTok{1}\OperatorTok{:}\DecValTok{13}\NormalTok{], scl))}
\KeywordTok{head}\NormalTok{(train)}
\CommentTok{#>   Alcohol Malic_acid   Ash Alcalinity_of_ash Magnesium Total_phenols}
\CommentTok{#> 1   0.571      0.206 0.417            0.0309     0.326         0.576}
\CommentTok{#> 2   0.561      0.320 0.701            0.4124     0.337         0.628}
\CommentTok{#> 3   0.879      0.239 0.610            0.3196     0.467         0.990}
\CommentTok{#> 4   0.582      0.366 0.807            0.5361     0.522         0.628}
\CommentTok{#> 5   0.834      0.202 0.583            0.2371     0.457         0.790}
\CommentTok{#> 6   0.884      0.223 0.583            0.2062     0.283         0.524}
\CommentTok{#>   Flavanoids Nonflavanoid_phenols Proanthocyanins Color_intensity   Hue}
\CommentTok{#> 1      0.511                0.245           0.274           0.265 0.463}
\CommentTok{#> 2      0.612                0.321           0.757           0.375 0.447}
\CommentTok{#> 3      0.665                0.208           0.558           0.556 0.309}
\CommentTok{#> 4      0.496                0.491           0.445           0.259 0.455}
\CommentTok{#> 5      0.643                0.396           0.492           0.467 0.463}
\CommentTok{#> 6      0.460                0.321           0.495           0.339 0.439}
\CommentTok{#>   OD280_OD315_of_diluted_wines Proline l1 l2 l3}
\CommentTok{#> 1                        0.780   0.551  1  0  0}
\CommentTok{#> 2                        0.696   0.647  1  0  0}
\CommentTok{#> 3                        0.799   0.857  1  0  0}
\CommentTok{#> 4                        0.608   0.326  1  0  0}
\CommentTok{#> 5                        0.579   0.836  1  0  0}
\CommentTok{#> 6                        0.846   0.722  1  0  0}
\end{Highlighting}
\end{Shaded}

\hypertarget{fitting-the-model-with-neuralnet}{%
\section{Fitting the model with neuralnet}\label{fitting-the-model-with-neuralnet}}

Now it is finally time to fit the model.

As you might remember from the old post I wrote, \texttt{neuralnet} does not like the formula y\textasciitilde{}.. Fear not, you can build the formula to be used in a simple step:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Set up formula}
\NormalTok{n <-}\StringTok{ }\KeywordTok{names}\NormalTok{(train)}
\NormalTok{f <-}\StringTok{ }\KeywordTok{as.formula}\NormalTok{(}\KeywordTok{paste}\NormalTok{(}\StringTok{"l1 + l2 + l3 ~"}\NormalTok{, }\KeywordTok{paste}\NormalTok{(n[}\OperatorTok{!}\NormalTok{n }\OperatorTok{%in%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"l1"}\NormalTok{,}\StringTok{"l2"}\NormalTok{,}\StringTok{"l3"}\NormalTok{)], }\DataTypeTok{collapse =} \StringTok{" + "}\NormalTok{)))}
\NormalTok{f}
\CommentTok{#> l1 + l2 + l3 ~ Alcohol + Malic_acid + Ash + Alcalinity_of_ash + }
\CommentTok{#>     Magnesium + Total_phenols + Flavanoids + Nonflavanoid_phenols + }
\CommentTok{#>     Proanthocyanins + Color_intensity + Hue + OD280_OD315_of_diluted_wines + }
\CommentTok{#>     Proline}
\end{Highlighting}
\end{Shaded}

Note that the characters in the vector are not pasted to the right of the ``\textasciitilde{}'' symbol.

Just remember to check that the formula is indeed correct and then you are good to go.

Let's train the neural network with the full dataset. It should take very little time to converge. If you did not standardize the predictors it could take a lot more though.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nn <-}\StringTok{ }\KeywordTok{neuralnet}\NormalTok{(f,}
                \DataTypeTok{data =}\NormalTok{ train,}
                \DataTypeTok{hidden =} \KeywordTok{c}\NormalTok{(}\DecValTok{13}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{3}\NormalTok{),}
                \DataTypeTok{act.fct =} \StringTok{"logistic"}\NormalTok{,}
                \DataTypeTok{linear.output =} \OtherTok{FALSE}\NormalTok{,}
                \DataTypeTok{lifesign =} \StringTok{"minimal"}\NormalTok{)}
\CommentTok{#> hidden: 13, 10, 3    thresh: 0.01    rep: 1/1    steps:      88  error: 0.03039  time: 0.06 secs}
\end{Highlighting}
\end{Shaded}

Note that I set the argument linear.output to FALSE in order to tell the model that I want to apply the activation function act.fct and that I am not doing a regression task. Then I set the activation function to logistic (which by the way is the default option) in order to apply the logistic function. The other available option is tanh but the model seems to perform a little worse with it so I opted for the default option. As far as I know these two are the only two available options, there is no ``relu'' function available although it seems to be a common activation function in other packages.

As far as the number of hidden neurons, I tried some combination and the one used seems to perform slightly better than the others (around 1\% of accuracy difference in cross validation score).

By using the in-built plot method you can get a visual take on what is actually happening inside the model, however the plot is not that helpful I think

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(nn)}
\end{Highlighting}
\end{Shaded}

Let's have a look at the accuracy on the training set:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Compute predictions}
\NormalTok{pr.nn <-}\StringTok{ }\KeywordTok{compute}\NormalTok{(nn, train[, }\DecValTok{1}\OperatorTok{:}\DecValTok{13}\NormalTok{])}

\CommentTok{# Extract results}
\NormalTok{pr.nn_ <-}\StringTok{ }\NormalTok{pr.nn}\OperatorTok{$}\NormalTok{net.result}
\KeywordTok{head}\NormalTok{(pr.nn_)}
\CommentTok{#>       [,1]    [,2]     [,3]}
\CommentTok{#> [1,] 0.990 0.00317 6.99e-06}
\CommentTok{#> [2,] 0.991 0.00233 8.69e-06}
\CommentTok{#> [3,] 0.991 0.00210 8.65e-06}
\CommentTok{#> [4,] 0.986 0.00442 8.74e-06}
\CommentTok{#> [5,] 0.992 0.00212 8.32e-06}
\CommentTok{#> [6,] 0.992 0.00214 8.34e-06}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Accuracy (training set)}
\NormalTok{original_values <-}\StringTok{ }\KeywordTok{max.col}\NormalTok{(train[, }\DecValTok{14}\OperatorTok{:}\DecValTok{16}\NormalTok{])}
\NormalTok{pr.nn_}\DecValTok{2}\NormalTok{ <-}\StringTok{ }\KeywordTok{max.col}\NormalTok{(pr.nn_)}
\KeywordTok{mean}\NormalTok{(pr.nn_}\DecValTok{2} \OperatorTok{==}\StringTok{ }\NormalTok{original_values)}
\CommentTok{#> [1] 1}
\end{Highlighting}
\end{Shaded}

100\% not bad! But wait, this may be because our model over fitted the data, furthermore evaluating accuracy on the training set is kind of cheating since the model already ``knows'' (or should know) the answers. In order to assess the ``true accuracy'' of the model you need to perform some kind of cross validation.

\hypertarget{cross-validating-the-classifier}{%
\section{Cross validating the classifier}\label{cross-validating-the-classifier}}

Let's crossvalidate the model using the evergreen 10 fold cross validation with the following train and test split: 95\% of the dataset will be used as training set while the remaining 5\% as test set.

Just out of curiosity I decided to run a LOOCV round too. In case you'd like to run this cross validation technique, just set the proportion variable to 0.995: this will select just one observation for as test set and leave all the other observations as training set. Running LOOCV you should get similar results to the 10 fold cross validation.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Set seed for reproducibility purposes}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{500}\NormalTok{)}
\CommentTok{# 10 fold cross validation}
\NormalTok{k <-}\StringTok{ }\DecValTok{10}
\CommentTok{# Results from cv}
\NormalTok{outs <-}\StringTok{ }\OtherTok{NULL}
\CommentTok{# Train test split proportions}
\NormalTok{proportion <-}\StringTok{ }\FloatTok{0.95} \CommentTok{# Set to 0.995 for LOOCV}

\CommentTok{# Crossvalidate, go!}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{k)}
\NormalTok{\{}
\NormalTok{    index <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\KeywordTok{nrow}\NormalTok{(train), }\KeywordTok{round}\NormalTok{(proportion}\OperatorTok{*}\KeywordTok{nrow}\NormalTok{(train)))}
\NormalTok{    train_cv <-}\StringTok{ }\NormalTok{train[index, ]}
\NormalTok{    test_cv <-}\StringTok{ }\NormalTok{train[}\OperatorTok{-}\NormalTok{index, ]}
\NormalTok{    nn_cv <-}\StringTok{ }\KeywordTok{neuralnet}\NormalTok{(f,}
                        \DataTypeTok{data =}\NormalTok{ train_cv,}
                        \DataTypeTok{hidden =} \KeywordTok{c}\NormalTok{(}\DecValTok{13}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{3}\NormalTok{),}
                        \DataTypeTok{act.fct =} \StringTok{"logistic"}\NormalTok{,}
                        \DataTypeTok{linear.output =} \OtherTok{FALSE}\NormalTok{)}
    
    \CommentTok{# Compute predictions}
\NormalTok{    pr.nn <-}\StringTok{ }\KeywordTok{compute}\NormalTok{(nn_cv, test_cv[, }\DecValTok{1}\OperatorTok{:}\DecValTok{13}\NormalTok{])}
    \CommentTok{# Extract results}
\NormalTok{    pr.nn_ <-}\StringTok{ }\NormalTok{pr.nn}\OperatorTok{$}\NormalTok{net.result}
    \CommentTok{# Accuracy (test set)}
\NormalTok{    original_values <-}\StringTok{ }\KeywordTok{max.col}\NormalTok{(test_cv[, }\DecValTok{14}\OperatorTok{:}\DecValTok{16}\NormalTok{])}
\NormalTok{    pr.nn_}\DecValTok{2}\NormalTok{ <-}\StringTok{ }\KeywordTok{max.col}\NormalTok{(pr.nn_)}
\NormalTok{    outs[i] <-}\StringTok{ }\KeywordTok{mean}\NormalTok{(pr.nn_}\DecValTok{2} \OperatorTok{==}\StringTok{ }\NormalTok{original_values)}
\NormalTok{\}}

\KeywordTok{mean}\NormalTok{(outs)}
\CommentTok{#> [1] 0.978}
\end{Highlighting}
\end{Shaded}

98.8\%, awesome! Next time when you are invited to a relaxing evening that includes a wine tasting competition I think you should definitely bring your laptop as a contestant!

Aside from that poor taste joke, (I made it again!), indeed this dataset is not the most challenging, I think with some more tweaking a better cross validation score could be achieved. Nevertheless I hope you found this tutorial useful. A gist with the entire code for this tutorial can be found here.

Thank you for reading this article, please feel free to leave a comment if you have any questions or suggestions and share the post with others if you find it useful.

Notes:

\hypertarget{build-a-fully-connected-neural-network-from-scratch}{%
\chapter{Build a fully connected neural network from scratch}\label{build-a-fully-connected-neural-network-from-scratch}}

\hypertarget{introduction-2}{%
\section{Introduction}\label{introduction-2}}

\url{http://www.parallelr.com/r-deep-neural-network-from-scratch/}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(neuralnet)}

\CommentTok{# Copyright 2016: www.ParallelR.com}
\CommentTok{# Parallel Blog : R For Deep Learning (I): Build Fully Connected Neural Network From Scratch}
\CommentTok{# Classification by 2-layers DNN and tested by iris dataset}
\CommentTok{# Author: Peng Zhao, patric.zhao@gmail.com}

\CommentTok{# Prediction}
\NormalTok{predict.dnn <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(model, }\DataTypeTok{data =}\NormalTok{ X.test) \{}
  \CommentTok{# new data, transfer to matrix}
\NormalTok{  new.data <-}\StringTok{ }\KeywordTok{data.matrix}\NormalTok{(data)}
  
  \CommentTok{# Feed Forwad}
\NormalTok{  hidden.layer <-}\StringTok{ }\KeywordTok{sweep}\NormalTok{(new.data }\OperatorTok{%*%}\StringTok{ }\NormalTok{model}\OperatorTok{$}\NormalTok{W1 ,}\DecValTok{2}\NormalTok{, model}\OperatorTok{$}\NormalTok{b1, }\StringTok{'+'}\NormalTok{)}
  \CommentTok{# neurons : Rectified Linear}
\NormalTok{  hidden.layer <-}\StringTok{ }\KeywordTok{pmax}\NormalTok{(hidden.layer, }\DecValTok{0}\NormalTok{)}
\NormalTok{  score <-}\StringTok{ }\KeywordTok{sweep}\NormalTok{(hidden.layer }\OperatorTok{%*%}\StringTok{ }\NormalTok{model}\OperatorTok{$}\NormalTok{W2, }\DecValTok{2}\NormalTok{, model}\OperatorTok{$}\NormalTok{b2, }\StringTok{'+'}\NormalTok{)}
  
  \CommentTok{# Loss Function: softmax}
\NormalTok{  score.exp <-}\StringTok{ }\KeywordTok{exp}\NormalTok{(score)}
\NormalTok{  probs <-}\KeywordTok{sweep}\NormalTok{(score.exp, }\DecValTok{1}\NormalTok{, }\KeywordTok{rowSums}\NormalTok{(score.exp), }\StringTok{'/'}\NormalTok{) }
  
  \CommentTok{# select max possiblity}
\NormalTok{  labels.predicted <-}\StringTok{ }\KeywordTok{max.col}\NormalTok{(probs)}
  \KeywordTok{return}\NormalTok{(labels.predicted)}
\NormalTok{\}}

\CommentTok{# Train: build and train a 2-layers neural network }
\NormalTok{train.dnn <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x, y, }\DataTypeTok{traindata=}\NormalTok{data, }\DataTypeTok{testdata=}\OtherTok{NULL}\NormalTok{,}
                  \DataTypeTok{model =} \OtherTok{NULL}\NormalTok{,}
                  \CommentTok{# set hidden layers and neurons}
                  \CommentTok{# currently, only support 1 hidden layer}
                  \DataTypeTok{hidden=}\KeywordTok{c}\NormalTok{(}\DecValTok{6}\NormalTok{), }
                  \CommentTok{# max iteration steps}
                  \DataTypeTok{maxit=}\DecValTok{2000}\NormalTok{,}
                  \CommentTok{# delta loss }
                  \DataTypeTok{abstol=}\FloatTok{1e-2}\NormalTok{,}
                  \CommentTok{# learning rate}
                  \DataTypeTok{lr =} \FloatTok{1e-2}\NormalTok{,}
                  \CommentTok{# regularization rate}
                  \DataTypeTok{reg =} \FloatTok{1e-3}\NormalTok{,}
                  \CommentTok{# show results every 'display' step}
                  \DataTypeTok{display =} \DecValTok{100}\NormalTok{,}
                  \DataTypeTok{random.seed =} \DecValTok{1}\NormalTok{)}
\NormalTok{\{}
  \CommentTok{# to make the case reproducible.}
  \KeywordTok{set.seed}\NormalTok{(random.seed)}
  
  \CommentTok{# total number of training set}
\NormalTok{  N <-}\StringTok{ }\KeywordTok{nrow}\NormalTok{(traindata)}
  
  \CommentTok{# extract the data and label}
  \CommentTok{# don't need atribute }
\NormalTok{  X <-}\StringTok{ }\KeywordTok{unname}\NormalTok{(}\KeywordTok{data.matrix}\NormalTok{(traindata[,x]))}
  \CommentTok{# correct categories represented by integer }
\NormalTok{  Y <-}\StringTok{ }\NormalTok{traindata[,y]}
  \ControlFlowTok{if}\NormalTok{(}\KeywordTok{is.factor}\NormalTok{(Y)) \{ Y <-}\StringTok{ }\KeywordTok{as.integer}\NormalTok{(Y) \}}
  \CommentTok{# create index for both row and col}
  \CommentTok{# create index for both row and col}
\NormalTok{  Y.len   <-}\StringTok{ }\KeywordTok{length}\NormalTok{(}\KeywordTok{unique}\NormalTok{(Y))}
\NormalTok{  Y.set   <-}\StringTok{ }\KeywordTok{sort}\NormalTok{(}\KeywordTok{unique}\NormalTok{(Y))}
\NormalTok{  Y.index <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\NormalTok{N, }\KeywordTok{match}\NormalTok{(Y, Y.set))}

  \CommentTok{# create model or get model from parameter}
  \ControlFlowTok{if}\NormalTok{(}\KeywordTok{is.null}\NormalTok{(model)) \{}
       \CommentTok{# number of input features}
\NormalTok{       D <-}\StringTok{ }\KeywordTok{ncol}\NormalTok{(X)}
       \CommentTok{# number of categories for classification}
\NormalTok{       K <-}\StringTok{ }\KeywordTok{length}\NormalTok{(}\KeywordTok{unique}\NormalTok{(Y))}
\NormalTok{       H <-}\StringTok{  }\NormalTok{hidden}
  
       \CommentTok{# create and init weights and bias }
\NormalTok{       W1 <-}\StringTok{ }\FloatTok{0.01}\OperatorTok{*}\KeywordTok{matrix}\NormalTok{(}\KeywordTok{rnorm}\NormalTok{(D}\OperatorTok{*}\NormalTok{H), }\DataTypeTok{nrow=}\NormalTok{D, }\DataTypeTok{ncol=}\NormalTok{H)}
\NormalTok{       b1 <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DataTypeTok{nrow=}\DecValTok{1}\NormalTok{, }\DataTypeTok{ncol=}\NormalTok{H)}
  
\NormalTok{       W2 <-}\StringTok{ }\FloatTok{0.01}\OperatorTok{*}\KeywordTok{matrix}\NormalTok{(}\KeywordTok{rnorm}\NormalTok{(H}\OperatorTok{*}\NormalTok{K), }\DataTypeTok{nrow=}\NormalTok{H, }\DataTypeTok{ncol=}\NormalTok{K)}
\NormalTok{       b2 <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DataTypeTok{nrow=}\DecValTok{1}\NormalTok{, }\DataTypeTok{ncol=}\NormalTok{K)}
\NormalTok{  \} }\ControlFlowTok{else}\NormalTok{ \{}
\NormalTok{       D  <-}\StringTok{ }\NormalTok{model}\OperatorTok{$}\NormalTok{D}
\NormalTok{       K  <-}\StringTok{ }\NormalTok{model}\OperatorTok{$}\NormalTok{K}
\NormalTok{       H  <-}\StringTok{ }\NormalTok{model}\OperatorTok{$}\NormalTok{H}
\NormalTok{       W1 <-}\StringTok{ }\NormalTok{model}\OperatorTok{$}\NormalTok{W1}
\NormalTok{       b1 <-}\StringTok{ }\NormalTok{model}\OperatorTok{$}\NormalTok{b1}
\NormalTok{       W2 <-}\StringTok{ }\NormalTok{model}\OperatorTok{$}\NormalTok{W2}
\NormalTok{       b2 <-}\StringTok{ }\NormalTok{model}\OperatorTok{$}\NormalTok{b2}
\NormalTok{  \}}
  
  \CommentTok{# use all train data to update weights since it's a small dataset}
\NormalTok{  batchsize <-}\StringTok{ }\NormalTok{N}
  \CommentTok{# init loss to a very big value}
\NormalTok{  loss <-}\StringTok{ }\DecValTok{100000}
  
  \CommentTok{# Training the network}
\NormalTok{  i <-}\StringTok{ }\DecValTok{0}
  \ControlFlowTok{while}\NormalTok{(i }\OperatorTok{<}\StringTok{ }\NormalTok{maxit }\OperatorTok{&&}\StringTok{ }\NormalTok{loss }\OperatorTok{>}\StringTok{ }\NormalTok{abstol ) \{}
    
    \CommentTok{# iteration index}
\NormalTok{    i <-}\StringTok{ }\NormalTok{i }\OperatorTok{+}\DecValTok{1}
    
    \CommentTok{# forward ....}
    \CommentTok{# 1 indicate row, 2 indicate col}
\NormalTok{    hidden.layer <-}\StringTok{ }\KeywordTok{sweep}\NormalTok{(X }\OperatorTok{%*%}\StringTok{ }\NormalTok{W1 ,}\DecValTok{2}\NormalTok{, b1, }\StringTok{'+'}\NormalTok{)}
    \CommentTok{# neurons : ReLU}
\NormalTok{    hidden.layer <-}\StringTok{ }\KeywordTok{pmax}\NormalTok{(hidden.layer, }\DecValTok{0}\NormalTok{)}
\NormalTok{    score <-}\StringTok{ }\KeywordTok{sweep}\NormalTok{(hidden.layer }\OperatorTok{%*%}\StringTok{ }\NormalTok{W2, }\DecValTok{2}\NormalTok{, b2, }\StringTok{'+'}\NormalTok{)}
    
    \CommentTok{# softmax}
\NormalTok{    score.exp <-}\StringTok{ }\KeywordTok{exp}\NormalTok{(score)}
    \CommentTok{# debug}
\NormalTok{    probs <-}\StringTok{ }\NormalTok{score.exp}\OperatorTok{/}\KeywordTok{rowSums}\NormalTok{(score.exp)}
    
    \CommentTok{# compute the loss}
\NormalTok{    corect.logprobs <-}\StringTok{ }\OperatorTok{-}\KeywordTok{log}\NormalTok{(probs[Y.index])}
\NormalTok{    data.loss  <-}\StringTok{ }\KeywordTok{sum}\NormalTok{(corect.logprobs)}\OperatorTok{/}\NormalTok{batchsize}
\NormalTok{    reg.loss   <-}\StringTok{ }\FloatTok{0.5}\OperatorTok{*}\NormalTok{reg}\OperatorTok{*}\StringTok{ }\NormalTok{(}\KeywordTok{sum}\NormalTok{(W1}\OperatorTok{*}\NormalTok{W1) }\OperatorTok{+}\StringTok{ }\KeywordTok{sum}\NormalTok{(W2}\OperatorTok{*}\NormalTok{W2))}
\NormalTok{    loss <-}\StringTok{ }\NormalTok{data.loss }\OperatorTok{+}\StringTok{ }\NormalTok{reg.loss}
    
    \CommentTok{# display results and update model}
    \ControlFlowTok{if}\NormalTok{( i }\OperatorTok{%%}\StringTok{ }\NormalTok{display }\OperatorTok{==}\StringTok{ }\DecValTok{0}\NormalTok{) \{}
        \ControlFlowTok{if}\NormalTok{(}\OperatorTok{!}\KeywordTok{is.null}\NormalTok{(testdata)) \{}
\NormalTok{            model <-}\StringTok{ }\KeywordTok{list}\NormalTok{( }\DataTypeTok{D =}\NormalTok{ D,}
                           \DataTypeTok{H =}\NormalTok{ H,}
                           \DataTypeTok{K =}\NormalTok{ K,}
                           \CommentTok{# weights and bias}
                           \DataTypeTok{W1 =}\NormalTok{ W1, }
                           \DataTypeTok{b1 =}\NormalTok{ b1, }
                           \DataTypeTok{W2 =}\NormalTok{ W2, }
                           \DataTypeTok{b2 =}\NormalTok{ b2)}
\NormalTok{            labs <-}\StringTok{ }\KeywordTok{predict.dnn}\NormalTok{(model, testdata[,}\OperatorTok{-}\NormalTok{y])}
\NormalTok{            accuracy <-}\StringTok{ }\KeywordTok{mean}\NormalTok{(}\KeywordTok{as.integer}\NormalTok{(testdata[,y]) }\OperatorTok{==}\StringTok{ }\NormalTok{Y.set[labs])}
            \KeywordTok{cat}\NormalTok{(i, loss, accuracy, }\StringTok{"}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\NormalTok{        \} }\ControlFlowTok{else}\NormalTok{ \{}
            \KeywordTok{cat}\NormalTok{(i, loss, }\StringTok{"}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\NormalTok{        \}}
\NormalTok{    \}}
    
    \CommentTok{# backward ....}
\NormalTok{    dscores <-}\StringTok{ }\NormalTok{probs}
\NormalTok{    dscores[Y.index] <-}\StringTok{ }\NormalTok{dscores[Y.index] }\DecValTok{-1}
\NormalTok{    dscores <-}\StringTok{ }\NormalTok{dscores }\OperatorTok{/}\StringTok{ }\NormalTok{batchsize}
    
    
\NormalTok{    dW2 <-}\StringTok{ }\KeywordTok{t}\NormalTok{(hidden.layer) }\OperatorTok{%*%}\StringTok{ }\NormalTok{dscores }
\NormalTok{    db2 <-}\StringTok{ }\KeywordTok{colSums}\NormalTok{(dscores)}
    
\NormalTok{    dhidden <-}\StringTok{ }\NormalTok{dscores }\OperatorTok{%*%}\StringTok{ }\KeywordTok{t}\NormalTok{(W2)}
\NormalTok{    dhidden[hidden.layer }\OperatorTok{<=}\StringTok{ }\DecValTok{0}\NormalTok{] <-}\StringTok{ }\DecValTok{0}
    
\NormalTok{    dW1 <-}\StringTok{ }\KeywordTok{t}\NormalTok{(X) }\OperatorTok{%*%}\StringTok{ }\NormalTok{dhidden}
\NormalTok{    db1 <-}\StringTok{ }\KeywordTok{colSums}\NormalTok{(dhidden) }
    
    \CommentTok{# update ....}
\NormalTok{    dW2 <-}\StringTok{ }\NormalTok{dW2 }\OperatorTok{+}\StringTok{ }\NormalTok{reg}\OperatorTok{*}\NormalTok{W2}
\NormalTok{    dW1 <-}\StringTok{ }\NormalTok{dW1  }\OperatorTok{+}\StringTok{ }\NormalTok{reg}\OperatorTok{*}\NormalTok{W1}
    
\NormalTok{    W1 <-}\StringTok{ }\NormalTok{W1 }\OperatorTok{-}\StringTok{ }\NormalTok{lr }\OperatorTok{*}\StringTok{ }\NormalTok{dW1}
\NormalTok{    b1 <-}\StringTok{ }\NormalTok{b1 }\OperatorTok{-}\StringTok{ }\NormalTok{lr }\OperatorTok{*}\StringTok{ }\NormalTok{db1}
    
\NormalTok{    W2 <-}\StringTok{ }\NormalTok{W2 }\OperatorTok{-}\StringTok{ }\NormalTok{lr }\OperatorTok{*}\StringTok{ }\NormalTok{dW2}
\NormalTok{    b2 <-}\StringTok{ }\NormalTok{b2 }\OperatorTok{-}\StringTok{ }\NormalTok{lr }\OperatorTok{*}\StringTok{ }\NormalTok{db2}
    
   
    
\NormalTok{  \}}
  
  \CommentTok{# final results}
  \CommentTok{# creat list to store learned parameters}
  \CommentTok{# you can add more parameters for debug and visualization}
  \CommentTok{# such as residuals, fitted.values ...}
\NormalTok{  model <-}\StringTok{ }\KeywordTok{list}\NormalTok{( }\DataTypeTok{D =}\NormalTok{ D,}
                 \DataTypeTok{H =}\NormalTok{ H,}
                 \DataTypeTok{K =}\NormalTok{ K,}
                 \CommentTok{# weights and bias}
                 \DataTypeTok{W1=}\NormalTok{ W1, }
                 \DataTypeTok{b1=}\NormalTok{ b1, }
                 \DataTypeTok{W2=}\NormalTok{ W2, }
                 \DataTypeTok{b2=}\NormalTok{ b2)}
    
  \KeywordTok{return}\NormalTok{(model)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{########################################################################}
\CommentTok{# testing}
\CommentTok{#######################################################################}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}

\CommentTok{# 0. EDA}
\KeywordTok{summary}\NormalTok{(iris)}
\CommentTok{#>   Sepal.Length   Sepal.Width    Petal.Length   Petal.Width }
\CommentTok{#>  Min.   :4.30   Min.   :2.00   Min.   :1.00   Min.   :0.1  }
\CommentTok{#>  1st Qu.:5.10   1st Qu.:2.80   1st Qu.:1.60   1st Qu.:0.3  }
\CommentTok{#>  Median :5.80   Median :3.00   Median :4.35   Median :1.3  }
\CommentTok{#>  Mean   :5.84   Mean   :3.06   Mean   :3.76   Mean   :1.2  }
\CommentTok{#>  3rd Qu.:6.40   3rd Qu.:3.30   3rd Qu.:5.10   3rd Qu.:1.8  }
\CommentTok{#>  Max.   :7.90   Max.   :4.40   Max.   :6.90   Max.   :2.5  }
\CommentTok{#>        Species  }
\CommentTok{#>  setosa    :50  }
\CommentTok{#>  versicolor:50  }
\CommentTok{#>  virginica :50  }
\CommentTok{#>                 }
\CommentTok{#>                 }
\CommentTok{#> }
\KeywordTok{plot}\NormalTok{(iris)}

\CommentTok{# 1. split data into test/train}
\NormalTok{samp <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{50}\NormalTok{,}\DecValTok{25}\NormalTok{), }\KeywordTok{sample}\NormalTok{(}\DecValTok{51}\OperatorTok{:}\DecValTok{100}\NormalTok{,}\DecValTok{25}\NormalTok{), }\KeywordTok{sample}\NormalTok{(}\DecValTok{101}\OperatorTok{:}\DecValTok{150}\NormalTok{,}\DecValTok{25}\NormalTok{))}

\CommentTok{# 2. train model}
\NormalTok{ ir.model <-}\StringTok{ }\KeywordTok{train.dnn}\NormalTok{(}\DataTypeTok{x=}\DecValTok{1}\OperatorTok{:}\DecValTok{4}\NormalTok{, }\DataTypeTok{y=}\DecValTok{5}\NormalTok{, }\DataTypeTok{traindata=}\NormalTok{iris[samp,], }\DataTypeTok{testdata=}\NormalTok{iris[}\OperatorTok{-}\NormalTok{samp,], }\DataTypeTok{hidden=}\DecValTok{10}\NormalTok{, }\DataTypeTok{maxit=}\DecValTok{2000}\NormalTok{, }\DataTypeTok{display=}\DecValTok{50}\NormalTok{)}
\CommentTok{#> 50 1.1 0.333 }
\CommentTok{#> 100 1.1 0.333 }
\CommentTok{#> 150 1.09 0.333 }
\CommentTok{#> 200 1.08 0.333 }
\CommentTok{#> 250 1.05 0.333 }
\CommentTok{#> 300 1 0.333 }
\CommentTok{#> 350 0.933 0.667 }
\CommentTok{#> 400 0.855 0.667 }
\CommentTok{#> 450 0.775 0.667 }
\CommentTok{#> 500 0.689 0.667 }
\CommentTok{#> 550 0.611 0.68 }
\CommentTok{#> 600 0.552 0.693 }
\CommentTok{#> 650 0.507 0.747 }
\CommentTok{#> 700 0.473 0.84 }
\CommentTok{#> 750 0.445 0.88 }
\CommentTok{#> 800 0.421 0.92 }
\CommentTok{#> 850 0.399 0.947 }
\CommentTok{#> 900 0.379 0.96 }
\CommentTok{#> 950 0.36 0.96 }
\CommentTok{#> 1000 0.341 0.973 }
\CommentTok{#> 1050 0.324 0.973 }
\CommentTok{#> 1100 0.307 0.973 }
\CommentTok{#> 1150 0.292 0.973 }
\CommentTok{#> 1200 0.277 0.973 }
\CommentTok{#> 1250 0.263 0.973 }
\CommentTok{#> 1300 0.25 0.973 }
\CommentTok{#> 1350 0.238 0.973 }
\CommentTok{#> 1400 0.227 0.973 }
\CommentTok{#> 1450 0.216 0.973 }
\CommentTok{#> 1500 0.207 0.973 }
\CommentTok{#> 1550 0.198 0.973 }
\CommentTok{#> 1600 0.19 0.973 }
\CommentTok{#> 1650 0.183 0.973 }
\CommentTok{#> 1700 0.176 0.973 }
\CommentTok{#> 1750 0.17 0.973 }
\CommentTok{#> 1800 0.164 0.973 }
\CommentTok{#> 1850 0.158 0.973 }
\CommentTok{#> 1900 0.153 0.973 }
\CommentTok{#> 1950 0.149 0.973 }
\CommentTok{#> 2000 0.144 0.973}
\CommentTok{# ir.model <- train.dnn(x=1:4, y=5, traindata=iris[samp,], hidden=6, maxit=2000, display=50)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{clssification-901-build_fully_connected_nn_from_scratch-nnet_files/figure-latex/unnamed-chunk-3-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# 3. prediction}
\CommentTok{# }\AlertTok{NOTE}\CommentTok{: if the predict is factor, we need to transfer the number into class manually.}
\CommentTok{#       To make the code clear, I don't write this change into predict.dnn function.}
\NormalTok{labels.dnn <-}\StringTok{ }\KeywordTok{predict.dnn}\NormalTok{(ir.model, iris[}\OperatorTok{-}\NormalTok{samp, }\DecValTok{-5}\NormalTok{])}

\CommentTok{# 4. verify the results}
\KeywordTok{table}\NormalTok{(iris[}\OperatorTok{-}\NormalTok{samp,}\DecValTok{5}\NormalTok{], labels.dnn)}
\CommentTok{#>             labels.dnn}
\CommentTok{#>               1  2  3}
\CommentTok{#>   setosa     25  0  0}
\CommentTok{#>   versicolor  0 23  2}
\CommentTok{#>   virginica   0  0 25}
\CommentTok{#          labels.dnn}
\CommentTok{#            1  2  3}
\CommentTok{#setosa     25  0  0}
\CommentTok{#versicolor  0 24  1}
\CommentTok{#virginica   0  0 25}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#accuracy}
\KeywordTok{mean}\NormalTok{(}\KeywordTok{as.integer}\NormalTok{(iris[}\OperatorTok{-}\NormalTok{samp, }\DecValTok{5}\NormalTok{]) }\OperatorTok{==}\StringTok{ }\NormalTok{labels.dnn)}
\CommentTok{#> [1] 0.973}
\CommentTok{# 0.98}

\CommentTok{# 5. compare with nnet}
\KeywordTok{library}\NormalTok{(nnet)}
\NormalTok{ird <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\KeywordTok{rbind}\NormalTok{(iris3[,,}\DecValTok{1}\NormalTok{], iris3[,,}\DecValTok{2}\NormalTok{], iris3[,,}\DecValTok{3}\NormalTok{]),}
                  \DataTypeTok{species =} \KeywordTok{factor}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\KeywordTok{rep}\NormalTok{(}\StringTok{"s"}\NormalTok{,}\DecValTok{50}\NormalTok{), }\KeywordTok{rep}\NormalTok{(}\StringTok{"c"}\NormalTok{, }\DecValTok{50}\NormalTok{), }\KeywordTok{rep}\NormalTok{(}\StringTok{"v"}\NormalTok{, }\DecValTok{50}\NormalTok{))))}
\NormalTok{ir.nn2 <-}\StringTok{ }\KeywordTok{nnet}\NormalTok{(species }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ ird, }\DataTypeTok{subset =}\NormalTok{ samp, }\DataTypeTok{size =} \DecValTok{6}\NormalTok{, }\DataTypeTok{rang =} \FloatTok{0.1}\NormalTok{,}
               \DataTypeTok{decay =} \FloatTok{1e-2}\NormalTok{, }\DataTypeTok{maxit =} \DecValTok{2000}\NormalTok{)}
\CommentTok{#> # weights:  51}
\CommentTok{#> initial  value 82.293110 }
\CommentTok{#> iter  10 value 29.196376}
\CommentTok{#> iter  20 value 5.446284}
\CommentTok{#> iter  30 value 4.782022}
\CommentTok{#> iter  40 value 4.379729}
\CommentTok{#> iter  50 value 4.188725}
\CommentTok{#> iter  60 value 4.120587}
\CommentTok{#> iter  70 value 4.091706}
\CommentTok{#> iter  80 value 4.086017}
\CommentTok{#> iter  90 value 4.081664}
\CommentTok{#> iter 100 value 4.074111}
\CommentTok{#> iter 110 value 4.072894}
\CommentTok{#> iter 120 value 4.069011}
\CommentTok{#> iter 130 value 4.067690}
\CommentTok{#> iter 140 value 4.067633}
\CommentTok{#> final  value 4.067633 }
\CommentTok{#> converged}

           
\NormalTok{labels.nnet <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(ir.nn2, ird[}\OperatorTok{-}\NormalTok{samp,], }\DataTypeTok{type=}\StringTok{"class"}\NormalTok{)}
\KeywordTok{table}\NormalTok{(ird}\OperatorTok{$}\NormalTok{species[}\OperatorTok{-}\NormalTok{samp], labels.nnet)}
\CommentTok{#>    labels.nnet}
\CommentTok{#>      c  s  v}
\CommentTok{#>   c 23  0  2}
\CommentTok{#>   s  0 25  0}
\CommentTok{#>   v  0  0 25}
\CommentTok{#  labels.nnet}
\CommentTok{#   c  s  v}
\CommentTok{#c 22  0  3}
\CommentTok{#s  0 25  0}
\CommentTok{#v  3  0 22}

\CommentTok{# accuracy}
\KeywordTok{mean}\NormalTok{(ird}\OperatorTok{$}\NormalTok{species[}\OperatorTok{-}\NormalTok{samp] }\OperatorTok{==}\StringTok{ }\NormalTok{labels.nnet)}
\CommentTok{#> [1] 0.973}
\CommentTok{# 0.96}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{# Visualization}
\CommentTok{# the output from screen, copy and paste here.}
\NormalTok{data1 <-}\StringTok{ }\NormalTok{(}\StringTok{"i loss accuracy}
\StringTok{50 1.098421 0.3333333 }
\StringTok{100 1.098021 0.3333333 }
\StringTok{150 1.096843 0.3333333 }
\StringTok{200 1.093393 0.3333333 }
\StringTok{250 1.084069 0.3333333 }
\StringTok{300 1.063278 0.3333333 }
\StringTok{350 1.027273 0.3333333 }
\StringTok{400 0.9707605 0.64 }
\StringTok{450 0.8996356 0.6666667 }
\StringTok{500 0.8335469 0.6666667 }
\StringTok{550 0.7662386 0.6666667 }
\StringTok{600 0.6914156 0.6666667 }
\StringTok{650 0.6195753 0.68 }
\StringTok{700 0.5620381 0.68 }
\StringTok{750 0.5184008 0.7333333 }
\StringTok{800 0.4844815 0.84 }
\StringTok{850 0.4568258 0.8933333 }
\StringTok{900 0.4331083 0.92 }
\StringTok{950 0.4118948 0.9333333 }
\StringTok{1000 0.392368 0.96 }
\StringTok{1050 0.3740457 0.96 }
\StringTok{1100 0.3566594 0.96 }
\StringTok{1150 0.3400993 0.9866667 }
\StringTok{1200 0.3243276 0.9866667 }
\StringTok{1250 0.3093422 0.9866667 }
\StringTok{1300 0.2951787 0.9866667 }
\StringTok{1350 0.2818472 0.9866667 }
\StringTok{1400 0.2693641 0.9866667 }
\StringTok{1450 0.2577245 0.9866667 }
\StringTok{1500 0.2469068 0.9866667 }
\StringTok{1550 0.2368819 0.9866667 }
\StringTok{1600 0.2276124 0.9866667 }
\StringTok{1650 0.2190535 0.9866667 }
\StringTok{1700 0.2111565 0.9866667 }
\StringTok{1750 0.2038719 0.9866667 }
\StringTok{1800 0.1971507 0.9866667 }
\StringTok{1850 0.1909452 0.9866667 }
\StringTok{1900 0.1852105 0.9866667 }
\StringTok{1950 0.1799045 0.9866667 }
\StringTok{2000 0.1749881 0.9866667  "}\NormalTok{)}

\NormalTok{data.v <-}\StringTok{ }\KeywordTok{read.table}\NormalTok{(}\DataTypeTok{text=}\NormalTok{data1, }\DataTypeTok{header=}\NormalTok{T)}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mar=}\KeywordTok{c}\NormalTok{(}\FloatTok{5.1}\NormalTok{, }\FloatTok{4.1}\NormalTok{, }\FloatTok{4.1}\NormalTok{, }\FloatTok{4.1}\NormalTok{))}
\KeywordTok{plot}\NormalTok{(}\DataTypeTok{x=}\NormalTok{data.v}\OperatorTok{$}\NormalTok{i, }\DataTypeTok{y=}\NormalTok{data.v}\OperatorTok{$}\NormalTok{loss, }\DataTypeTok{type=}\StringTok{"o"}\NormalTok{, }\DataTypeTok{col=}\StringTok{"blue"}\NormalTok{, }\DataTypeTok{pch=}\DecValTok{16}\NormalTok{, }
     \DataTypeTok{main=}\StringTok{"IRIS loss and accuracy by 2-layers DNN"}\NormalTok{,}
     \DataTypeTok{ylim=}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{1.2}\NormalTok{),}
     \DataTypeTok{xlab=}\StringTok{""}\NormalTok{,}
     \DataTypeTok{ylab=}\StringTok{""}\NormalTok{,}
     \DataTypeTok{axe =}\NormalTok{F)}
\KeywordTok{lines}\NormalTok{(}\DataTypeTok{x=}\NormalTok{data.v}\OperatorTok{$}\NormalTok{i, }\DataTypeTok{y=}\NormalTok{data.v}\OperatorTok{$}\NormalTok{accuracy, }\DataTypeTok{type=}\StringTok{"o"}\NormalTok{, }\DataTypeTok{col=}\StringTok{"red"}\NormalTok{, }\DataTypeTok{pch=}\DecValTok{1}\NormalTok{)}
\KeywordTok{box}\NormalTok{()}
\KeywordTok{axis}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DataTypeTok{at=}\KeywordTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{2000}\NormalTok{,}\DataTypeTok{by=}\DecValTok{200}\NormalTok{))}
\KeywordTok{axis}\NormalTok{(}\DecValTok{4}\NormalTok{, }\DataTypeTok{at=}\KeywordTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{,}\FloatTok{1.0}\NormalTok{,}\DataTypeTok{by=}\FloatTok{0.1}\NormalTok{))}
\KeywordTok{axis}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DataTypeTok{at=}\KeywordTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{,}\FloatTok{1.2}\NormalTok{,}\DataTypeTok{by=}\FloatTok{0.1}\NormalTok{))}
\KeywordTok{mtext}\NormalTok{(}\StringTok{"training step"}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DataTypeTok{line=}\DecValTok{3}\NormalTok{)}
\KeywordTok{mtext}\NormalTok{(}\StringTok{"loss of training set"}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DataTypeTok{line=}\FloatTok{2.5}\NormalTok{)}
\KeywordTok{mtext}\NormalTok{(}\StringTok{"accuracy of testing set"}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DataTypeTok{line=}\DecValTok{2}\NormalTok{)}

\KeywordTok{legend}\NormalTok{(}\StringTok{"bottomleft"}\NormalTok{, }
       \DataTypeTok{legend =} \KeywordTok{c}\NormalTok{(}\StringTok{"loss"}\NormalTok{, }\StringTok{"accuracy"}\NormalTok{),}
       \DataTypeTok{pch =} \KeywordTok{c}\NormalTok{(}\DecValTok{16}\NormalTok{,}\DecValTok{1}\NormalTok{),}
       \DataTypeTok{col =} \KeywordTok{c}\NormalTok{(}\StringTok{"blue"}\NormalTok{,}\StringTok{"red"}\NormalTok{),}
       \DataTypeTok{lwd=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{clssification-901-build_fully_connected_nn_from_scratch-nnet_files/figure-latex/unnamed-chunk-6-1} \end{center}

\hypertarget{classification-and-regression-with-h2o-deep-learning}{%
\chapter{Classification and Regression with H2O Deep Learning}\label{classification-and-regression-with-h2o-deep-learning}}

\hypertarget{introduction-3}{%
\section{Introduction}\label{introduction-3}}

Source: \url{http://docs.h2o.ai/h2o-tutorials/latest-stable/tutorials/deeplearning/index.html}

Repo: \url{https://github.com/h2oai/h2o-tutorials}

This tutorial shows how a H2O Deep Learning model can be used to do supervised classification and regression. A great tutorial about Deep Learning is given by Quoc Le here and here. This tutorial covers usage of H2O from R. A python version of this tutorial will be available as well in a separate document. This file is available in plain R, R markdown and regular markdown formats, and the plots are available as PDF files. All documents are available on Github.

If run from plain R, execute R in the directory of this script. If run from RStudio, be sure to setwd() to the location of this script.\texttt{h2o.init()} starts H2O in R's current working directory. \texttt{h2o.importFile()} looks for files from the perspective of where H2O was started.

More examples and explanations can be found in our H2O Deep Learning booklet and on our H2O Github Repository. The PDF slide deck can be found on Github.

\hypertarget{h2o-r-package}{%
\section{H2O R Package}\label{h2o-r-package}}

Load the H2O R package:

Source: \url{http://docs.h2o.ai/h2o-tutorials/latest-stable/tutorials/deeplearning/index.html}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## R installation instructions are at http://h2o.ai/download}
\KeywordTok{library}\NormalTok{(h2o)}
\CommentTok{#> }
\CommentTok{#> ----------------------------------------------------------------------}
\CommentTok{#> }
\CommentTok{#> Your next step is to start H2O:}
\CommentTok{#>     > h2o.init()}
\CommentTok{#> }
\CommentTok{#> For H2O package documentation, ask for help:}
\CommentTok{#>     > ??h2o}
\CommentTok{#> }
\CommentTok{#> After starting H2O, you can use the Web UI at http://localhost:54321}
\CommentTok{#> For more information visit http://docs.h2o.ai}
\CommentTok{#> }
\CommentTok{#> ----------------------------------------------------------------------}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'h2o'}
\CommentTok{#> The following objects are masked from 'package:stats':}
\CommentTok{#> }
\CommentTok{#>     cor, sd, var}
\CommentTok{#> The following objects are masked from 'package:base':}
\CommentTok{#> }
\CommentTok{#>     &&, %*%, %in%, ||, apply, as.factor, as.numeric, colnames,}
\CommentTok{#>     colnames<-, ifelse, is.character, is.factor, is.numeric, log,}
\CommentTok{#>     log10, log1p, log2, round, signif, trunc}
\end{Highlighting}
\end{Shaded}

\hypertarget{start-h2o}{%
\section{Start H2O}\label{start-h2o}}

Start up a 1-node H2O server on your local machine, and allow it to use all CPU cores and up to 2GB of memory:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{h2o.init}\NormalTok{(}\DataTypeTok{nthreads=}\OperatorTok{-}\DecValTok{1}\NormalTok{, }\DataTypeTok{max_mem_size=}\StringTok{"2G"}\NormalTok{)}
\CommentTok{#> }
\CommentTok{#> H2O is not running yet, starting it now...}
\CommentTok{#> }
\CommentTok{#> Note:  In case of errors look at the following log files:}
\CommentTok{#>     /tmp/RtmpgHej5Z/h2o_datascience_started_from_r.out}
\CommentTok{#>     /tmp/RtmpgHej5Z/h2o_datascience_started_from_r.err}
\CommentTok{#> }
\CommentTok{#> }
\CommentTok{#> Starting H2O JVM and connecting: . Connection successful!}
\CommentTok{#> }
\CommentTok{#> R is connected to the H2O cluster: }
\CommentTok{#>     H2O cluster uptime:         1 seconds 298 milliseconds }
\CommentTok{#>     H2O cluster timezone:       America/Chicago }
\CommentTok{#>     H2O data parsing timezone:  UTC }
\CommentTok{#>     H2O cluster version:        3.22.1.1 }
\CommentTok{#>     H2O cluster version age:    8 months and 21 days !!! }
\CommentTok{#>     H2O cluster name:           H2O_started_from_R_datascience_mwl453 }
\CommentTok{#>     H2O cluster total nodes:    1 }
\CommentTok{#>     H2O cluster total memory:   1.78 GB }
\CommentTok{#>     H2O cluster total cores:    8 }
\CommentTok{#>     H2O cluster allowed cores:  8 }
\CommentTok{#>     H2O cluster healthy:        TRUE }
\CommentTok{#>     H2O Connection ip:          localhost }
\CommentTok{#>     H2O Connection port:        54321 }
\CommentTok{#>     H2O Connection proxy:       NA }
\CommentTok{#>     H2O Internal Security:      FALSE }
\CommentTok{#>     H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4 }
\CommentTok{#>     R Version:                  R version 3.6.0 (2019-04-26)}
\CommentTok{#> Warning in h2o.clusterInfo(): }
\CommentTok{#> Your H2O cluster version is too old (8 months and 21 days)!}
\CommentTok{#> Please download and install the latest version from http://h2o.ai/download/}
\KeywordTok{h2o.removeAll}\NormalTok{() }\CommentTok{## clean slate - just in case the cluster was already running}
\CommentTok{#> [1] 0}
\end{Highlighting}
\end{Shaded}

The \texttt{h2o.deeplearning} function fits H2O's Deep Learning models from within R. We can run the example from the man page using the example function, or run a longer demonstration from the h2o package using the demo function::

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{args}\NormalTok{(h2o.deeplearning)}
\CommentTok{#> function (x, y, training_frame, model_id = NULL, validation_frame = NULL, }
\CommentTok{#>     nfolds = 0, keep_cross_validation_models = TRUE, keep_cross_validation_predictions = FALSE, }
\CommentTok{#>     keep_cross_validation_fold_assignment = FALSE, fold_assignment = c("AUTO", }
\CommentTok{#>         "Random", "Modulo", "Stratified"), fold_column = NULL, }
\CommentTok{#>     ignore_const_cols = TRUE, score_each_iteration = FALSE, weights_column = NULL, }
\CommentTok{#>     offset_column = NULL, balance_classes = FALSE, class_sampling_factors = NULL, }
\CommentTok{#>     max_after_balance_size = 5, max_hit_ratio_k = 0, checkpoint = NULL, }
\CommentTok{#>     pretrained_autoencoder = NULL, overwrite_with_best_model = TRUE, }
\CommentTok{#>     use_all_factor_levels = TRUE, standardize = TRUE, activation = c("Tanh", }
\CommentTok{#>         "TanhWithDropout", "Rectifier", "RectifierWithDropout", }
\CommentTok{#>         "Maxout", "MaxoutWithDropout"), hidden = c(200, 200), }
\CommentTok{#>     epochs = 10, train_samples_per_iteration = -2, target_ratio_comm_to_comp = 0.05, }
\CommentTok{#>     seed = -1, adaptive_rate = TRUE, rho = 0.99, epsilon = 1e-08, }
\CommentTok{#>     rate = 0.005, rate_annealing = 1e-06, rate_decay = 1, momentum_start = 0, }
\CommentTok{#>     momentum_ramp = 1e+06, momentum_stable = 0, nesterov_accelerated_gradient = TRUE, }
\CommentTok{#>     input_dropout_ratio = 0, hidden_dropout_ratios = NULL, l1 = 0, }
\CommentTok{#>     l2 = 0, max_w2 = 3.4028235e+38, initial_weight_distribution = c("UniformAdaptive", }
\CommentTok{#>         "Uniform", "Normal"), initial_weight_scale = 1, initial_weights = NULL, }
\CommentTok{#>     initial_biases = NULL, loss = c("Automatic", "CrossEntropy", }
\CommentTok{#>         "Quadratic", "Huber", "Absolute", "Quantile"), distribution = c("AUTO", }
\CommentTok{#>         "bernoulli", "multinomial", "gaussian", "poisson", "gamma", }
\CommentTok{#>         "tweedie", "laplace", "quantile", "huber"), quantile_alpha = 0.5, }
\CommentTok{#>     tweedie_power = 1.5, huber_alpha = 0.9, score_interval = 5, }
\CommentTok{#>     score_training_samples = 10000, score_validation_samples = 0, }
\CommentTok{#>     score_duty_cycle = 0.1, classification_stop = 0, regression_stop = 1e-06, }
\CommentTok{#>     stopping_rounds = 5, stopping_metric = c("AUTO", "deviance", }
\CommentTok{#>         "logloss", "MSE", "RMSE", "MAE", "RMSLE", "AUC", "lift_top_group", }
\CommentTok{#>         "misclassification", "mean_per_class_error", "custom", }
\CommentTok{#>         "custom_increasing"), stopping_tolerance = 0, max_runtime_secs = 0, }
\CommentTok{#>     score_validation_sampling = c("Uniform", "Stratified"), diagnostics = TRUE, }
\CommentTok{#>     fast_mode = TRUE, force_load_balance = TRUE, variable_importances = TRUE, }
\CommentTok{#>     replicate_training_data = TRUE, single_node_mode = FALSE, }
\CommentTok{#>     shuffle_training_data = FALSE, missing_values_handling = c("MeanImputation", }
\CommentTok{#>         "Skip"), quiet_mode = FALSE, autoencoder = FALSE, sparse = FALSE, }
\CommentTok{#>     col_major = FALSE, average_activation = 0, sparsity_beta = 0, }
\CommentTok{#>     max_categorical_features = 2147483647, reproducible = FALSE, }
\CommentTok{#>     export_weights_and_biases = FALSE, mini_batch_size = 1, categorical_encoding = c("AUTO", }
\CommentTok{#>         "Enum", "OneHotInternal", "OneHotExplicit", "Binary", }
\CommentTok{#>         "Eigen", "LabelEncoder", "SortByResponse", "EnumLimited"), }
\CommentTok{#>     elastic_averaging = FALSE, elastic_averaging_moving_rate = 0.9, }
\CommentTok{#>     elastic_averaging_regularization = 0.001, export_checkpoints_dir = NULL, }
\CommentTok{#>     verbose = FALSE) }
\CommentTok{#> NULL}
\ControlFlowTok{if}\NormalTok{ (}\KeywordTok{interactive}\NormalTok{()) }\KeywordTok{help}\NormalTok{(h2o.deeplearning)}
\KeywordTok{example}\NormalTok{(h2o.deeplearning)}
\CommentTok{#> }
\CommentTok{#> h2.dpl> ## No test: }
\CommentTok{#> h2.dpl> ##D library(h2o)}
\CommentTok{#> h2.dpl> ##D h2o.init()}
\CommentTok{#> h2.dpl> ##D iris_hf <- as.h2o(iris)}
\CommentTok{#> h2.dpl> ##D iris_dl <- h2o.deeplearning(x = 1:4, y = 5, training_frame = iris_hf, seed=123456)}
\CommentTok{#> h2.dpl> ##D }
\CommentTok{#> h2.dpl> ##D # now make a prediction}
\CommentTok{#> h2.dpl> ##D predictions <- h2o.predict(iris_dl, iris_hf)}
\CommentTok{#> h2.dpl> ## End(No test)}
\CommentTok{#> h2.dpl> }
\CommentTok{#> h2.dpl> }
\CommentTok{#> h2.dpl>}
\ControlFlowTok{if}\NormalTok{ (}\KeywordTok{interactive}\NormalTok{()) }\KeywordTok{demo}\NormalTok{(h2o.deeplearning)  }\CommentTok{#requires user interaction}
\end{Highlighting}
\end{Shaded}

While \textbf{H2O Deep Learning} has many parameters, it was designed to be just as easy to use as the other supervised training methods in H2O. Early stopping, automatic data standardization and handling of categorical variables and missing values and adaptive learning rates (per weight) reduce the amount of parameters the user has to specify. Often, it's just the number and sizes of hidden layers, the number of epochs and the activation function and maybe some regularization techniques.

\hypertarget{lets-have-some-fun-first-decision-boundaries}{%
\section{Let's have some fun first: Decision Boundaries}\label{lets-have-some-fun-first-decision-boundaries}}

We start with a small dataset representing red and black dots on a plane, arranged in the shape of two nested spirals. Then we task H2O's machine learning methods to separate the red and black dots, i.e., recognize each spiral as such by assigning each point in the plane to one of the two spirals.

We visualize the nature of H2O Deep Learning (DL), H2O's tree methods (GBM/DRF) and H2O's generalized linear modeling (GLM) by plotting the decision boundary between the red and black spirals:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# setwd("~/h2o-tutorials/tutorials/deeplearning") ##For RStudio}
\NormalTok{spiral <-}\StringTok{ }\KeywordTok{h2o.importFile}\NormalTok{(}\DataTypeTok{path =} \KeywordTok{normalizePath}\NormalTok{(}\KeywordTok{file.path}\NormalTok{(data_raw_dir, }\StringTok{"spiral.csv"}\NormalTok{)))}
\CommentTok{#> }
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|}\StringTok{                                                                 }\ErrorTok{|}\StringTok{   }\DecValTok{0}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=================================================================|}\StringTok{ }\DecValTok{100}\NormalTok{%}
\NormalTok{grid   <-}\StringTok{ }\KeywordTok{h2o.importFile}\NormalTok{(}\DataTypeTok{path =} \KeywordTok{normalizePath}\NormalTok{(}\KeywordTok{file.path}\NormalTok{(data_raw_dir, }\StringTok{"grid.csv"}\NormalTok{)))}
\CommentTok{#> }
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|}\StringTok{                                                                 }\ErrorTok{|}\StringTok{   }\DecValTok{0}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=================================================================|}\StringTok{ }\DecValTok{100}\NormalTok{%}

\CommentTok{# Define helper to plot contours}
\NormalTok{plotC <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(name, model, }\DataTypeTok{data=}\NormalTok{spiral, }\DataTypeTok{g=}\NormalTok{grid) \{}
\NormalTok{  data <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(data) }\CommentTok{#get data from into R}
\NormalTok{  pred <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(}\KeywordTok{h2o.predict}\NormalTok{(model, g))}
\NormalTok{  n=}\FloatTok{0.5}\OperatorTok{*}\NormalTok{(}\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{nrow}\NormalTok{(g))}\OperatorTok{-}\DecValTok{1}\NormalTok{); d <-}\StringTok{ }\FloatTok{1.5}\NormalTok{; h <-}\StringTok{ }\NormalTok{d}\OperatorTok{*}\NormalTok{(}\OperatorTok{-}\NormalTok{n}\OperatorTok{:}\NormalTok{n)}\OperatorTok{/}\NormalTok{n}
  \KeywordTok{plot}\NormalTok{(data[,}\OperatorTok{-}\DecValTok{3}\NormalTok{],}\DataTypeTok{pch=}\DecValTok{19}\NormalTok{,}\DataTypeTok{col=}\NormalTok{data[,}\DecValTok{3}\NormalTok{],}\DataTypeTok{cex=}\FloatTok{0.5}\NormalTok{,}
       \DataTypeTok{xlim=}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\NormalTok{d,d),}\DataTypeTok{ylim=}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\NormalTok{d,d),}\DataTypeTok{main=}\NormalTok{name)}
  \KeywordTok{contour}\NormalTok{(h,h,}\DataTypeTok{z=}\KeywordTok{array}\NormalTok{(}\KeywordTok{ifelse}\NormalTok{(pred[,}\DecValTok{1}\NormalTok{]}\OperatorTok{==}\StringTok{"Red"}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{),}
          \DataTypeTok{dim=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\OperatorTok{*}\NormalTok{n}\OperatorTok{+}\DecValTok{1}\NormalTok{,}\DecValTok{2}\OperatorTok{*}\NormalTok{n}\OperatorTok{+}\DecValTok{1}\NormalTok{)),}\DataTypeTok{col=}\StringTok{"blue"}\NormalTok{,}\DataTypeTok{lwd=}\DecValTok{2}\NormalTok{,}\DataTypeTok{add=}\NormalTok{T)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

We build a few different models:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#dev.new(noRStudioGD=FALSE) #direct plotting output to a new window}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{)) }\CommentTok{#set up the canvas for 2x2 plots}
\KeywordTok{plotC}\NormalTok{( }\StringTok{"DL"}\NormalTok{, }\KeywordTok{h2o.deeplearning}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,spiral,}\DataTypeTok{epochs=}\FloatTok{1e3}\NormalTok{))}
\KeywordTok{plotC}\NormalTok{(}\StringTok{"GBM"}\NormalTok{, }\KeywordTok{h2o.gbm}\NormalTok{         (}\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,spiral))}
\KeywordTok{plotC}\NormalTok{(}\StringTok{"DRF"}\NormalTok{, }\KeywordTok{h2o.randomForest}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,spiral))}
\KeywordTok{plotC}\NormalTok{(}\StringTok{"GLM"}\NormalTok{, }\KeywordTok{h2o.glm}\NormalTok{         (}\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,spiral,}\DataTypeTok{family=}\StringTok{"binomial"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{comparison-classification_regression_with_h2o_deep_learning_files/figure-latex/deeplearning_gbm_drf_glm-1} \end{center}

Let's investigate some more Deep Learning models. First, we explore the evolution over training time (number of passes over the data), and we use checkpointing to continue training the same model:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#dev.new(noRStudioGD=FALSE) #direct plotting output to a new window}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{)) }\CommentTok{#set up the canvas for 2x2 plots}
\NormalTok{ep <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{250}\NormalTok{,}\DecValTok{500}\NormalTok{,}\DecValTok{750}\NormalTok{)}
\KeywordTok{plotC}\NormalTok{(}\KeywordTok{paste0}\NormalTok{(}\StringTok{"DL "}\NormalTok{,ep[}\DecValTok{1}\NormalTok{],}\StringTok{" epochs"}\NormalTok{),}
      \KeywordTok{h2o.deeplearning}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,spiral,}\DataTypeTok{epochs=}\NormalTok{ep[}\DecValTok{1}\NormalTok{],}
                              \DataTypeTok{model_id=}\StringTok{"dl_1"}\NormalTok{))}
\KeywordTok{plotC}\NormalTok{(}\KeywordTok{paste0}\NormalTok{(}\StringTok{"DL "}\NormalTok{,ep[}\DecValTok{2}\NormalTok{],}\StringTok{" epochs"}\NormalTok{),}
      \KeywordTok{h2o.deeplearning}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,spiral,}\DataTypeTok{epochs=}\NormalTok{ep[}\DecValTok{2}\NormalTok{],}
            \DataTypeTok{checkpoint=}\StringTok{"dl_1"}\NormalTok{,}\DataTypeTok{model_id=}\StringTok{"dl_2"}\NormalTok{))}
\KeywordTok{plotC}\NormalTok{(}\KeywordTok{paste0}\NormalTok{(}\StringTok{"DL "}\NormalTok{,ep[}\DecValTok{3}\NormalTok{],}\StringTok{" epochs"}\NormalTok{),}
      \KeywordTok{h2o.deeplearning}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,spiral,}\DataTypeTok{epochs=}\NormalTok{ep[}\DecValTok{3}\NormalTok{],}
            \DataTypeTok{checkpoint=}\StringTok{"dl_2"}\NormalTok{,}\DataTypeTok{model_id=}\StringTok{"dl_3"}\NormalTok{))}
\KeywordTok{plotC}\NormalTok{(}\KeywordTok{paste0}\NormalTok{(}\StringTok{"DL "}\NormalTok{,ep[}\DecValTok{4}\NormalTok{],}\StringTok{" epochs"}\NormalTok{),}
      \KeywordTok{h2o.deeplearning}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,spiral,}\DataTypeTok{epochs=}\NormalTok{ep[}\DecValTok{4}\NormalTok{],}
            \DataTypeTok{checkpoint=}\StringTok{"dl_3"}\NormalTok{,}\DataTypeTok{model_id=}\StringTok{"dl_4"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{comparison-classification_regression_with_h2o_deep_learning_files/figure-latex/deeplearning_evolution_training_time-1} \end{center}

You can see how the network learns the structure of the spirals with enough training time. We explore different network architectures next:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#dev.new(noRStudioGD=FALSE) #direct plotting output to a new window}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{)) }\CommentTok{#set up the canvas for 2x2 plots}
\ControlFlowTok{for}\NormalTok{ (hidden }\ControlFlowTok{in} \KeywordTok{list}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{11}\NormalTok{,}\DecValTok{13}\NormalTok{,}\DecValTok{17}\NormalTok{,}\DecValTok{19}\NormalTok{),}\KeywordTok{c}\NormalTok{(}\DecValTok{42}\NormalTok{,}\DecValTok{42}\NormalTok{,}\DecValTok{42}\NormalTok{),}\KeywordTok{c}\NormalTok{(}\DecValTok{200}\NormalTok{,}\DecValTok{200}\NormalTok{),}\KeywordTok{c}\NormalTok{(}\DecValTok{1000}\NormalTok{))) \{}
  \KeywordTok{plotC}\NormalTok{(}\KeywordTok{paste0}\NormalTok{(}\StringTok{"DL hidden="}\NormalTok{,}\KeywordTok{paste0}\NormalTok{(hidden, }\DataTypeTok{collapse=}\StringTok{"x"}\NormalTok{)),}
        \KeywordTok{h2o.deeplearning}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{ ,spiral, }\DataTypeTok{hidden=}\NormalTok{hidden, }\DataTypeTok{epochs=}\DecValTok{500}\NormalTok{))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{comparison-classification_regression_with_h2o_deep_learning_files/figure-latex/deeplearning_for_loop_hidden-1} \end{center}

It is clear that different configurations can achieve similar performance, and that tuning will be required for optimal performance. Next, we compare between different activation functions, including one with 50\% dropout regularization in the hidden layers:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#dev.new(noRStudioGD=FALSE) #direct plotting output to a new window}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{)) }\CommentTok{#set up the canvas for 2x2 plots}

\ControlFlowTok{for}\NormalTok{ (act }\ControlFlowTok{in} \KeywordTok{c}\NormalTok{(}\StringTok{"Tanh"}\NormalTok{, }\StringTok{"Maxout"}\NormalTok{, }\StringTok{"Rectifier"}\NormalTok{, }\StringTok{"RectifierWithDropout"}\NormalTok{)) \{}
  \KeywordTok{plotC}\NormalTok{(}\KeywordTok{paste0}\NormalTok{(}\StringTok{"DL "}\NormalTok{,act,}\StringTok{" activation"}\NormalTok{), }
        \KeywordTok{h2o.deeplearning}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{, spiral,}
              \DataTypeTok{activation =}\NormalTok{ act, }
              \DataTypeTok{hidden =} \KeywordTok{c}\NormalTok{(}\DecValTok{100}\NormalTok{,}\DecValTok{100}\NormalTok{), }
              \DataTypeTok{epochs =} \DecValTok{1000}\NormalTok{))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{comparison-classification_regression_with_h2o_deep_learning_files/figure-latex/compare_activation_functions-1} \end{center}

Clearly, the dropout rate was too high or the number of epochs was too low for the last configuration, which often ends up performing the best on larger datasets where generalization is important.

More information about the parameters can be found in the H2O Deep Learning booklet.

\hypertarget{cover-type-dataset}{%
\section{Cover Type Dataset}\label{cover-type-dataset}}

We important the full cover type dataset (581k rows, 13 columns, 10 numerical, 3 categorical). We also split the data 3 ways: 60\% for training, 20\% for validation (hyper parameter tuning) and 20\% for final testing.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df <-}\StringTok{ }\KeywordTok{h2o.importFile}\NormalTok{(}\DataTypeTok{path =} \KeywordTok{normalizePath}\NormalTok{(}\KeywordTok{file.path}\NormalTok{(data_raw_dir, }\StringTok{"covtype.full.csv"}\NormalTok{)))}
\CommentTok{#> }
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|}\StringTok{                                                                 }\ErrorTok{|}\StringTok{   }\DecValTok{0}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|====================}\StringTok{                                             }\ErrorTok{|}\StringTok{  }\DecValTok{31}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=================================================================|}\StringTok{ }\DecValTok{100}\NormalTok{%}
\KeywordTok{dim}\NormalTok{(df)}
\CommentTok{#> [1] 581012     13}
\NormalTok{df}
\CommentTok{#>   Elevation Aspect Slope Horizontal_Distance_To_Hydrology}
\CommentTok{#> 1      3066    124     5                                0}
\CommentTok{#> 2      3136     32    20                              450}
\CommentTok{#> 3      2655     28    14                               42}
\CommentTok{#> 4      3191     45    19                              323}
\CommentTok{#> 5      3217     80    13                               30}
\CommentTok{#> 6      3119    293    13                               30}
\CommentTok{#>   Vertical_Distance_To_Hydrology Horizontal_Distance_To_Roadways}
\CommentTok{#> 1                              0                            1533}
\CommentTok{#> 2                            -38                            1290}
\CommentTok{#> 3                              8                            1890}
\CommentTok{#> 4                             88                            3932}
\CommentTok{#> 5                              1                            3901}
\CommentTok{#> 6                             10                            4810}
\CommentTok{#>   Hillshade_9am Hillshade_Noon Hillshade_3pm}
\CommentTok{#> 1           229            236           141}
\CommentTok{#> 2           211            193           111}
\CommentTok{#> 3           214            209           128}
\CommentTok{#> 4           221            195           100}
\CommentTok{#> 5           237            217           109}
\CommentTok{#> 6           182            237           194}
\CommentTok{#>   Horizontal_Distance_To_Fire_Points Wilderness_Area Soil_Type Cover_Type}
\CommentTok{#> 1                                459          area_0   type_22    class_1}
\CommentTok{#> 2                               1112          area_0   type_28    class_1}
\CommentTok{#> 3                               1001          area_2    type_9    class_2}
\CommentTok{#> 4                               2919          area_0   type_39    class_2}
\CommentTok{#> 5                               2859          area_0   type_22    class_7}
\CommentTok{#> 6                               1200          area_0   type_21    class_1}
\CommentTok{#> }
\CommentTok{#> [581012 rows x 13 columns]}
\NormalTok{splits <-}\StringTok{ }\KeywordTok{h2o.splitFrame}\NormalTok{(df, }\KeywordTok{c}\NormalTok{(}\FloatTok{0.6}\NormalTok{, }\FloatTok{0.2}\NormalTok{), }\DataTypeTok{seed=}\DecValTok{1234}\NormalTok{)}
\NormalTok{train  <-}\StringTok{ }\KeywordTok{h2o.assign}\NormalTok{(splits[[}\DecValTok{1}\NormalTok{]], }\StringTok{"train.hex"}\NormalTok{) }\CommentTok{# 60%}
\NormalTok{valid  <-}\StringTok{ }\KeywordTok{h2o.assign}\NormalTok{(splits[[}\DecValTok{2}\NormalTok{]], }\StringTok{"valid.hex"}\NormalTok{) }\CommentTok{# 20%}
\NormalTok{test   <-}\StringTok{ }\KeywordTok{h2o.assign}\NormalTok{(splits[[}\DecValTok{3}\NormalTok{]], }\StringTok{"test.hex"}\NormalTok{)  }\CommentTok{# 20%}
\end{Highlighting}
\end{Shaded}

Here's a scalable way to do scatter plots via binning (works for categorical and numeric columns) to get more familiar with the dataset.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#dev.new(noRStudioGD=FALSE) #direct plotting output to a new window}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)) }\CommentTok{# reset canvas}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{h2o.tabulate}\NormalTok{(df, }\StringTok{"Elevation"}\NormalTok{,                       }\StringTok{"Cover_Type"}\NormalTok{))}
\CommentTok{#> Registered S3 methods overwritten by 'ggplot2':}
\CommentTok{#>   method         from }
\CommentTok{#>   [.quosures     rlang}
\CommentTok{#>   c.quosures     rlang}
\CommentTok{#>   print.quosures rlang}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{h2o.tabulate}\NormalTok{(df, }\StringTok{"Horizontal_Distance_To_Roadways"}\NormalTok{, }\StringTok{"Cover_Type"}\NormalTok{))}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{h2o.tabulate}\NormalTok{(df, }\StringTok{"Soil_Type"}\NormalTok{,                       }\StringTok{"Cover_Type"}\NormalTok{))}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{h2o.tabulate}\NormalTok{(df, }\StringTok{"Horizontal_Distance_To_Roadways"}\NormalTok{, }\StringTok{"Elevation"}\NormalTok{ ))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{comparison-classification_regression_with_h2o_deep_learning_files/figure-latex/plot_cover_type-1} \includegraphics[width=0.7\linewidth]{comparison-classification_regression_with_h2o_deep_learning_files/figure-latex/plot_cover_type-2} \includegraphics[width=0.7\linewidth]{comparison-classification_regression_with_h2o_deep_learning_files/figure-latex/plot_cover_type-3} \includegraphics[width=0.7\linewidth]{comparison-classification_regression_with_h2o_deep_learning_files/figure-latex/plot_cover_type-4} \end{center}

\hypertarget{first-run-of-h2o-deep-learning}{%
\subsection{First Run of H2O Deep Learning}\label{first-run-of-h2o-deep-learning}}

Let's run our first Deep Learning model on the covtype dataset. We want to predict the \texttt{Cover\_Type} column, a categorical feature with 7 levels, and the Deep Learning model will be tasked to perform (multi-class) classification. It uses the other 12 predictors of the dataset, of which 10 are numerical, and 2 are categorical with a total of 44 levels. We can expect the Deep Learning model to have 56 input neurons (after automatic one-hot encoding).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{response <-}\StringTok{ "Cover_Type"}
\NormalTok{predictors <-}\StringTok{ }\KeywordTok{setdiff}\NormalTok{(}\KeywordTok{names}\NormalTok{(df), response)}
\NormalTok{predictors}
\CommentTok{#>  [1] "Elevation"                         }
\CommentTok{#>  [2] "Aspect"                            }
\CommentTok{#>  [3] "Slope"                             }
\CommentTok{#>  [4] "Horizontal_Distance_To_Hydrology"  }
\CommentTok{#>  [5] "Vertical_Distance_To_Hydrology"    }
\CommentTok{#>  [6] "Horizontal_Distance_To_Roadways"   }
\CommentTok{#>  [7] "Hillshade_9am"                     }
\CommentTok{#>  [8] "Hillshade_Noon"                    }
\CommentTok{#>  [9] "Hillshade_3pm"                     }
\CommentTok{#> [10] "Horizontal_Distance_To_Fire_Points"}
\CommentTok{#> [11] "Wilderness_Area"                   }
\CommentTok{#> [12] "Soil_Type"}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{train_df <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(train)}
\KeywordTok{str}\NormalTok{(train_df)}
\CommentTok{#> 'data.frame':    349015 obs. of  13 variables:}
\CommentTok{#>  $ Elevation                         : int  3136 3217 3119 2679 3261 2885 3227 2843 2853 2883 ...}
\CommentTok{#>  $ Aspect                            : int  32 80 293 48 322 26 32 12 124 177 ...}
\CommentTok{#>  $ Slope                             : int  20 13 13 7 13 9 6 18 12 9 ...}
\CommentTok{#>  $ Horizontal_Distance_To_Hydrology  : int  450 30 30 150 30 192 108 335 30 426 ...}
\CommentTok{#>  $ Vertical_Distance_To_Hydrology    : int  -38 1 10 24 5 38 13 50 -5 126 ...}
\CommentTok{#>  $ Horizontal_Distance_To_Roadways   : int  1290 3901 4810 1588 5701 3271 5542 2642 1485 2139 ...}
\CommentTok{#>  $ Hillshade_9am                     : int  211 237 182 223 186 216 219 199 240 225 ...}
\CommentTok{#>  $ Hillshade_Noon                    : int  193 217 237 224 226 220 227 201 231 246 ...}
\CommentTok{#>  $ Hillshade_3pm                     : int  111 109 194 136 180 140 145 135 119 153 ...}
\CommentTok{#>  $ Horizontal_Distance_To_Fire_Points: int  1112 2859 1200 6265 769 2643 765 1719 2497 713 ...}
\CommentTok{#>  $ Wilderness_Area                   : Factor w/ 4 levels "area_0","area_1",..: 1 1 1 1 1 1 1 3 3 3 ...}
\CommentTok{#>  $ Soil_Type                         : Factor w/ 40 levels "type_0","type_1",..: 22 16 15 4 15 22 15 27 12 25 ...}
\CommentTok{#>  $ Cover_Type                        : Factor w/ 7 levels "class_1","class_2",..: 1 7 1 2 1 2 1 2 1 2 ...}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{valid_df <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(valid)}
\KeywordTok{str}\NormalTok{(valid_df)}
\CommentTok{#> 'data.frame':    116018 obs. of  13 variables:}
\CommentTok{#>  $ Elevation                         : int  3066 2655 2902 2994 2697 2990 3237 2884 2972 2696 ...}
\CommentTok{#>  $ Aspect                            : int  124 28 304 61 93 59 135 71 100 169 ...}
\CommentTok{#>  $ Slope                             : int  5 14 22 9 9 12 14 9 4 10 ...}
\CommentTok{#>  $ Horizontal_Distance_To_Hydrology  : int  0 42 511 391 306 108 240 459 175 323 ...}
\CommentTok{#>  $ Vertical_Distance_To_Hydrology    : int  0 8 18 57 -2 10 -11 141 13 149 ...}
\CommentTok{#>  $ Horizontal_Distance_To_Roadways   : int  1533 1890 1273 4286 553 2190 1189 1214 5031 2452 ...}
\CommentTok{#>  $ Hillshade_9am                     : int  229 214 155 227 234 229 241 231 227 228 ...}
\CommentTok{#>  $ Hillshade_Noon                    : int  236 209 223 222 227 215 233 222 234 244 ...}
\CommentTok{#>  $ Hillshade_3pm                     : int  141 128 206 128 125 117 118 124 142 148 ...}
\CommentTok{#>  $ Horizontal_Distance_To_Fire_Points: int  459 1001 1347 1928 1716 1048 2748 1355 6198 1044 ...}
\CommentTok{#>  $ Wilderness_Area                   : Factor w/ 4 levels "area_0","area_1",..: 1 3 3 1 1 3 1 3 1 3 ...}
\CommentTok{#>  $ Soil_Type                         : Factor w/ 39 levels "type_0","type_1",..: 15 39 25 4 4 25 14 25 11 23 ...}
\CommentTok{#>  $ Cover_Type                        : Factor w/ 7 levels "class_1","class_2",..: 1 2 2 2 2 2 1 2 1 3 ...}
\end{Highlighting}
\end{Shaded}

To keep it fast, we only run for one epoch (one pass over the training data).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m1 <-}\StringTok{ }\KeywordTok{h2o.deeplearning}\NormalTok{(}
  \DataTypeTok{model_id=}\StringTok{"dl_model_first"}\NormalTok{, }
  \DataTypeTok{training_frame =}\NormalTok{ train, }
  \DataTypeTok{validation_frame =}\NormalTok{ valid,   }\CommentTok{## validation dataset: used for scoring and early stopping}
  \DataTypeTok{x =}\NormalTok{ predictors,}
  \DataTypeTok{y =}\NormalTok{ response,}
  \CommentTok{#activation="Rectifier",  ## default}
  \CommentTok{#hidden=c(200,200),       ## default: 2 hidden layers with 200 neurons each}
  \DataTypeTok{epochs =} \DecValTok{1}\NormalTok{,}
  \DataTypeTok{variable_importances=}\NormalTok{T    }\CommentTok{## not enabled by default}
\NormalTok{)}
\CommentTok{#> }
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|}\StringTok{                                                                 }\ErrorTok{|}\StringTok{   }\DecValTok{0}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|======}\StringTok{                                                           }\ErrorTok{|}\StringTok{  }\DecValTok{10}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=============}\StringTok{                                                    }\ErrorTok{|}\StringTok{  }\DecValTok{20}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|===================}\StringTok{                                              }\ErrorTok{|}\StringTok{  }\DecValTok{30}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==========================}\StringTok{                                       }\ErrorTok{|}\StringTok{  }\DecValTok{40}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|================================}\StringTok{                                 }\ErrorTok{|}\StringTok{  }\DecValTok{50}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=======================================}\StringTok{                          }\ErrorTok{|}\StringTok{  }\DecValTok{60}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=============================================}\StringTok{                    }\ErrorTok{|}\StringTok{  }\DecValTok{70}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|====================================================}\StringTok{             }\ErrorTok{|}\StringTok{  }\DecValTok{80}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==========================================================}\StringTok{       }\ErrorTok{|}\StringTok{  }\DecValTok{90}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=================================================================|}\StringTok{ }\DecValTok{100}\NormalTok{%}
\KeywordTok{summary}\NormalTok{(m1)}
\CommentTok{#> Model Details:}
\CommentTok{#> ==============}
\CommentTok{#> }
\CommentTok{#> H2OMultinomialModel: deeplearning}
\CommentTok{#> Model Key:  dl_model_first }
\CommentTok{#> Status of Neuron Layers: predicting Cover_Type, 7-class classification, multinomial distribution, CrossEntropy loss, 53,007 weights/biases, 634.3 KB, 383,527 training samples, mini-batch size 1}
\CommentTok{#>   layer units      type dropout       l1       l2 mean_rate rate_rms}
\CommentTok{#> 1     1    56     Input  0.00 %       NA       NA        NA       NA}
\CommentTok{#> 2     2   200 Rectifier  0.00 % 0.000000 0.000000  0.053201 0.216052}
\CommentTok{#> 3     3   200 Rectifier  0.00 % 0.000000 0.000000  0.010359 0.008217}
\CommentTok{#> 4     4     7   Softmax      NA 0.000000 0.000000  0.098231 0.258950}
\CommentTok{#>   momentum mean_weight weight_rms mean_bias bias_rms}
\CommentTok{#> 1       NA          NA         NA        NA       NA}
\CommentTok{#> 2 0.000000   -0.011749   0.117278 -0.011060 0.115161}
\CommentTok{#> 3 0.000000   -0.024653   0.119464  0.614590 0.471928}
\CommentTok{#> 4 0.000000   -0.409800   0.505029 -0.575337 0.206118}
\CommentTok{#> }
\CommentTok{#> H2OMultinomialMetrics: deeplearning}
\CommentTok{#> ** Reported on training data. **}
\CommentTok{#> ** Metrics reported on temporary training frame with 9864 samples **}
\CommentTok{#> }
\CommentTok{#> Training Set Metrics: }
\CommentTok{#> =====================}
\CommentTok{#> }
\CommentTok{#> MSE: (Extract with `h2o.mse`) 0.126}
\CommentTok{#> RMSE: (Extract with `h2o.rmse`) 0.355}
\CommentTok{#> Logloss: (Extract with `h2o.logloss`) 0.411}
\CommentTok{#> Mean Per-Class Error: 0.307}
\CommentTok{#> Confusion Matrix: Extract with `h2o.confusionMatrix(<model>,train = TRUE)`)}
\CommentTok{#> =========================================================================}
\CommentTok{#> Confusion Matrix: Row labels: Actual class; Column labels: Predicted class}
\CommentTok{#>         class_1 class_2 class_3 class_4 class_5 class_6 class_7  Error}
\CommentTok{#> class_1    2806     739       1       0      10       1      17 0.2149}
\CommentTok{#> class_2     338    4417      27       0      20      47       0 0.0891}
\CommentTok{#> class_3       0      23     487       5       3      78       0 0.1829}
\CommentTok{#> class_4       0       0      14      26       0       4       0 0.4091}
\CommentTok{#> class_5       4      92       2       0      69       0       0 0.5868}
\CommentTok{#> class_6       0      35      62       4       0     179       0 0.3607}
\CommentTok{#> class_7      94      15       0       0       0       0     245 0.3079}
\CommentTok{#> Totals     3242    5321     593      35     102     309     262 0.1658}
\CommentTok{#>                    Rate}
\CommentTok{#> class_1 =   768 / 3,574}
\CommentTok{#> class_2 =   432 / 4,849}
\CommentTok{#> class_3 =     109 / 596}
\CommentTok{#> class_4 =       18 / 44}
\CommentTok{#> class_5 =      98 / 167}
\CommentTok{#> class_6 =     101 / 280}
\CommentTok{#> class_7 =     109 / 354}
\CommentTok{#> Totals  = 1,635 / 9,864}
\CommentTok{#> }
\CommentTok{#> Hit Ratio Table: Extract with `h2o.hit_ratio_table(<model>,train = TRUE)`}
\CommentTok{#> =======================================================================}
\CommentTok{#> Top-7 Hit Ratios: }
\CommentTok{#>   k hit_ratio}
\CommentTok{#> 1 1  0.834246}
\CommentTok{#> 2 2  0.983779}
\CommentTok{#> 3 3  0.998479}
\CommentTok{#> 4 4  0.999595}
\CommentTok{#> 5 5  1.000000}
\CommentTok{#> 6 6  1.000000}
\CommentTok{#> 7 7  1.000000}
\CommentTok{#> }
\CommentTok{#> }
\CommentTok{#> H2OMultinomialMetrics: deeplearning}
\CommentTok{#> ** Reported on validation data. **}
\CommentTok{#> ** Metrics reported on full validation frame **}
\CommentTok{#> }
\CommentTok{#> Validation Set Metrics: }
\CommentTok{#> =====================}
\CommentTok{#> }
\CommentTok{#> Extract validation frame with `h2o.getFrame("valid.hex")`}
\CommentTok{#> MSE: (Extract with `h2o.mse`) 0.131}
\CommentTok{#> RMSE: (Extract with `h2o.rmse`) 0.362}
\CommentTok{#> Logloss: (Extract with `h2o.logloss`) 0.425}
\CommentTok{#> Mean Per-Class Error: 0.33}
\CommentTok{#> Confusion Matrix: Extract with `h2o.confusionMatrix(<model>,valid = TRUE)`)}
\CommentTok{#> =========================================================================}
\CommentTok{#> Confusion Matrix: Row labels: Actual class; Column labels: Predicted class}
\CommentTok{#>         class_1 class_2 class_3 class_4 class_5 class_6 class_7  Error}
\CommentTok{#> class_1   32950    9250       1       0     102      18     179 0.2247}
\CommentTok{#> class_2    4033   51142     345       4     317     530       9 0.0929}
\CommentTok{#> class_3       0     393    5642      65      15    1028       0 0.2101}
\CommentTok{#> class_4       0       0     202     302       0      58       0 0.4626}
\CommentTok{#> class_5      48    1043      53       0     715      11       0 0.6176}
\CommentTok{#> class_6      10     480     742      26       5    2201       0 0.3646}
\CommentTok{#> class_7    1236     138       0       0       0       0    2725 0.3352}
\CommentTok{#> Totals    38277   62446    6985     397    1154    3846    2913 0.1753}
\CommentTok{#>                       Rate}
\CommentTok{#> class_1 =   9,550 / 42,500}
\CommentTok{#> class_2 =   5,238 / 56,380}
\CommentTok{#> class_3 =    1,501 / 7,143}
\CommentTok{#> class_4 =        260 / 562}
\CommentTok{#> class_5 =    1,155 / 1,870}
\CommentTok{#> class_6 =    1,263 / 3,464}
\CommentTok{#> class_7 =    1,374 / 4,099}
\CommentTok{#> Totals  = 20,341 / 116,018}
\CommentTok{#> }
\CommentTok{#> Hit Ratio Table: Extract with `h2o.hit_ratio_table(<model>,valid = TRUE)`}
\CommentTok{#> =======================================================================}
\CommentTok{#> Top-7 Hit Ratios: }
\CommentTok{#>   k hit_ratio}
\CommentTok{#> 1 1  0.824674}
\CommentTok{#> 2 2  0.982830}
\CommentTok{#> 3 3  0.998043}
\CommentTok{#> 4 4  0.999647}
\CommentTok{#> 5 5  0.999991}
\CommentTok{#> 6 6  0.999991}
\CommentTok{#> 7 7  1.000000}
\CommentTok{#> }
\CommentTok{#> }
\CommentTok{#> }
\CommentTok{#> }
\CommentTok{#> Scoring History: }
\CommentTok{#>             timestamp   duration training_speed  epochs iterations}
\CommentTok{#> 1 2019-09-18 16:05:54  0.000 sec             NA 0.00000          0}
\CommentTok{#> 2 2019-09-18 16:06:00  7.612 sec   6138 obs/sec 0.10136          1}
\CommentTok{#> 3 2019-09-18 16:06:19 26.612 sec   9090 obs/sec 0.60059          6}
\CommentTok{#> 4 2019-09-18 16:06:35 42.982 sec  10173 obs/sec 1.09888         11}
\CommentTok{#>         samples training_rmse training_logloss training_r2}
\CommentTok{#> 1      0.000000            NA               NA          NA}
\CommentTok{#> 2  35375.000000       0.43680          0.59573     0.90174}
\CommentTok{#> 3 209616.000000       0.39819          0.49481     0.91834}
\CommentTok{#> 4 383527.000000       0.35493          0.41107     0.93512}
\CommentTok{#>   training_classification_error validation_rmse validation_logloss}
\CommentTok{#> 1                            NA              NA                 NA}
\CommentTok{#> 2                       0.25466         0.44181            0.60760}
\CommentTok{#> 3                       0.21665         0.40487            0.50994}
\CommentTok{#> 4                       0.16575         0.36237            0.42453}
\CommentTok{#>   validation_r2 validation_classification_error}
\CommentTok{#> 1            NA                              NA}
\CommentTok{#> 2       0.89994                         0.26078}
\CommentTok{#> 3       0.91598                         0.22440}
\CommentTok{#> 4       0.93269                         0.17533}
\CommentTok{#> }
\CommentTok{#> Variable Importances: (Extract with `h2o.varimp`) }
\CommentTok{#> =================================================}
\CommentTok{#> }
\CommentTok{#> Variable Importances: }
\CommentTok{#>                             variable relative_importance scaled_importance}
\CommentTok{#> 1             Wilderness_Area.area_0            1.000000          1.000000}
\CommentTok{#> 2                          Elevation            0.910638          0.910638}
\CommentTok{#> 3    Horizontal_Distance_To_Roadways            0.902916          0.902916}
\CommentTok{#> 4 Horizontal_Distance_To_Fire_Points            0.850998          0.850998}
\CommentTok{#> 5             Wilderness_Area.area_2            0.832705          0.832705}
\CommentTok{#>   percentage}
\CommentTok{#> 1   0.033119}
\CommentTok{#> 2   0.030160}
\CommentTok{#> 3   0.029904}
\CommentTok{#> 4   0.028184}
\CommentTok{#> 5   0.027579}
\CommentTok{#> }
\CommentTok{#> ---}
\CommentTok{#>                       variable relative_importance scaled_importance}
\CommentTok{#> 51            Soil_Type.type_5            0.433003          0.433003}
\CommentTok{#> 52                       Slope            0.376897          0.376897}
\CommentTok{#> 53               Hillshade_3pm            0.361976          0.361976}
\CommentTok{#> 54                      Aspect            0.269958          0.269958}
\CommentTok{#> 55       Soil_Type.missing(NA)            0.000000          0.000000}
\CommentTok{#> 56 Wilderness_Area.missing(NA)            0.000000          0.000000}
\CommentTok{#>    percentage}
\CommentTok{#> 51   0.014341}
\CommentTok{#> 52   0.012483}
\CommentTok{#> 53   0.011988}
\CommentTok{#> 54   0.008941}
\CommentTok{#> 55   0.000000}
\CommentTok{#> 56   0.000000}
\end{Highlighting}
\end{Shaded}

Inspect the model in Flow for more information about model building etc. by issuing a cell with the content getModel ``dl\_model\_first'', and pressing Ctrl-Enter.

\hypertarget{variable-importances}{%
\subsection{Variable Importances}\label{variable-importances}}

Variable importances for Neural Network models are notoriously difficult to compute, and there are many pitfalls. H2O Deep Learning has implemented the method of Gedeon, and returns relative variable importances in descending order of importance.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(}\KeywordTok{as.data.frame}\NormalTok{(}\KeywordTok{h2o.varimp}\NormalTok{(m1)))}
\CommentTok{#>                             variable relative_importance scaled_importance}
\CommentTok{#> 1             Wilderness_Area.area_0               1.000             1.000}
\CommentTok{#> 2                          Elevation               0.911             0.911}
\CommentTok{#> 3    Horizontal_Distance_To_Roadways               0.903             0.903}
\CommentTok{#> 4 Horizontal_Distance_To_Fire_Points               0.851             0.851}
\CommentTok{#> 5             Wilderness_Area.area_2               0.833             0.833}
\CommentTok{#> 6             Wilderness_Area.area_1               0.737             0.737}
\CommentTok{#>   percentage}
\CommentTok{#> 1     0.0331}
\CommentTok{#> 2     0.0302}
\CommentTok{#> 3     0.0299}
\CommentTok{#> 4     0.0282}
\CommentTok{#> 5     0.0276}
\CommentTok{#> 6     0.0244}
\end{Highlighting}
\end{Shaded}

\hypertarget{early-stopping}{%
\subsection{Early Stopping}\label{early-stopping}}

Now we run another, smaller network, and we let it stop automatically once the misclassification rate converges (specifically, if the moving average of length 2 does not improve by at least 1\% for 2 consecutive scoring events). We also sample the validation set to 10,000 rows for faster scoring.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m2 <-}\StringTok{ }\KeywordTok{h2o.deeplearning}\NormalTok{(}
  \DataTypeTok{model_id=}\StringTok{"dl_model_faster"}\NormalTok{, }
  \DataTypeTok{training_frame=}\NormalTok{train, }
  \DataTypeTok{validation_frame=}\NormalTok{valid,}
  \DataTypeTok{x=}\NormalTok{predictors,}
  \DataTypeTok{y=}\NormalTok{response,}
  \DataTypeTok{hidden=}\KeywordTok{c}\NormalTok{(}\DecValTok{32}\NormalTok{,}\DecValTok{32}\NormalTok{,}\DecValTok{32}\NormalTok{),                  }\CommentTok{## small network, runs faster}
  \DataTypeTok{epochs=}\DecValTok{1000000}\NormalTok{,                      }\CommentTok{## hopefully converges earlier...}
  \DataTypeTok{score_validation_samples=}\DecValTok{10000}\NormalTok{,      }\CommentTok{## sample the validation dataset (faster)}
  \DataTypeTok{stopping_rounds=}\DecValTok{2}\NormalTok{,}
  \DataTypeTok{stopping_metric=}\StringTok{"misclassification"}\NormalTok{, }\CommentTok{## could be "MSE","logloss","r2"}
  \DataTypeTok{stopping_tolerance=}\FloatTok{0.01}
\NormalTok{)}
\CommentTok{#> }
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|}\StringTok{                                                                 }\ErrorTok{|}\StringTok{   }\DecValTok{0}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=================================================================|}\StringTok{ }\DecValTok{100}\NormalTok{%}
\KeywordTok{summary}\NormalTok{(m2)}
\CommentTok{#> Model Details:}
\CommentTok{#> ==============}
\CommentTok{#> }
\CommentTok{#> H2OMultinomialModel: deeplearning}
\CommentTok{#> Model Key:  dl_model_faster }
\CommentTok{#> Status of Neuron Layers: predicting Cover_Type, 7-class classification, multinomial distribution, CrossEntropy loss, 4,167 weights/biases, 59.0 KB, 4,598,167 training samples, mini-batch size 1}
\CommentTok{#>   layer units      type dropout       l1       l2 mean_rate rate_rms}
\CommentTok{#> 1     1    56     Input  0.00 %       NA       NA        NA       NA}
\CommentTok{#> 2     2    32 Rectifier  0.00 % 0.000000 0.000000  0.043019 0.202104}
\CommentTok{#> 3     3    32 Rectifier  0.00 % 0.000000 0.000000  0.000429 0.000214}
\CommentTok{#> 4     4    32 Rectifier  0.00 % 0.000000 0.000000  0.000959 0.002080}
\CommentTok{#> 5     5     7   Softmax      NA 0.000000 0.000000  0.109826 0.286061}
\CommentTok{#>   momentum mean_weight weight_rms mean_bias bias_rms}
\CommentTok{#> 1       NA          NA         NA        NA       NA}
\CommentTok{#> 2 0.000000   -0.002224   0.263971  0.362323 0.224006}
\CommentTok{#> 3 0.000000   -0.055808   0.339650  0.712228 0.500067}
\CommentTok{#> 4 0.000000   -0.019305   0.416608  0.565933 0.755517}
\CommentTok{#> 5 0.000000   -2.929996   2.806343 -2.028375 0.757954}
\CommentTok{#> }
\CommentTok{#> H2OMultinomialMetrics: deeplearning}
\CommentTok{#> ** Reported on training data. **}
\CommentTok{#> ** Metrics reported on temporary training frame with 9983 samples **}
\CommentTok{#> }
\CommentTok{#> Training Set Metrics: }
\CommentTok{#> =====================}
\CommentTok{#> }
\CommentTok{#> MSE: (Extract with `h2o.mse`) 0.117}
\CommentTok{#> RMSE: (Extract with `h2o.rmse`) 0.342}
\CommentTok{#> Logloss: (Extract with `h2o.logloss`) 0.385}
\CommentTok{#> Mean Per-Class Error: 0.318}
\CommentTok{#> Confusion Matrix: Extract with `h2o.confusionMatrix(<model>,train = TRUE)`)}
\CommentTok{#> =========================================================================}
\CommentTok{#> Confusion Matrix: Row labels: Actual class; Column labels: Predicted class}
\CommentTok{#>         class_1 class_2 class_3 class_4 class_5 class_6 class_7  Error}
\CommentTok{#> class_1    3012     606       2       0       1       1      27 0.1746}
\CommentTok{#> class_2     445    4402      25       0      13      12       0 0.1011}
\CommentTok{#> class_3       1      42     485       2       1      44       0 0.1565}
\CommentTok{#> class_4       0       0      16      32       0       9       0 0.4386}
\CommentTok{#> class_5       8      95       5       0      49       4       0 0.6957}
\CommentTok{#> class_6       1      48      91       0       1     163       0 0.4638}
\CommentTok{#> class_7      61       6       0       0       0       0     273 0.1971}
\CommentTok{#> Totals     3528    5199     624      34      65     233     300 0.1570}
\CommentTok{#>                    Rate}
\CommentTok{#> class_1 =   637 / 3,649}
\CommentTok{#> class_2 =   495 / 4,897}
\CommentTok{#> class_3 =      90 / 575}
\CommentTok{#> class_4 =       25 / 57}
\CommentTok{#> class_5 =     112 / 161}
\CommentTok{#> class_6 =     141 / 304}
\CommentTok{#> class_7 =      67 / 340}
\CommentTok{#> Totals  = 1,567 / 9,983}
\CommentTok{#> }
\CommentTok{#> Hit Ratio Table: Extract with `h2o.hit_ratio_table(<model>,train = TRUE)`}
\CommentTok{#> =======================================================================}
\CommentTok{#> Top-7 Hit Ratios: }
\CommentTok{#>   k hit_ratio}
\CommentTok{#> 1 1  0.843033}
\CommentTok{#> 2 2  0.985275}
\CommentTok{#> 3 3  0.997496}
\CommentTok{#> 4 4  0.999699}
\CommentTok{#> 5 5  1.000000}
\CommentTok{#> 6 6  1.000000}
\CommentTok{#> 7 7  1.000000}
\CommentTok{#> }
\CommentTok{#> }
\CommentTok{#> H2OMultinomialMetrics: deeplearning}
\CommentTok{#> ** Reported on validation data. **}
\CommentTok{#> ** Metrics reported on temporary validation frame with 9914 samples **}
\CommentTok{#> }
\CommentTok{#> Validation Set Metrics: }
\CommentTok{#> =====================}
\CommentTok{#> }
\CommentTok{#> MSE: (Extract with `h2o.mse`) 0.119}
\CommentTok{#> RMSE: (Extract with `h2o.rmse`) 0.345}
\CommentTok{#> Logloss: (Extract with `h2o.logloss`) 0.398}
\CommentTok{#> Mean Per-Class Error: 0.321}
\CommentTok{#> Confusion Matrix: Extract with `h2o.confusionMatrix(<model>,valid = TRUE)`)}
\CommentTok{#> =========================================================================}
\CommentTok{#> Confusion Matrix: Row labels: Actual class; Column labels: Predicted class}
\CommentTok{#>         class_1 class_2 class_3 class_4 class_5 class_6 class_7  Error}
\CommentTok{#> class_1    3055     632       0       0       1       2      32 0.1792}
\CommentTok{#> class_2     427    4287      30       0      10       9       0 0.0999}
\CommentTok{#> class_3       3      38     475       2       0      53       0 0.1681}
\CommentTok{#> class_4       0       0      26      24       0       3       0 0.5472}
\CommentTok{#> class_5      10      85       2       0      52       2       0 0.6556}
\CommentTok{#> class_6       3      53      75       0       0     177       0 0.4253}
\CommentTok{#> class_7      55       5       0       0       0       0     286 0.1734}
\CommentTok{#> Totals     3553    5100     608      26      63     246     318 0.1572}
\CommentTok{#>                    Rate}
\CommentTok{#> class_1 =   667 / 3,722}
\CommentTok{#> class_2 =   476 / 4,763}
\CommentTok{#> class_3 =      96 / 571}
\CommentTok{#> class_4 =       29 / 53}
\CommentTok{#> class_5 =      99 / 151}
\CommentTok{#> class_6 =     131 / 308}
\CommentTok{#> class_7 =      60 / 346}
\CommentTok{#> Totals  = 1,558 / 9,914}
\CommentTok{#> }
\CommentTok{#> Hit Ratio Table: Extract with `h2o.hit_ratio_table(<model>,valid = TRUE)`}
\CommentTok{#> =======================================================================}
\CommentTok{#> Top-7 Hit Ratios: }
\CommentTok{#>   k hit_ratio}
\CommentTok{#> 1 1  0.842849}
\CommentTok{#> 2 2  0.984567}
\CommentTok{#> 3 3  0.997882}
\CommentTok{#> 4 4  0.999597}
\CommentTok{#> 5 5  1.000000}
\CommentTok{#> 6 6  1.000000}
\CommentTok{#> 7 7  1.000000}
\CommentTok{#> }
\CommentTok{#> }
\CommentTok{#> }
\CommentTok{#> }
\CommentTok{#> Scoring History: }
\CommentTok{#>              timestamp   duration training_speed   epochs iterations}
\CommentTok{#> 1  2019-09-18 16:06:37  0.000 sec             NA  0.00000          0}
\CommentTok{#> 2  2019-09-18 16:06:39  1.448 sec  75582 obs/sec  0.28586          1}
\CommentTok{#> 3  2019-09-18 16:06:44  6.663 sec  92486 obs/sec  1.71768          6}
\CommentTok{#> 4  2019-09-18 16:06:49 11.922 sec  93953 obs/sec  3.15012         11}
\CommentTok{#> 5  2019-09-18 16:06:55 17.361 sec  99338 obs/sec  4.86878         17}
\CommentTok{#> 6  2019-09-18 16:07:00 23.259 sec 100074 obs/sec  6.58486         23}
\CommentTok{#> 7  2019-09-18 16:07:06 28.996 sec 101143 obs/sec  8.30562         29}
\CommentTok{#> 8  2019-09-18 16:07:11 34.122 sec 100745 obs/sec  9.74073         34}
\CommentTok{#> 9  2019-09-18 16:07:17 39.718 sec 101739 obs/sec 11.45787         40}
\CommentTok{#> 10 2019-09-18 16:07:22 44.953 sec 103308 obs/sec 13.17470         46}
\CommentTok{#> 11 2019-09-18 16:07:22 45.006 sec 103301 obs/sec 13.17470         46}
\CommentTok{#>           samples training_rmse training_logloss training_r2}
\CommentTok{#> 1        0.000000            NA               NA          NA}
\CommentTok{#> 2    99769.000000       0.43523          0.59986     0.90174}
\CommentTok{#> 3   599495.000000       0.38618          0.47742     0.92264}
\CommentTok{#> 4  1099438.000000       0.36709          0.43553     0.93010}
\CommentTok{#> 5  1699276.000000       0.35992          0.42033     0.93281}
\CommentTok{#> 6  2298214.000000       0.34863          0.40001     0.93695}
\CommentTok{#> 7  2898786.000000       0.34210          0.38546     0.93929}
\CommentTok{#> 8  3399660.000000       0.34143          0.38734     0.93953}
\CommentTok{#> 9  3998967.000000       0.34014          0.38340     0.93999}
\CommentTok{#> 10 4598167.000000       0.34099          0.38570     0.93969}
\CommentTok{#> 11 4598167.000000       0.34210          0.38546     0.93929}
\CommentTok{#>    training_classification_error validation_rmse validation_logloss}
\CommentTok{#> 1                             NA              NA                 NA}
\CommentTok{#> 2                        0.25103         0.43601            0.60595}
\CommentTok{#> 3                        0.20114         0.38781            0.48037}
\CommentTok{#> 4                        0.17981         0.36861            0.43709}
\CommentTok{#> 5                        0.17219         0.36435            0.42946}
\CommentTok{#> 6                        0.16398         0.35273            0.40957}
\CommentTok{#> 7                        0.15697         0.34521            0.39827}
\CommentTok{#> 8                        0.15777         0.34679            0.40252}
\CommentTok{#> 9                        0.15637         0.34528            0.39537}
\CommentTok{#> 10                       0.15917         0.34271            0.39400}
\CommentTok{#> 11                       0.15697         0.34521            0.39827}
\CommentTok{#>    validation_r2 validation_classification_error}
\CommentTok{#> 1             NA                              NA}
\CommentTok{#> 2        0.90299                         0.25136}
\CommentTok{#> 3        0.92326                         0.20345}
\CommentTok{#> 4        0.93067                         0.18186}
\CommentTok{#> 5        0.93226                         0.17823}
\CommentTok{#> 6        0.93651                         0.16724}
\CommentTok{#> 7        0.93919                         0.15715}
\CommentTok{#> 8        0.93863                         0.16068}
\CommentTok{#> 9        0.93916                         0.16038}
\CommentTok{#> 10       0.94006                         0.15877}
\CommentTok{#> 11       0.93919                         0.15715}
\CommentTok{#> }
\CommentTok{#> Variable Importances: (Extract with `h2o.varimp`) }
\CommentTok{#> =================================================}
\CommentTok{#> }
\CommentTok{#> Variable Importances: }
\CommentTok{#>                             variable relative_importance scaled_importance}
\CommentTok{#> 1                          Elevation            1.000000          1.000000}
\CommentTok{#> 2    Horizontal_Distance_To_Roadways            0.921223          0.921223}
\CommentTok{#> 3 Horizontal_Distance_To_Fire_Points            0.888336          0.888336}
\CommentTok{#> 4             Wilderness_Area.area_3            0.876854          0.876854}
\CommentTok{#> 5                  Soil_Type.type_32            0.852413          0.852413}
\CommentTok{#>   percentage}
\CommentTok{#> 1   0.033561}
\CommentTok{#> 2   0.030917}
\CommentTok{#> 3   0.029814}
\CommentTok{#> 4   0.029428}
\CommentTok{#> 5   0.028608}
\CommentTok{#> }
\CommentTok{#> ---}
\CommentTok{#>                            variable relative_importance scaled_importance}
\CommentTok{#> 51   Vertical_Distance_To_Hydrology            0.353224          0.353224}
\CommentTok{#> 52 Horizontal_Distance_To_Hydrology            0.313158          0.313158}
\CommentTok{#> 53                            Slope            0.218501          0.218501}
\CommentTok{#> 54                           Aspect            0.076046          0.076046}
\CommentTok{#> 55            Soil_Type.missing(NA)            0.000000          0.000000}
\CommentTok{#> 56      Wilderness_Area.missing(NA)            0.000000          0.000000}
\CommentTok{#>    percentage}
\CommentTok{#> 51   0.011855}
\CommentTok{#> 52   0.010510}
\CommentTok{#> 53   0.007333}
\CommentTok{#> 54   0.002552}
\CommentTok{#> 55   0.000000}
\CommentTok{#> 56   0.000000}
\KeywordTok{plot}\NormalTok{(m2)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{comparison-classification_regression_with_h2o_deep_learning_files/figure-latex/dl_ctype_early_stopping-1} \end{center}

\hypertarget{adaptive-learning-rate}{%
\subsection{Adaptive Learning Rate}\label{adaptive-learning-rate}}

By default, H2O Deep Learning uses an adaptive learning rate (ADADELTA) for its stochastic gradient descent optimization. There are only two tuning parameters for this method: rho and epsilon, which balance the global and local search efficiencies. rho is the similarity to prior weight updates (similar to momentum), and epsilon is a parameter that prevents the optimization to get stuck in local optima. Defaults are rho=0.99 and epsilon=1e-8. For cases where convergence speed is very important, it might make sense to perform a few runs to optimize these two parameters (e.g., with rho in c(0.9,0.95,0.99,0.999) and epsilon in c(1e-10,1e-8,1e-6,1e-4)). Of course, as always with grid searches, caution has to be applied when extrapolating grid search results to a different parameter regime (e.g., for more epochs or different layer topologies or activation functions, etc.).

If adaptive\_rate is disabled, several manual learning rate parameters become important: rate, rate\_annealing, rate\_decay, momentum\_start, momentum\_ramp, momentum\_stable and \texttt{nesterov\_accelerated\_gradient}, the discussion of which we leave to H2O Deep Learning booklet.

\hypertarget{tuning}{%
\subsection{Tuning}\label{tuning}}

With some tuning, it is possible to obtain less than 10\% test set error rate in about one minute. Error rates of below 5\% are possible with larger models. Note that deep tree methods can be more effective for this dataset than Deep Learning, as they directly partition the space into sectors, which seems to be needed here.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m3 <-}\StringTok{ }\KeywordTok{h2o.deeplearning}\NormalTok{(}
  \DataTypeTok{model_id=}\StringTok{"dl_model_tuned"}\NormalTok{, }
  \DataTypeTok{training_frame=}\NormalTok{train, }
  \DataTypeTok{validation_frame=}\NormalTok{valid, }
  \DataTypeTok{x=}\NormalTok{predictors, }
  \DataTypeTok{y=}\NormalTok{response, }
  \DataTypeTok{overwrite_with_best_model=}\NormalTok{F,    }\CommentTok{## Return final model after 10 epochs, even if not the best}
  \DataTypeTok{hidden=}\KeywordTok{c}\NormalTok{(}\DecValTok{128}\NormalTok{,}\DecValTok{128}\NormalTok{,}\DecValTok{128}\NormalTok{),          }\CommentTok{## more hidden layers -> more complex interactions}
  \DataTypeTok{epochs=}\DecValTok{10}\NormalTok{,                      }\CommentTok{## to keep it short enough}
  \DataTypeTok{score_validation_samples=}\DecValTok{10000}\NormalTok{, }\CommentTok{## downsample validation set for faster scoring}
  \DataTypeTok{score_duty_cycle=}\FloatTok{0.025}\NormalTok{,         }\CommentTok{## don't score more than 2.5% of the wall time}
  \DataTypeTok{adaptive_rate=}\NormalTok{F,                }\CommentTok{## manually tuned learning rate}
  \DataTypeTok{rate=}\FloatTok{0.01}\NormalTok{, }
  \DataTypeTok{rate_annealing=}\FloatTok{2e-6}\NormalTok{,            }
  \DataTypeTok{momentum_start=}\FloatTok{0.2}\NormalTok{,             }\CommentTok{## manually tuned momentum}
  \DataTypeTok{momentum_stable=}\FloatTok{0.4}\NormalTok{, }
  \DataTypeTok{momentum_ramp=}\FloatTok{1e7}\NormalTok{, }
  \DataTypeTok{l1=}\FloatTok{1e-5}\NormalTok{,                        }\CommentTok{## add some L1/L2 regularization}
  \DataTypeTok{l2=}\FloatTok{1e-5}\NormalTok{,}
  \DataTypeTok{max_w2=}\DecValTok{10}                       \CommentTok{## helps stability for Rectifier}
\NormalTok{) }
\CommentTok{#> }
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|}\StringTok{                                                                 }\ErrorTok{|}\StringTok{   }\DecValTok{0}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==}\StringTok{                                                               }\ErrorTok{|}\StringTok{   }\DecValTok{3}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|====}\StringTok{                                                             }\ErrorTok{|}\StringTok{   }\DecValTok{6}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|======}\StringTok{                                                           }\ErrorTok{|}\StringTok{   }\DecValTok{9}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=======}\StringTok{                                                          }\ErrorTok{|}\StringTok{  }\DecValTok{11}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=========}\StringTok{                                                        }\ErrorTok{|}\StringTok{  }\DecValTok{14}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|===========}\StringTok{                                                      }\ErrorTok{|}\StringTok{  }\DecValTok{17}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=============}\StringTok{                                                    }\ErrorTok{|}\StringTok{  }\DecValTok{20}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|===============}\StringTok{                                                  }\ErrorTok{|}\StringTok{  }\DecValTok{23}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=================}\StringTok{                                                }\ErrorTok{|}\StringTok{  }\DecValTok{26}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|===================}\StringTok{                                              }\ErrorTok{|}\StringTok{  }\DecValTok{29}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|====================}\StringTok{                                             }\ErrorTok{|}\StringTok{  }\DecValTok{32}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|======================}\StringTok{                                           }\ErrorTok{|}\StringTok{  }\DecValTok{34}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|========================}\StringTok{                                         }\ErrorTok{|}\StringTok{  }\DecValTok{37}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==========================}\StringTok{                                       }\ErrorTok{|}\StringTok{  }\DecValTok{40}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|============================}\StringTok{                                     }\ErrorTok{|}\StringTok{  }\DecValTok{43}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==============================}\StringTok{                                   }\ErrorTok{|}\StringTok{  }\DecValTok{46}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|================================}\StringTok{                                 }\ErrorTok{|}\StringTok{  }\DecValTok{49}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==================================}\StringTok{                               }\ErrorTok{|}\StringTok{  }\DecValTok{52}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|===================================}\StringTok{                              }\ErrorTok{|}\StringTok{  }\DecValTok{54}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=====================================}\StringTok{                            }\ErrorTok{|}\StringTok{  }\DecValTok{57}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=======================================}\StringTok{                          }\ErrorTok{|}\StringTok{  }\DecValTok{60}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=========================================}\StringTok{                        }\ErrorTok{|}\StringTok{  }\DecValTok{63}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|===========================================}\StringTok{                      }\ErrorTok{|}\StringTok{  }\DecValTok{66}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=============================================}\StringTok{                    }\ErrorTok{|}\StringTok{  }\DecValTok{69}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|===============================================}\StringTok{                  }\ErrorTok{|}\StringTok{  }\DecValTok{72}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|================================================}\StringTok{                 }\ErrorTok{|}\StringTok{  }\DecValTok{74}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==================================================}\StringTok{               }\ErrorTok{|}\StringTok{  }\DecValTok{77}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|====================================================}\StringTok{             }\ErrorTok{|}\StringTok{  }\DecValTok{80}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|======================================================}\StringTok{           }\ErrorTok{|}\StringTok{  }\DecValTok{83}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|========================================================}\StringTok{         }\ErrorTok{|}\StringTok{  }\DecValTok{86}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==========================================================}\StringTok{       }\ErrorTok{|}\StringTok{  }\DecValTok{89}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|============================================================}\StringTok{     }\ErrorTok{|}\StringTok{  }\DecValTok{92}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=============================================================}\StringTok{    }\ErrorTok{|}\StringTok{  }\DecValTok{95}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|===============================================================}\StringTok{  }\ErrorTok{|}\StringTok{  }\DecValTok{97}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=================================================================|}\StringTok{ }\DecValTok{100}\NormalTok{%}
\KeywordTok{summary}\NormalTok{(m3)}
\CommentTok{#> Model Details:}
\CommentTok{#> ==============}
\CommentTok{#> }
\CommentTok{#> H2OMultinomialModel: deeplearning}
\CommentTok{#> Model Key:  dl_model_tuned }
\CommentTok{#> Status of Neuron Layers: predicting Cover_Type, 7-class classification, multinomial distribution, CrossEntropy loss, 41,223 weights/biases, 334.1 KB, 3,502,089 training samples, mini-batch size 1}
\CommentTok{#>   layer units      type dropout       l1       l2 mean_rate rate_rms}
\CommentTok{#> 1     1    56     Input  0.00 %       NA       NA        NA       NA}
\CommentTok{#> 2     2   128 Rectifier  0.00 % 0.000010 0.000010  0.001249 0.000000}
\CommentTok{#> 3     3   128 Rectifier  0.00 % 0.000010 0.000010  0.001249 0.000000}
\CommentTok{#> 4     4   128 Rectifier  0.00 % 0.000010 0.000010  0.001249 0.000000}
\CommentTok{#> 5     5     7   Softmax      NA 0.000010 0.000010  0.001249 0.000000}
\CommentTok{#>   momentum mean_weight weight_rms mean_bias bias_rms}
\CommentTok{#> 1       NA          NA         NA        NA       NA}
\CommentTok{#> 2 0.270042   -0.010916   0.315808  0.010861 0.308801}
\CommentTok{#> 3 0.270042   -0.055477   0.221490  0.867268 0.348294}
\CommentTok{#> 4 0.270042   -0.063210   0.215936  0.804884 0.191174}
\CommentTok{#> 5 0.270042   -0.021527   0.269892  0.004505 0.794439}
\CommentTok{#> }
\CommentTok{#> H2OMultinomialMetrics: deeplearning}
\CommentTok{#> ** Reported on training data. **}
\CommentTok{#> ** Metrics reported on temporary training frame with 9995 samples **}
\CommentTok{#> }
\CommentTok{#> Training Set Metrics: }
\CommentTok{#> =====================}
\CommentTok{#> }
\CommentTok{#> MSE: (Extract with `h2o.mse`) 0.0574}
\CommentTok{#> RMSE: (Extract with `h2o.rmse`) 0.24}
\CommentTok{#> Logloss: (Extract with `h2o.logloss`) 0.187}
\CommentTok{#> Mean Per-Class Error: 0.128}
\CommentTok{#> Confusion Matrix: Extract with `h2o.confusionMatrix(<model>,train = TRUE)`)}
\CommentTok{#> =========================================================================}
\CommentTok{#> Confusion Matrix: Row labels: Actual class; Column labels: Predicted class}
\CommentTok{#>         class_1 class_2 class_3 class_4 class_5 class_6 class_7  Error}
\CommentTok{#> class_1    3231     367       0       0       2       2      13 0.1062}
\CommentTok{#> class_2     192    4632      10       0      10      13       3 0.0469}
\CommentTok{#> class_3       0      13     571       8       2      25       0 0.0775}
\CommentTok{#> class_4       0       0       4      35       0       4       0 0.1860}
\CommentTok{#> class_5       2      31       3       0     127       0       0 0.2209}
\CommentTok{#> class_6       1      15      36       1       0     296       0 0.1519}
\CommentTok{#> class_7      30       7       0       0       0       0     309 0.1069}
\CommentTok{#> Totals     3456    5065     624      44     141     340     325 0.0794}
\CommentTok{#>                  Rate}
\CommentTok{#> class_1 = 384 / 3,615}
\CommentTok{#> class_2 = 228 / 4,860}
\CommentTok{#> class_3 =    48 / 619}
\CommentTok{#> class_4 =      8 / 43}
\CommentTok{#> class_5 =    36 / 163}
\CommentTok{#> class_6 =    53 / 349}
\CommentTok{#> class_7 =    37 / 346}
\CommentTok{#> Totals  = 794 / 9,995}
\CommentTok{#> }
\CommentTok{#> Hit Ratio Table: Extract with `h2o.hit_ratio_table(<model>,train = TRUE)`}
\CommentTok{#> =======================================================================}
\CommentTok{#> Top-7 Hit Ratios: }
\CommentTok{#>   k hit_ratio}
\CommentTok{#> 1 1  0.920560}
\CommentTok{#> 2 2  0.996498}
\CommentTok{#> 3 3  0.999700}
\CommentTok{#> 4 4  1.000000}
\CommentTok{#> 5 5  1.000000}
\CommentTok{#> 6 6  1.000000}
\CommentTok{#> 7 7  1.000000}
\CommentTok{#> }
\CommentTok{#> }
\CommentTok{#> H2OMultinomialMetrics: deeplearning}
\CommentTok{#> ** Reported on validation data. **}
\CommentTok{#> ** Metrics reported on temporary validation frame with 10023 samples **}
\CommentTok{#> }
\CommentTok{#> Validation Set Metrics: }
\CommentTok{#> =====================}
\CommentTok{#> }
\CommentTok{#> MSE: (Extract with `h2o.mse`) 0.0616}
\CommentTok{#> RMSE: (Extract with `h2o.rmse`) 0.248}
\CommentTok{#> Logloss: (Extract with `h2o.logloss`) 0.208}
\CommentTok{#> Mean Per-Class Error: 0.161}
\CommentTok{#> Confusion Matrix: Extract with `h2o.confusionMatrix(<model>,valid = TRUE)`)}
\CommentTok{#> =========================================================================}
\CommentTok{#> Confusion Matrix: Row labels: Actual class; Column labels: Predicted class}
\CommentTok{#>         class_1 class_2 class_3 class_4 class_5 class_6 class_7  Error}
\CommentTok{#> class_1    3300     400       0       0       4       0      25 0.1150}
\CommentTok{#> class_2     168    4628       8       0      13      16       3 0.0430}
\CommentTok{#> class_3       0      19     561       6       1      26       0 0.0848}
\CommentTok{#> class_4       0       0      10      30       0       2       0 0.2857}
\CommentTok{#> class_5       3      45       4       0     109       0       0 0.3230}
\CommentTok{#> class_6       0      18      39       2       0     237       0 0.1993}
\CommentTok{#> class_7      22       5       0       0       0       0     319 0.0780}
\CommentTok{#> Totals     3493    5115     622      38     127     281     347 0.0837}
\CommentTok{#>                   Rate}
\CommentTok{#> class_1 =  429 / 3,729}
\CommentTok{#> class_2 =  208 / 4,836}
\CommentTok{#> class_3 =     52 / 613}
\CommentTok{#> class_4 =      12 / 42}
\CommentTok{#> class_5 =     52 / 161}
\CommentTok{#> class_6 =     59 / 296}
\CommentTok{#> class_7 =     27 / 346}
\CommentTok{#> Totals  = 839 / 10,023}
\CommentTok{#> }
\CommentTok{#> Hit Ratio Table: Extract with `h2o.hit_ratio_table(<model>,valid = TRUE)`}
\CommentTok{#> =======================================================================}
\CommentTok{#> Top-7 Hit Ratios: }
\CommentTok{#>   k hit_ratio}
\CommentTok{#> 1 1  0.916293}
\CommentTok{#> 2 2  0.995909}
\CommentTok{#> 3 3  0.999601}
\CommentTok{#> 4 4  1.000000}
\CommentTok{#> 5 5  1.000000}
\CommentTok{#> 6 6  1.000000}
\CommentTok{#> 7 7  1.000000}
\CommentTok{#> }
\CommentTok{#> }
\CommentTok{#> }
\CommentTok{#> }
\CommentTok{#> Scoring History: }
\CommentTok{#>              timestamp          duration training_speed   epochs}
\CommentTok{#> 1  2019-09-18 16:07:23         0.000 sec             NA  0.00000}
\CommentTok{#> 2  2019-09-18 16:07:29         6.416 sec  16447 obs/sec  0.28623}
\CommentTok{#> 3  2019-09-18 16:07:42        19.613 sec  20981 obs/sec  1.14684}
\CommentTok{#> 4  2019-09-18 16:07:51        27.872 sec  22163 obs/sec  1.72020}
\CommentTok{#> 5  2019-09-18 16:08:02        38.887 sec  21091 obs/sec  2.29155}
\CommentTok{#> 6  2019-09-18 16:08:10        46.794 sec  21912 obs/sec  2.86555}
\CommentTok{#> 7  2019-09-18 16:08:20        57.297 sec  23276 obs/sec  3.72887}
\CommentTok{#> 8  2019-09-18 16:08:32  1 min  9.478 sec  23582 obs/sec  4.58847}
\CommentTok{#> 9  2019-09-18 16:08:40  1 min 17.269 sec  23866 obs/sec  5.16196}
\CommentTok{#> 10 2019-09-18 16:08:52  1 min 28.655 sec  24252 obs/sec  6.02399}
\CommentTok{#> 11 2019-09-18 16:08:59  1 min 36.499 sec  24402 obs/sec  6.59606}
\CommentTok{#> 12 2019-09-18 16:09:07  1 min 44.628 sec  24473 obs/sec  7.16889}
\CommentTok{#> 13 2019-09-18 16:09:19  1 min 55.714 sec  24767 obs/sec  8.02787}
\CommentTok{#> 14 2019-09-18 16:09:30  2 min  6.976 sec  24977 obs/sec  8.88889}
\CommentTok{#> 15 2019-09-18 16:09:40  2 min 17.375 sec  25311 obs/sec  9.74901}
\CommentTok{#> 16 2019-09-18 16:09:44  2 min 20.928 sec  25412 obs/sec 10.03421}
\CommentTok{#>    iterations        samples training_rmse training_logloss training_r2}
\CommentTok{#> 1           0       0.000000            NA               NA          NA}
\CommentTok{#> 2           1   99900.000000       0.42461          0.55871     0.91016}
\CommentTok{#> 3           4  400265.000000       0.36386          0.41433     0.93402}
\CommentTok{#> 4           6  600375.000000       0.33460          0.35497     0.94421}
\CommentTok{#> 5           8  799786.000000       0.31826          0.32369     0.94953}
\CommentTok{#> 6          10 1000119.000000       0.30627          0.30096     0.95326}
\CommentTok{#> 7          13 1301430.000000       0.29274          0.27498     0.95729}
\CommentTok{#> 8          16 1601446.000000       0.28144          0.25803     0.96053}
\CommentTok{#> 9          18 1801600.000000       0.27123          0.24053     0.96334}
\CommentTok{#> 10         21 2102464.000000       0.26230          0.22390     0.96572}
\CommentTok{#> 11         23 2302125.000000       0.25662          0.21526     0.96718}
\CommentTok{#> 12         25 2502050.000000       0.25657          0.21498     0.96720}
\CommentTok{#> 13         28 2801846.000000       0.24881          0.20293     0.96915}
\CommentTok{#> 14         31 3102356.000000       0.24224          0.19232     0.97076}
\CommentTok{#> 15         34 3402549.000000       0.24020          0.18927     0.97125}
\CommentTok{#> 16         35 3502089.000000       0.23962          0.18680     0.97139}
\CommentTok{#>    training_classification_error validation_rmse validation_logloss}
\CommentTok{#> 1                             NA              NA                 NA}
\CommentTok{#> 2                        0.24022         0.42163            0.55128}
\CommentTok{#> 3                        0.17659         0.37069            0.42487}
\CommentTok{#> 4                        0.15078         0.33673            0.36105}
\CommentTok{#> 5                        0.13487         0.32572            0.34029}
\CommentTok{#> 6                        0.12786         0.31685            0.32451}
\CommentTok{#> 7                        0.11556         0.30173            0.29507}
\CommentTok{#> 8                        0.10665         0.28967            0.27707}
\CommentTok{#> 9                        0.09835         0.28672            0.27253}
\CommentTok{#> 10                       0.09285         0.27291            0.24511}
\CommentTok{#> 11                       0.08754         0.26546            0.23595}
\CommentTok{#> 12                       0.08884         0.26914            0.24172}
\CommentTok{#> 13                       0.08434         0.25941            0.22542}
\CommentTok{#> 14                       0.07964         0.25119            0.21287}
\CommentTok{#> 15                       0.07764         0.25087            0.21213}
\CommentTok{#> 16                       0.07944         0.24821            0.20775}
\CommentTok{#>    validation_r2 validation_classification_error}
\CommentTok{#> 1             NA                              NA}
\CommentTok{#> 2        0.90783                         0.23755}
\CommentTok{#> 3        0.92876                         0.18557}
\CommentTok{#> 4        0.94121                         0.15175}
\CommentTok{#> 5        0.94499                         0.14128}
\CommentTok{#> 6        0.94795                         0.13738}
\CommentTok{#> 7        0.95280                         0.12132}
\CommentTok{#> 8        0.95649                         0.11254}
\CommentTok{#> 9        0.95738                         0.11055}
\CommentTok{#> 10       0.96138                         0.10157}
\CommentTok{#> 11       0.96346                         0.09339}
\CommentTok{#> 12       0.96244                         0.09768}
\CommentTok{#> 13       0.96511                         0.09099}
\CommentTok{#> 14       0.96729                         0.08321}
\CommentTok{#> 15       0.96737                         0.08560}
\CommentTok{#> 16       0.96806                         0.08371}
\CommentTok{#> }
\CommentTok{#> Variable Importances: (Extract with `h2o.varimp`) }
\CommentTok{#> =================================================}
\CommentTok{#> }
\CommentTok{#> Variable Importances: }
\CommentTok{#>                             variable relative_importance scaled_importance}
\CommentTok{#> 1                          Elevation            1.000000          1.000000}
\CommentTok{#> 2 Horizontal_Distance_To_Fire_Points            0.962107          0.962107}
\CommentTok{#> 3    Horizontal_Distance_To_Roadways            0.955372          0.955372}
\CommentTok{#> 4             Wilderness_Area.area_0            0.628632          0.628632}
\CommentTok{#> 5   Horizontal_Distance_To_Hydrology            0.589965          0.589965}
\CommentTok{#>   percentage}
\CommentTok{#> 1   0.047713}
\CommentTok{#> 2   0.045905}
\CommentTok{#> 3   0.045583}
\CommentTok{#> 4   0.029994}
\CommentTok{#> 5   0.028149}
\CommentTok{#> }
\CommentTok{#> ---}
\CommentTok{#>                       variable relative_importance scaled_importance}
\CommentTok{#> 51           Soil_Type.type_13            0.170216          0.170216}
\CommentTok{#> 52            Soil_Type.type_6            0.158149          0.158149}
\CommentTok{#> 53           Soil_Type.type_14            0.148051          0.148051}
\CommentTok{#> 54           Soil_Type.type_35            0.147837          0.147837}
\CommentTok{#> 55       Soil_Type.missing(NA)            0.000000          0.000000}
\CommentTok{#> 56 Wilderness_Area.missing(NA)            0.000000          0.000000}
\CommentTok{#>    percentage}
\CommentTok{#> 51   0.008121}
\CommentTok{#> 52   0.007546}
\CommentTok{#> 53   0.007064}
\CommentTok{#> 54   0.007054}
\CommentTok{#> 55   0.000000}
\CommentTok{#> 56   0.000000}
\end{Highlighting}
\end{Shaded}

Let's compare the training error with the validation and test set errors

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{h2o.performance}\NormalTok{(m3, }\DataTypeTok{train=}\NormalTok{T)          }\CommentTok{## sampled training data (from model building)}
\CommentTok{#> H2OMultinomialMetrics: deeplearning}
\CommentTok{#> ** Reported on training data. **}
\CommentTok{#> ** Metrics reported on temporary training frame with 9995 samples **}
\CommentTok{#> }
\CommentTok{#> Training Set Metrics: }
\CommentTok{#> =====================}
\CommentTok{#> }
\CommentTok{#> MSE: (Extract with `h2o.mse`) 0.0574}
\CommentTok{#> RMSE: (Extract with `h2o.rmse`) 0.24}
\CommentTok{#> Logloss: (Extract with `h2o.logloss`) 0.187}
\CommentTok{#> Mean Per-Class Error: 0.128}
\CommentTok{#> Confusion Matrix: Extract with `h2o.confusionMatrix(<model>,train = TRUE)`)}
\CommentTok{#> =========================================================================}
\CommentTok{#> Confusion Matrix: Row labels: Actual class; Column labels: Predicted class}
\CommentTok{#>         class_1 class_2 class_3 class_4 class_5 class_6 class_7  Error}
\CommentTok{#> class_1    3231     367       0       0       2       2      13 0.1062}
\CommentTok{#> class_2     192    4632      10       0      10      13       3 0.0469}
\CommentTok{#> class_3       0      13     571       8       2      25       0 0.0775}
\CommentTok{#> class_4       0       0       4      35       0       4       0 0.1860}
\CommentTok{#> class_5       2      31       3       0     127       0       0 0.2209}
\CommentTok{#> class_6       1      15      36       1       0     296       0 0.1519}
\CommentTok{#> class_7      30       7       0       0       0       0     309 0.1069}
\CommentTok{#> Totals     3456    5065     624      44     141     340     325 0.0794}
\CommentTok{#>                  Rate}
\CommentTok{#> class_1 = 384 / 3,615}
\CommentTok{#> class_2 = 228 / 4,860}
\CommentTok{#> class_3 =    48 / 619}
\CommentTok{#> class_4 =      8 / 43}
\CommentTok{#> class_5 =    36 / 163}
\CommentTok{#> class_6 =    53 / 349}
\CommentTok{#> class_7 =    37 / 346}
\CommentTok{#> Totals  = 794 / 9,995}
\CommentTok{#> }
\CommentTok{#> Hit Ratio Table: Extract with `h2o.hit_ratio_table(<model>,train = TRUE)`}
\CommentTok{#> =======================================================================}
\CommentTok{#> Top-7 Hit Ratios: }
\CommentTok{#>   k hit_ratio}
\CommentTok{#> 1 1  0.920560}
\CommentTok{#> 2 2  0.996498}
\CommentTok{#> 3 3  0.999700}
\CommentTok{#> 4 4  1.000000}
\CommentTok{#> 5 5  1.000000}
\CommentTok{#> 6 6  1.000000}
\CommentTok{#> 7 7  1.000000}
\KeywordTok{h2o.performance}\NormalTok{(m3, }\DataTypeTok{valid=}\NormalTok{T)          }\CommentTok{## sampled validation data (from model building)}
\CommentTok{#> H2OMultinomialMetrics: deeplearning}
\CommentTok{#> ** Reported on validation data. **}
\CommentTok{#> ** Metrics reported on temporary validation frame with 10023 samples **}
\CommentTok{#> }
\CommentTok{#> Validation Set Metrics: }
\CommentTok{#> =====================}
\CommentTok{#> }
\CommentTok{#> MSE: (Extract with `h2o.mse`) 0.0616}
\CommentTok{#> RMSE: (Extract with `h2o.rmse`) 0.248}
\CommentTok{#> Logloss: (Extract with `h2o.logloss`) 0.208}
\CommentTok{#> Mean Per-Class Error: 0.161}
\CommentTok{#> Confusion Matrix: Extract with `h2o.confusionMatrix(<model>,valid = TRUE)`)}
\CommentTok{#> =========================================================================}
\CommentTok{#> Confusion Matrix: Row labels: Actual class; Column labels: Predicted class}
\CommentTok{#>         class_1 class_2 class_3 class_4 class_5 class_6 class_7  Error}
\CommentTok{#> class_1    3300     400       0       0       4       0      25 0.1150}
\CommentTok{#> class_2     168    4628       8       0      13      16       3 0.0430}
\CommentTok{#> class_3       0      19     561       6       1      26       0 0.0848}
\CommentTok{#> class_4       0       0      10      30       0       2       0 0.2857}
\CommentTok{#> class_5       3      45       4       0     109       0       0 0.3230}
\CommentTok{#> class_6       0      18      39       2       0     237       0 0.1993}
\CommentTok{#> class_7      22       5       0       0       0       0     319 0.0780}
\CommentTok{#> Totals     3493    5115     622      38     127     281     347 0.0837}
\CommentTok{#>                   Rate}
\CommentTok{#> class_1 =  429 / 3,729}
\CommentTok{#> class_2 =  208 / 4,836}
\CommentTok{#> class_3 =     52 / 613}
\CommentTok{#> class_4 =      12 / 42}
\CommentTok{#> class_5 =     52 / 161}
\CommentTok{#> class_6 =     59 / 296}
\CommentTok{#> class_7 =     27 / 346}
\CommentTok{#> Totals  = 839 / 10,023}
\CommentTok{#> }
\CommentTok{#> Hit Ratio Table: Extract with `h2o.hit_ratio_table(<model>,valid = TRUE)`}
\CommentTok{#> =======================================================================}
\CommentTok{#> Top-7 Hit Ratios: }
\CommentTok{#>   k hit_ratio}
\CommentTok{#> 1 1  0.916293}
\CommentTok{#> 2 2  0.995909}
\CommentTok{#> 3 3  0.999601}
\CommentTok{#> 4 4  1.000000}
\CommentTok{#> 5 5  1.000000}
\CommentTok{#> 6 6  1.000000}
\CommentTok{#> 7 7  1.000000}
\KeywordTok{h2o.performance}\NormalTok{(m3, }\DataTypeTok{newdata=}\NormalTok{train)    }\CommentTok{## full training data}
\CommentTok{#> H2OMultinomialMetrics: deeplearning}
\CommentTok{#> }
\CommentTok{#> Test Set Metrics: }
\CommentTok{#> =====================}
\CommentTok{#> }
\CommentTok{#> MSE: (Extract with `h2o.mse`) 0.0558}
\CommentTok{#> RMSE: (Extract with `h2o.rmse`) 0.236}
\CommentTok{#> Logloss: (Extract with `h2o.logloss`) 0.184}
\CommentTok{#> Mean Per-Class Error: 0.13}
\CommentTok{#> Confusion Matrix: Extract with `h2o.confusionMatrix(<model>, <data>)`)}
\CommentTok{#> =========================================================================}
\CommentTok{#> Confusion Matrix: Row labels: Actual class; Column labels: Predicted class}
\CommentTok{#>         class_1 class_2 class_3 class_4 class_5 class_6 class_7  Error}
\CommentTok{#> class_1  113843   12621       1       0     100      21     534 0.1044}
\CommentTok{#> class_2    5569  163436     393       3     440     426      75 0.0405}
\CommentTok{#> class_3      11     511   19911     126      36     847       0 0.0714}
\CommentTok{#> class_4       0       0     258    1330       0      70       0 0.1978}
\CommentTok{#> class_5      59    1289      81       0    4269      22       0 0.2537}
\CommentTok{#> class_6      36     338    1245      63       7    8744       0 0.1619}
\CommentTok{#> class_7     880     131       0       0       1       0   11288 0.0823}
\CommentTok{#> Totals   120398  178326   21889    1522    4853   10130   11897 0.0751}
\CommentTok{#>                       Rate}
\CommentTok{#> class_1 = 13,277 / 127,120}
\CommentTok{#> class_2 =  6,906 / 170,342}
\CommentTok{#> class_3 =   1,531 / 21,442}
\CommentTok{#> class_4 =      328 / 1,658}
\CommentTok{#> class_5 =    1,451 / 5,720}
\CommentTok{#> class_6 =   1,689 / 10,433}
\CommentTok{#> class_7 =   1,012 / 12,300}
\CommentTok{#> Totals  = 26,194 / 349,015}
\CommentTok{#> }
\CommentTok{#> Hit Ratio Table: Extract with `h2o.hit_ratio_table(<model>, <data>)`}
\CommentTok{#> =======================================================================}
\CommentTok{#> Top-7 Hit Ratios: }
\CommentTok{#>   k hit_ratio}
\CommentTok{#> 1 1  0.924949}
\CommentTok{#> 2 2  0.996814}
\CommentTok{#> 3 3  0.999762}
\CommentTok{#> 4 4  0.999968}
\CommentTok{#> 5 5  1.000000}
\CommentTok{#> 6 6  1.000000}
\CommentTok{#> 7 7  1.000000}
\KeywordTok{h2o.performance}\NormalTok{(m3, }\DataTypeTok{newdata=}\NormalTok{valid)    }\CommentTok{## full validation data}
\CommentTok{#> H2OMultinomialMetrics: deeplearning}
\CommentTok{#> }
\CommentTok{#> Test Set Metrics: }
\CommentTok{#> =====================}
\CommentTok{#> }
\CommentTok{#> MSE: (Extract with `h2o.mse`) 0.0614}
\CommentTok{#> RMSE: (Extract with `h2o.rmse`) 0.248}
\CommentTok{#> Logloss: (Extract with `h2o.logloss`) 0.203}
\CommentTok{#> Mean Per-Class Error: 0.146}
\CommentTok{#> Confusion Matrix: Extract with `h2o.confusionMatrix(<model>, <data>)`)}
\CommentTok{#> =========================================================================}
\CommentTok{#> Confusion Matrix: Row labels: Actual class; Column labels: Predicted class}
\CommentTok{#>         class_1 class_2 class_3 class_4 class_5 class_6 class_7  Error}
\CommentTok{#> class_1   37738    4523       0       0      33       3     203 0.1120}
\CommentTok{#> class_2    2063   53774     146       1     191     174      31 0.0462}
\CommentTok{#> class_3       2     203    6571      50      12     305       0 0.0801}
\CommentTok{#> class_4       0       0      94     436       0      32       0 0.2242}
\CommentTok{#> class_5      33     465      40       0    1327       5       0 0.2904}
\CommentTok{#> class_6       5     155     422      22       4    2856       0 0.1755}
\CommentTok{#> class_7     336      39       0       0       1       0    3723 0.0917}
\CommentTok{#> Totals    40177   59159    7273     509    1568    3375    3957 0.0827}
\CommentTok{#>                      Rate}
\CommentTok{#> class_1 =  4,762 / 42,500}
\CommentTok{#> class_2 =  2,606 / 56,380}
\CommentTok{#> class_3 =     572 / 7,143}
\CommentTok{#> class_4 =       126 / 562}
\CommentTok{#> class_5 =     543 / 1,870}
\CommentTok{#> class_6 =     608 / 3,464}
\CommentTok{#> class_7 =     376 / 4,099}
\CommentTok{#> Totals  = 9,593 / 116,018}
\CommentTok{#> }
\CommentTok{#> Hit Ratio Table: Extract with `h2o.hit_ratio_table(<model>, <data>)`}
\CommentTok{#> =======================================================================}
\CommentTok{#> Top-7 Hit Ratios: }
\CommentTok{#>   k hit_ratio}
\CommentTok{#> 1 1  0.917315}
\CommentTok{#> 2 2  0.995966}
\CommentTok{#> 3 3  0.999707}
\CommentTok{#> 4 4  0.999965}
\CommentTok{#> 5 5  0.999991}
\CommentTok{#> 6 6  1.000000}
\CommentTok{#> 7 7  1.000000}
\KeywordTok{h2o.performance}\NormalTok{(m3, }\DataTypeTok{newdata=}\NormalTok{test)     }\CommentTok{## full test data}
\CommentTok{#> H2OMultinomialMetrics: deeplearning}
\CommentTok{#> }
\CommentTok{#> Test Set Metrics: }
\CommentTok{#> =====================}
\CommentTok{#> }
\CommentTok{#> MSE: (Extract with `h2o.mse`) 0.0612}
\CommentTok{#> RMSE: (Extract with `h2o.rmse`) 0.247}
\CommentTok{#> Logloss: (Extract with `h2o.logloss`) 0.202}
\CommentTok{#> Mean Per-Class Error: 0.145}
\CommentTok{#> Confusion Matrix: Extract with `h2o.confusionMatrix(<model>, <data>)`)}
\CommentTok{#> =========================================================================}
\CommentTok{#> Confusion Matrix: Row labels: Actual class; Column labels: Predicted class}
\CommentTok{#>         class_1 class_2 class_3 class_4 class_5 class_6 class_7  Error}
\CommentTok{#> class_1   37519    4459       0       0      33       6     203 0.1113}
\CommentTok{#> class_2    2136   53937     140       0     165     173      28 0.0467}
\CommentTok{#> class_3       2     209    6601      61      16     280       0 0.0792}
\CommentTok{#> class_4       0       0      99     410       0      18       0 0.2220}
\CommentTok{#> class_5      25     460      25       0    1381      12       0 0.2743}
\CommentTok{#> class_6      26     131     454      25       5    2829       0 0.1847}
\CommentTok{#> class_7     354      43       0       0       0       0    3714 0.0966}
\CommentTok{#> Totals    40062   59239    7319     496    1600    3318    3945 0.0827}
\CommentTok{#>                      Rate}
\CommentTok{#> class_1 =  4,701 / 42,220}
\CommentTok{#> class_2 =  2,642 / 56,579}
\CommentTok{#> class_3 =     568 / 7,169}
\CommentTok{#> class_4 =       117 / 527}
\CommentTok{#> class_5 =     522 / 1,903}
\CommentTok{#> class_6 =     641 / 3,470}
\CommentTok{#> class_7 =     397 / 4,111}
\CommentTok{#> Totals  = 9,588 / 115,979}
\CommentTok{#> }
\CommentTok{#> Hit Ratio Table: Extract with `h2o.hit_ratio_table(<model>, <data>)`}
\CommentTok{#> =======================================================================}
\CommentTok{#> Top-7 Hit Ratios: }
\CommentTok{#>   k hit_ratio}
\CommentTok{#> 1 1  0.917330}
\CommentTok{#> 2 2  0.996068}
\CommentTok{#> 3 3  0.999724}
\CommentTok{#> 4 4  0.999948}
\CommentTok{#> 5 5  1.000000}
\CommentTok{#> 6 6  1.000000}
\CommentTok{#> 7 7  1.000000}
\end{Highlighting}
\end{Shaded}

To confirm that the reported confusion matrix on the validation set (here, the test set) was correct, we make a prediction on the test set and compare the confusion matrices explicitly:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pred <-}\StringTok{ }\KeywordTok{h2o.predict}\NormalTok{(m3, test)}
\CommentTok{#> }
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|}\StringTok{                                                                 }\ErrorTok{|}\StringTok{   }\DecValTok{0}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=================================================================|}\StringTok{ }\DecValTok{100}\NormalTok{%}
\NormalTok{pred}
\CommentTok{#>   predict  class_1 class_2  class_3  class_4  class_5  class_6  class_7}
\CommentTok{#> 1 class_2 1.59e-01 0.84040 7.73e-04 3.68e-05 4.52e-05 8.77e-05 3.38e-05}
\CommentTok{#> 2 class_1 1.00e+00 0.00045 2.91e-07 2.38e-06 2.82e-08 2.94e-07 4.82e-07}
\CommentTok{#> 3 class_1 8.99e-01 0.10147 4.57e-07 3.85e-07 3.55e-09 7.77e-08 1.57e-07}
\CommentTok{#> 4 class_1 9.93e-01 0.00703 1.60e-06 4.74e-08 4.90e-08 1.58e-08 5.77e-06}
\CommentTok{#> 5 class_2 1.22e-02 0.98737 3.25e-05 4.04e-06 2.68e-04 8.21e-05 7.75e-07}
\CommentTok{#> 6 class_5 3.06e-05 0.22890 2.46e-09 1.96e-06 7.71e-01 1.64e-08 4.48e-08}
\CommentTok{#> }
\CommentTok{#> [115979 rows x 8 columns]}
\NormalTok{test}\OperatorTok{$}\NormalTok{Accuracy <-}\StringTok{ }\NormalTok{pred}\OperatorTok{$}\NormalTok{predict }\OperatorTok{==}\StringTok{ }\NormalTok{test}\OperatorTok{$}\NormalTok{Cover_Type}
\DecValTok{1}\OperatorTok{-}\KeywordTok{mean}\NormalTok{(test}\OperatorTok{$}\NormalTok{Accuracy)}
\CommentTok{#> [1] 0.0827}
\end{Highlighting}
\end{Shaded}

\hypertarget{hyper-parameter-tuning-with-grid-search}{%
\subsection{Hyper-parameter Tuning with Grid Search}\label{hyper-parameter-tuning-with-grid-search}}

Since there are a lot of parameters that can impact model accuracy, hyper-parameter tuning is especially important for Deep Learning:

For speed, we will only train on the first 10,000 rows of the training dataset:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sampled_train=train[}\DecValTok{1}\OperatorTok{:}\DecValTok{10000}\NormalTok{,]}
\end{Highlighting}
\end{Shaded}

The simplest hyperparameter search method is a brute-force scan of the full Cartesian product of all combinations specified by a grid search:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{hyper_params <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}
  \DataTypeTok{hidden=}\KeywordTok{list}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{32}\NormalTok{,}\DecValTok{32}\NormalTok{,}\DecValTok{32}\NormalTok{),}\KeywordTok{c}\NormalTok{(}\DecValTok{64}\NormalTok{,}\DecValTok{64}\NormalTok{)),}
  \DataTypeTok{input_dropout_ratio=}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\FloatTok{0.05}\NormalTok{),}
  \DataTypeTok{rate=}\KeywordTok{c}\NormalTok{(}\FloatTok{0.01}\NormalTok{,}\FloatTok{0.02}\NormalTok{),}
  \DataTypeTok{rate_annealing=}\KeywordTok{c}\NormalTok{(}\FloatTok{1e-8}\NormalTok{,}\FloatTok{1e-7}\NormalTok{,}\FloatTok{1e-6}\NormalTok{)}
\NormalTok{)}
\NormalTok{hyper_params}
\CommentTok{#> $hidden}
\CommentTok{#> $hidden[[1]]}
\CommentTok{#> [1] 32 32 32}
\CommentTok{#> }
\CommentTok{#> $hidden[[2]]}
\CommentTok{#> [1] 64 64}
\CommentTok{#> }
\CommentTok{#> }
\CommentTok{#> $input_dropout_ratio}
\CommentTok{#> [1] 0.00 0.05}
\CommentTok{#> }
\CommentTok{#> $rate}
\CommentTok{#> [1] 0.01 0.02}
\CommentTok{#> }
\CommentTok{#> $rate_annealing}
\CommentTok{#> [1] 1e-08 1e-07 1e-06}
\NormalTok{grid <-}\StringTok{ }\KeywordTok{h2o.grid}\NormalTok{(}
  \DataTypeTok{algorithm=}\StringTok{"deeplearning"}\NormalTok{,}
  \DataTypeTok{grid_id=}\StringTok{"dl_grid"}\NormalTok{, }
  \DataTypeTok{training_frame=}\NormalTok{sampled_train,}
  \DataTypeTok{validation_frame=}\NormalTok{valid, }
  \DataTypeTok{x=}\NormalTok{predictors, }
  \DataTypeTok{y=}\NormalTok{response,}
  \DataTypeTok{epochs=}\DecValTok{10}\NormalTok{,}
  \DataTypeTok{stopping_metric=}\StringTok{"misclassification"}\NormalTok{,}
  \DataTypeTok{stopping_tolerance=}\FloatTok{1e-2}\NormalTok{,        }\CommentTok{## stop when misclassification does not improve by >=1% for 2 scoring events}
  \DataTypeTok{stopping_rounds=}\DecValTok{2}\NormalTok{,}
  \DataTypeTok{score_validation_samples=}\DecValTok{10000}\NormalTok{, }\CommentTok{## downsample validation set for faster scoring}
  \DataTypeTok{score_duty_cycle=}\FloatTok{0.025}\NormalTok{,         }\CommentTok{## don't score more than 2.5% of the wall time}
  \DataTypeTok{adaptive_rate=}\NormalTok{F,                }\CommentTok{## manually tuned learning rate}
  \DataTypeTok{momentum_start=}\FloatTok{0.5}\NormalTok{,             }\CommentTok{## manually tuned momentum}
  \DataTypeTok{momentum_stable=}\FloatTok{0.9}\NormalTok{, }
  \DataTypeTok{momentum_ramp=}\FloatTok{1e7}\NormalTok{, }
  \DataTypeTok{l1=}\FloatTok{1e-5}\NormalTok{,}
  \DataTypeTok{l2=}\FloatTok{1e-5}\NormalTok{,}
  \DataTypeTok{activation=}\KeywordTok{c}\NormalTok{(}\StringTok{"Rectifier"}\NormalTok{),}
  \DataTypeTok{max_w2=}\DecValTok{10}\NormalTok{,                      }\CommentTok{## can help improve stability for Rectifier}
  \DataTypeTok{hyper_params=}\NormalTok{hyper_params}
\NormalTok{)}
\CommentTok{#> }
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|}\StringTok{                                                                 }\ErrorTok{|}\StringTok{   }\DecValTok{0}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|===}\StringTok{                                                              }\ErrorTok{|}\StringTok{   }\DecValTok{4}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=====}\StringTok{                                                            }\ErrorTok{|}\StringTok{   }\DecValTok{7}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=======}\StringTok{                                                          }\ErrorTok{|}\StringTok{  }\DecValTok{11}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=========}\StringTok{                                                        }\ErrorTok{|}\StringTok{  }\DecValTok{15}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|============}\StringTok{                                                     }\ErrorTok{|}\StringTok{  }\DecValTok{18}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|===============}\StringTok{                                                  }\ErrorTok{|}\StringTok{  }\DecValTok{23}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==================}\StringTok{                                               }\ErrorTok{|}\StringTok{  }\DecValTok{28}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=====================}\StringTok{                                            }\ErrorTok{|}\StringTok{  }\DecValTok{32}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=======================}\StringTok{                                          }\ErrorTok{|}\StringTok{  }\DecValTok{36}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=========================}\StringTok{                                        }\ErrorTok{|}\StringTok{  }\DecValTok{38}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|===========================}\StringTok{                                      }\ErrorTok{|}\StringTok{  }\DecValTok{41}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|============================}\StringTok{                                     }\ErrorTok{|}\StringTok{  }\DecValTok{44}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==============================}\StringTok{                                   }\ErrorTok{|}\StringTok{  }\DecValTok{47}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|================================}\StringTok{                                 }\ErrorTok{|}\StringTok{  }\DecValTok{49}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|===================================}\StringTok{                              }\ErrorTok{|}\StringTok{  }\DecValTok{54}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=====================================}\StringTok{                            }\ErrorTok{|}\StringTok{  }\DecValTok{56}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|========================================}\StringTok{                         }\ErrorTok{|}\StringTok{  }\DecValTok{62}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|===========================================}\StringTok{                      }\ErrorTok{|}\StringTok{  }\DecValTok{66}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==============================================}\StringTok{                   }\ErrorTok{|}\StringTok{  }\DecValTok{70}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|================================================}\StringTok{                 }\ErrorTok{|}\StringTok{  }\DecValTok{74}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|===================================================}\StringTok{              }\ErrorTok{|}\StringTok{  }\DecValTok{78}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=====================================================}\StringTok{            }\ErrorTok{|}\StringTok{  }\DecValTok{81}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|========================================================}\StringTok{         }\ErrorTok{|}\StringTok{  }\DecValTok{85}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|===========================================================}\StringTok{      }\ErrorTok{|}\StringTok{  }\DecValTok{90}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==============================================================}\StringTok{   }\ErrorTok{|}\StringTok{  }\DecValTok{95}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|================================================================}\StringTok{ }\ErrorTok{|}\StringTok{  }\DecValTok{99}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=================================================================|}\StringTok{ }\DecValTok{100}\NormalTok{%}
\NormalTok{grid}
\CommentTok{#> H2O Grid Details}
\CommentTok{#> ================}
\CommentTok{#> }
\CommentTok{#> Grid ID: dl_grid }
\CommentTok{#> Used hyper parameters: }
\CommentTok{#>   -  hidden }
\CommentTok{#>   -  input_dropout_ratio }
\CommentTok{#>   -  rate }
\CommentTok{#>   -  rate_annealing }
\CommentTok{#> Number of models: 24 }
\CommentTok{#> Number of failed models: 0 }
\CommentTok{#> }
\CommentTok{#> Hyper-Parameter Search Summary: ordered by increasing logloss}
\CommentTok{#>     hidden input_dropout_ratio rate rate_annealing        model_ids}
\CommentTok{#> 1 [64, 64]                 0.0 0.01         1.0E-8  dl_grid_model_2}
\CommentTok{#> 2 [64, 64]                 0.0 0.01         1.0E-7 dl_grid_model_10}
\CommentTok{#> 3 [64, 64]                0.05 0.01         1.0E-7 dl_grid_model_12}
\CommentTok{#> 4 [64, 64]                 0.0 0.01         1.0E-6 dl_grid_model_18}
\CommentTok{#> 5 [64, 64]                 0.0 0.02         1.0E-7 dl_grid_model_14}
\CommentTok{#>              logloss}
\CommentTok{#> 1  0.554375812035957}
\CommentTok{#> 2 0.5615972989064305}
\CommentTok{#> 3 0.5774132175393003}
\CommentTok{#> 4 0.5795132556056228}
\CommentTok{#> 5 0.5836529554680269}
\CommentTok{#> }
\CommentTok{#> ---}
\CommentTok{#>          hidden input_dropout_ratio rate rate_annealing        model_ids}
\CommentTok{#> 19 [32, 32, 32]                 0.0 0.02         1.0E-6 dl_grid_model_21}
\CommentTok{#> 20 [32, 32, 32]                0.05 0.01         1.0E-8  dl_grid_model_3}
\CommentTok{#> 21 [32, 32, 32]                0.05 0.02         1.0E-7 dl_grid_model_15}
\CommentTok{#> 22 [32, 32, 32]                0.05 0.02         1.0E-6 dl_grid_model_23}
\CommentTok{#> 23 [32, 32, 32]                 0.0 0.02         1.0E-7 dl_grid_model_13}
\CommentTok{#> 24 [32, 32, 32]                0.05 0.02         1.0E-8  dl_grid_model_7}
\CommentTok{#>               logloss}
\CommentTok{#> 19 0.6332954204607399}
\CommentTok{#> 20 0.6333300727746808}
\CommentTok{#> 21 0.6383509237900457}
\CommentTok{#> 22 0.6399458760574194}
\CommentTok{#> 23  0.642640090400582}
\CommentTok{#> 24 0.6454225103454274}
\end{Highlighting}
\end{Shaded}

Let's see which model had the lowest validation error:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{grid <-}\StringTok{ }\KeywordTok{h2o.getGrid}\NormalTok{(}\StringTok{"dl_grid"}\NormalTok{,}\DataTypeTok{sort_by=}\StringTok{"err"}\NormalTok{,}\DataTypeTok{decreasing=}\OtherTok{FALSE}\NormalTok{)}
\NormalTok{grid}
\CommentTok{#> H2O Grid Details}
\CommentTok{#> ================}
\CommentTok{#> }
\CommentTok{#> Grid ID: dl_grid }
\CommentTok{#> Used hyper parameters: }
\CommentTok{#>   -  hidden }
\CommentTok{#>   -  input_dropout_ratio }
\CommentTok{#>   -  rate }
\CommentTok{#>   -  rate_annealing }
\CommentTok{#> Number of models: 24 }
\CommentTok{#> Number of failed models: 0 }
\CommentTok{#> }
\CommentTok{#> Hyper-Parameter Search Summary: ordered by increasing err}
\CommentTok{#>         hidden input_dropout_ratio rate rate_annealing        model_ids}
\CommentTok{#> 1     [64, 64]                 0.0 0.01         1.0E-8  dl_grid_model_2}
\CommentTok{#> 2     [64, 64]                 0.0 0.01         1.0E-7 dl_grid_model_10}
\CommentTok{#> 3     [64, 64]                 0.0 0.01         1.0E-6 dl_grid_model_18}
\CommentTok{#> 4     [64, 64]                 0.0 0.02         1.0E-6 dl_grid_model_22}
\CommentTok{#> 5 [32, 32, 32]                0.05 0.01         1.0E-6 dl_grid_model_19}
\CommentTok{#>                   err}
\CommentTok{#> 1 0.24254592912358197}
\CommentTok{#> 2  0.2509792106056041}
\CommentTok{#> 3 0.25175175175175174}
\CommentTok{#> 4  0.2526158445440957}
\CommentTok{#> 5   0.252856433184302}
\CommentTok{#> }
\CommentTok{#> ---}
\CommentTok{#>          hidden input_dropout_ratio rate rate_annealing        model_ids}
\CommentTok{#> 19 [32, 32, 32]                 0.0 0.02         1.0E-7 dl_grid_model_13}
\CommentTok{#> 20 [32, 32, 32]                 0.0 0.02         1.0E-8  dl_grid_model_5}
\CommentTok{#> 21     [64, 64]                0.05 0.01         1.0E-6 dl_grid_model_20}
\CommentTok{#> 22 [32, 32, 32]                0.05 0.01         1.0E-7 dl_grid_model_11}
\CommentTok{#> 23     [64, 64]                0.05 0.02         1.0E-7 dl_grid_model_16}
\CommentTok{#> 24 [32, 32, 32]                0.05 0.02         1.0E-8  dl_grid_model_7}
\CommentTok{#>                   err}
\CommentTok{#> 19 0.2667524497673958}
\CommentTok{#> 20 0.2668004012036108}
\CommentTok{#> 21 0.2683467741935484}
\CommentTok{#> 22 0.2696741854636591}
\CommentTok{#> 23 0.2705015923566879}
\CommentTok{#> 24 0.2841873938667466}

\CommentTok{## To see what other "sort_by" criteria are allowed}
\CommentTok{#grid <- h2o.getGrid("dl_grid",sort_by="wrong_thing",decreasing=FALSE)}

\CommentTok{## Sort by logloss}
\KeywordTok{h2o.getGrid}\NormalTok{(}\StringTok{"dl_grid"}\NormalTok{,}\DataTypeTok{sort_by=}\StringTok{"logloss"}\NormalTok{,}\DataTypeTok{decreasing=}\OtherTok{FALSE}\NormalTok{)}
\CommentTok{#> H2O Grid Details}
\CommentTok{#> ================}
\CommentTok{#> }
\CommentTok{#> Grid ID: dl_grid }
\CommentTok{#> Used hyper parameters: }
\CommentTok{#>   -  hidden }
\CommentTok{#>   -  input_dropout_ratio }
\CommentTok{#>   -  rate }
\CommentTok{#>   -  rate_annealing }
\CommentTok{#> Number of models: 24 }
\CommentTok{#> Number of failed models: 0 }
\CommentTok{#> }
\CommentTok{#> Hyper-Parameter Search Summary: ordered by increasing logloss}
\CommentTok{#>     hidden input_dropout_ratio rate rate_annealing        model_ids}
\CommentTok{#> 1 [64, 64]                 0.0 0.01         1.0E-8  dl_grid_model_2}
\CommentTok{#> 2 [64, 64]                 0.0 0.01         1.0E-7 dl_grid_model_10}
\CommentTok{#> 3 [64, 64]                0.05 0.01         1.0E-7 dl_grid_model_12}
\CommentTok{#> 4 [64, 64]                 0.0 0.01         1.0E-6 dl_grid_model_18}
\CommentTok{#> 5 [64, 64]                 0.0 0.02         1.0E-7 dl_grid_model_14}
\CommentTok{#>              logloss}
\CommentTok{#> 1  0.554375812035957}
\CommentTok{#> 2 0.5615972989064305}
\CommentTok{#> 3 0.5774132175393003}
\CommentTok{#> 4 0.5795132556056228}
\CommentTok{#> 5 0.5836529554680269}
\CommentTok{#> }
\CommentTok{#> ---}
\CommentTok{#>          hidden input_dropout_ratio rate rate_annealing        model_ids}
\CommentTok{#> 19 [32, 32, 32]                 0.0 0.02         1.0E-6 dl_grid_model_21}
\CommentTok{#> 20 [32, 32, 32]                0.05 0.01         1.0E-8  dl_grid_model_3}
\CommentTok{#> 21 [32, 32, 32]                0.05 0.02         1.0E-7 dl_grid_model_15}
\CommentTok{#> 22 [32, 32, 32]                0.05 0.02         1.0E-6 dl_grid_model_23}
\CommentTok{#> 23 [32, 32, 32]                 0.0 0.02         1.0E-7 dl_grid_model_13}
\CommentTok{#> 24 [32, 32, 32]                0.05 0.02         1.0E-8  dl_grid_model_7}
\CommentTok{#>               logloss}
\CommentTok{#> 19 0.6332954204607399}
\CommentTok{#> 20 0.6333300727746808}
\CommentTok{#> 21 0.6383509237900457}
\CommentTok{#> 22 0.6399458760574194}
\CommentTok{#> 23  0.642640090400582}
\CommentTok{#> 24 0.6454225103454274}

\CommentTok{## Find the best model and its full set of parameters}
\NormalTok{grid}\OperatorTok{@}\NormalTok{summary_table[}\DecValTok{1}\NormalTok{,]}
\CommentTok{#> Hyper-Parameter Search Summary: ordered by increasing err}
\CommentTok{#>     hidden input_dropout_ratio rate rate_annealing       model_ids}
\CommentTok{#> 1 [64, 64]                 0.0 0.01         1.0E-8 dl_grid_model_2}
\CommentTok{#>                   err}
\CommentTok{#> 1 0.24254592912358197}
\NormalTok{best_model <-}\StringTok{ }\KeywordTok{h2o.getModel}\NormalTok{(grid}\OperatorTok{@}\NormalTok{model_ids[[}\DecValTok{1}\NormalTok{]])}
\NormalTok{best_model}
\CommentTok{#> Model Details:}
\CommentTok{#> ==============}
\CommentTok{#> }
\CommentTok{#> H2OMultinomialModel: deeplearning}
\CommentTok{#> Model ID:  dl_grid_model_2 }
\CommentTok{#> Status of Neuron Layers: predicting Cover_Type, 7-class classification, multinomial distribution, CrossEntropy loss, 8,263 weights/biases, 72.5 KB, 100,000 training samples, mini-batch size 1}
\CommentTok{#>   layer units      type dropout       l1       l2 mean_rate rate_rms}
\CommentTok{#> 1     1    56     Input  0.00 %       NA       NA        NA       NA}
\CommentTok{#> 2     2    64 Rectifier  0.00 % 0.000010 0.000010  0.009990 0.000000}
\CommentTok{#> 3     3    64 Rectifier  0.00 % 0.000010 0.000010  0.009990 0.000000}
\CommentTok{#> 4     4     7   Softmax      NA 0.000010 0.000010  0.009990 0.000000}
\CommentTok{#>   momentum mean_weight weight_rms mean_bias bias_rms}
\CommentTok{#> 1       NA          NA         NA        NA       NA}
\CommentTok{#> 2 0.504000   -0.008767   0.215177  0.132772 0.155228}
\CommentTok{#> 3 0.504000   -0.057464   0.188338  0.852400 0.168772}
\CommentTok{#> 4 0.504000    0.007717   0.391073  0.013818 0.573407}
\CommentTok{#> }
\CommentTok{#> }
\CommentTok{#> H2OMultinomialMetrics: deeplearning}
\CommentTok{#> ** Reported on training data. **}
\CommentTok{#> ** Metrics reported on full training frame **}
\CommentTok{#> }
\CommentTok{#> Training Set Metrics: }
\CommentTok{#> =====================}
\CommentTok{#> }
\CommentTok{#> Extract training frame with `h2o.getFrame("RTMP_sid_8ba3_9")`}
\CommentTok{#> MSE: (Extract with `h2o.mse`) 0.16}
\CommentTok{#> RMSE: (Extract with `h2o.rmse`) 0.4}
\CommentTok{#> Logloss: (Extract with `h2o.logloss`) 0.495}
\CommentTok{#> Mean Per-Class Error: 0.403}
\CommentTok{#> Confusion Matrix: Extract with `h2o.confusionMatrix(<model>,train = TRUE)`)}
\CommentTok{#> =========================================================================}
\CommentTok{#> Confusion Matrix: Row labels: Actual class; Column labels: Predicted class}
\CommentTok{#>         class_1 class_2 class_3 class_4 class_5 class_6 class_7  Error}
\CommentTok{#> class_1    2505    1123       1       0       0       2      57 0.3208}
\CommentTok{#> class_2     377    4397      33       0       7      20       1 0.0906}
\CommentTok{#> class_3       0      51     469       7       5      98       0 0.2556}
\CommentTok{#> class_4       0       0      18      22       0       4       0 0.5000}
\CommentTok{#> class_5       7     121       1       0      27       0       0 0.8269}
\CommentTok{#> class_6       0      79      88       0       0     142       0 0.5405}
\CommentTok{#> class_7      91       5       0       0       0       0     242 0.2840}
\CommentTok{#> Totals     2980    5776     610      29      39     266     300 0.2196}
\CommentTok{#>                     Rate}
\CommentTok{#> class_1 =  1,183 / 3,688}
\CommentTok{#> class_2 =    438 / 4,835}
\CommentTok{#> class_3 =      161 / 630}
\CommentTok{#> class_4 =        22 / 44}
\CommentTok{#> class_5 =      129 / 156}
\CommentTok{#> class_6 =      167 / 309}
\CommentTok{#> class_7 =       96 / 338}
\CommentTok{#> Totals  = 2,196 / 10,000}
\CommentTok{#> }
\CommentTok{#> Hit Ratio Table: Extract with `h2o.hit_ratio_table(<model>,train = TRUE)`}
\CommentTok{#> =======================================================================}
\CommentTok{#> Top-7 Hit Ratios: }
\CommentTok{#>   k hit_ratio}
\CommentTok{#> 1 1  0.780400}
\CommentTok{#> 2 2  0.982500}
\CommentTok{#> 3 3  0.997800}
\CommentTok{#> 4 4  0.999500}
\CommentTok{#> 5 5  1.000000}
\CommentTok{#> 6 6  1.000000}
\CommentTok{#> 7 7  1.000000}
\CommentTok{#> }
\CommentTok{#> }
\CommentTok{#> H2OMultinomialMetrics: deeplearning}
\CommentTok{#> ** Reported on validation data. **}
\CommentTok{#> ** Metrics reported on temporary validation frame with 9961 samples **}
\CommentTok{#> }
\CommentTok{#> Validation Set Metrics: }
\CommentTok{#> =====================}
\CommentTok{#> }
\CommentTok{#> MSE: (Extract with `h2o.mse`) 0.178}
\CommentTok{#> RMSE: (Extract with `h2o.rmse`) 0.422}
\CommentTok{#> Logloss: (Extract with `h2o.logloss`) 0.554}
\CommentTok{#> Mean Per-Class Error: 0.43}
\CommentTok{#> Confusion Matrix: Extract with `h2o.confusionMatrix(<model>,valid = TRUE)`)}
\CommentTok{#> =========================================================================}
\CommentTok{#> Confusion Matrix: Row labels: Actual class; Column labels: Predicted class}
\CommentTok{#>         class_1 class_2 class_3 class_4 class_5 class_6 class_7  Error}
\CommentTok{#> class_1    2375    1217       0       0       0       1      46 0.3473}
\CommentTok{#> class_2     459    4289      36       1       4      30       3 0.1105}
\CommentTok{#> class_3       0      55     426      20       4     106       0 0.3028}
\CommentTok{#> class_4       0       0      21      22       0       5       0 0.5417}
\CommentTok{#> class_5      11     124       1       0      18       1       0 0.8839}
\CommentTok{#> class_6       0      89      74       2       2     135       0 0.5530}
\CommentTok{#> class_7     103       1       0       0       0       0     280 0.2708}
\CommentTok{#> Totals     2948    5775     558      45      28     278     329 0.2425}
\CommentTok{#>                    Rate}
\CommentTok{#> class_1 = 1,264 / 3,639}
\CommentTok{#> class_2 =   533 / 4,822}
\CommentTok{#> class_3 =     185 / 611}
\CommentTok{#> class_4 =       26 / 48}
\CommentTok{#> class_5 =     137 / 155}
\CommentTok{#> class_6 =     167 / 302}
\CommentTok{#> class_7 =     104 / 384}
\CommentTok{#> Totals  = 2,416 / 9,961}
\CommentTok{#> }
\CommentTok{#> Hit Ratio Table: Extract with `h2o.hit_ratio_table(<model>,valid = TRUE)`}
\CommentTok{#> =======================================================================}
\CommentTok{#> Top-7 Hit Ratios: }
\CommentTok{#>   k hit_ratio}
\CommentTok{#> 1 1  0.757454}
\CommentTok{#> 2 2  0.974701}
\CommentTok{#> 3 3  0.996587}
\CommentTok{#> 4 4  0.999297}
\CommentTok{#> 5 5  1.000000}
\CommentTok{#> 6 6  1.000000}
\CommentTok{#> 7 7  1.000000}

\KeywordTok{print}\NormalTok{(best_model}\OperatorTok{@}\NormalTok{allparameters)}
\CommentTok{#> $model_id}
\CommentTok{#> [1] "dl_grid_model_2"}
\CommentTok{#> }
\CommentTok{#> $training_frame}
\CommentTok{#> [1] "RTMP_sid_8ba3_9"}
\CommentTok{#> }
\CommentTok{#> $validation_frame}
\CommentTok{#> [1] "valid.hex"}
\CommentTok{#> }
\CommentTok{#> $nfolds}
\CommentTok{#> [1] 0}
\CommentTok{#> }
\CommentTok{#> $keep_cross_validation_models}
\CommentTok{#> [1] TRUE}
\CommentTok{#> }
\CommentTok{#> $keep_cross_validation_predictions}
\CommentTok{#> [1] FALSE}
\CommentTok{#> }
\CommentTok{#> $keep_cross_validation_fold_assignment}
\CommentTok{#> [1] FALSE}
\CommentTok{#> }
\CommentTok{#> $fold_assignment}
\CommentTok{#> [1] "AUTO"}
\CommentTok{#> }
\CommentTok{#> $ignore_const_cols}
\CommentTok{#> [1] TRUE}
\CommentTok{#> }
\CommentTok{#> $score_each_iteration}
\CommentTok{#> [1] FALSE}
\CommentTok{#> }
\CommentTok{#> $balance_classes}
\CommentTok{#> [1] FALSE}
\CommentTok{#> }
\CommentTok{#> $max_after_balance_size}
\CommentTok{#> [1] 5}
\CommentTok{#> }
\CommentTok{#> $max_confusion_matrix_size}
\CommentTok{#> [1] 20}
\CommentTok{#> }
\CommentTok{#> $max_hit_ratio_k}
\CommentTok{#> [1] 0}
\CommentTok{#> }
\CommentTok{#> $overwrite_with_best_model}
\CommentTok{#> [1] TRUE}
\CommentTok{#> }
\CommentTok{#> $use_all_factor_levels}
\CommentTok{#> [1] TRUE}
\CommentTok{#> }
\CommentTok{#> $standardize}
\CommentTok{#> [1] TRUE}
\CommentTok{#> }
\CommentTok{#> $activation}
\CommentTok{#> [1] "Rectifier"}
\CommentTok{#> }
\CommentTok{#> $hidden}
\CommentTok{#> [1] 64 64}
\CommentTok{#> }
\CommentTok{#> $epochs}
\CommentTok{#> [1] 10}
\CommentTok{#> }
\CommentTok{#> $train_samples_per_iteration}
\CommentTok{#> [1] -2}
\CommentTok{#> }
\CommentTok{#> $target_ratio_comm_to_comp}
\CommentTok{#> [1] 0.05}
\CommentTok{#> }
\CommentTok{#> $seed}
\CommentTok{#> [1] -3.09e+17}
\CommentTok{#> }
\CommentTok{#> $adaptive_rate}
\CommentTok{#> [1] FALSE}
\CommentTok{#> }
\CommentTok{#> $rho}
\CommentTok{#> [1] 0.99}
\CommentTok{#> }
\CommentTok{#> $epsilon}
\CommentTok{#> [1] 1e-08}
\CommentTok{#> }
\CommentTok{#> $rate}
\CommentTok{#> [1] 0.01}
\CommentTok{#> }
\CommentTok{#> $rate_annealing}
\CommentTok{#> [1] 1e-08}
\CommentTok{#> }
\CommentTok{#> $rate_decay}
\CommentTok{#> [1] 1}
\CommentTok{#> }
\CommentTok{#> $momentum_start}
\CommentTok{#> [1] 0.5}
\CommentTok{#> }
\CommentTok{#> $momentum_ramp}
\CommentTok{#> [1] 1e+07}
\CommentTok{#> }
\CommentTok{#> $momentum_stable}
\CommentTok{#> [1] 0.9}
\CommentTok{#> }
\CommentTok{#> $nesterov_accelerated_gradient}
\CommentTok{#> [1] TRUE}
\CommentTok{#> }
\CommentTok{#> $input_dropout_ratio}
\CommentTok{#> [1] 0}
\CommentTok{#> }
\CommentTok{#> $l1}
\CommentTok{#> [1] 1e-05}
\CommentTok{#> }
\CommentTok{#> $l2}
\CommentTok{#> [1] 1e-05}
\CommentTok{#> }
\CommentTok{#> $max_w2}
\CommentTok{#> [1] 10}
\CommentTok{#> }
\CommentTok{#> $initial_weight_distribution}
\CommentTok{#> [1] "UniformAdaptive"}
\CommentTok{#> }
\CommentTok{#> $initial_weight_scale}
\CommentTok{#> [1] 1}
\CommentTok{#> }
\CommentTok{#> $loss}
\CommentTok{#> [1] "Automatic"}
\CommentTok{#> }
\CommentTok{#> $distribution}
\CommentTok{#> [1] "AUTO"}
\CommentTok{#> }
\CommentTok{#> $quantile_alpha}
\CommentTok{#> [1] 0.5}
\CommentTok{#> }
\CommentTok{#> $tweedie_power}
\CommentTok{#> [1] 1.5}
\CommentTok{#> }
\CommentTok{#> $huber_alpha}
\CommentTok{#> [1] 0.9}
\CommentTok{#> }
\CommentTok{#> $score_interval}
\CommentTok{#> [1] 5}
\CommentTok{#> }
\CommentTok{#> $score_training_samples}
\CommentTok{#> [1] 10000}
\CommentTok{#> }
\CommentTok{#> $score_validation_samples}
\CommentTok{#> [1] 10000}
\CommentTok{#> }
\CommentTok{#> $score_duty_cycle}
\CommentTok{#> [1] 0.025}
\CommentTok{#> }
\CommentTok{#> $classification_stop}
\CommentTok{#> [1] 0}
\CommentTok{#> }
\CommentTok{#> $regression_stop}
\CommentTok{#> [1] 1e-06}
\CommentTok{#> }
\CommentTok{#> $stopping_rounds}
\CommentTok{#> [1] 2}
\CommentTok{#> }
\CommentTok{#> $stopping_metric}
\CommentTok{#> [1] "misclassification"}
\CommentTok{#> }
\CommentTok{#> $stopping_tolerance}
\CommentTok{#> [1] 0.01}
\CommentTok{#> }
\CommentTok{#> $max_runtime_secs}
\CommentTok{#> [1] 1.8e+308}
\CommentTok{#> }
\CommentTok{#> $score_validation_sampling}
\CommentTok{#> [1] "Uniform"}
\CommentTok{#> }
\CommentTok{#> $diagnostics}
\CommentTok{#> [1] TRUE}
\CommentTok{#> }
\CommentTok{#> $fast_mode}
\CommentTok{#> [1] TRUE}
\CommentTok{#> }
\CommentTok{#> $force_load_balance}
\CommentTok{#> [1] TRUE}
\CommentTok{#> }
\CommentTok{#> $variable_importances}
\CommentTok{#> [1] TRUE}
\CommentTok{#> }
\CommentTok{#> $replicate_training_data}
\CommentTok{#> [1] TRUE}
\CommentTok{#> }
\CommentTok{#> $single_node_mode}
\CommentTok{#> [1] FALSE}
\CommentTok{#> }
\CommentTok{#> $shuffle_training_data}
\CommentTok{#> [1] FALSE}
\CommentTok{#> }
\CommentTok{#> $missing_values_handling}
\CommentTok{#> [1] "MeanImputation"}
\CommentTok{#> }
\CommentTok{#> $quiet_mode}
\CommentTok{#> [1] FALSE}
\CommentTok{#> }
\CommentTok{#> $autoencoder}
\CommentTok{#> [1] FALSE}
\CommentTok{#> }
\CommentTok{#> $sparse}
\CommentTok{#> [1] FALSE}
\CommentTok{#> }
\CommentTok{#> $col_major}
\CommentTok{#> [1] FALSE}
\CommentTok{#> }
\CommentTok{#> $average_activation}
\CommentTok{#> [1] 0}
\CommentTok{#> }
\CommentTok{#> $sparsity_beta}
\CommentTok{#> [1] 0}
\CommentTok{#> }
\CommentTok{#> $max_categorical_features}
\CommentTok{#> [1] 2147483647}
\CommentTok{#> }
\CommentTok{#> $reproducible}
\CommentTok{#> [1] FALSE}
\CommentTok{#> }
\CommentTok{#> $export_weights_and_biases}
\CommentTok{#> [1] FALSE}
\CommentTok{#> }
\CommentTok{#> $mini_batch_size}
\CommentTok{#> [1] 1}
\CommentTok{#> }
\CommentTok{#> $categorical_encoding}
\CommentTok{#> [1] "AUTO"}
\CommentTok{#> }
\CommentTok{#> $elastic_averaging}
\CommentTok{#> [1] FALSE}
\CommentTok{#> }
\CommentTok{#> $elastic_averaging_moving_rate}
\CommentTok{#> [1] 0.9}
\CommentTok{#> }
\CommentTok{#> $elastic_averaging_regularization}
\CommentTok{#> [1] 0.001}
\CommentTok{#> }
\CommentTok{#> $x}
\CommentTok{#>  [1] "Soil_Type"                         }
\CommentTok{#>  [2] "Wilderness_Area"                   }
\CommentTok{#>  [3] "Elevation"                         }
\CommentTok{#>  [4] "Aspect"                            }
\CommentTok{#>  [5] "Slope"                             }
\CommentTok{#>  [6] "Horizontal_Distance_To_Hydrology"  }
\CommentTok{#>  [7] "Vertical_Distance_To_Hydrology"    }
\CommentTok{#>  [8] "Horizontal_Distance_To_Roadways"   }
\CommentTok{#>  [9] "Hillshade_9am"                     }
\CommentTok{#> [10] "Hillshade_Noon"                    }
\CommentTok{#> [11] "Hillshade_3pm"                     }
\CommentTok{#> [12] "Horizontal_Distance_To_Fire_Points"}
\CommentTok{#> }
\CommentTok{#> $y}
\CommentTok{#> [1] "Cover_Type"}
\KeywordTok{print}\NormalTok{(}\KeywordTok{h2o.performance}\NormalTok{(best_model, }\DataTypeTok{valid=}\NormalTok{T))}
\CommentTok{#> H2OMultinomialMetrics: deeplearning}
\CommentTok{#> ** Reported on validation data. **}
\CommentTok{#> ** Metrics reported on temporary validation frame with 9961 samples **}
\CommentTok{#> }
\CommentTok{#> Validation Set Metrics: }
\CommentTok{#> =====================}
\CommentTok{#> }
\CommentTok{#> MSE: (Extract with `h2o.mse`) 0.178}
\CommentTok{#> RMSE: (Extract with `h2o.rmse`) 0.422}
\CommentTok{#> Logloss: (Extract with `h2o.logloss`) 0.554}
\CommentTok{#> Mean Per-Class Error: 0.43}
\CommentTok{#> Confusion Matrix: Extract with `h2o.confusionMatrix(<model>,valid = TRUE)`)}
\CommentTok{#> =========================================================================}
\CommentTok{#> Confusion Matrix: Row labels: Actual class; Column labels: Predicted class}
\CommentTok{#>         class_1 class_2 class_3 class_4 class_5 class_6 class_7  Error}
\CommentTok{#> class_1    2375    1217       0       0       0       1      46 0.3473}
\CommentTok{#> class_2     459    4289      36       1       4      30       3 0.1105}
\CommentTok{#> class_3       0      55     426      20       4     106       0 0.3028}
\CommentTok{#> class_4       0       0      21      22       0       5       0 0.5417}
\CommentTok{#> class_5      11     124       1       0      18       1       0 0.8839}
\CommentTok{#> class_6       0      89      74       2       2     135       0 0.5530}
\CommentTok{#> class_7     103       1       0       0       0       0     280 0.2708}
\CommentTok{#> Totals     2948    5775     558      45      28     278     329 0.2425}
\CommentTok{#>                    Rate}
\CommentTok{#> class_1 = 1,264 / 3,639}
\CommentTok{#> class_2 =   533 / 4,822}
\CommentTok{#> class_3 =     185 / 611}
\CommentTok{#> class_4 =       26 / 48}
\CommentTok{#> class_5 =     137 / 155}
\CommentTok{#> class_6 =     167 / 302}
\CommentTok{#> class_7 =     104 / 384}
\CommentTok{#> Totals  = 2,416 / 9,961}
\CommentTok{#> }
\CommentTok{#> Hit Ratio Table: Extract with `h2o.hit_ratio_table(<model>,valid = TRUE)`}
\CommentTok{#> =======================================================================}
\CommentTok{#> Top-7 Hit Ratios: }
\CommentTok{#>   k hit_ratio}
\CommentTok{#> 1 1  0.757454}
\CommentTok{#> 2 2  0.974701}
\CommentTok{#> 3 3  0.996587}
\CommentTok{#> 4 4  0.999297}
\CommentTok{#> 5 5  1.000000}
\CommentTok{#> 6 6  1.000000}
\CommentTok{#> 7 7  1.000000}
\KeywordTok{print}\NormalTok{(}\KeywordTok{h2o.logloss}\NormalTok{(best_model, }\DataTypeTok{valid=}\NormalTok{T))}
\CommentTok{#> [1] 0.554}
\end{Highlighting}
\end{Shaded}

\hypertarget{random-hyper-parameter-search}{%
\subsection{Random Hyper-Parameter Search}\label{random-hyper-parameter-search}}

Often, hyper-parameter search for more than 4 parameters can be done more efficiently with random parameter search than with grid search. Basically, chances are good to find one of many good models in less time than performing an exhaustive grid search. We simply build up to max\_models models with parameters drawn randomly from user-specified distributions (here, uniform). For this example, we use the adaptive learning rate and focus on tuning the network architecture and the regularization parameters. We also let the grid search stop automatically once the performance at the top of the leaderboard doesn't change much anymore, i.e., once the search has converged.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{hyper_params <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}
  \DataTypeTok{activation=}\KeywordTok{c}\NormalTok{(}\StringTok{"Rectifier"}\NormalTok{,}\StringTok{"Tanh"}\NormalTok{,}\StringTok{"Maxout"}\NormalTok{,}\StringTok{"RectifierWithDropout"}\NormalTok{,}\StringTok{"TanhWithDropout"}\NormalTok{,}\StringTok{"MaxoutWithDropout"}\NormalTok{),}
  \DataTypeTok{hidden=}\KeywordTok{list}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{20}\NormalTok{,}\DecValTok{20}\NormalTok{),}\KeywordTok{c}\NormalTok{(}\DecValTok{50}\NormalTok{,}\DecValTok{50}\NormalTok{),}\KeywordTok{c}\NormalTok{(}\DecValTok{30}\NormalTok{,}\DecValTok{30}\NormalTok{,}\DecValTok{30}\NormalTok{),}\KeywordTok{c}\NormalTok{(}\DecValTok{25}\NormalTok{,}\DecValTok{25}\NormalTok{,}\DecValTok{25}\NormalTok{,}\DecValTok{25}\NormalTok{)),}
  \DataTypeTok{input_dropout_ratio=}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\FloatTok{0.05}\NormalTok{),}
  \DataTypeTok{l1=}\KeywordTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{,}\FloatTok{1e-4}\NormalTok{,}\FloatTok{1e-6}\NormalTok{),}
  \DataTypeTok{l2=}\KeywordTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{,}\FloatTok{1e-4}\NormalTok{,}\FloatTok{1e-6}\NormalTok{)}
\NormalTok{)}
\NormalTok{hyper_params}

\CommentTok{## Stop once the top 5 models are within 1% of each other (i.e., the windowed average varies less than 1%)}
\NormalTok{search_criteria =}\StringTok{ }\KeywordTok{list}\NormalTok{(}\DataTypeTok{strategy =} \StringTok{"RandomDiscrete"}\NormalTok{, }\DataTypeTok{max_runtime_secs =} \DecValTok{360}\NormalTok{, }\DataTypeTok{max_models =} \DecValTok{100}\NormalTok{, }\DataTypeTok{seed=}\DecValTok{1234567}\NormalTok{, }\DataTypeTok{stopping_rounds=}\DecValTok{5}\NormalTok{, }\DataTypeTok{stopping_tolerance=}\FloatTok{1e-2}\NormalTok{)}
\NormalTok{dl_random_grid <-}\StringTok{ }\KeywordTok{h2o.grid}\NormalTok{(}
  \DataTypeTok{algorithm=}\StringTok{"deeplearning"}\NormalTok{,}
  \DataTypeTok{grid_id =} \StringTok{"dl_grid_random"}\NormalTok{,}
  \DataTypeTok{training_frame=}\NormalTok{sampled_train,}
  \DataTypeTok{validation_frame=}\NormalTok{valid, }
  \DataTypeTok{x=}\NormalTok{predictors, }
  \DataTypeTok{y=}\NormalTok{response,}
  \DataTypeTok{epochs=}\DecValTok{1}\NormalTok{,}
  \DataTypeTok{stopping_metric=}\StringTok{"logloss"}\NormalTok{,}
  \DataTypeTok{stopping_tolerance=}\FloatTok{1e-2}\NormalTok{,        }\CommentTok{## stop when logloss does not improve by >=1% for 2 scoring events}
  \DataTypeTok{stopping_rounds=}\DecValTok{2}\NormalTok{,}
  \DataTypeTok{score_validation_samples=}\DecValTok{10000}\NormalTok{, }\CommentTok{## downsample validation set for faster scoring}
  \DataTypeTok{score_duty_cycle=}\FloatTok{0.025}\NormalTok{,         }\CommentTok{## don't score more than 2.5% of the wall time}
  \DataTypeTok{max_w2=}\DecValTok{10}\NormalTok{,                      }\CommentTok{## can help improve stability for Rectifier}
  \DataTypeTok{hyper_params =}\NormalTok{ hyper_params,}
  \DataTypeTok{search_criteria =}\NormalTok{ search_criteria}
\NormalTok{)                                }
\NormalTok{grid <-}\StringTok{ }\KeywordTok{h2o.getGrid}\NormalTok{(}\StringTok{"dl_grid_random"}\NormalTok{,}\DataTypeTok{sort_by=}\StringTok{"logloss"}\NormalTok{,}\DataTypeTok{decreasing=}\OtherTok{FALSE}\NormalTok{)}
\NormalTok{grid}

\NormalTok{grid}\OperatorTok{@}\NormalTok{summary_table[}\DecValTok{1}\NormalTok{,]}
\NormalTok{best_model <-}\StringTok{ }\KeywordTok{h2o.getModel}\NormalTok{(grid}\OperatorTok{@}\NormalTok{model_ids[[}\DecValTok{1}\NormalTok{]]) }\CommentTok{## model with lowest logloss}
\NormalTok{best_model}
\end{Highlighting}
\end{Shaded}

Let's look at the model with the lowest validation misclassification rate:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{grid <-}\StringTok{ }\KeywordTok{h2o.getGrid}\NormalTok{(}\StringTok{"dl_grid"}\NormalTok{,}\DataTypeTok{sort_by=}\StringTok{"err"}\NormalTok{,}\DataTypeTok{decreasing=}\OtherTok{FALSE}\NormalTok{)}
\NormalTok{best_model <-}\StringTok{ }\KeywordTok{h2o.getModel}\NormalTok{(grid}\OperatorTok{@}\NormalTok{model_ids[[}\DecValTok{1}\NormalTok{]]) }\CommentTok{## model with lowest classification error (on validation, since it was available during training)}
\KeywordTok{h2o.confusionMatrix}\NormalTok{(best_model,}\DataTypeTok{valid=}\NormalTok{T)}
\CommentTok{#> Confusion Matrix: Row labels: Actual class; Column labels: Predicted class}
\CommentTok{#>         class_1 class_2 class_3 class_4 class_5 class_6 class_7  Error}
\CommentTok{#> class_1    2375    1217       0       0       0       1      46 0.3473}
\CommentTok{#> class_2     459    4289      36       1       4      30       3 0.1105}
\CommentTok{#> class_3       0      55     426      20       4     106       0 0.3028}
\CommentTok{#> class_4       0       0      21      22       0       5       0 0.5417}
\CommentTok{#> class_5      11     124       1       0      18       1       0 0.8839}
\CommentTok{#> class_6       0      89      74       2       2     135       0 0.5530}
\CommentTok{#> class_7     103       1       0       0       0       0     280 0.2708}
\CommentTok{#> Totals     2948    5775     558      45      28     278     329 0.2425}
\CommentTok{#>                    Rate}
\CommentTok{#> class_1 = 1,264 / 3,639}
\CommentTok{#> class_2 =   533 / 4,822}
\CommentTok{#> class_3 =     185 / 611}
\CommentTok{#> class_4 =       26 / 48}
\CommentTok{#> class_5 =     137 / 155}
\CommentTok{#> class_6 =     167 / 302}
\CommentTok{#> class_7 =     104 / 384}
\CommentTok{#> Totals  = 2,416 / 9,961}
\NormalTok{best_params <-}\StringTok{ }\NormalTok{best_model}\OperatorTok{@}\NormalTok{allparameters}
\NormalTok{best_params}\OperatorTok{$}\NormalTok{activation}
\CommentTok{#> [1] "Rectifier"}
\NormalTok{best_params}\OperatorTok{$}\NormalTok{hidden}
\CommentTok{#> [1] 64 64}
\NormalTok{best_params}\OperatorTok{$}\NormalTok{input_dropout_ratio}
\CommentTok{#> [1] 0}
\NormalTok{best_params}\OperatorTok{$}\NormalTok{l1}
\CommentTok{#> [1] 1e-05}
\NormalTok{best_params}\OperatorTok{$}\NormalTok{l2}
\CommentTok{#> [1] 1e-05}
\end{Highlighting}
\end{Shaded}

\hypertarget{checkpointing}{%
\subsection{Checkpointing}\label{checkpointing}}

Let's continue training the manually tuned model from before, for 2 more epochs. Note that since many important parameters such as epochs, l1, l2, max\_w2, score\_interval, train\_samples\_per\_iteration, input\_dropout\_ratio, hidden\_dropout\_ratios, score\_duty\_cycle, classification\_stop, regression\_stop, variable\_importances, force\_load\_balance can be modified between checkpoint restarts, it is best to specify as many parameters as possible explicitly.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{max_epochs <-}\StringTok{ }\DecValTok{12} \CommentTok{## Add two more epochs}
\NormalTok{m_cont <-}\StringTok{ }\KeywordTok{h2o.deeplearning}\NormalTok{(}
  \DataTypeTok{model_id=}\StringTok{"dl_model_tuned_continued"}\NormalTok{, }
  \DataTypeTok{checkpoint=}\StringTok{"dl_model_tuned"}\NormalTok{, }
  \DataTypeTok{training_frame=}\NormalTok{train, }
  \DataTypeTok{validation_frame=}\NormalTok{valid, }
  \DataTypeTok{x=}\NormalTok{predictors, }
  \DataTypeTok{y=}\NormalTok{response, }
  \DataTypeTok{hidden=}\KeywordTok{c}\NormalTok{(}\DecValTok{128}\NormalTok{,}\DecValTok{128}\NormalTok{,}\DecValTok{128}\NormalTok{),          }\CommentTok{## more hidden layers -> more complex interactions}
  \DataTypeTok{epochs=}\NormalTok{max_epochs,              }\CommentTok{## hopefully long enough to converge (otherwise restart again)}
  \DataTypeTok{stopping_metric=}\StringTok{"logloss"}\NormalTok{,      }\CommentTok{## logloss is directly optimized by Deep Learning}
  \DataTypeTok{stopping_tolerance=}\FloatTok{1e-2}\NormalTok{,        }\CommentTok{## stop when validation logloss does not improve by >=1% for 2 scoring events}
  \DataTypeTok{stopping_rounds=}\DecValTok{2}\NormalTok{,}
  \DataTypeTok{score_validation_samples=}\DecValTok{10000}\NormalTok{, }\CommentTok{## downsample validation set for faster scoring}
  \DataTypeTok{score_duty_cycle=}\FloatTok{0.025}\NormalTok{,         }\CommentTok{## don't score more than 2.5% of the wall time}
  \DataTypeTok{adaptive_rate=}\NormalTok{F,                }\CommentTok{## manually tuned learning rate}
  \DataTypeTok{rate=}\FloatTok{0.01}\NormalTok{, }
  \DataTypeTok{rate_annealing=}\FloatTok{2e-6}\NormalTok{,            }
  \DataTypeTok{momentum_start=}\FloatTok{0.2}\NormalTok{,             }\CommentTok{## manually tuned momentum}
  \DataTypeTok{momentum_stable=}\FloatTok{0.4}\NormalTok{, }
  \DataTypeTok{momentum_ramp=}\FloatTok{1e7}\NormalTok{, }
  \DataTypeTok{l1=}\FloatTok{1e-5}\NormalTok{,                        }\CommentTok{## add some L1/L2 regularization}
  \DataTypeTok{l2=}\FloatTok{1e-5}\NormalTok{,}
  \DataTypeTok{max_w2=}\DecValTok{10}                       \CommentTok{## helps stability for Rectifier}
\NormalTok{) }
\KeywordTok{summary}\NormalTok{(m_cont)}
\KeywordTok{plot}\NormalTok{(m_cont)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{comparison-classification_regression_with_h2o_deep_learning_files/figure-latex/checpointing-1} \end{center}

Once we are satisfied with the results, we can save the model to disk (on the cluster). In this example, we store the model in a directory called \texttt{mybest\_deeplearning\_covtype\_model}, which will be created for us since force=TRUE.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{path <-}\StringTok{ }\KeywordTok{h2o.saveModel}\NormalTok{(m_cont, }
          \DataTypeTok{path =} \KeywordTok{file.path}\NormalTok{(data_out_dir, }\StringTok{"mybest_deeplearning_covtype_model"}\NormalTok{), }\DataTypeTok{force=}\OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

It can be loaded later with the following command:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{print}\NormalTok{(path)}
\CommentTok{#> [1] "/home/datascience/repos/machine-learning-rsuite/export/mybest_deeplearning_covtype_model/dl_model_tuned_continued"}
\NormalTok{m_loaded <-}\StringTok{ }\KeywordTok{h2o.loadModel}\NormalTok{(path)}
\KeywordTok{summary}\NormalTok{(m_loaded)}
\CommentTok{#> Model Details:}
\CommentTok{#> ==============}
\CommentTok{#> }
\CommentTok{#> H2OMultinomialModel: deeplearning}
\CommentTok{#> Model Key:  dl_model_tuned_continued }
\CommentTok{#> Status of Neuron Layers: predicting Cover_Type, 7-class classification, multinomial distribution, CrossEntropy loss, 41,223 weights/biases, 334.1 KB, 4,202,367 training samples, mini-batch size 1}
\CommentTok{#>   layer units      type dropout       l1       l2 mean_rate rate_rms}
\CommentTok{#> 1     1    56     Input  0.00 %       NA       NA        NA       NA}
\CommentTok{#> 2     2   128 Rectifier  0.00 % 0.000010 0.000010  0.001063 0.000000}
\CommentTok{#> 3     3   128 Rectifier  0.00 % 0.000010 0.000010  0.001063 0.000000}
\CommentTok{#> 4     4   128 Rectifier  0.00 % 0.000010 0.000010  0.001063 0.000000}
\CommentTok{#> 5     5     7   Softmax      NA 0.000010 0.000010  0.001063 0.000000}
\CommentTok{#>   momentum mean_weight weight_rms mean_bias bias_rms}
\CommentTok{#> 1       NA          NA         NA        NA       NA}
\CommentTok{#> 2 0.284047   -0.010841   0.324725  0.008575 0.320140}
\CommentTok{#> 3 0.284047   -0.055705   0.226045  0.869899 0.364565}
\CommentTok{#> 4 0.284047   -0.063183   0.221199  0.804981 0.194941}
\CommentTok{#> 5 0.284047   -0.022196   0.270699  0.003139 0.790763}
\CommentTok{#> }
\CommentTok{#> H2OMultinomialMetrics: deeplearning}
\CommentTok{#> ** Reported on training data. **}
\CommentTok{#> ** Metrics reported on temporary training frame with 9933 samples **}
\CommentTok{#> }
\CommentTok{#> Training Set Metrics: }
\CommentTok{#> =====================}
\CommentTok{#> }
\CommentTok{#> MSE: (Extract with `h2o.mse`) 0.0482}
\CommentTok{#> RMSE: (Extract with `h2o.rmse`) 0.22}
\CommentTok{#> Logloss: (Extract with `h2o.logloss`) 0.162}
\CommentTok{#> Mean Per-Class Error: 0.113}
\CommentTok{#> Confusion Matrix: Extract with `h2o.confusionMatrix(<model>,train = TRUE)`)}
\CommentTok{#> =========================================================================}
\CommentTok{#> Confusion Matrix: Row labels: Actual class; Column labels: Predicted class}
\CommentTok{#>         class_1 class_2 class_3 class_4 class_5 class_6 class_7  Error}
\CommentTok{#> class_1    3344     262       0       0       2       1       9 0.0757}
\CommentTok{#> class_2     182    4666       9       0      15       7       0 0.0437}
\CommentTok{#> class_3       0       4     578       2       2      18       0 0.0430}
\CommentTok{#> class_4       0       0       8      52       0       2       0 0.1613}
\CommentTok{#> class_5       6      26       5       0     137       0       0 0.2126}
\CommentTok{#> class_6       2      13      27       3       0     221       0 0.1692}
\CommentTok{#> class_7      24       4       0       0       0       0     302 0.0848}
\CommentTok{#> Totals     3558    4975     627      57     156     249     311 0.0637}
\CommentTok{#>                  Rate}
\CommentTok{#> class_1 = 274 / 3,618}
\CommentTok{#> class_2 = 213 / 4,879}
\CommentTok{#> class_3 =    26 / 604}
\CommentTok{#> class_4 =     10 / 62}
\CommentTok{#> class_5 =    37 / 174}
\CommentTok{#> class_6 =    45 / 266}
\CommentTok{#> class_7 =    28 / 330}
\CommentTok{#> Totals  = 633 / 9,933}
\CommentTok{#> }
\CommentTok{#> Hit Ratio Table: Extract with `h2o.hit_ratio_table(<model>,train = TRUE)`}
\CommentTok{#> =======================================================================}
\CommentTok{#> Top-7 Hit Ratios: }
\CommentTok{#>   k hit_ratio}
\CommentTok{#> 1 1  0.936273}
\CommentTok{#> 2 2  0.997684}
\CommentTok{#> 3 3  0.999698}
\CommentTok{#> 4 4  1.000000}
\CommentTok{#> 5 5  1.000000}
\CommentTok{#> 6 6  1.000000}
\CommentTok{#> 7 7  1.000000}
\CommentTok{#> }
\CommentTok{#> }
\CommentTok{#> H2OMultinomialMetrics: deeplearning}
\CommentTok{#> ** Reported on validation data. **}
\CommentTok{#> ** Metrics reported on temporary validation frame with 10032 samples **}
\CommentTok{#> }
\CommentTok{#> Validation Set Metrics: }
\CommentTok{#> =====================}
\CommentTok{#> }
\CommentTok{#> MSE: (Extract with `h2o.mse`) 0.053}
\CommentTok{#> RMSE: (Extract with `h2o.rmse`) 0.23}
\CommentTok{#> Logloss: (Extract with `h2o.logloss`) 0.178}
\CommentTok{#> Mean Per-Class Error: 0.15}
\CommentTok{#> Confusion Matrix: Extract with `h2o.confusionMatrix(<model>,valid = TRUE)`)}
\CommentTok{#> =========================================================================}
\CommentTok{#> Confusion Matrix: Row labels: Actual class; Column labels: Predicted class}
\CommentTok{#>         class_1 class_2 class_3 class_4 class_5 class_6 class_7  Error}
\CommentTok{#> class_1    3354     269       1       0       2       0       6 0.0765}
\CommentTok{#> class_2     189    4664      15       0      17      13       3 0.0484}
\CommentTok{#> class_3       0      18     596       4       0      25       0 0.0731}
\CommentTok{#> class_4       0       0      10      32       0       4       0 0.3043}
\CommentTok{#> class_5       6      37       2       0     108       0       0 0.2941}
\CommentTok{#> class_6       0      15      38       1       0     235       0 0.1869}
\CommentTok{#> class_7      22       2       0       0       0       0     344 0.0652}
\CommentTok{#> Totals     3571    5005     662      37     127     277     353 0.0697}
\CommentTok{#>                   Rate}
\CommentTok{#> class_1 =  278 / 3,632}
\CommentTok{#> class_2 =  237 / 4,901}
\CommentTok{#> class_3 =     47 / 643}
\CommentTok{#> class_4 =      14 / 46}
\CommentTok{#> class_5 =     45 / 153}
\CommentTok{#> class_6 =     54 / 289}
\CommentTok{#> class_7 =     24 / 368}
\CommentTok{#> Totals  = 699 / 10,032}
\CommentTok{#> }
\CommentTok{#> Hit Ratio Table: Extract with `h2o.hit_ratio_table(<model>,valid = TRUE)`}
\CommentTok{#> =======================================================================}
\CommentTok{#> Top-7 Hit Ratios: }
\CommentTok{#>   k hit_ratio}
\CommentTok{#> 1 1  0.930323}
\CommentTok{#> 2 2  0.996312}
\CommentTok{#> 3 3  0.999801}
\CommentTok{#> 4 4  1.000000}
\CommentTok{#> 5 5  1.000000}
\CommentTok{#> 6 6  1.000000}
\CommentTok{#> 7 7  1.000000}
\CommentTok{#> }
\CommentTok{#> }
\CommentTok{#> }
\CommentTok{#> }
\CommentTok{#> Scoring History: }
\CommentTok{#>              timestamp          duration training_speed   epochs}
\CommentTok{#> 1  2019-09-18 16:07:23         0.000 sec             NA  0.00000}
\CommentTok{#> 2  2019-09-18 16:07:29         6.416 sec  16447 obs/sec  0.28623}
\CommentTok{#> 3  2019-09-18 16:07:42        19.613 sec  20981 obs/sec  1.14684}
\CommentTok{#> 4  2019-09-18 16:07:51        27.872 sec  22163 obs/sec  1.72020}
\CommentTok{#> 5  2019-09-18 16:08:02        38.887 sec  21091 obs/sec  2.29155}
\CommentTok{#> 6  2019-09-18 16:08:10        46.794 sec  21912 obs/sec  2.86555}
\CommentTok{#> 7  2019-09-18 16:08:20        57.297 sec  23276 obs/sec  3.72887}
\CommentTok{#> 8  2019-09-18 16:08:32  1 min  9.478 sec  23582 obs/sec  4.58847}
\CommentTok{#> 9  2019-09-18 16:08:40  1 min 17.269 sec  23866 obs/sec  5.16196}
\CommentTok{#> 10 2019-09-18 16:08:52  1 min 28.655 sec  24252 obs/sec  6.02399}
\CommentTok{#> 11 2019-09-18 16:08:59  1 min 36.499 sec  24402 obs/sec  6.59606}
\CommentTok{#> 12 2019-09-18 16:09:07  1 min 44.628 sec  24473 obs/sec  7.16889}
\CommentTok{#> 13 2019-09-18 16:09:19  1 min 55.714 sec  24767 obs/sec  8.02787}
\CommentTok{#> 14 2019-09-18 16:09:30  2 min  6.976 sec  24977 obs/sec  8.88889}
\CommentTok{#> 15 2019-09-18 16:09:40  2 min 17.375 sec  25311 obs/sec  9.74901}
\CommentTok{#> 16 2019-09-18 16:09:44  2 min 20.928 sec  25412 obs/sec 10.03421}
\CommentTok{#> 17 2019-09-18 16:11:02  2 min 24.360 sec  25543 obs/sec 10.32054}
\CommentTok{#> 18 2019-09-18 16:11:10  2 min 32.791 sec  25483 obs/sec 10.89361}
\CommentTok{#> 19 2019-09-18 16:11:23  2 min 45.257 sec  25404 obs/sec 11.75408}
\CommentTok{#> 20 2019-09-18 16:11:27  2 min 49.472 sec  25396 obs/sec 12.04065}
\CommentTok{#>    iterations        samples training_rmse training_logloss training_r2}
\CommentTok{#> 1           0       0.000000            NA               NA          NA}
\CommentTok{#> 2           1   99900.000000       0.42461          0.55871     0.91016}
\CommentTok{#> 3           4  400265.000000       0.36386          0.41433     0.93402}
\CommentTok{#> 4           6  600375.000000       0.33460          0.35497     0.94421}
\CommentTok{#> 5           8  799786.000000       0.31826          0.32369     0.94953}
\CommentTok{#> 6          10 1000119.000000       0.30627          0.30096     0.95326}
\CommentTok{#> 7          13 1301430.000000       0.29274          0.27498     0.95729}
\CommentTok{#> 8          16 1601446.000000       0.28144          0.25803     0.96053}
\CommentTok{#> 9          18 1801600.000000       0.27123          0.24053     0.96334}
\CommentTok{#> 10         21 2102464.000000       0.26230          0.22390     0.96572}
\CommentTok{#> 11         23 2302125.000000       0.25662          0.21526     0.96718}
\CommentTok{#> 12         25 2502050.000000       0.25657          0.21498     0.96720}
\CommentTok{#> 13         28 2801846.000000       0.24881          0.20293     0.96915}
\CommentTok{#> 14         31 3102356.000000       0.24224          0.19232     0.97076}
\CommentTok{#> 15         34 3402549.000000       0.24020          0.18927     0.97125}
\CommentTok{#> 16         35 3502089.000000       0.23962          0.18680     0.97139}
\CommentTok{#> 17         36 3602023.000000       0.23663          0.18390     0.96998}
\CommentTok{#> 18         38 3802034.000000       0.22964          0.17594     0.97173}
\CommentTok{#> 19         41 4102351.000000       0.22488          0.16809     0.97289}
\CommentTok{#> 20         42 4202367.000000       0.21963          0.16190     0.97414}
\CommentTok{#>    training_classification_error validation_rmse validation_logloss}
\CommentTok{#> 1                             NA              NA                 NA}
\CommentTok{#> 2                        0.24022         0.42163            0.55128}
\CommentTok{#> 3                        0.17659         0.37069            0.42487}
\CommentTok{#> 4                        0.15078         0.33673            0.36105}
\CommentTok{#> 5                        0.13487         0.32572            0.34029}
\CommentTok{#> 6                        0.12786         0.31685            0.32451}
\CommentTok{#> 7                        0.11556         0.30173            0.29507}
\CommentTok{#> 8                        0.10665         0.28967            0.27707}
\CommentTok{#> 9                        0.09835         0.28672            0.27253}
\CommentTok{#> 10                       0.09285         0.27291            0.24511}
\CommentTok{#> 11                       0.08754         0.26546            0.23595}
\CommentTok{#> 12                       0.08884         0.26914            0.24172}
\CommentTok{#> 13                       0.08434         0.25941            0.22542}
\CommentTok{#> 14                       0.07964         0.25119            0.21287}
\CommentTok{#> 15                       0.07764         0.25087            0.21213}
\CommentTok{#> 16                       0.07944         0.24821            0.20775}
\CommentTok{#> 17                       0.07390         0.24246            0.19481}
\CommentTok{#> 18                       0.06735         0.23945            0.19052}
\CommentTok{#> 19                       0.06856         0.23635            0.18518}
\CommentTok{#> 20                       0.06373         0.23021            0.17798}
\CommentTok{#>    validation_r2 validation_classification_error}
\CommentTok{#> 1             NA                              NA}
\CommentTok{#> 2        0.90783                         0.23755}
\CommentTok{#> 3        0.92876                         0.18557}
\CommentTok{#> 4        0.94121                         0.15175}
\CommentTok{#> 5        0.94499                         0.14128}
\CommentTok{#> 6        0.94795                         0.13738}
\CommentTok{#> 7        0.95280                         0.12132}
\CommentTok{#> 8        0.95649                         0.11254}
\CommentTok{#> 9        0.95738                         0.11055}
\CommentTok{#> 10       0.96138                         0.10157}
\CommentTok{#> 11       0.96346                         0.09339}
\CommentTok{#> 12       0.96244                         0.09768}
\CommentTok{#> 13       0.96511                         0.09099}
\CommentTok{#> 14       0.96729                         0.08321}
\CommentTok{#> 15       0.96737                         0.08560}
\CommentTok{#> 16       0.96806                         0.08371}
\CommentTok{#> 17       0.96996                         0.07935}
\CommentTok{#> 18       0.97070                         0.07626}
\CommentTok{#> 19       0.97145                         0.07646}
\CommentTok{#> 20       0.97291                         0.06968}
\CommentTok{#> }
\CommentTok{#> Variable Importances: (Extract with `h2o.varimp`) }
\CommentTok{#> =================================================}
\CommentTok{#> }
\CommentTok{#> Variable Importances: }
\CommentTok{#>                             variable relative_importance scaled_importance}
\CommentTok{#> 1                          Elevation            1.000000          1.000000}
\CommentTok{#> 2 Horizontal_Distance_To_Fire_Points            0.965839          0.965839}
\CommentTok{#> 3    Horizontal_Distance_To_Roadways            0.960288          0.960288}
\CommentTok{#> 4             Wilderness_Area.area_0            0.628117          0.628117}
\CommentTok{#> 5   Horizontal_Distance_To_Hydrology            0.592000          0.592000}
\CommentTok{#>   percentage}
\CommentTok{#> 1   0.047730}
\CommentTok{#> 2   0.046100}
\CommentTok{#> 3   0.045835}
\CommentTok{#> 4   0.029980}
\CommentTok{#> 5   0.028256}
\CommentTok{#> }
\CommentTok{#> ---}
\CommentTok{#>                       variable relative_importance scaled_importance}
\CommentTok{#> 51           Soil_Type.type_13            0.165256          0.165256}
\CommentTok{#> 52            Soil_Type.type_6            0.154398          0.154398}
\CommentTok{#> 53           Soil_Type.type_14            0.144444          0.144444}
\CommentTok{#> 54           Soil_Type.type_35            0.144247          0.144247}
\CommentTok{#> 55       Soil_Type.missing(NA)            0.000000          0.000000}
\CommentTok{#> 56 Wilderness_Area.missing(NA)            0.000000          0.000000}
\CommentTok{#>    percentage}
\CommentTok{#> 51   0.007888}
\CommentTok{#> 52   0.007369}
\CommentTok{#> 53   0.006894}
\CommentTok{#> 54   0.006885}
\CommentTok{#> 55   0.000000}
\CommentTok{#> 56   0.000000}
\end{Highlighting}
\end{Shaded}

This model is fully functional and can be inspected, restarted, or used to score a dataset, etc. Note that binary compatibility between H2O versions is currently not guaranteed.

\hypertarget{cross-validation}{%
\subsection{Cross-Validation}\label{cross-validation}}

For N-fold cross-validation, specify \texttt{nfolds\textgreater{}1} instead of (or in addition to) a validation frame, and \texttt{N+1} models will be built: 1 model on the full training data, and N models with each 1/N-th of the data held out (there are different holdout strategies). Those N models then score on the held out data, and their combined predictions on the full training data are scored to get the cross-validation metrics.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dlmodel <-}\StringTok{ }\KeywordTok{h2o.deeplearning}\NormalTok{(}
  \DataTypeTok{x=}\NormalTok{predictors,}
  \DataTypeTok{y=}\NormalTok{response, }
  \DataTypeTok{training_frame=}\NormalTok{train,}
  \DataTypeTok{hidden=}\KeywordTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{,}\DecValTok{10}\NormalTok{),}
  \DataTypeTok{epochs=}\DecValTok{1}\NormalTok{,}
  \DataTypeTok{nfolds=}\DecValTok{5}\NormalTok{,}
  \DataTypeTok{fold_assignment=}\StringTok{"Modulo"} \CommentTok{# can be "AUTO", "Modulo", "Random" or "Stratified"}
\NormalTok{  )}
\NormalTok{dlmodel}
\end{Highlighting}
\end{Shaded}

N-fold cross-validation is especially useful with early stopping, as the main model will pick the ideal number of epochs from the convergence behavior of the cross-validation models.

\hypertarget{regression-and-binary-classification}{%
\section{Regression and Binary Classification}\label{regression-and-binary-classification}}

Assume we want to turn the multi-class problem above into a binary classification problem. We create a binary response as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{train}\OperatorTok{$}\NormalTok{bin_response <-}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(train[,response] }\OperatorTok{==}\StringTok{ "class_1"}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Let's build a quick model and inspect the model:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dlmodel <-}\StringTok{ }\KeywordTok{h2o.deeplearning}\NormalTok{(}
  \DataTypeTok{x=}\NormalTok{predictors,}
  \DataTypeTok{y=}\StringTok{"bin_response"}\NormalTok{, }
  \DataTypeTok{training_frame=}\NormalTok{train,}
  \DataTypeTok{hidden=}\KeywordTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{,}\DecValTok{10}\NormalTok{),}
  \DataTypeTok{epochs=}\FloatTok{0.1}
\NormalTok{)}
\KeywordTok{summary}\NormalTok{(dlmodel)}
\end{Highlighting}
\end{Shaded}

Instead of a binary classification model, we find a regression model (H2ORegressionModel) that contains only 1 output neuron (instead of 2). The reason is that the response was a numerical feature (ordinal numbers 0 and 1), and H2O Deep Learning was run with distribution=AUTO, which defaulted to a Gaussian regression problem for a real-valued response. H2O Deep Learning supports regression for distributions other than Gaussian such as Poisson, Gamma, Tweedie, Laplace. It also supports Huber loss and per-row offsets specified via an offset\_column. We refer to our H2O Deep Learning regression code examples for more information.

To perform classification, the response must first be turned into a categorical (factor) feature:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{train}\OperatorTok{$}\NormalTok{bin_response <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(train}\OperatorTok{$}\NormalTok{bin_response) }\CommentTok{##make categorical}
\NormalTok{dlmodel <-}\StringTok{ }\KeywordTok{h2o.deeplearning}\NormalTok{(}
  \DataTypeTok{x=}\NormalTok{predictors,}
  \DataTypeTok{y=}\StringTok{"bin_response"}\NormalTok{, }
  \DataTypeTok{training_frame=}\NormalTok{train,}
  \DataTypeTok{hidden=}\KeywordTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{,}\DecValTok{10}\NormalTok{),}
  \DataTypeTok{epochs=}\FloatTok{0.1}
  \CommentTok{#balance_classes=T    ## enable this for high class imbalance}
\NormalTok{)}
\KeywordTok{summary}\NormalTok{(dlmodel) }\CommentTok{## Now the model metrics contain AUC for binary classification}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{h2o.performance}\NormalTok{(dlmodel)) }\CommentTok{## display ROC curve}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{comparison-classification_regression_with_h2o_deep_learning_files/figure-latex/bin_class_as_factor-1} \end{center}

Now the model performs (binary) classification, and has multiple (2) output neurons.

\hypertarget{unsupervised-anomaly-detection}{%
\section{Unsupervised Anomaly detection}\label{unsupervised-anomaly-detection}}

For instructions on how to build unsupervised models with H2O Deep Learning, we refer to our previous Tutorial on Anomaly Detection with H2O Deep Learning and our MNIST Anomaly detection code example, as well as our Stacked AutoEncoder R code example and another one for Unsupervised Pretraining with an AutoEncoder R code example.

\hypertarget{h2o-deep-learning-tips-tricks}{%
\section{H2O Deep Learning Tips \& Tricks}\label{h2o-deep-learning-tips-tricks}}

\hypertarget{performance-tuning}{%
\subsection{Performance Tuning}\label{performance-tuning}}

The Definitive H2O Deep Learning Performance Tuning blog post covers many of the following points that affect the computational efficiency, so it's highly recommended.

\hypertarget{activation-functions}{%
\subsection{Activation Functions}\label{activation-functions}}

While sigmoids have been used historically for neural networks, H2O Deep Learning implements Tanh, a scaled and shifted variant of the sigmoid which is symmetric around 0. Since its output values are bounded by -1..1, the stability of the neural network is rarely endangered. However, the derivative of the tanh function is always non-zero and back-propagation (training) of the weights is more computationally expensive than for rectified linear units, or Rectifier, which is max(0,x) and has vanishing gradient for x\textless{}=0, leading to much faster training speed for large networks and is often the fastest path to accuracy on larger problems. In case you encounter instabilities with the Rectifier (in which case model building is automatically aborted), try a limited value to re-scale the weights: max\_w2=10. The Maxout activation function is computationally more expensive, but can lead to higher accuracy. It is a generalized version of the Rectifier with two non-zero channels. In practice, the Rectifier (and RectifierWithDropout, see below) is the most versatile and performant option for most problems.

\hypertarget{generalization-techniques}{%
\subsection{Generalization Techniques}\label{generalization-techniques}}

L1 and L2 penalties can be applied by specifying the l1 and l2 parameters. Intuition: L1 lets only strong weights survive (constant pulling force towards zero), while L2 prevents any single weight from getting too big. Dropout has recently been introduced as a powerful generalization technique, and is available as a parameter per layer, including the input layer. \texttt{input\_dropout\_ratio} controls the amount of input layer neurons that are randomly dropped (set to zero), while \texttt{hidden\_dropout\_ratios} are specified for each hidden layer. The former controls overfitting with respect to the input data (useful for high-dimensional noisy data), while the latter controls overfitting of the learned features. Note that \texttt{hidden\_dropout\_ratios} require the activation function to end with \ldots{}WithDropout.

\hypertarget{early-stopping-and-optimizing-for-lowest-validation-error}{%
\subsection{Early stopping and optimizing for lowest validation error}\label{early-stopping-and-optimizing-for-lowest-validation-error}}

By default, Deep Learning training stops when the stopping\_metric does not improve by at least stopping\_tolerance (0.01 means 1\% improvement) for stopping\_rounds consecutive scoring events on the training (or validation) data. By default, \texttt{overwrite\_with\_best\_model} is enabled and the model returned after training for the specified number of epochs (or after stopping early due to convergence) is the model that has the best training set error (according to the metric specified by stopping\_metric), or, if a validation set is provided, the lowest validation set error. Note that the training or validation set errors can be based on a subset of the training or validation data, depending on the values for \texttt{score\_validation\_samples} or \texttt{score\_training\_samples}, see below. For early stopping on a predefined error rate on the training data (accuracy for classification or MSE for regression), specify \texttt{classification\_stop} or \texttt{regression\_stop.}

\hypertarget{training-samples-per-mapreduce-iteration}{%
\subsection{Training Samples per MapReduce Iteration}\label{training-samples-per-mapreduce-iteration}}

The parameter \texttt{train\_samples\_per\_iteration} matters especially in multi-node operation. It controls the number of rows trained on for each \texttt{MapReduce} iteration. Depending on the value selected, one MapReduce pass can sample observations, and multiple such passes are needed to train for one epoch. All H2O compute nodes then communicate to agree on the best model coefficients (weights/biases) so far, and the model may then be scored (controlled by other parameters below). The default value of -2 indicates auto-tuning, which attemps to keep the communication overhead at 5\% of the total runtime. The parameter \texttt{target\_ratio\_comm\_to\_comp} controls this ratio. This parameter is explained in more detail in the H2O Deep Learning booklet,

\hypertarget{categorical-data}{%
\subsection{Categorical Data}\label{categorical-data}}

For categorical data, a feature with K factor levels is automatically one-hot encoded (horizontalized) into \texttt{K-1} input neurons. Hence, the input neuron layer can grow substantially for datasets with high factor counts. In these cases, it might make sense to reduce the number of hidden neurons in the first hidden layer, such that large numbers of factor levels can be handled. In the limit of 1 neuron in the first hidden layer, the resulting model is similar to logistic regression with stochastic gradient descent, except that for classification problems, there's still a softmax output layer, and that the activation function is not necessarily a sigmoid (Tanh). If variable importances are computed, it is recommended to turn on \texttt{use\_all\_factor\_levels} (K input neurons for K levels). The experimental option max\_categorical\_features uses feature hashing to reduce the number of input neurons via the hash trick at the expense of hash collisions and reduced accuracy. Another way to reduce the dimensionality of the (categorical) features is to use \texttt{h2o.glrm()}, we refer to the GLRM tutorial for more details.

\hypertarget{sparse-data}{%
\subsection{Sparse Data}\label{sparse-data}}

If the input data is sparse (many zeros), then it might make sense to enable the sparse option. This will result in the input not being standardized (0 mean, 1 variance), but only de-scaled (1 variance) and 0 values remain 0, leading to more efficient back-propagation. Sparsity is also a reason why CPU implementations can be faster than GPU implementations, because they can take advantage of if/else statements more effectively.

\hypertarget{missing-values}{%
\subsection{Missing Values}\label{missing-values}}

H2O Deep Learning automatically does mean imputation for missing values during training (leaving the input layer activation at 0 after standardizing the values). For testing, missing test set values are also treated the same way by default. See the \texttt{h2o.impute} function to do your own mean imputation.

\hypertarget{loss-functions-distributions-offsets-observation-weights}{%
\subsection{Loss functions, Distributions, Offsets, Observation Weights}\label{loss-functions-distributions-offsets-observation-weights}}

H2O Deep Learning supports advanced statistical features such as multiple loss functions, non-Gaussian distributions, per-row offsets and observation weights. In addition to Gaussian distributions and Squared loss, H2O Deep Learning supports Poisson, Gamma, Tweedie and Laplace distributions. It also supports Absolute and Huber loss and per-row offsets specified via an \texttt{offset\_column.} Observation weights are supported via a user-specified weights\_column.

We refer to our H2O Deep Learning R test code examples for more information.

\hypertarget{exporting-weights-and-biases}{%
\subsection{Exporting Weights and Biases}\label{exporting-weights-and-biases}}

The model parameters (weights connecting two adjacent layers and per-neuron bias terms) can be stored as H2O Frames (like a dataset) by enabling \texttt{export\_weights\_and\_biases}, and they can be accessed as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iris_dl <-}\StringTok{ }\KeywordTok{h2o.deeplearning}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{4}\NormalTok{,}\DecValTok{5}\NormalTok{,}\KeywordTok{as.h2o}\NormalTok{(iris),}
             \DataTypeTok{export_weights_and_biases=}\NormalTok{T)}
\CommentTok{#> }
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|}\StringTok{                                                                 }\ErrorTok{|}\StringTok{   }\DecValTok{0}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=================================================================|}\StringTok{ }\DecValTok{100}\NormalTok{%}
\CommentTok{#> }
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|}\StringTok{                                                                 }\ErrorTok{|}\StringTok{   }\DecValTok{0}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|======}\StringTok{                                                           }\ErrorTok{|}\StringTok{  }\DecValTok{10}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=================================================================|}\StringTok{ }\DecValTok{100}\NormalTok{%}
\KeywordTok{h2o.weights}\NormalTok{(iris_dl, }\DataTypeTok{matrix_id=}\DecValTok{1}\NormalTok{)}
\CommentTok{#>   Sepal.Length Sepal.Width Petal.Length Petal.Width}
\CommentTok{#> 1      0.12783     0.03120       0.1501     -0.0262}
\CommentTok{#> 2     -0.16487     0.00398      -0.1796     -0.0128}
\CommentTok{#> 3     -0.00595     0.02936       0.0112      0.0463}
\CommentTok{#> 4      0.01206    -0.03346       0.1536      0.0380}
\CommentTok{#> 5     -0.04686     0.01739      -0.2189     -0.1949}
\CommentTok{#> 6      0.10783    -0.16339       0.1475      0.1661}
\CommentTok{#> }
\CommentTok{#> [200 rows x 4 columns]}
\KeywordTok{h2o.weights}\NormalTok{(iris_dl, }\DataTypeTok{matrix_id=}\DecValTok{2}\NormalTok{)}
\CommentTok{#>        C1      C2      C3      C4      C5      C6       C7        C8}
\CommentTok{#> 1 -0.0421  0.0873  0.1032  0.0493  0.0777 -0.0860  0.01497 -0.040998}
\CommentTok{#> 2  0.1119  0.1184 -0.0523 -0.0383  0.0672 -0.0472 -0.07735 -0.085760}
\CommentTok{#> 3  0.1133 -0.0271 -0.0645  0.1013 -0.0116  0.0647  0.01483 -0.019384}
\CommentTok{#> 4 -0.0903 -0.0469  0.0915  0.0645  0.0126 -0.1322 -0.04695 -0.111394}
\CommentTok{#> 5 -0.0873 -0.0587 -0.0131  0.1224  0.0581 -0.0571 -0.00122 -0.059171}
\CommentTok{#> 6 -0.0819  0.1211 -0.0613 -0.1020  0.0667  0.0548  0.04662  0.000408}
\CommentTok{#>        C9     C10      C11     C12     C13     C14     C15     C16     C17}
\CommentTok{#> 1 -0.0440 -0.0364 -0.01144 -0.0795 -0.0798 -0.0840 -0.0385 -0.1067  0.0893}
\CommentTok{#> 2  0.0692  0.1071 -0.01064  0.0891  0.0865 -0.0776 -0.0719  0.1095  0.0445}
\CommentTok{#> 3  0.0127 -0.0461  0.00277  0.0351 -0.1386  0.0272 -0.1249 -0.0810 -0.0707}
\CommentTok{#> 4  0.0668  0.1368 -0.02408  0.0327 -0.0458  0.1283 -0.0919 -0.1094  0.1187}
\CommentTok{#> 5  0.0952 -0.0570 -0.10554 -0.0821  0.0320 -0.0738 -0.0300 -0.0941 -0.1113}
\CommentTok{#> 6 -0.0351  0.1179 -0.02703 -0.0672 -0.0164  0.1170  0.0970 -0.0470  0.1090}
\CommentTok{#>        C18      C19     C20     C21     C22     C23      C24      C25}
\CommentTok{#> 1  0.08689  0.02225  0.1206 -0.0690  0.0319 -0.0364 -0.05090 -0.05037}
\CommentTok{#> 2  0.09734 -0.08062  0.0399  0.1027 -0.0833 -0.0547  0.09267  0.05185}
\CommentTok{#> 3 -0.11916 -0.08284  0.0336  0.0633  0.0649 -0.0346  0.00571 -0.08496}
\CommentTok{#> 4  0.00632  0.09803 -0.0212 -0.0665  0.1359 -0.0759 -0.12357  0.03661}
\CommentTok{#> 5  0.00186  0.09125 -0.0818  0.0858  0.0255 -0.0576 -0.08107 -0.00722}
\CommentTok{#> 6  0.01870 -0.00543  0.0419  0.0401  0.0926  0.0635 -0.06597 -0.08127}
\CommentTok{#>       C26     C27     C28      C29      C30      C31      C32     C33}
\CommentTok{#> 1  0.0282  0.0887 -0.1214  0.09143  0.07200 -0.09157  0.07595 -0.0390}
\CommentTok{#> 2 -0.0758  0.1104 -0.0496 -0.00459 -0.08874 -0.10924  0.07171 -0.0205}
\CommentTok{#> 3  0.0528  0.1088 -0.0221 -0.01530 -0.09284  0.03791 -0.08658  0.0346}
\CommentTok{#> 4  0.0493  0.0433 -0.1015  0.05354 -0.00643  0.09072 -0.00161  0.1203}
\CommentTok{#> 5  0.0997  0.0786  0.1118 -0.12147 -0.06114 -0.00352  0.05894 -0.1160}
\CommentTok{#> 6  0.0797 -0.1283 -0.0613 -0.09978 -0.12424 -0.05458  0.01594  0.1211}
\CommentTok{#>        C34      C35      C36      C37     C38     C39      C40      C41}
\CommentTok{#> 1 -0.06196 -0.06911 -0.03342 -0.08844 -0.1164  0.1217 -0.07643 -0.10717}
\CommentTok{#> 2 -0.08745  0.03487 -0.09796  0.10160 -0.0941  0.0745 -0.00918  0.00853}
\CommentTok{#> 3 -0.01081  0.10516  0.08577  0.00760 -0.0621  0.0186 -0.03089  0.03620}
\CommentTok{#> 4 -0.00596  0.00550 -0.00148 -0.00358 -0.0737  0.1133  0.00721  0.06511}
\CommentTok{#> 5 -0.08903 -0.08898  0.07231 -0.12475  0.0700 -0.0559 -0.07811  0.01823}
\CommentTok{#> 6  0.07347  0.00651 -0.07097 -0.04557 -0.0422  0.0601  0.01943  0.05346}
\CommentTok{#>       C42     C43     C44       C45      C46     C47       C48     C49}
\CommentTok{#> 1  0.1151  0.0743 -0.0653  0.094533  0.05526 -0.0310 -0.114233 -0.0790}
\CommentTok{#> 2 -0.0446  0.0355 -0.0629  0.070812 -0.01438 -0.0738 -0.000069 -0.0478}
\CommentTok{#> 3  0.1098 -0.1006 -0.1304  0.022230  0.12149 -0.1015 -0.006622  0.0831}
\CommentTok{#> 4  0.0510 -0.0849 -0.0572 -0.042663  0.04502 -0.0483 -0.012013 -0.0328}
\CommentTok{#> 5 -0.0316  0.0986 -0.0379  0.109019 -0.00642  0.0898 -0.083742  0.1087}
\CommentTok{#> 6  0.0960  0.0660 -0.0464 -0.000126 -0.07409  0.0543 -0.084661 -0.0680}
\CommentTok{#>       C50      C51     C52     C53     C54     C55     C56      C57}
\CommentTok{#> 1 -0.0330 -0.04943 -0.0151  0.0577  0.0245  0.1126  0.0388  0.00627}
\CommentTok{#> 2 -0.1076  0.00191  0.1210 -0.1144  0.0196  0.0305  0.0809  0.01624}
\CommentTok{#> 3 -0.1060  0.01020  0.0533  0.0120 -0.0686 -0.1305  0.0652  0.04340}
\CommentTok{#> 4  0.0859  0.07703  0.0797 -0.0728 -0.0238  0.1029 -0.0212  0.05030}
\CommentTok{#> 5 -0.0671 -0.06745  0.0118 -0.0486  0.1161 -0.1342  0.0166  0.04059}
\CommentTok{#> 6 -0.1244 -0.00897 -0.1195 -0.0428  0.0445  0.0880  0.0517 -0.01197}
\CommentTok{#>       C58     C59     C60       C61      C62       C63     C64      C65}
\CommentTok{#> 1  0.0771 -0.0522 -0.0470 -0.028394  0.04790  0.089048  0.1037  0.10457}
\CommentTok{#> 2  0.0209  0.0191  0.0108 -0.083047  0.02710 -0.036491  0.0331 -0.04738}
\CommentTok{#> 3 -0.0616 -0.0437 -0.0958 -0.021860  0.06576 -0.000653  0.0492  0.09300}
\CommentTok{#> 4  0.0684 -0.1045  0.0723 -0.019187  0.00277  0.103602  0.1091  0.00414}
\CommentTok{#> 5  0.0169  0.0305 -0.0518 -0.032521  0.05497 -0.035296  0.0571 -0.09255}
\CommentTok{#> 6 -0.0423 -0.0164 -0.0377 -0.000426 -0.06389 -0.020087 -0.0795  0.02603}
\CommentTok{#>        C66      C67      C68     C69     C70      C71       C72     C73}
\CommentTok{#> 1  0.04723  0.10220 -0.01884  0.0420 -0.1123  0.00192  0.000253 -0.0646}
\CommentTok{#> 2  0.06892 -0.05563  0.00178 -0.0879 -0.1032 -0.05863 -0.065380 -0.0230}
\CommentTok{#> 3 -0.10661  0.06551  0.05542 -0.0832  0.1130 -0.10356  0.040896 -0.1076}
\CommentTok{#> 4  0.00948 -0.04648 -0.08551 -0.0721 -0.0779 -0.05955  0.087991 -0.0468}
\CommentTok{#> 5  0.05514 -0.03107  0.00995 -0.1101  0.0811 -0.08852 -0.104251  0.0470}
\CommentTok{#> 6 -0.02936 -0.00475  0.09422 -0.0618 -0.1000 -0.07336  0.101635  0.0921}
\CommentTok{#>       C74       C75     C76      C77      C78     C79      C80     C81}
\CommentTok{#> 1  0.0358  0.000627 -0.0685  0.11645  0.00531 -0.0642  0.00757 -0.1068}
\CommentTok{#> 2 -0.0603  0.016079 -0.0933 -0.02089  0.04972  0.0150  0.01555 -0.1015}
\CommentTok{#> 3 -0.0793 -0.135173  0.0466 -0.11461  0.04855 -0.1068  0.06307 -0.1207}
\CommentTok{#> 4 -0.0736  0.006731  0.0666  0.00716  0.12196 -0.0983 -0.05322  0.0350}
\CommentTok{#> 5  0.0375  0.042804  0.1167  0.04143 -0.04692  0.0831 -0.05423  0.0132}
\CommentTok{#> 6  0.1221  0.060883 -0.0243 -0.05036  0.08884  0.0256  0.08862 -0.1145}
\CommentTok{#>        C82     C83     C84     C85     C86     C87     C88      C89}
\CommentTok{#> 1  0.01389  0.1201 -0.1042  0.1306 -0.0659  0.0650  0.0582 -0.03086}
\CommentTok{#> 2  0.07088  0.0582 -0.0326  0.0509 -0.0702 -0.0580 -0.1076 -0.01377}
\CommentTok{#> 3  0.00152  0.0559  0.0627  0.0205  0.0829  0.0265  0.0307 -0.06123}
\CommentTok{#> 4  0.01129 -0.0511 -0.0505 -0.0690  0.0693 -0.0191 -0.0213 -0.12633}
\CommentTok{#> 5  0.05991 -0.0795 -0.0289 -0.0987 -0.0941 -0.0835  0.0312  0.04035}
\CommentTok{#> 6 -0.10004 -0.0861 -0.0957  0.0451 -0.0505 -0.1033 -0.0481 -0.00323}
\CommentTok{#>        C90     C91     C92     C93     C94     C95      C96       C97}
\CommentTok{#> 1 -0.00502 -0.1054  0.0972  0.0430  0.1160  0.0360 -0.10116  0.085933}
\CommentTok{#> 2  0.07973 -0.0826 -0.0399  0.0633 -0.0168 -0.0929  0.04439 -0.023410}
\CommentTok{#> 3 -0.13139  0.0867  0.1259 -0.0856 -0.0247 -0.0253  0.11184 -0.005639}
\CommentTok{#> 4  0.05243  0.0805 -0.0131  0.0397 -0.0867  0.0110  0.00245 -0.000453}
\CommentTok{#> 5  0.02458  0.1052 -0.0288 -0.0848 -0.0588  0.0236  0.09611 -0.113343}
\CommentTok{#> 6  0.10122  0.0657  0.0977 -0.0403 -0.0478 -0.0938  0.01367 -0.058340}
\CommentTok{#>        C98     C99    C100    C101     C102     C103     C104    C105}
\CommentTok{#> 1  0.00461  0.0705  0.1287 -0.0550  0.05422 -0.07768  0.02050 -0.0653}
\CommentTok{#> 2 -0.11635 -0.0704  0.0242 -0.0670  0.11632 -0.10687 -0.09043  0.0360}
\CommentTok{#> 3  0.08957 -0.0241 -0.1062 -0.0217 -0.02884  0.01308  0.11537 -0.0986}
\CommentTok{#> 4 -0.06628  0.0722  0.0186  0.0667 -0.12297 -0.06988 -0.00369  0.1183}
\CommentTok{#> 5  0.11663 -0.0931 -0.0904  0.0801 -0.00345  0.08317  0.07236 -0.1240}
\CommentTok{#> 6  0.08525 -0.0530  0.0171  0.0429  0.09091 -0.00673 -0.11552  0.0672}
\CommentTok{#>       C106     C107     C108    C109    C110    C111      C112    C113}
\CommentTok{#> 1 -0.09330  0.02565 -0.00609  0.0421  0.0404 -0.0339 -0.061360 -0.0129}
\CommentTok{#> 2  0.00233 -0.10024  0.07455  0.1123  0.0782 -0.0796 -0.053981  0.1139}
\CommentTok{#> 3 -0.08723  0.06189  0.02327 -0.0539  0.1081  0.0665 -0.013018  0.0388}
\CommentTok{#> 4 -0.07975 -0.00139 -0.09287  0.1114  0.0061 -0.0122  0.000636 -0.1064}
\CommentTok{#> 5  0.05724  0.11264 -0.07843  0.0791 -0.0388 -0.0812  0.109622 -0.1046}
\CommentTok{#> 6  0.01225 -0.04051  0.09262 -0.0923 -0.0910 -0.1184  0.015054  0.0370}
\CommentTok{#>      C114    C115    C116     C117     C118    C119    C120     C121}
\CommentTok{#> 1  0.0893  0.0318  0.0375  0.00543 -0.01216  0.0135  0.0240  0.00895}
\CommentTok{#> 2  0.0414 -0.0866  0.0855  0.04991  0.00184  0.0450 -0.1195  0.00779}
\CommentTok{#> 3 -0.0986  0.0376 -0.0783 -0.04182 -0.05241 -0.0714 -0.1106  0.01534}
\CommentTok{#> 4  0.0177 -0.0981  0.1494 -0.08319 -0.08920 -0.1165  0.0156  0.04423}
\CommentTok{#> 5  0.0923 -0.0688  0.0919  0.10526  0.05222  0.0934 -0.0385  0.07644}
\CommentTok{#> 6 -0.0994 -0.0622 -0.0614 -0.05617 -0.07816  0.1023 -0.1049 -0.03828}
\CommentTok{#>      C122    C123    C124     C125    C126    C127    C128      C129}
\CommentTok{#> 1 -0.0335  0.1154  0.1009  0.03531  0.0556  0.0737 -0.0666  0.000563}
\CommentTok{#> 2  0.1008 -0.0403  0.0946 -0.02287 -0.1053 -0.1182  0.1143 -0.096493}
\CommentTok{#> 3  0.0874  0.0992 -0.0144 -0.09176  0.1292  0.0420 -0.0367 -0.090754}
\CommentTok{#> 4 -0.0635  0.0289 -0.1272 -0.03690 -0.0300 -0.0610 -0.0134 -0.039173}
\CommentTok{#> 5 -0.0932  0.1091  0.0984  0.00253  0.0197  0.0260 -0.0603 -0.017103}
\CommentTok{#> 6  0.0918  0.0160 -0.0480 -0.04069 -0.0793  0.0929  0.0262 -0.073559}
\CommentTok{#>      C130    C131    C132    C133    C134    C135     C136    C137    C138}
\CommentTok{#> 1  0.0246  0.1229  0.0864  0.1192  0.0623 -0.0356  0.11440 -0.0989 -0.0306}
\CommentTok{#> 2 -0.0604 -0.0908 -0.0645  0.0526 -0.1100 -0.0432 -0.06490 -0.0338 -0.0895}
\CommentTok{#> 3  0.0867  0.0756 -0.0393 -0.0711 -0.0171  0.0225  0.04992  0.0273 -0.0794}
\CommentTok{#> 4 -0.0863 -0.1113  0.0506  0.1127  0.0898  0.0926 -0.00305 -0.0603  0.0491}
\CommentTok{#> 5 -0.1191 -0.0296  0.1191 -0.0897 -0.1058 -0.0639 -0.04895 -0.1334  0.0492}
\CommentTok{#> 6  0.0828  0.1045  0.0292 -0.0478  0.0347 -0.0346  0.01157  0.0841  0.1166}
\CommentTok{#>       C139    C140    C141    C142     C143   C144     C145    C146}
\CommentTok{#> 1  0.09971  0.0721  0.1283  0.0655 -0.07622 0.0774 -0.00777 -0.1106}
\CommentTok{#> 2 -0.12212  0.1015  0.0482  0.0144  0.11786 0.0594  0.05931  0.0790}
\CommentTok{#> 3 -0.00736 -0.1160 -0.0587  0.0827 -0.00540 0.0567  0.04123  0.1136}
\CommentTok{#> 4 -0.09201 -0.0390  0.1229  0.0410  0.00899 0.0158  0.03078  0.0143}
\CommentTok{#> 5 -0.12324 -0.0770 -0.0071 -0.0346  0.05664 0.0436 -0.05304 -0.0263}
\CommentTok{#> 6  0.01993 -0.1186 -0.0551 -0.0554 -0.06803 0.0495  0.03765 -0.0977}
\CommentTok{#>       C147     C148    C149     C150    C151    C152    C153    C154}
\CommentTok{#> 1 -0.11650  0.01603 -0.0302 -0.08573  0.0208 -0.0340  0.0298  0.1109}
\CommentTok{#> 2  0.08605 -0.08017 -0.1151 -0.05466  0.1123  0.0945  0.0436  0.0124}
\CommentTok{#> 3 -0.06558 -0.00397 -0.0124 -0.08881  0.0701  0.0891 -0.1048  0.0175}
\CommentTok{#> 4  0.11890 -0.01553  0.1045  0.11076  0.0632  0.1212  0.0481 -0.0416}
\CommentTok{#> 5  0.00616 -0.11141  0.0861 -0.00303 -0.0825 -0.0742  0.0414  0.0187}
\CommentTok{#> 6 -0.10398  0.01735 -0.1259 -0.05250  0.0746 -0.0263  0.0047 -0.0628}
\CommentTok{#>       C155    C156     C157     C158     C159    C160    C161     C162}
\CommentTok{#> 1  0.00587  0.0538 -0.05591 -0.11223  0.07053 -0.0918 -0.0505  0.11202}
\CommentTok{#> 2  0.08482 -0.0800  0.06757 -0.08783 -0.11604 -0.0271  0.0914 -0.00459}
\CommentTok{#> 3 -0.04435  0.0557  0.02164 -0.00889  0.10257 -0.1334  0.0442 -0.15218}
\CommentTok{#> 4  0.03777 -0.0772 -0.07832 -0.01604  0.07322 -0.0335 -0.0392 -0.04962}
\CommentTok{#> 5 -0.09842 -0.0510 -0.00942 -0.07867  0.00362  0.0667 -0.0435  0.03897}
\CommentTok{#> 6 -0.11152  0.0301 -0.08371 -0.08597 -0.04411 -0.0377  0.0805 -0.08440}
\CommentTok{#>      C163    C164    C165     C166     C167     C168     C169     C170}
\CommentTok{#> 1 -0.0470 -0.1106  0.0137 -0.07625 -0.08372  0.06794 -0.05559 -0.00891}
\CommentTok{#> 2 -0.0453  0.0631 -0.0585  0.07988 -0.09018  0.07817 -0.00242  0.05795}
\CommentTok{#> 3  0.0915 -0.0425 -0.0180  0.05048 -0.10667  0.04892 -0.09012  0.01890}
\CommentTok{#> 4  0.0994  0.0545 -0.0491  0.00206 -0.00162  0.00811  0.00430 -0.07632}
\CommentTok{#> 5  0.0912  0.0103 -0.0578 -0.12141  0.10732 -0.07513  0.05090 -0.05027}
\CommentTok{#> 6 -0.0833 -0.0736  0.0423  0.05401 -0.09769 -0.09727  0.00999 -0.05769}
\CommentTok{#>      C171    C172     C173      C174     C175      C176    C177    C178}
\CommentTok{#> 1  0.0726  0.0765 -0.03345  0.000539  0.08297 -0.112874  0.0249 -0.0846}
\CommentTok{#> 2  0.0852 -0.0946  0.00369 -0.001933 -0.04844 -0.000873 -0.0222  0.1083}
\CommentTok{#> 3 -0.0489 -0.0098 -0.05220 -0.125355  0.06163  0.036121 -0.0991  0.0600}
\CommentTok{#> 4  0.1137 -0.0593 -0.07113  0.071049  0.14532 -0.039208 -0.0231  0.0804}
\CommentTok{#> 5 -0.1167  0.0036  0.11619  0.053279  0.00691  0.073048 -0.0764 -0.0716}
\CommentTok{#> 6  0.0388 -0.0853  0.08495  0.113081  0.10110 -0.003853  0.0535 -0.0728}
\CommentTok{#>       C179    C180    C181     C182    C183    C184    C185    C186}
\CommentTok{#> 1  0.04217  0.0465  0.0304  0.03261  0.0131  0.0623 -0.0636  0.0916}
\CommentTok{#> 2 -0.02873  0.0898  0.0659  0.12176  0.0276  0.0171 -0.0393 -0.0519}
\CommentTok{#> 3  0.11343 -0.0846 -0.0591 -0.02017 -0.0415  0.0336  0.0377  0.0125}
\CommentTok{#> 4 -0.05080  0.0606 -0.1005  0.03859 -0.0835  0.1018  0.1164 -0.1084}
\CommentTok{#> 5  0.10064 -0.0454 -0.0571  0.00791 -0.0175 -0.0192 -0.0703  0.1143}
\CommentTok{#> 6  0.00256  0.0593 -0.0606  0.03251 -0.0578  0.0902  0.0369  0.0586}
\CommentTok{#>      C187    C188    C189    C190    C191     C192      C193    C194}
\CommentTok{#> 1  0.0724 -0.0587  0.0473 -0.0453 -0.0438  0.00929 -0.000643 -0.1103}
\CommentTok{#> 2 -0.0252 -0.0643 -0.0810  0.1143 -0.1079  0.07810 -0.087180  0.1000}
\CommentTok{#> 3  0.0830  0.0139  0.0488 -0.0124 -0.0988  0.00301 -0.034779  0.0513}
\CommentTok{#> 4 -0.0563  0.0360  0.0426 -0.0663  0.0785 -0.13081 -0.080189 -0.0118}
\CommentTok{#> 5 -0.0826  0.1059  0.0636  0.1159  0.0531  0.08274  0.085099  0.0776}
\CommentTok{#> 6  0.0352  0.0441  0.0647  0.0866  0.0559 -0.01666  0.099376  0.0890}
\CommentTok{#>       C195     C196    C197    C198    C199     C200}
\CommentTok{#> 1 -0.10481  0.12203  0.0654  0.0294  0.1198  0.03634}
\CommentTok{#> 2  0.01139  0.08274 -0.0113  0.0285  0.0897  0.03249}
\CommentTok{#> 3  0.11617 -0.10325 -0.0720 -0.0209 -0.0356  0.00624}
\CommentTok{#> 4  0.00178  0.04353  0.0831 -0.0182  0.0367  0.01303}
\CommentTok{#> 5 -0.08615  0.00641  0.0811  0.0264 -0.0919 -0.02750}
\CommentTok{#> 6 -0.02408  0.11783 -0.1302  0.0737 -0.0638  0.01573}
\CommentTok{#> }
\CommentTok{#> [200 rows x 200 columns]}
\KeywordTok{h2o.weights}\NormalTok{(iris_dl, }\DataTypeTok{matrix_id=}\DecValTok{3}\NormalTok{)}
\CommentTok{#>        C1      C2     C3     C4     C5    C6    C7       C8      C9   C10}
\CommentTok{#> 1 -0.2943 -0.0221 -0.216  0.667  0.357 0.208 0.287  0.00148 -0.0769 0.102}
\CommentTok{#> 2  0.0351 -0.1697 -0.185  0.621 -0.309 0.493 0.148 -0.68544 -0.1141 0.278}
\CommentTok{#> 3 -0.6050 -0.0399  0.055 -0.612  0.637 0.110 0.475 -0.65817 -0.5060 0.127}
\CommentTok{#>      C11     C12    C13   C14    C15    C16    C17    C18   C19     C20}
\CommentTok{#> 1  0.534  0.1479 -0.138 0.188  0.288  0.342 -0.401 -0.310 0.608 -0.4064}
\CommentTok{#> 2 -0.474 -0.4538  0.108 0.157 -0.556  0.174 -0.269 -0.560 0.281  0.4576}
\CommentTok{#> 3  0.510  0.0867  0.320 0.191  0.599 -0.654  0.586 -0.208 0.233 -0.0573}
\CommentTok{#>     C21     C22     C23    C24      C25    C26    C27   C28    C29    C30}
\CommentTok{#> 1 0.327  0.6139 -0.5003  0.300 -0.64122  0.684  0.551 0.572  0.228 -0.678}
\CommentTok{#> 2 0.624 -0.3150  0.1056 -0.585  0.53114 -0.638 -0.643 0.218 -0.449 -0.139}
\CommentTok{#> 3 0.139  0.0531  0.0538 -0.417 -0.00424 -0.585 -0.394 0.185  0.398  0.225}
\CommentTok{#>      C31    C32    C33     C34    C35     C36     C37     C38    C39}
\CommentTok{#> 1  0.667 -0.664 -0.586 -0.6566 -0.139 -0.0632 -0.4918 -0.0054 -0.213}
\CommentTok{#> 2 -0.594 -0.613  0.428 -0.0573  0.645 -0.5789  0.0298  0.4150 -0.318}
\CommentTok{#> 3  0.299  0.151 -0.490 -0.2106 -0.595  0.1842  0.1678 -0.1932  0.366}
\CommentTok{#>      C40   C41    C42     C43     C44    C45     C46    C47     C48    C49}
\CommentTok{#> 1  0.380 0.691  0.494  0.3176 -0.5060 -0.362 -0.0296  0.559 -0.3768  0.655}
\CommentTok{#> 2 -0.529 0.244  0.185 -0.0341  0.0761  0.010 -0.5667 -0.631  0.0877  0.327}
\CommentTok{#> 3 -0.274 0.120 -0.543  0.3044 -0.3509  0.489 -0.6235  0.154  0.4751 -0.159}
\CommentTok{#>       C50    C51    C52     C53    C54    C55    C56    C57    C58    C59}
\CommentTok{#> 1 -0.2676  0.155  0.278  0.5425 -0.127  0.394  0.383 -0.266 -0.575 -0.282}
\CommentTok{#> 2 -0.0669 -0.324  0.121  0.0785  0.501 -0.534 -0.449  0.435  0.670 -0.539}
\CommentTok{#> 3  0.0818  0.331 -0.237 -0.1026 -0.644 -0.331  0.355 -0.303 -0.428  0.355}
\CommentTok{#>      C60      C61    C62    C63    C64     C65    C66     C67    C68}
\CommentTok{#> 1  0.562 -0.27855 -0.530 -0.478 -0.629  0.0709 -0.454  0.0481  0.438}
\CommentTok{#> 2  0.316 -0.58491  0.175  0.390 -0.242  0.6663  0.483 -0.5829 -0.354}
\CommentTok{#> 3 -0.364 -0.00287 -0.489 -0.121  0.366 -0.6349  0.534 -0.4585  0.264}
\CommentTok{#>       C69    C70    C71   C72    C73    C74   C75     C76    C77     C78}
\CommentTok{#> 1 -0.0764 -0.598 -0.127 0.538  0.241 -0.558 0.456 -0.1107  0.465  0.6176}
\CommentTok{#> 2  0.5509 -0.441 -0.228 0.615 -0.087  0.387 0.423 -0.0731 -0.519 -0.0891}
\CommentTok{#> 3  0.6281 -0.417  0.229 0.539  0.494  0.211 0.613  0.3096  0.379 -0.1622}
\CommentTok{#>       C79    C80   C81    C82    C83    C84    C85    C86     C87    C88}
\CommentTok{#> 1 -0.6572  0.503 0.102 -0.661  0.398  0.137  0.300  0.412  0.0238  0.357}
\CommentTok{#> 2  0.0468 -0.377 0.179 -0.576 -0.178 -0.188  0.635 -0.417 -0.5376 -0.208}
\CommentTok{#> 3 -0.1629  0.474 0.298  0.473  0.462  0.130 -0.132  0.209  0.2179  0.372}
\CommentTok{#>      C89    C90     C91    C92     C93    C94    C95    C96    C97    C98}
\CommentTok{#> 1 -0.514 -0.167 -0.1140 -0.370  0.6681  0.417  0.686 -0.167 -0.304 -0.422}
\CommentTok{#> 2 -0.146 -0.566  0.0277  0.260 -0.0437  0.260  0.427  0.092 -0.260  0.668}
\CommentTok{#> 3 -0.509 -0.265  0.0538 -0.422 -0.2022 -0.579 -0.035 -0.637 -0.592  0.357}
\CommentTok{#>      C99   C100   C101    C102   C103   C104   C105   C106   C107   C108}
\CommentTok{#> 1 -0.216  0.623 -0.597  0.3520 -0.270  0.174 -0.319 0.1834 -0.464 -0.597}
\CommentTok{#> 2  0.286 -0.496 -0.371 -0.0504  0.500 -0.132 -0.430 0.0512 -0.464 -0.351}
\CommentTok{#> 3  0.225  0.603  0.218  0.0431  0.316  0.171 -0.469 0.3045 -0.179 -0.563}
\CommentTok{#>     C109   C110   C111   C112   C113   C114  C115   C116  C117   C118}
\CommentTok{#> 1 -0.440 -0.233  0.303 -0.455 -0.517 -0.331 0.273  0.160 0.202 -0.617}
\CommentTok{#> 2  0.335 -0.256 -0.484  0.053 -0.234 -0.482 0.143  0.357 0.309 -0.235}
\CommentTok{#> 3 -0.577 -0.194  0.672  0.439  0.516 -0.561 0.150 -0.435 0.125  0.179}
\CommentTok{#>     C119   C120    C121    C122    C123   C124   C125   C126   C127   C128}
\CommentTok{#> 1 -0.315  0.428 -0.0395 -0.5965  0.0295  0.240  0.447 -0.470  0.293  0.455}
\CommentTok{#> 2  0.158 -0.346  0.2760 -0.3515  0.3489 -0.215 -0.206 -0.531  0.602  0.604}
\CommentTok{#> 3  0.339  0.133  0.1634  0.0247 -0.2497 -0.360  0.287  0.550 -0.235 -0.230}
\CommentTok{#>     C129   C130    C131   C132   C133   C134   C135   C136    C137  C138}
\CommentTok{#> 1 -0.397 -0.164 -0.1708 -0.440  0.616  0.132  0.648 0.1048 -0.6893 0.633}
\CommentTok{#> 2  0.557 -0.198  0.4765  0.485 -0.673 -0.658 -0.151 0.0063 -0.0139 0.190}
\CommentTok{#> 3  0.115 -0.379 -0.0537  0.158 -0.597 -0.584 -0.473 0.3004  0.4636 0.582}
\CommentTok{#>     C139   C140  C141    C142    C143   C144   C145   C146    C147   C148}
\CommentTok{#> 1 -0.229 -0.124 0.585  0.1181  0.0607 -0.533  0.263  0.560  0.4851 -0.261}
\CommentTok{#> 2  0.326 -0.370 0.328  0.2604 -0.0418  0.289  0.471  0.635 -0.0526 -0.384}
\CommentTok{#> 3 -0.515  0.601 0.614 -0.0346 -0.4742 -0.285 -0.616 -0.566  0.5417 -0.152}
\CommentTok{#>      C149   C150   C151   C152   C153    C154   C155   C156    C157}
\CommentTok{#> 1 -0.5325 0.6738  0.449 -0.310 -0.605 -0.0212 -0.134 -0.161  0.1530}
\CommentTok{#> 2  0.4986 0.0228 -0.107  0.653 -0.423 -0.4857  0.623 -0.187 -0.0688}
\CommentTok{#> 3  0.0983 0.5357  0.420 -0.353  0.198  0.0816 -0.364  0.475 -0.0502}
\CommentTok{#>      C158    C159   C160   C161    C162    C163   C164   C165   C166}
\CommentTok{#> 1 -0.2934  0.2854 0.4478  0.543  0.0992  0.0752 -0.517  0.559 -0.650}
\CommentTok{#> 2 -0.3228 -0.4128 0.3912 -0.340 -0.3029  0.6183 -0.346 -0.575  0.183}
\CommentTok{#> 3 -0.0565 -0.0511 0.0593 -0.244  0.6380 -0.4106 -0.258  0.589  0.535}
\CommentTok{#>     C167   C168   C169   C170   C171   C172    C173   C174   C175    C176}
\CommentTok{#> 1 -0.613 -0.429 -0.558  0.228 -0.528 -0.358  0.2361 -0.660 -0.495 -0.0473}
\CommentTok{#> 2  0.552 -0.584  0.602 -0.477  0.352 -0.602  0.0166  0.446  0.265  0.2689}
\CommentTok{#> 3 -0.286 -0.634 -0.264  0.350 -0.513 -0.518 -0.5546  0.335 -0.143  0.0734}
\CommentTok{#>     C177   C178   C179    C180      C181   C182   C183   C184   C185}
\CommentTok{#> 1 -0.247 -0.509  0.547 -0.4437  0.553396  0.374 -0.192 -0.518 -0.669}
\CommentTok{#> 2 -0.310  0.231 -0.317 -0.3347  0.357966 -0.352 -0.410 -0.415  0.173}
\CommentTok{#> 3 -0.396 -0.468 -0.538  0.0667 -0.000532 -0.632 -0.468  0.250  0.645}
\CommentTok{#>       C186   C187   C188   C189   C190   C191   C192  C193   C194    C195}
\CommentTok{#> 1 -0.05092 -0.140 -0.108 -0.252 -0.160 -0.577  0.123 0.390 0.3165 -0.2131}
\CommentTok{#> 2 -0.30534  0.662 -0.429 -0.571  0.670 -0.654 -0.317 0.252 0.4049 -0.0293}
\CommentTok{#> 3  0.00476  0.284 -0.181  0.333 -0.245 -0.207 -0.168 0.142 0.0543 -0.1701}
\CommentTok{#>     C196    C197   C198    C199   C200}
\CommentTok{#> 1 -0.024 -0.0569 -0.115 -0.0083 -0.649}
\CommentTok{#> 2  0.312  0.5508  0.167 -0.4435 -0.455}
\CommentTok{#> 3  0.433 -0.2022  0.631  0.0865 -0.579}
\CommentTok{#> }
\CommentTok{#> [3 rows x 200 columns]}
\KeywordTok{h2o.biases}\NormalTok{(iris_dl,  }\DataTypeTok{vector_id=}\DecValTok{1}\NormalTok{)}
\CommentTok{#>      C1}
\CommentTok{#> 1 0.492}
\CommentTok{#> 2 0.467}
\CommentTok{#> 3 0.477}
\CommentTok{#> 4 0.474}
\CommentTok{#> 5 0.491}
\CommentTok{#> 6 0.499}
\CommentTok{#> }
\CommentTok{#> [200 rows x 1 column]}
\KeywordTok{h2o.biases}\NormalTok{(iris_dl,  }\DataTypeTok{vector_id=}\DecValTok{2}\NormalTok{)}
\CommentTok{#>      C1}
\CommentTok{#> 1 1.006}
\CommentTok{#> 2 0.995}
\CommentTok{#> 3 0.994}
\CommentTok{#> 4 1.005}
\CommentTok{#> 5 0.995}
\CommentTok{#> 6 0.997}
\CommentTok{#> }
\CommentTok{#> [200 rows x 1 column]}
\KeywordTok{h2o.biases}\NormalTok{(iris_dl,  }\DataTypeTok{vector_id=}\DecValTok{3}\NormalTok{)}
\CommentTok{#>          C1}
\CommentTok{#> 1  0.000386}
\CommentTok{#> 2  0.001556}
\CommentTok{#> 3 -0.002351}
\CommentTok{#> }
\CommentTok{#> [3 rows x 1 column]}
\CommentTok{#plot weights connecting `Sepal.Length` to first hidden neurons}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{as.data.frame}\NormalTok{(}\KeywordTok{h2o.weights}\NormalTok{(iris_dl,  }\DataTypeTok{matrix_id=}\DecValTok{1}\NormalTok{))[,}\DecValTok{1}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{comparison-classification_regression_with_h2o_deep_learning_files/figure-latex/export_weight_biases-1} \end{center}

\hypertarget{reproducibility}{%
\subsection{Reproducibility}\label{reproducibility}}

Every run of DeepLearning results in different results since multithreading is done via Hogwild! that benefits from intentional lock-free race conditions between threads. To get reproducible results for small datasets and testing purposes, set \texttt{reproducible=T} and set \texttt{seed=1337} (pick any integer). This will not work for big data for technical reasons, and is probably also not desired because of the significant slowdown (runs on 1 core only).

\hypertarget{scoring-on-trainingvalidation-sets-during-training}{%
\subsection{Scoring on Training/Validation Sets During Training}\label{scoring-on-trainingvalidation-sets-during-training}}

The training and/or validation set errors can be based on a subset of the training or validation data, depending on the values for \texttt{score\_validation\_samples} (defaults to 0: all) or \texttt{score\_training\_samples} (defaults to 10,000 rows, since the training error is only used for early stopping and monitoring). For large datasets, Deep Learning can automatically sample the validation set to avoid spending too much time in scoring during training, especially since scoring results are not currently displayed in the model returned to R.

Note that the default value of \texttt{score\_duty\_cycle=0.1} limits the amount of time spent in scoring to 10\%, so a large number of scoring samples won't slow down overall training progress too much, but it will always score once after the first MapReduce iteration, and once at the end of training.

Stratified sampling of the validation dataset can help with scoring on datasets with class imbalance. Note that this option also requires \texttt{balance\_classes} to be enabled (used to over/under-sample the training dataset, based on the max. relative size of the resulting training dataset, max\_after\_balance\_size):

More information can be found in the H2O Deep Learning booklet, in our H2O SlideShare Presentations, our H2O YouTube channel, as well as on our H2O Github Repository, especially in our H2O Deep Learning R tests, and H2O Deep Learning Python tests.

\hypertarget{all-done-shutdown-h2o}{%
\section{All done, shutdown H2O}\label{all-done-shutdown-h2o}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{h2o.shutdown}\NormalTok{(}\DataTypeTok{prompt=}\OtherTok{FALSE}\NormalTok{)}
\CommentTok{#> [1] TRUE}
\end{Highlighting}
\end{Shaded}

\hypertarget{sensitivity-analysis-for-neural-networks}{%
\chapter{Sensitivity analysis for neural networks}\label{sensitivity-analysis-for-neural-networks}}

\hypertarget{introduction-4}{%
\section{Introduction}\label{introduction-4}}

\url{https://beckmw.wordpress.com/tag/nnet/}

I've made quite a few blog posts about neural networks and some of the diagnostic tools that can be used to `demystify' the information contained in these models. Frankly, I'm kind of sick of writing about neural networks but I wanted to share one last tool I've implemented in R. I'm a strong believer that supervised neural networks can be used for much more than prediction, as is the common assumption by most researchers. I hope that my collection of posts, including this one, has shown the versatility of these models to develop inference into causation. To date, I've authored posts on visualizing neural networks, animating neural networks, and determining importance of model inputs. This post will describe a function for a sensitivity analysis of a neural network. Specifically, I will describe an approach to evaluate the form of the relationship of a response variable with the explanatory variables used in the model.

The general goal of a sensitivity analysis is similar to evaluating relative importance of explanatory variables, with a few important distinctions. For both analyses, we are interested in the relationships between explanatory and response variables as described by the model in the hope that the neural network has explained some real-world phenomenon. Using Garson's algorithm,1 we can get an idea of the magnitude and sign of the relationship between variables relative to each other. Conversely, the sensitivity analysis allows us to obtain information about the form of the relationship between variables rather than a categorical description, such as variable x is positively and strongly related to y. For example, how does a response variable change in relation to increasing or decreasing values of a given explanatory variable? Is it a linear response, non-linear, uni-modal, no response, etc.? Furthermore, how does the form of the response change given values of the other explanatory variables in the model? We might expect that the relationship between a response and explanatory variable might differ given the context of the other explanatory variables (i.e., an interaction may be present). The sensitivity analysis can provide this information.

As with most of my posts, I've created the sensitivity analysis function using ideas from other people that are much more clever than me. I've simply converted these ideas into a useful form in R. Ultimate credit for the sensitivity analysis goes to Sovan Lek (and colleagues), who developed the approach in the mid-1990s. The `Lek-profile method' is described briefly in Lek et al.~19962 and in more detail in Gevrey et al.~2003.3 I'll provide a brief summary here since the method is pretty simple. In fact, the profile method can be extended to any statistical model and is not specific to neural networks, although it is one of few methods used to evaluate the latter. For any statistical model where multiple response variables are related to multiple explanatory variables, we choose one response and one explanatory variable. We obtain predictions of the response variable across the range of values for the given explanatory variable. All other explanatory variables are held constant at a given set of respective values (e.g., minimum, 20th percentile, maximum). The final product is a set of response curves for one response variable across the range of values for one explanatory variable, while holding all other explanatory variables constant. This is implemented in R by creating a matrix of values for explanatory variables where the number of rows is the number of observations and the number of columns is the number of explanatory variables. All explanatory variables are held at their mean (or other constant value) while the variable of interest is sequenced from its minimum to maximum value across the range of observations. This matrix (actually a data frame) is then used to predict values of the response variable from a fitted model object. This is repeated for different variables.

I'll illustrate the function using simulated data, as I've done in previous posts. The exception here is that I'll be using two response variables instead of one. The two response variables are linear combinations of eight explanatory variables, with random error components taken from a normal distribution. The relationships between the variables are determined by the arbitrary set of parameters (\texttt{parms1} and \texttt{parms2}). The explanatory variables are partially correlated and taken from a multivariate normal distribution.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{require}\NormalTok{(clusterGeneration)}
\CommentTok{#> Loading required package: clusterGeneration}
\CommentTok{#> Loading required package: MASS}
\KeywordTok{require}\NormalTok{(nnet)}
\CommentTok{#> Loading required package: nnet}
  
\CommentTok{#define number of variables and observations}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{2}\NormalTok{)}
\NormalTok{num.vars<-}\DecValTok{8}
\NormalTok{num.obs<-}\DecValTok{10000}
  
\CommentTok{#define correlation matrix for explanatory variables }
\CommentTok{#define actual parameter values}
\NormalTok{cov.mat<-}\KeywordTok{genPositiveDefMat}\NormalTok{(num.vars,}\DataTypeTok{covMethod=}\KeywordTok{c}\NormalTok{(}\StringTok{"unifcorrmat"}\NormalTok{))}\OperatorTok{$}\NormalTok{Sigma}
\NormalTok{rand.vars<-}\KeywordTok{mvrnorm}\NormalTok{(num.obs,}\KeywordTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{,num.vars),}\DataTypeTok{Sigma=}\NormalTok{cov.mat)}
\NormalTok{parms1<-}\KeywordTok{runif}\NormalTok{(num.vars,}\OperatorTok{-}\DecValTok{10}\NormalTok{,}\DecValTok{10}\NormalTok{)}
\NormalTok{y1<-rand.vars }\OperatorTok{%*%}\StringTok{ }\KeywordTok{matrix}\NormalTok{(parms1) }\OperatorTok{+}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(num.obs,}\DataTypeTok{sd=}\DecValTok{20}\NormalTok{)}
\NormalTok{parms2<-}\KeywordTok{runif}\NormalTok{(num.vars,}\OperatorTok{-}\DecValTok{10}\NormalTok{,}\DecValTok{10}\NormalTok{)}
\NormalTok{y2<-rand.vars }\OperatorTok{%*%}\StringTok{ }\KeywordTok{matrix}\NormalTok{(parms2) }\OperatorTok{+}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(num.obs,}\DataTypeTok{sd=}\DecValTok{20}\NormalTok{)}
 
\CommentTok{#prep data and create neural network}
\NormalTok{rand.vars<-}\KeywordTok{data.frame}\NormalTok{(rand.vars)}
\NormalTok{resp<-}\KeywordTok{apply}\NormalTok{(}\KeywordTok{cbind}\NormalTok{(y1,y2),}\DecValTok{2}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(y) (y}\OperatorTok{-}\KeywordTok{min}\NormalTok{(y))}\OperatorTok{/}\NormalTok{(}\KeywordTok{max}\NormalTok{(y)}\OperatorTok{-}\KeywordTok{min}\NormalTok{(y)))}
\NormalTok{resp<-}\KeywordTok{data.frame}\NormalTok{(resp)}
\KeywordTok{names}\NormalTok{(resp)<-}\KeywordTok{c}\NormalTok{(}\StringTok{'Y1'}\NormalTok{,}\StringTok{'Y2'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mod1 <-}\StringTok{ }\KeywordTok{nnet}\NormalTok{(rand.vars,resp,}\DataTypeTok{size=}\DecValTok{8}\NormalTok{,}\DataTypeTok{linout=}\NormalTok{T)}
\CommentTok{#> # weights:  90}
\CommentTok{#> initial  value 30121.205794 }
\CommentTok{#> iter  10 value 130.537462}
\CommentTok{#> iter  20 value 57.187090}
\CommentTok{#> iter  30 value 47.285919}
\CommentTok{#> iter  40 value 42.778564}
\CommentTok{#> iter  50 value 39.837784}
\CommentTok{#> iter  60 value 36.694632}
\CommentTok{#> iter  70 value 35.140948}
\CommentTok{#> iter  80 value 34.268819}
\CommentTok{#> iter  90 value 33.772282}
\CommentTok{#> iter 100 value 33.472654}
\CommentTok{#> final  value 33.472654 }
\CommentTok{#> stopped after 100 iterations}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#import the function from Github}
\KeywordTok{library}\NormalTok{(devtools)}

\CommentTok{# source_url('https://gist.githubusercontent.com/fawda123/7471137/raw/466c1474d0a505ff044412703516c34f1a4684a5/nnet_plot_update.r')}
\KeywordTok{source}\NormalTok{(}\StringTok{"nnet_plot_update.r"}\NormalTok{)}
 
\CommentTok{#plot each model}
\KeywordTok{plot.nnet}\NormalTok{(mod1)}
\CommentTok{#> Loading required package: scales}
\CommentTok{#> Loading required package: reshape}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{meta_905-regression_-sensitivity_analysis_nn_files/figure-latex/unnamed-chunk-4-1} \end{center}

\hypertarget{the-lek-profile-function}{%
\section{The Lek profile function}\label{the-lek-profile-function}}

We've created a neural network that hopefully describes the relationship of two response variables with eight explanatory variables. The sensitivity analysis lets us visualize these relationships. The Lek profile function can be used once we have a neural network model in our workspace. The function is imported and used as follows:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# source('https://gist.githubusercontent.com/fawda123/6860630/raw/b8bf4a6c88d6b392b1bfa6ef24759ae98f31877c/lek_fun.r')}
\KeywordTok{source}\NormalTok{(}\StringTok{"lek_fun.r"}\NormalTok{)}

\KeywordTok{lek.fun}\NormalTok{(mod1)}
\CommentTok{#> Loading required package: ggplot2}
\CommentTok{#> Registered S3 methods overwritten by 'ggplot2':}
\CommentTok{#>   method         from }
\CommentTok{#>   [.quosures     rlang}
\CommentTok{#>   c.quosures     rlang}
\CommentTok{#>   print.quosures rlang}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{meta_905-regression_-sensitivity_analysis_nn_files/figure-latex/unnamed-chunk-5-1} \end{center}

\begin{quote}
Fig: Sensitivity analysis of the two response variables in the neural network model to individual explanatory variables. Splits represent the quantile values at which the remaining explanatory variables were held constant. The function can be obtained \href{https://gist.githubusercontent.com/fawda123/6860630/raw/b8bf4a6c88d6b392b1bfa6ef24759ae98f31877c/lek_fun.r}{here}
\end{quote}

By default, the function runs a sensitivity analysis for all variables. This creates a busy plot so we may want to look at specific variables of interest. Maybe we want to evaluate different quantile values as well. These options can be changed using the arguments.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{lek.fun}\NormalTok{(mod1,}\DataTypeTok{var.sens=}\KeywordTok{c}\NormalTok{(}\StringTok{'X2'}\NormalTok{,}\StringTok{'X5'}\NormalTok{),}\DataTypeTok{split.vals=}\KeywordTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DataTypeTok{by=}\FloatTok{0.05}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{meta_905-regression_-sensitivity_analysis_nn_files/figure-latex/unnamed-chunk-6-1} \end{center}

\begin{quote}
Fig: Sensitivity analysis of the two response variables in relation to explanatory variables X2 and X5 and different quantile values for the remaining variables.
\end{quote}

The function also returns a ggplot2 object that can be further modified. You may prefer a different theme, color, or line type, for example.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p1<-}\KeywordTok{lek.fun}\NormalTok{(mod1)}
\KeywordTok{class}\NormalTok{(p1)}
\CommentTok{#> [1] "gg"     "ggplot"}
\CommentTok{# [1] "gg"     "ggplot"}
 
\NormalTok{p1 }\OperatorTok{+}\StringTok{ }
\StringTok{   }\KeywordTok{theme_bw}\NormalTok{() }\OperatorTok{+}
\StringTok{   }\KeywordTok{scale_colour_brewer}\NormalTok{(}\DataTypeTok{palette=}\StringTok{"PuBu"}\NormalTok{) }\OperatorTok{+}
\StringTok{   }\KeywordTok{scale_linetype_manual}\NormalTok{(}\DataTypeTok{values=}\KeywordTok{rep}\NormalTok{(}\StringTok{'dashed'}\NormalTok{,}\DecValTok{6}\NormalTok{)) }\OperatorTok{+}
\StringTok{   }\KeywordTok{scale_size_manual}\NormalTok{(}\DataTypeTok{values=}\KeywordTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{6}\NormalTok{))}
\CommentTok{#> Scale for 'linetype' is already present. Adding another scale for}
\CommentTok{#> 'linetype', which will replace the existing scale.}
\CommentTok{#> Scale for 'size' is already present. Adding another scale for 'size',}
\CommentTok{#> which will replace the existing scale.}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{meta_905-regression_-sensitivity_analysis_nn_files/figure-latex/unnamed-chunk-7-1} \end{center}

\hypertarget{getting-a-dataframe-from-lek}{%
\section{\texorpdfstring{Getting a dataframe from \texttt{lek}}{Getting a dataframe from lek}}\label{getting-a-dataframe-from-lek}}

Finally, the actual values from the sensitivity analysis can be returned if you'd prefer that instead. The output is a data frame in long form that was created using melt.list from the reshape package for compatibility with ggplot2. The six columns indicate values for explanatory variables on the x-axes, names of the response variables, predicted values of the response variables, quantiles at which other explanatory variables were held constant, and names of the explanatory variables on the x-axes.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(}\KeywordTok{lek.fun}\NormalTok{(mod1,}\DataTypeTok{val.out =} \OtherTok{TRUE}\NormalTok{))}
\CommentTok{#>   Explanatory resp.name Response Splits exp.name}
\CommentTok{#> 1       -9.58        Y1    0.466      0       X1}
\CommentTok{#> 2       -9.39        Y1    0.466      0       X1}
\CommentTok{#> 3       -9.19        Y1    0.467      0       X1}
\CommentTok{#> 4       -9.00        Y1    0.467      0       X1}
\CommentTok{#> 5       -8.81        Y1    0.468      0       X1}
\CommentTok{#> 6       -8.62        Y1    0.468      0       X1}
\end{Highlighting}
\end{Shaded}

\hypertarget{the-lek-function-works-with-lm}{%
\section{\texorpdfstring{The \texttt{lek} function works with \texttt{lm}}{The lek function works with lm}}\label{the-lek-function-works-with-lm}}

I mentioned earlier that the function is not unique to neural networks and can work with other models created in R. I haven't done an extensive test of the function, but I'm fairly certain that it will work if the model object has a predict method (e.g., predict.lm). Here's an example using the function to evaluate a multiple linear regression for one of the response variables.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mod2 <-}\KeywordTok{lm}\NormalTok{(Y1 }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =} \KeywordTok{cbind}\NormalTok{(resp[,}\StringTok{'Y1'}\NormalTok{, }\DataTypeTok{drop =}\NormalTok{ F], rand.vars))}
\KeywordTok{lek.fun}\NormalTok{(mod2)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{meta_905-regression_-sensitivity_analysis_nn_files/figure-latex/unnamed-chunk-9-1} \end{center}

This function has little relevance for conventional models like linear regression since a wealth of \texttt{diagnostic} tools are already available (e.g., effects plots, add/drop procedures, outlier tests, etc.). The application of the function to neural networks provides insight into the relationships described by the models, insights that to my knowledge, cannot be obtained using current tools in R. This post concludes my contribution of diagnostic tools for neural networks in R and I hope that they have been useful to some of you. I have spent the last year or so working with neural networks and my opinion of their utility is mixed. I see advantages in the use of highly flexible computer-based algorithms, although in most cases similar conclusions can be made using more conventional analyses. I suggest that neural networks only be used \emph{if there is an extremely high sample size} and other methods have proven inconclusive. Feel free to voice your opinions or suggestions in the comments.

\hypertarget{lek-function-works-with-rsnns}{%
\section{\texorpdfstring{\texttt{lek} function works with \texttt{RSNNS}}{lek function works with RSNNS}}\label{lek-function-works-with-rsnns}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{require}\NormalTok{(clusterGeneration)}
\KeywordTok{require}\NormalTok{(RSNNS)}
\CommentTok{#> Loading required package: RSNNS}
\CommentTok{#> Loading required package: Rcpp}
\KeywordTok{require}\NormalTok{(devtools)}
 
\CommentTok{#define number of variables and observations}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{2}\NormalTok{)}
\NormalTok{num.vars<-}\DecValTok{8}
\NormalTok{num.obs<-}\DecValTok{10000}
 
\CommentTok{#define correlation matrix for explanatory variables }
\CommentTok{#define actual parameter values}
\NormalTok{cov.mat <-}\KeywordTok{genPositiveDefMat}\NormalTok{(num.vars,}\DataTypeTok{covMethod=}\KeywordTok{c}\NormalTok{(}\StringTok{"unifcorrmat"}\NormalTok{))}\OperatorTok{$}\NormalTok{Sigma}
\NormalTok{rand.vars <-}\KeywordTok{mvrnorm}\NormalTok{(num.obs,}\KeywordTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{,num.vars),}\DataTypeTok{Sigma=}\NormalTok{cov.mat)}
\NormalTok{parms1 <-}\KeywordTok{runif}\NormalTok{(num.vars,}\OperatorTok{-}\DecValTok{10}\NormalTok{,}\DecValTok{10}\NormalTok{)}
\NormalTok{y1 <-rand.vars }\OperatorTok{%*%}\StringTok{ }\KeywordTok{matrix}\NormalTok{(parms1) }\OperatorTok{+}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(num.obs,}\DataTypeTok{sd=}\DecValTok{20}\NormalTok{)}
\NormalTok{parms2 <-}\KeywordTok{runif}\NormalTok{(num.vars,}\OperatorTok{-}\DecValTok{10}\NormalTok{,}\DecValTok{10}\NormalTok{)}
\NormalTok{y2 <-rand.vars }\OperatorTok{%*%}\StringTok{ }\KeywordTok{matrix}\NormalTok{(parms2) }\OperatorTok{+}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(num.obs,}\DataTypeTok{sd=}\DecValTok{20}\NormalTok{)}
 
\CommentTok{#prep data and create neural network}
\NormalTok{rand.vars <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(rand.vars)}
\NormalTok{resp <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(}\KeywordTok{cbind}\NormalTok{(y1,y2),}\DecValTok{2}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(y) (y}\OperatorTok{-}\KeywordTok{min}\NormalTok{(y))}\OperatorTok{/}\NormalTok{(}\KeywordTok{max}\NormalTok{(y)}\OperatorTok{-}\KeywordTok{min}\NormalTok{(y)))}
\NormalTok{resp <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(resp)}
\KeywordTok{names}\NormalTok{(resp)<-}\KeywordTok{c}\NormalTok{(}\StringTok{'Y1'}\NormalTok{,}\StringTok{'Y2'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tibble}\OperatorTok{::}\KeywordTok{as_tibble}\NormalTok{(rand.vars)}
\CommentTok{#> # A tibble: 10,000 x 8}
\CommentTok{#>        X1     X2     X3     X4     X5    X6    X7     X8}
\CommentTok{#>     <dbl>  <dbl>  <dbl>  <dbl>  <dbl> <dbl> <dbl>  <dbl>}
\CommentTok{#> 1  1.61    2.13   2.13   3.97  -1.34   2.00  3.11 -2.55 }
\CommentTok{#> 2 -1.25    3.07  -0.325  1.61  -0.484  2.28  2.98 -1.71 }
\CommentTok{#> 3 -3.17   -1.29  -1.77  -1.66  -0.549 -3.19  1.07  1.81 }
\CommentTok{#> 4 -2.39    3.28  -3.42  -0.160 -1.52   2.67  7.05 -1.14 }
\CommentTok{#> 5 -1.55   -0.181 -1.14   2.27  -1.68  -1.67  3.08  0.334}
\CommentTok{#> 6  0.0690 -1.54  -2.98   2.84   1.42   1.31  1.82  2.07 }
\CommentTok{#> # ... with 9,994 more rows}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tibble}\OperatorTok{::}\KeywordTok{as_tibble}\NormalTok{(resp)}
\CommentTok{#> # A tibble: 10,000 x 2}
\CommentTok{#>      Y1    Y2}
\CommentTok{#>   <dbl> <dbl>}
\CommentTok{#> 1 0.461 0.500}
\CommentTok{#> 2 0.416 0.509}
\CommentTok{#> 3 0.534 0.675}
\CommentTok{#> 4 0.548 0.619}
\CommentTok{#> 5 0.519 0.659}
\CommentTok{#> 6 0.389 0.622}
\CommentTok{#> # ... with 9,994 more rows}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# create neural network model}
\NormalTok{mod2 <-}\StringTok{ }\KeywordTok{mlp}\NormalTok{(rand.vars, resp, }\DataTypeTok{size =} \DecValTok{8}\NormalTok{, }\DataTypeTok{linOut =}\NormalTok{ T)}
 
\CommentTok{#import sensitivity analysis function}
\KeywordTok{source_url}\NormalTok{(}\StringTok{'https://gist.githubusercontent.com/fawda123/6860630/raw/b8bf4a6c88d6b392b1bfa6ef24759ae98f31877c/lek_fun.r'}\NormalTok{)}
\CommentTok{#> SHA-1 hash of file is 4a2d33b94a08f46a94518207a4ae7cc412845222}
 
\CommentTok{#sensitivity analsyis, note 'exp.in' argument}
\KeywordTok{lek.fun}\NormalTok{(mod2, }\DataTypeTok{exp.in =}\NormalTok{ rand.vars)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{meta_905-regression_-sensitivity_analysis_nn_files/figure-latex/unnamed-chunk-13-1} \end{center}

\hypertarget{references}{%
\chapter{References}\label{references}}

1 Garson GD. 1991. Interpreting neural network connection weights. Artificial Intelligence Expert. 6:46--51.
2 Lek S, Delacoste M, Baran P, Dimopoulos I, Lauga J, Aulagnier S. 1996. Application of neural networks to modelling nonlinear relationships in Ecology. Ecological Modelling. 90:39-52.
3 Gevrey M, Dimopoulos I, Lek S. 2003. Review and comparison of methods to study the contribution of variables in artificial neural network models. Ecological Modelling. 160:249-264.

\hypertarget{regression-with-ann---yacht-hydrodynamics}{%
\chapter{Regression with ANN - Yacht Hydrodynamics}\label{regression-with-ann---yacht-hydrodynamics}}

\hypertarget{introduction-5}{%
\section{Introduction}\label{introduction-5}}

Regression ANNs predict an output variable as a function of the inputs. The input features (independent variables) can be categorical or numeric types, however, for regression ANNs, we require a numeric dependent variable. If the output variable is a categorical variable (or binary) the ANN will function as a classifier (see next tutorial).

Source: \url{http://uc-r.github.io/ann_regression}

In this tutorial we introduce a neural network used for numeric predictions and cover:

\begin{itemize}
\tightlist
\item
  Replication requirements: What you'll need to reproduce the analysis in this tutorial.
\item
  Data Preparation: Preparing our data.
\item
  1st Regression ANN: Constructing a 1-hidden layer ANN with 1 neuron.
\item
  Regression Hyperparameters: Tuning the model.
\item
  Wrapping Up: Final comments and some exercises to test your skills.
\end{itemize}

\hypertarget{replication-requirements}{%
\section{Replication Requirements}\label{replication-requirements}}

We require the following packages for the analysis.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}
\CommentTok{#> Registered S3 methods overwritten by 'ggplot2':}
\CommentTok{#>   method         from }
\CommentTok{#>   [.quosures     rlang}
\CommentTok{#>   c.quosures     rlang}
\CommentTok{#>   print.quosures rlang}
\CommentTok{#> Registered S3 method overwritten by 'rvest':}
\CommentTok{#>   method            from}
\CommentTok{#>   read_xml.response xml2}
\CommentTok{#> -- Attaching packages ----------------------------------------- tidyverse 1.2.1 --}
\CommentTok{#> v ggplot2 3.1.1       v purrr   0.3.2  }
\CommentTok{#> v tibble  2.1.1       v dplyr   0.8.0.1}
\CommentTok{#> v tidyr   0.8.3       v stringr 1.4.0  }
\CommentTok{#> v readr   1.3.1       v forcats 0.4.0}
\CommentTok{#> -- Conflicts -------------------------------------------- tidyverse_conflicts() --}
\CommentTok{#> x dplyr::filter() masks stats::filter()}
\CommentTok{#> x dplyr::lag()    masks stats::lag()}
\KeywordTok{library}\NormalTok{(neuralnet)}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'neuralnet'}
\CommentTok{#> The following object is masked from 'package:dplyr':}
\CommentTok{#> }
\CommentTok{#>     compute}
\KeywordTok{library}\NormalTok{(GGally)}
\CommentTok{#> Registered S3 method overwritten by 'GGally':}
\CommentTok{#>   method from   }
\CommentTok{#>   +.gg   ggplot2}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'GGally'}
\CommentTok{#> The following object is masked from 'package:dplyr':}
\CommentTok{#> }
\CommentTok{#>     nasa}
\end{Highlighting}
\end{Shaded}

\hypertarget{data-preparation}{%
\section{Data Preparation}\label{data-preparation}}

Our regression ANN will use the \textbf{Yacht Hydrodynamics} data set from UCI's Machine Learning Repository. The yacht data was provided by Dr.~Roberto Lopez email. This data set contains data contains results from 308 full-scale experiments performed at the Delft Ship Hydromechanics Laboratory where they test 22 different hull forms. Their experiment tested the effect of variations in the hull geometry and the ship's Froude number on the craft's residuary resistance per unit weight of displacement.

To begin we download the data from UCI.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{url <-}\StringTok{ 'http://archive.ics.uci.edu/ml/machine-learning-databases/00243/yacht_hydrodynamics.data'}

\NormalTok{Yacht_Data <-}\StringTok{ }\KeywordTok{read_table}\NormalTok{(}\DataTypeTok{file =}\NormalTok{ url,}
                         \DataTypeTok{col_names =} \KeywordTok{c}\NormalTok{(}\StringTok{'LongPos_COB'}\NormalTok{, }\StringTok{'Prismatic_Coeff'}\NormalTok{,}
                                       \StringTok{'Len_Disp_Ratio'}\NormalTok{, }\StringTok{'Beam_Draut_Ratio'}\NormalTok{, }
                                       \StringTok{'Length_Beam_Ratio'}\NormalTok{,}\StringTok{'Froude_Num'}\NormalTok{, }
                                       \StringTok{'Residuary_Resist'}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{na.omit}\NormalTok{()}
\CommentTok{#> Parsed with column specification:}
\CommentTok{#> cols(}
\CommentTok{#>   LongPos_COB = col_double(),}
\CommentTok{#>   Prismatic_Coeff = col_double(),}
\CommentTok{#>   Len_Disp_Ratio = col_double(),}
\CommentTok{#>   Beam_Draut_Ratio = col_double(),}
\CommentTok{#>   Length_Beam_Ratio = col_double(),}
\CommentTok{#>   Froude_Num = col_double(),}
\CommentTok{#>   Residuary_Resist = col_double()}
\CommentTok{#> )}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{glimpse}\NormalTok{(Yacht_Data)}
\CommentTok{#> Observations: 308}
\CommentTok{#> Variables: 7}
\CommentTok{#> $ LongPos_COB       <dbl> -2.3, -2.3, -2.3, -2.3, -2.3, -2.3, -2.3, -2...}
\CommentTok{#> $ Prismatic_Coeff   <dbl> 0.568, 0.568, 0.568, 0.568, 0.568, 0.568, 0....}
\CommentTok{#> $ Len_Disp_Ratio    <dbl> 4.78, 4.78, 4.78, 4.78, 4.78, 4.78, 4.78, 4....}
\CommentTok{#> $ Beam_Draut_Ratio  <dbl> 3.99, 3.99, 3.99, 3.99, 3.99, 3.99, 3.99, 3....}
\CommentTok{#> $ Length_Beam_Ratio <dbl> 3.17, 3.17, 3.17, 3.17, 3.17, 3.17, 3.17, 3....}
\CommentTok{#> $ Froude_Num        <dbl> 0.125, 0.150, 0.175, 0.200, 0.225, 0.250, 0....}
\CommentTok{#> $ Residuary_Resist  <dbl> 0.11, 0.27, 0.47, 0.78, 1.18, 1.82, 2.61, 3....}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# save the dataset locally}
\KeywordTok{write.csv}\NormalTok{(Yacht_Data, }\DataTypeTok{file =} \KeywordTok{file.path}\NormalTok{(data_raw_dir, }\StringTok{"yach_data.csv"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Prior to any data analysis lets take a look at the data set.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggpairs}\NormalTok{(Yacht_Data, }\DataTypeTok{title =} \StringTok{"Scatterplot Matrix of the Features of the Yacht Data Set"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{regression_142-neural_network_yacht_files/figure-latex/unnamed-chunk-6-1} \end{center}

Here we see an excellent summary of the variation of each feature in our data set. Draw your attention to the bottom-most strip of scatter-plots. This shows the residuary resistance as a function of the other data set features (independent experimental values). The greatest variation appears with the Froude Number feature. It will be interesting to see how this pattern appears in the subsequent regression ANNs.

Prior to regression ANN construction we first must split the Yacht data set into test and training data sets. Before we split, first scale each feature to fall in the
\texttt{{[}0,1{]}} interval.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Scale the Data}
\NormalTok{scale01 <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x)\{}
\NormalTok{  (x }\OperatorTok{-}\StringTok{ }\KeywordTok{min}\NormalTok{(x)) }\OperatorTok{/}\StringTok{ }\NormalTok{(}\KeywordTok{max}\NormalTok{(x) }\OperatorTok{-}\StringTok{ }\KeywordTok{min}\NormalTok{(x))}
\NormalTok{\}}

\NormalTok{Yacht_Data <-}\StringTok{ }\NormalTok{Yacht_Data }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate_all}\NormalTok{(scale01)}

\CommentTok{# Split into test and train sets}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{12345}\NormalTok{)}
\NormalTok{Yacht_Data_Train <-}\StringTok{ }\KeywordTok{sample_frac}\NormalTok{(}\DataTypeTok{tbl =}\NormalTok{ Yacht_Data, }\DataTypeTok{replace =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{size =} \FloatTok{0.80}\NormalTok{)}
\NormalTok{Yacht_Data_Test <-}\StringTok{ }\KeywordTok{anti_join}\NormalTok{(Yacht_Data, Yacht_Data_Train)}
\CommentTok{#> Joining, by = c("LongPos_COB", "Prismatic_Coeff", "Len_Disp_Ratio", "Beam_Draut_Ratio", "Length_Beam_Ratio", "Froude_Num", "Residuary_Resist")}
\end{Highlighting}
\end{Shaded}

The \texttt{scale01()} function maps each data observation onto the \texttt{{[}0,1{]}} interval as called in the dplyr \texttt{mutate\_all()} function. We then provided a seed for reproducible results and randomly extracted (without replacement) 80\% of the observations to build the \texttt{Yacht\_Data\_Train} data set. Using dplyr's \texttt{anti\_join()} function we extracted all the observations not within the \texttt{Yacht\_Data\_Train} data set as our test data set in \texttt{Yacht\_Data\_Test}.

\hypertarget{st-regression-ann}{%
\section{1st Regression ANN}\label{st-regression-ann}}

To begin we construct a 1-hidden layer ANN with 1 neuron, the simplest of all neural networks.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{12321}\NormalTok{)}
\NormalTok{Yacht_NN1 <-}\StringTok{ }\KeywordTok{neuralnet}\NormalTok{(Residuary_Resist }\OperatorTok{~}\StringTok{ }\NormalTok{LongPos_COB }\OperatorTok{+}\StringTok{ }\NormalTok{Prismatic_Coeff }\OperatorTok{+}\StringTok{ }
\StringTok{                         }\NormalTok{Len_Disp_Ratio }\OperatorTok{+}\StringTok{ }\NormalTok{Beam_Draut_Ratio }\OperatorTok{+}\StringTok{ }\NormalTok{Length_Beam_Ratio }\OperatorTok{+}
\StringTok{                         }\NormalTok{Froude_Num, }\DataTypeTok{data =}\NormalTok{ Yacht_Data_Train)}
\end{Highlighting}
\end{Shaded}

The \texttt{Yacht\_NN1} is a list containing all parameters of the regression ANN as well as the results of the neural network on the test data set. To view a diagram of the \texttt{Yacht\_NN1} use the \texttt{plot()} function.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(Yacht_NN1, }\DataTypeTok{rep =} \StringTok{'best'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{regression_142-neural_network_yacht_files/figure-latex/unnamed-chunk-9-1} \end{center}

This plot shows the weights learned by the \texttt{Yacht\_NN1} neural network, and displays the number of iterations before convergence, as well as the SSE of the training data set. To manually compute the SSE you can use the following:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{NN1_Train_SSE <-}\StringTok{ }\KeywordTok{sum}\NormalTok{((Yacht_NN1}\OperatorTok{$}\NormalTok{net.result }\OperatorTok{-}\StringTok{ }\NormalTok{Yacht_Data_Train[, }\DecValTok{7}\NormalTok{])}\OperatorTok{^}\DecValTok{2}\NormalTok{)}\OperatorTok{/}\DecValTok{2}
\KeywordTok{paste}\NormalTok{(}\StringTok{"SSE: "}\NormalTok{, }\KeywordTok{round}\NormalTok{(NN1_Train_SSE, }\DecValTok{4}\NormalTok{))}
\CommentTok{#> [1] "SSE:  0.0365"}
\CommentTok{## [1] "SSE:  0.0361"}
\end{Highlighting}
\end{Shaded}

This SSE is the error associated with the training data set. A superior metric for estimating the generalization capability of the ANN would be the SSE of the test data set. Recall, the test data set contains observations not used to train the \texttt{Yacht\_NN1\ ANN}. To calculate the test error, we first must run our test observations through the \texttt{Yacht\_NN1} ANN. This is accomplished with the neuralnet package \texttt{compute()} function, which takes as its first input the desired neural network object created by the \texttt{neuralnet()} function, and the second argument the test data set feature (independent variable(s)) values.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Test_NN1_Output <-}\StringTok{ }\KeywordTok{compute}\NormalTok{(Yacht_NN1, Yacht_Data_Test[, }\DecValTok{1}\OperatorTok{:}\DecValTok{6}\NormalTok{])}\OperatorTok{$}\NormalTok{net.result}
\NormalTok{NN1_Test_SSE <-}\StringTok{ }\KeywordTok{sum}\NormalTok{((Test_NN1_Output }\OperatorTok{-}\StringTok{ }\NormalTok{Yacht_Data_Test[, }\DecValTok{7}\NormalTok{])}\OperatorTok{^}\DecValTok{2}\NormalTok{)}\OperatorTok{/}\DecValTok{2}
\NormalTok{NN1_Test_SSE}
\CommentTok{#> [1] 0.0139}
\CommentTok{## [1] 0.008417631461}
\end{Highlighting}
\end{Shaded}

The \texttt{compute()} function outputs the response variable, in our case the \texttt{Residuary\_Resist}, as estimated by the neural network. Once we have the ANN estimated response we can compute the test SSE. Comparing the test error of 0.0084 to the training error of 0.0361 we see that in our case our test error is smaller than our training error.

\hypertarget{regression-hyperparameters}{%
\section{Regression Hyperparameters}\label{regression-hyperparameters}}

We have constructed the most basic of regression ANNs without modifying any of the default hyperparameters associated with the \texttt{neuralnet()} function. We should try and improve the network by modifying its basic structure and hyperparameter modification. To begin we will add depth to the hidden layer of the network, then we will change the activation function from the logistic to the tangent hyperbolicus (tanh) to determine if these modifications can improve the test data set SSE. When using the tanh activation function, we first must rescale the data from \([0,1]\) to \([-1,1]\) using the \texttt{rescale} package. For the purposes of this exercise we will use the same random seed for reproducible results, generally this is not a best practice.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# 2-Hidden Layers, Layer-1 4-neurons, Layer-2, 1-neuron, logistic activation}
\CommentTok{# function}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{12321}\NormalTok{)}
\NormalTok{Yacht_NN2 <-}\StringTok{ }\KeywordTok{neuralnet}\NormalTok{(Residuary_Resist }\OperatorTok{~}\StringTok{ }\NormalTok{LongPos_COB }\OperatorTok{+}\StringTok{ }\NormalTok{Prismatic_Coeff }\OperatorTok{+}\StringTok{ }\NormalTok{Len_Disp_Ratio }\OperatorTok{+}\StringTok{ }\NormalTok{Beam_Draut_Ratio }\OperatorTok{+}\StringTok{ }\NormalTok{Length_Beam_Ratio }\OperatorTok{+}\StringTok{ }\NormalTok{Froude_Num, }
                       \DataTypeTok{data =}\NormalTok{ Yacht_Data_Train, }
                       \DataTypeTok{hidden =} \KeywordTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{, }\DecValTok{1}\NormalTok{), }
                       \DataTypeTok{act.fct =} \StringTok{"logistic"}\NormalTok{)}

\CommentTok{## Training Error}
\NormalTok{NN2_Train_SSE <-}\StringTok{ }\KeywordTok{sum}\NormalTok{((Yacht_NN2}\OperatorTok{$}\NormalTok{net.result }\OperatorTok{-}\StringTok{ }\NormalTok{Yacht_Data_Train[, }\DecValTok{7}\NormalTok{])}\OperatorTok{^}\DecValTok{2}\NormalTok{)}\OperatorTok{/}\DecValTok{2}

\CommentTok{## Test Error}
\NormalTok{Test_NN2_Output <-}\StringTok{ }\KeywordTok{compute}\NormalTok{(Yacht_NN2, Yacht_Data_Test[, }\DecValTok{1}\OperatorTok{:}\DecValTok{6}\NormalTok{])}\OperatorTok{$}\NormalTok{net.result}
\NormalTok{NN2_Test_SSE <-}\StringTok{ }\KeywordTok{sum}\NormalTok{((Test_NN2_Output }\OperatorTok{-}\StringTok{ }\NormalTok{Yacht_Data_Test[, }\DecValTok{7}\NormalTok{])}\OperatorTok{^}\DecValTok{2}\NormalTok{)}\OperatorTok{/}\DecValTok{2}

\CommentTok{# Rescale for tanh activation function}
\NormalTok{scale11 <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x) \{}
\NormalTok{    (}\DecValTok{2} \OperatorTok{*}\StringTok{ }\NormalTok{((x }\OperatorTok{-}\StringTok{ }\KeywordTok{min}\NormalTok{(x))}\OperatorTok{/}\NormalTok{(}\KeywordTok{max}\NormalTok{(x) }\OperatorTok{-}\StringTok{ }\KeywordTok{min}\NormalTok{(x)))) }\OperatorTok{-}\StringTok{ }\DecValTok{1}
\NormalTok{\}}
\NormalTok{Yacht_Data_Train <-}\StringTok{ }\NormalTok{Yacht_Data_Train }\OperatorTok{%>%}\StringTok{ }\KeywordTok{mutate_all}\NormalTok{(scale11)}
\NormalTok{Yacht_Data_Test <-}\StringTok{ }\NormalTok{Yacht_Data_Test }\OperatorTok{%>%}\StringTok{ }\KeywordTok{mutate_all}\NormalTok{(scale11)}

\CommentTok{# 2-Hidden Layers, Layer-1 4-neurons, Layer-2, 1-neuron, tanh activation}
\CommentTok{# function}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{12321}\NormalTok{)}
\NormalTok{Yacht_NN3 <-}\StringTok{ }\KeywordTok{neuralnet}\NormalTok{(Residuary_Resist }\OperatorTok{~}\StringTok{ }\NormalTok{LongPos_COB }\OperatorTok{+}\StringTok{ }\NormalTok{Prismatic_Coeff }\OperatorTok{+}\StringTok{ }\NormalTok{Len_Disp_Ratio }\OperatorTok{+}\StringTok{ }\NormalTok{Beam_Draut_Ratio }\OperatorTok{+}\StringTok{ }\NormalTok{Length_Beam_Ratio }\OperatorTok{+}\StringTok{ }\NormalTok{Froude_Num, }
                       \DataTypeTok{data =}\NormalTok{ Yacht_Data_Train, }
                       \DataTypeTok{hidden =} \KeywordTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{, }\DecValTok{1}\NormalTok{), }
                       \DataTypeTok{act.fct =} \StringTok{"tanh"}\NormalTok{)}

\CommentTok{## Training Error}
\NormalTok{NN3_Train_SSE <-}\StringTok{ }\KeywordTok{sum}\NormalTok{((Yacht_NN3}\OperatorTok{$}\NormalTok{net.result }\OperatorTok{-}\StringTok{ }\NormalTok{Yacht_Data_Train[, }\DecValTok{7}\NormalTok{])}\OperatorTok{^}\DecValTok{2}\NormalTok{)}\OperatorTok{/}\DecValTok{2}

\CommentTok{## Test Error}
\NormalTok{Test_NN3_Output <-}\StringTok{ }\KeywordTok{compute}\NormalTok{(Yacht_NN3, Yacht_Data_Test[, }\DecValTok{1}\OperatorTok{:}\DecValTok{6}\NormalTok{])}\OperatorTok{$}\NormalTok{net.result}
\NormalTok{NN3_Test_SSE <-}\StringTok{ }\KeywordTok{sum}\NormalTok{((Test_NN3_Output }\OperatorTok{-}\StringTok{ }\NormalTok{Yacht_Data_Test[, }\DecValTok{7}\NormalTok{])}\OperatorTok{^}\DecValTok{2}\NormalTok{)}\OperatorTok{/}\DecValTok{2}

\CommentTok{# 1-Hidden Layer, 1-neuron, tanh activation function}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{12321}\NormalTok{)}
\NormalTok{Yacht_NN4 <-}\StringTok{ }\KeywordTok{neuralnet}\NormalTok{(Residuary_Resist }\OperatorTok{~}\StringTok{ }\NormalTok{LongPos_COB }\OperatorTok{+}\StringTok{ }\NormalTok{Prismatic_Coeff }\OperatorTok{+}\StringTok{ }\NormalTok{Len_Disp_Ratio }\OperatorTok{+}\StringTok{ }\NormalTok{Beam_Draut_Ratio }\OperatorTok{+}\StringTok{ }\NormalTok{Length_Beam_Ratio }\OperatorTok{+}\StringTok{ }\NormalTok{Froude_Num, }
                       \DataTypeTok{data =}\NormalTok{ Yacht_Data_Train, }
                       \DataTypeTok{act.fct =} \StringTok{"tanh"}\NormalTok{)}

\CommentTok{## Training Error}
\NormalTok{NN4_Train_SSE <-}\StringTok{ }\KeywordTok{sum}\NormalTok{((Yacht_NN4}\OperatorTok{$}\NormalTok{net.result }\OperatorTok{-}\StringTok{ }\NormalTok{Yacht_Data_Train[, }\DecValTok{7}\NormalTok{])}\OperatorTok{^}\DecValTok{2}\NormalTok{)}\OperatorTok{/}\DecValTok{2}

\CommentTok{## Test Error}
\NormalTok{Test_NN4_Output <-}\StringTok{ }\KeywordTok{compute}\NormalTok{(Yacht_NN4, Yacht_Data_Test[, }\DecValTok{1}\OperatorTok{:}\DecValTok{6}\NormalTok{])}\OperatorTok{$}\NormalTok{net.result}
\NormalTok{NN4_Test_SSE <-}\StringTok{ }\KeywordTok{sum}\NormalTok{((Test_NN4_Output }\OperatorTok{-}\StringTok{ }\NormalTok{Yacht_Data_Test[, }\DecValTok{7}\NormalTok{])}\OperatorTok{^}\DecValTok{2}\NormalTok{)}\OperatorTok{/}\DecValTok{2}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Bar plot of results}
\NormalTok{Regression_NN_Errors <-}\StringTok{ }\KeywordTok{tibble}\NormalTok{(}\DataTypeTok{Network =} \KeywordTok{rep}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"NN1"}\NormalTok{, }\StringTok{"NN2"}\NormalTok{, }\StringTok{"NN3"}\NormalTok{, }\StringTok{"NN4"}\NormalTok{), }\DataTypeTok{each =} \DecValTok{2}\NormalTok{), }
                               \DataTypeTok{DataSet =} \KeywordTok{rep}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"Train"}\NormalTok{, }\StringTok{"Test"}\NormalTok{), }\DataTypeTok{time =} \DecValTok{4}\NormalTok{), }
                               \DataTypeTok{SSE =} \KeywordTok{c}\NormalTok{(NN1_Train_SSE, NN1_Test_SSE, }
\NormalTok{                                       NN2_Train_SSE, NN2_Test_SSE, }
\NormalTok{                                       NN3_Train_SSE, NN3_Test_SSE, }
\NormalTok{                                       NN4_Train_SSE, NN4_Test_SSE))}

\NormalTok{Regression_NN_Errors }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(Network, SSE, }\DataTypeTok{fill =}\NormalTok{ DataSet)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_col}\NormalTok{(}\DataTypeTok{position =} \StringTok{"dodge"}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"Regression ANN's SSE"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{regression_142-neural_network_yacht_files/figure-latex/unnamed-chunk-13-1} \end{center}

As evident from the plot, we see that the best regression ANN we found was \texttt{Yacht\_NN2} with a training and test SSE of 0.0188 and 0.0057. We make this determination by the value of the training and test SSEs only. \texttt{Yacht\_NN2}'s structure is presented here:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(Yacht_NN2, }\DataTypeTok{rep =} \StringTok{"best"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{regression_142-neural_network_yacht_files/figure-latex/unnamed-chunk-14-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{12321}\NormalTok{)}
\NormalTok{Yacht_NN2 <-}\StringTok{ }\KeywordTok{neuralnet}\NormalTok{(Residuary_Resist }\OperatorTok{~}\StringTok{ }\NormalTok{LongPos_COB }\OperatorTok{+}\StringTok{ }\NormalTok{Prismatic_Coeff }\OperatorTok{+}\StringTok{ }\NormalTok{Len_Disp_Ratio }\OperatorTok{+}\StringTok{ }\NormalTok{Beam_Draut_Ratio }\OperatorTok{+}\StringTok{ }\NormalTok{Length_Beam_Ratio }\OperatorTok{+}\StringTok{ }\NormalTok{Froude_Num, }
                       \DataTypeTok{data =}\NormalTok{ Yacht_Data_Train, }
                       \DataTypeTok{hidden =} \KeywordTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{, }\DecValTok{1}\NormalTok{), }
                       \DataTypeTok{act.fct =} \StringTok{"logistic"}\NormalTok{, }
                       \DataTypeTok{rep =} \DecValTok{10}\NormalTok{)}

\KeywordTok{plot}\NormalTok{(Yacht_NN2, }\DataTypeTok{rep =} \StringTok{"best"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{regression_142-neural_network_yacht_files/figure-latex/unnamed-chunk-15-1} \end{center}

By setting the same seed, prior to running the 10 repetitions of ANNs, we force the software to reproduce the exact same \texttt{Yacht\_NN2} ANN for the first replication. The subsequent 9 generated ANNs, use a different random set of starting weights. Comparing the `best' of the 10 repetitions, to the Yacht\_NN2, we observe a decrease in training set error indicating we have a superior set of weights.

\hypertarget{wrapping-up}{%
\section{Wrapping Up}\label{wrapping-up}}

We have briefly covered regression ANNs in this tutorial. In the next tutorial we will cover classification ANNs. The neuralnet package used in this tutorial is one of many tools available for ANN implementation in R. Others include:

\begin{itemize}
\tightlist
\item
  nnet
\item
  autoencoder
\item
  caret
\item
  RSNNS
\item
  h2o
\end{itemize}

Before you move on to the next tutorial, test your new knowledge on the exercises that follow.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Why do we split the yacht data into a training and test data sets?
\item
  Re-load the Yacht Data from the UCI Machine learning repository yacht data without scaling. Run any regression ANN. What happens? Why do you think this happens?
\item
  After completing exercise question 1, re-scale the yacht data. Perform a simple linear regression fitting \texttt{Residuary\_Resist} as a function of all other features. Now run a regression neural network (see 1st Regression ANN section). Plot the regression ANN and compare the weights on the features in the ANN to the p-values for the regressors.
\item
  Build your own regression ANN using the scaled yacht data modifying one hyperparameter. Use \texttt{?neuralnet} to see the function options. Plot your ANN.
\end{enumerate}

\hypertarget{regression---cereals-dataset}{%
\chapter{Regression - cereals dataset}\label{regression---cereals-dataset}}

\hypertarget{introduction-6}{%
\section{Introduction}\label{introduction-6}}

Source: \url{https://www.analyticsvidhya.com/blog/2017/09/creating-visualizing-neural-network-in-r/}

Neural network is an information-processing machine and can be viewed as analogous to human nervous system. Just like human nervous system, which is made up of interconnected neurons, a neural network is made up of interconnected information processing units. The information processing units do not work in a linear manner. In fact, neural network draws its strength from parallel processing of information, which allows it to deal with non-linearity. Neural network becomes handy to infer meaning and detect patterns from complex data sets.

Neural network is considered as one of the most useful technique in the world of data analytics. However, it is complex and is often regarded as a black box, i.e.~users view the input and output of a neural network but remain clueless about the knowledge generating process. We hope that the article will help readers learn about the internal mechanism of a neural network and get hands-on experience to implement it in R.

\hypertarget{the-basics-of-neural-networks}{%
\section{The Basics of Neural Networks}\label{the-basics-of-neural-networks}}

A neural network is a model characterized by an activation function, which is used by interconnected information processing units to transform input into output. A neural network has always been compared to human nervous system. Information in passed through interconnected units analogous to information passage through neurons in humans. The first layer of the neural network receives the raw input, processes it and passes the processed information to the hidden layers. The hidden layer passes the information to the last layer, which produces the output. The advantage of neural network is that it is adaptive in nature. It learns from the information provided, i.e.~trains itself from the data, which has a known outcome and optimizes its weights for a better prediction in situations with unknown outcome.

A perceptron, viz. single layer neural network, is the most basic form of a neural network. A perceptron receives multidimensional input and processes it using a weighted summation and an activation function. It is trained using a labeled data and learning algorithm that optimize the weights in the summation processor. A major limitation of perceptron model is its inability to deal with non-linearity. A multilayered neural network overcomes this limitation and helps solve non-linear problems. The input layer connects with hidden layer, which in turn connects to the output layer. The connections are weighted and weights are optimized using a learning rule.

There are many learning rules that are used with neural network:

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\tightlist
\item
  least mean square;
\item
  gradient descent;
\item
  newton's rule;
\item
  conjugate gradient etc.
\end{enumerate}

The learning rules can be used in conjunction with backpropgation error method. The learning rule is used to calculate the error at the output unit. This error is backpropagated to all the units such that the error at each unit is proportional to the contribution of that unit towards total error at the output unit. The errors at each unit are then used to optimize the weight at each connection. Figure 1 displays the structure of a simple neural network model for better understanding.

\hypertarget{fitting-a-neural-network-in-r}{%
\section{Fitting a Neural Network in R}\label{fitting-a-neural-network-in-r}}

Now we will fit a neural network model in R. In this article, we use a subset of cereal dataset shared by Carnegie Mellon University (CMU). The details of the dataset are on the following link: \url{http://lib.stat.cmu.edu/DASL/Datafiles/Cereals.html}. The objective is to predict rating of the cereals variables such as calories, proteins, fat etc. The R script is provided side by side and is commented for better understanding of the user. . The data is in .csv format and can be downloaded by clicking: cereals.

Please set working directory in R using setwd( ) function, and keep cereal.csv in the working directory. We use rating as the dependent variable and calories, proteins, fat, sodium and fiber as the independent variables. We divide the data into training and test set. Training set is used to find the relationship between dependent and independent variables while the test set assesses the performance of the model. We use 60\% of the dataset as training set. The assignment of the data to training and test set is done using random sampling. We perform random sampling on R using sample ( ) function. We have used set.seed( ) to generate same random sample everytime and maintain consistency. We will use the index variable while fitting neural network to create training and test data sets. The R script is as follows:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## Creating index variable }

\CommentTok{# Read the Data}
\NormalTok{data =}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\KeywordTok{file.path}\NormalTok{(data_raw_dir, }\StringTok{"cereals.csv"}\NormalTok{), }\DataTypeTok{header=}\NormalTok{T)}

\CommentTok{# Random sampling}
\NormalTok{samplesize =}\StringTok{ }\FloatTok{0.60} \OperatorTok{*}\StringTok{ }\KeywordTok{nrow}\NormalTok{(data)}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{80}\NormalTok{)}
\NormalTok{index =}\StringTok{ }\KeywordTok{sample}\NormalTok{( }\KeywordTok{seq_len}\NormalTok{ ( }\KeywordTok{nrow}\NormalTok{ ( data ) ), }\DataTypeTok{size =}\NormalTok{ samplesize )}

\CommentTok{# Create training and test set  }
\NormalTok{datatrain =}\StringTok{ }\NormalTok{data[ index, ]}
\NormalTok{datatest =}\StringTok{ }\NormalTok{data[ }\OperatorTok{-}\NormalTok{index, ]}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{glimpse}\NormalTok{(data)}
\CommentTok{#> Observations: 75}
\CommentTok{#> Variables: 6}
\CommentTok{#> $ calories <int> 70, 120, 70, 50, 110, 110, 130, 90, 90, 120, 110, 120...}
\CommentTok{#> $ protein  <int> 4, 3, 4, 4, 2, 2, 3, 2, 3, 1, 6, 1, 3, 1, 2, 2, 1, 1,...}
\CommentTok{#> $ fat      <int> 1, 5, 1, 0, 2, 0, 2, 1, 0, 2, 2, 3, 2, 1, 0, 0, 0, 1,...}
\CommentTok{#> $ sodium   <int> 130, 15, 260, 140, 180, 125, 210, 200, 210, 220, 290,...}
\CommentTok{#> $ fiber    <dbl> 10.0, 2.0, 9.0, 14.0, 1.5, 1.0, 2.0, 4.0, 5.0, 0.0, 2...}
\CommentTok{#> $ rating   <dbl> 68.4, 34.0, 59.4, 93.7, 29.5, 33.2, 37.0, 49.1, 53.3,...}
\end{Highlighting}
\end{Shaded}

Now we fit a neural network on our data. We use neuralnet library for the analysis. The first step is to scale the cereal dataset. The scaling of data is essential because otherwise a variable may have large impact on the prediction variable only because of its scale. Using unscaled may lead to meaningless results. The common techniques to scale data are: min-max normalization, Z-score normalization, median and MAD, and tan-h estimators. The min-max normalization transforms the data into a common range, thus removing the scaling effect from all the variables. Unlike Z-score normalization and median and MAD method, the min-max method retains the original distribution of the variables. We use min-max normalization to scale the data. The R script for scaling the data is as follows.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## Scale data for neural network}

\NormalTok{max =}\StringTok{ }\KeywordTok{apply}\NormalTok{(data , }\DecValTok{2}\NormalTok{ , max)}
\NormalTok{min =}\StringTok{ }\KeywordTok{apply}\NormalTok{(data, }\DecValTok{2}\NormalTok{ , min)}
\NormalTok{scaled =}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(}\KeywordTok{scale}\NormalTok{(data, }\DataTypeTok{center =}\NormalTok{ min, }\DataTypeTok{scale =}\NormalTok{ max }\OperatorTok{-}\StringTok{ }\NormalTok{min))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## Fit neural network }

\CommentTok{# install library}
\CommentTok{# install.packages("neuralnet ")}

\CommentTok{# load library}
\KeywordTok{library}\NormalTok{(neuralnet)}

\CommentTok{# creating training and test set}
\NormalTok{trainNN =}\StringTok{ }\NormalTok{scaled[index , ]}
\NormalTok{testNN =}\StringTok{ }\NormalTok{scaled[}\OperatorTok{-}\NormalTok{index , ]}

\CommentTok{# fit neural network}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{2}\NormalTok{)}
\NormalTok{NN =}\StringTok{ }\KeywordTok{neuralnet}\NormalTok{(rating }\OperatorTok{~}\StringTok{ }\NormalTok{calories }\OperatorTok{+}\StringTok{ }\NormalTok{protein }\OperatorTok{+}\StringTok{ }\NormalTok{fat }\OperatorTok{+}\StringTok{ }\NormalTok{sodium }\OperatorTok{+}\StringTok{ }\NormalTok{fiber, }
\NormalTok{               trainNN, }\DataTypeTok{hidden =} \DecValTok{3}\NormalTok{ , }\DataTypeTok{linear.output =}\NormalTok{ T )}

\CommentTok{# plot neural network}
\KeywordTok{plot}\NormalTok{(NN)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## Prediction using neural network}

\NormalTok{predict_testNN =}\StringTok{ }\KeywordTok{compute}\NormalTok{(NN, testNN[,}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{5}\NormalTok{)])}
\NormalTok{predict_testNN =}\StringTok{ }\NormalTok{(predict_testNN}\OperatorTok{$}\NormalTok{net.result }\OperatorTok{*}\StringTok{ }\NormalTok{(}\KeywordTok{max}\NormalTok{(data}\OperatorTok{$}\NormalTok{rating) }\OperatorTok{-}\StringTok{ }\KeywordTok{min}\NormalTok{(data}\OperatorTok{$}\NormalTok{rating))) }\OperatorTok{+}\StringTok{ }\KeywordTok{min}\NormalTok{(data}\OperatorTok{$}\NormalTok{rating)}

\KeywordTok{plot}\NormalTok{(datatest}\OperatorTok{$}\NormalTok{rating, predict_testNN, }\DataTypeTok{col=}\StringTok{'blue'}\NormalTok{, }\DataTypeTok{pch=}\DecValTok{16}\NormalTok{, }\DataTypeTok{ylab =} \StringTok{"predicted rating NN"}\NormalTok{, }\DataTypeTok{xlab =} \StringTok{"real rating"}\NormalTok{)}

\KeywordTok{abline}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{)}

\CommentTok{# Calculate Root Mean Square Error (RMSE)}
\NormalTok{RMSE.NN =}\StringTok{ }\NormalTok{(}\KeywordTok{sum}\NormalTok{((datatest}\OperatorTok{$}\NormalTok{rating }\OperatorTok{-}\StringTok{ }\NormalTok{predict_testNN)}\OperatorTok{^}\DecValTok{2}\NormalTok{) }\OperatorTok{/}\StringTok{ }\KeywordTok{nrow}\NormalTok{(datatest)) }\OperatorTok{^}\StringTok{ }\FloatTok{0.5}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{regression_144-nn_cereals_neuralnet_files/figure-latex/predict_nn-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## Cross validation of neural network model}

\CommentTok{# install relevant libraries}
\CommentTok{# install.packages("boot")}
\CommentTok{# install.packages("plyr")}

\CommentTok{# Load libraries}
\KeywordTok{library}\NormalTok{(boot)}
\KeywordTok{library}\NormalTok{(plyr)}

\CommentTok{# Initialize variables}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{50}\NormalTok{)}
\NormalTok{k =}\StringTok{ }\DecValTok{100}
\NormalTok{RMSE.NN =}\StringTok{ }\OtherTok{NULL}

\NormalTok{List =}\StringTok{ }\KeywordTok{list}\NormalTok{( )}

\CommentTok{# Fit neural network model within nested for loop}
\ControlFlowTok{for}\NormalTok{(j }\ControlFlowTok{in} \DecValTok{10}\OperatorTok{:}\DecValTok{65}\NormalTok{)\{}
    \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{k) \{}
\NormalTok{        index =}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\KeywordTok{nrow}\NormalTok{(data),j )}

\NormalTok{        trainNN =}\StringTok{ }\NormalTok{scaled[index,]}
\NormalTok{        testNN =}\StringTok{ }\NormalTok{scaled[}\OperatorTok{-}\NormalTok{index,]}
\NormalTok{        datatest =}\StringTok{ }\NormalTok{data[}\OperatorTok{-}\NormalTok{index,]}

\NormalTok{        NN =}\StringTok{ }\KeywordTok{neuralnet}\NormalTok{(rating }\OperatorTok{~}\StringTok{ }\NormalTok{calories }\OperatorTok{+}\StringTok{ }\NormalTok{protein }\OperatorTok{+}\StringTok{ }\NormalTok{fat }\OperatorTok{+}\StringTok{ }\NormalTok{sodium }\OperatorTok{+}\StringTok{ }\NormalTok{fiber, trainNN, }\DataTypeTok{hidden =} \DecValTok{3}\NormalTok{, }\DataTypeTok{linear.output=}\NormalTok{ T)}
\NormalTok{        predict_testNN =}\StringTok{ }\KeywordTok{compute}\NormalTok{(NN,testNN[,}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{5}\NormalTok{)])}
\NormalTok{        predict_testNN =}\StringTok{ }\NormalTok{(predict_testNN}\OperatorTok{$}\NormalTok{net.result}\OperatorTok{*}\NormalTok{(}\KeywordTok{max}\NormalTok{(data}\OperatorTok{$}\NormalTok{rating)}\OperatorTok{-}\KeywordTok{min}\NormalTok{(data}\OperatorTok{$}\NormalTok{rating)))}\OperatorTok{+}\KeywordTok{min}\NormalTok{(data}\OperatorTok{$}\NormalTok{rating)}

\NormalTok{        RMSE.NN [i]<-}\StringTok{ }\NormalTok{(}\KeywordTok{sum}\NormalTok{((datatest}\OperatorTok{$}\NormalTok{rating }\OperatorTok{-}\StringTok{ }\NormalTok{predict_testNN)}\OperatorTok{^}\DecValTok{2}\NormalTok{)}\OperatorTok{/}\KeywordTok{nrow}\NormalTok{(datatest))}\OperatorTok{^}\FloatTok{0.5}
\NormalTok{    \}}
\NormalTok{    List[[j]] =}\StringTok{ }\NormalTok{RMSE.NN}
\NormalTok{\}}

\NormalTok{Matrix.RMSE =}\StringTok{ }\KeywordTok{do.call}\NormalTok{(cbind, List)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## Prepare boxplot}
\KeywordTok{boxplot}\NormalTok{(Matrix.RMSE[,}\DecValTok{56}\NormalTok{], }\DataTypeTok{ylab =} \StringTok{"RMSE"}\NormalTok{, }\DataTypeTok{main =} \StringTok{"RMSE BoxPlot (length of traning set = 65)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{regression_144-nn_cereals_neuralnet_files/figure-latex/boxplot-1} \end{center}



\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## Variation of median RMSE }
\CommentTok{# install.packages("matrixStats")}
\KeywordTok{library}\NormalTok{(matrixStats)}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'matrixStats'}
\CommentTok{#> The following object is masked from 'package:plyr':}
\CommentTok{#> }
\CommentTok{#>     count}

\NormalTok{med =}\StringTok{ }\KeywordTok{colMedians}\NormalTok{(Matrix.RMSE)}

\NormalTok{X =}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\DecValTok{10}\NormalTok{,}\DecValTok{65}\NormalTok{)}

\KeywordTok{plot}\NormalTok{ (med}\OperatorTok{~}\NormalTok{X, }\DataTypeTok{type =} \StringTok{"l"}\NormalTok{, }\DataTypeTok{xlab =} \StringTok{"length of training set"}\NormalTok{, }\DataTypeTok{ylab =} \StringTok{"median RMSE"}\NormalTok{, }\DataTypeTok{main =} \StringTok{"Variation of RMSE with length of training set"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{regression_144-nn_cereals_neuralnet_files/figure-latex/var-rmse-1} 

}

\caption{Variation of RMSE}\label{fig:var-rmse}
\end{figure}

Figure \ref{fig:var-rmse}) shows that the median RMSE of our model decreases as the length of the training the set. This is an important result. The reader must remember that the model accuracy is dependent on the length of training set. The performance of neural network model is sensitive to training-test split.

\hypertarget{end-notes}{%
\section{End Notes}\label{end-notes}}

The article discusses the theoretical aspects of a neural network, its implementation in R and post training evaluation. Neural network is inspired from biological nervous system. Similar to nervous system the information is passed through layers of processors. The significance of variables is represented by weights of each connection. The article provides basic understanding of back propagation algorithm, which is used to assign these weights. In this article we also implement neural network on R. We use a publically available dataset shared by CMU. The aim is to predict the rating of cereals using information such as calories, fat, protein etc. After constructing the neural network we evaluate the model for accuracy and robustness. We compute RMSE and perform cross-validation analysis. In cross validation, we check the variation in model accuracy as the length of training set is changed. We consider training sets with length 10 to 65. For each length a 100 samples are random picked and median RMSE is calculated. We show that model accuracy increases when training set is large. Before using the model for prediction, it is important to check the robustness of performance through cross validation.

The article provides a quick review neural network and is a useful reference for data enthusiasts. We have provided commented R code throughout the article to help readers with hands on experience of using neural networks.

\hypertarget{fitting-a-neural-network}{%
\chapter{Fitting a neural network}\label{fitting-a-neural-network}}

\hypertarget{introduction-7}{%
\section{Introduction}\label{introduction-7}}

\url{https://www.r-bloggers.com/fitting-a-neural-network-in-r-neuralnet-package/}

\url{https://datascienceplus.com/fitting-neural-network-in-r/}

Neural networks have always been one of the fascinating machine learning models in my opinion, not only because of the fancy backpropagation algorithm but also because of their complexity (think of deep learning with many hidden layers) and structure inspired by the brain.

Neural networks have not always been popular, partly because they were, and still are in some cases, computationally expensive and partly because they did not seem to yield better results when compared with simpler methods such as support vector machines (SVMs). Nevertheless, Neural Networks have, once again, raised attention and become popular.

Update: We published another post about Network analysis at DataScience+ Network analysis of Game of Thrones

In this post, we are going to fit a simple neural network using the neuralnet package and fit a linear model as a comparison.

\hypertarget{the-dataset-1}{%
\section{The dataset}\label{the-dataset-1}}

We are going to use the Boston dataset in the MASS package.
The Boston dataset is a collection of data about housing values in the suburbs of Boston. Our goal is to predict the median value of owner-occupied homes (medv) using all the other continuous variables available.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{500}\NormalTok{)}
\KeywordTok{library}\NormalTok{(MASS)}
\NormalTok{data <-}\StringTok{ }\NormalTok{Boston}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{glimpse}\NormalTok{(data)}
\CommentTok{#> Observations: 506}
\CommentTok{#> Variables: 14}
\CommentTok{#> $ crim    <dbl> 0.00632, 0.02731, 0.02729, 0.03237, 0.06905, 0.02985, ...}
\CommentTok{#> $ zn      <dbl> 18.0, 0.0, 0.0, 0.0, 0.0, 0.0, 12.5, 12.5, 12.5, 12.5,...}
\CommentTok{#> $ indus   <dbl> 2.31, 7.07, 7.07, 2.18, 2.18, 2.18, 7.87, 7.87, 7.87, ...}
\CommentTok{#> $ chas    <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...}
\CommentTok{#> $ nox     <dbl> 0.538, 0.469, 0.469, 0.458, 0.458, 0.458, 0.524, 0.524...}
\CommentTok{#> $ rm      <dbl> 6.58, 6.42, 7.18, 7.00, 7.15, 6.43, 6.01, 6.17, 5.63, ...}
\CommentTok{#> $ age     <dbl> 65.2, 78.9, 61.1, 45.8, 54.2, 58.7, 66.6, 96.1, 100.0,...}
\CommentTok{#> $ dis     <dbl> 4.09, 4.97, 4.97, 6.06, 6.06, 6.06, 5.56, 5.95, 6.08, ...}
\CommentTok{#> $ rad     <int> 1, 2, 2, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 4, 4, 4, 4, 4, ...}
\CommentTok{#> $ tax     <dbl> 296, 242, 242, 222, 222, 222, 311, 311, 311, 311, 311,...}
\CommentTok{#> $ ptratio <dbl> 15.3, 17.8, 17.8, 18.7, 18.7, 18.7, 15.2, 15.2, 15.2, ...}
\CommentTok{#> $ black   <dbl> 397, 397, 393, 395, 397, 394, 396, 397, 387, 387, 393,...}
\CommentTok{#> $ lstat   <dbl> 4.98, 9.14, 4.03, 2.94, 5.33, 5.21, 12.43, 19.15, 29.9...}
\CommentTok{#> $ medv    <dbl> 24.0, 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, ...}
\end{Highlighting}
\end{Shaded}

First we need to check that no datapoint is missing, otherwise we need to fix the dataset.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{apply}\NormalTok{(data,}\DecValTok{2}\NormalTok{,}\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{sum}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(x)))}
\CommentTok{#>    crim      zn   indus    chas     nox      rm     age     dis     rad }
\CommentTok{#>       0       0       0       0       0       0       0       0       0 }
\CommentTok{#>     tax ptratio   black   lstat    medv }
\CommentTok{#>       0       0       0       0       0}
\end{Highlighting}
\end{Shaded}

There is no missing data, good. We proceed by randomly splitting the data into a train and a test set, then we fit a linear regression model and test it on the test set. Note that I am using the gml() function instead of the lm() this will become useful later when cross validating the linear model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{index <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\KeywordTok{nrow}\NormalTok{(data),}\KeywordTok{round}\NormalTok{(}\FloatTok{0.75}\OperatorTok{*}\KeywordTok{nrow}\NormalTok{(data)))}
\NormalTok{train <-}\StringTok{ }\NormalTok{data[index,]}
\NormalTok{test <-}\StringTok{ }\NormalTok{data[}\OperatorTok{-}\NormalTok{index,]}
\NormalTok{lm.fit <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(medv}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{train)}
\KeywordTok{summary}\NormalTok{(lm.fit)}
\CommentTok{#> }
\CommentTok{#> Call:}
\CommentTok{#> glm(formula = medv ~ ., data = train)}
\CommentTok{#> }
\CommentTok{#> Deviance Residuals: }
\CommentTok{#>     Min       1Q   Median       3Q      Max  }
\CommentTok{#> -15.211   -2.559   -0.655    1.828   29.711  }
\CommentTok{#> }
\CommentTok{#> Coefficients:}
\CommentTok{#>              Estimate Std. Error t value Pr(>|t|)    }
\CommentTok{#> (Intercept)  31.11170    5.45981    5.70  2.5e-08 ***}
\CommentTok{#> crim         -0.11137    0.03326   -3.35  0.00090 ***}
\CommentTok{#> zn            0.04263    0.01431    2.98  0.00308 ** }
\CommentTok{#> indus         0.00148    0.06745    0.02  0.98247    }
\CommentTok{#> chas          1.75684    0.98109    1.79  0.07417 .  }
\CommentTok{#> nox         -18.18485    4.47157   -4.07  5.8e-05 ***}
\CommentTok{#> rm            4.76034    0.48047    9.91  < 2e-16 ***}
\CommentTok{#> age          -0.01344    0.01410   -0.95  0.34119    }
\CommentTok{#> dis          -1.55375    0.21893   -7.10  6.7e-12 ***}
\CommentTok{#> rad           0.28818    0.07202    4.00  7.6e-05 ***}
\CommentTok{#> tax          -0.01374    0.00406   -3.38  0.00079 ***}
\CommentTok{#> ptratio      -0.94755    0.14012   -6.76  5.4e-11 ***}
\CommentTok{#> black         0.00950    0.00290    3.28  0.00115 ** }
\CommentTok{#> lstat        -0.38890    0.05973   -6.51  2.5e-10 ***}
\CommentTok{#> ---}
\CommentTok{#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1}
\CommentTok{#> }
\CommentTok{#> (Dispersion parameter for gaussian family taken to be 20.2)}
\CommentTok{#> }
\CommentTok{#>     Null deviance: 32463.5  on 379  degrees of freedom}
\CommentTok{#> Residual deviance:  7407.1  on 366  degrees of freedom}
\CommentTok{#> AIC: 2237}
\CommentTok{#> }
\CommentTok{#> Number of Fisher Scoring iterations: 2}
\NormalTok{pr.lm <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(lm.fit,test)}
\NormalTok{MSE.lm <-}\StringTok{ }\KeywordTok{sum}\NormalTok{((pr.lm }\OperatorTok{-}\StringTok{ }\NormalTok{test}\OperatorTok{$}\NormalTok{medv)}\OperatorTok{^}\DecValTok{2}\NormalTok{)}\OperatorTok{/}\KeywordTok{nrow}\NormalTok{(test)}
\end{Highlighting}
\end{Shaded}

The sample(x,size) function simply outputs a vector of the specified size of randomly selected samples from the vector x. By default the sampling is without replacement: index is essentially a random vector of indeces.
Since we are dealing with a regression problem, we are going to use the mean squared error (MSE) as a measure of how much our predictions are far away from the real data.

\hypertarget{preparing-to-fit-the-neural-network}{%
\section{Preparing to fit the neural network}\label{preparing-to-fit-the-neural-network}}

Before fitting a neural network, some preparation need to be done. Neural networks are not that easy to train and tune.

As a first step, we are going to address data preprocessing.
It is good practice to normalize your data before training a neural network. I cannot emphasize enough how important this step is: depending on your dataset, avoiding normalization may lead to useless results or to a very difficult training process (most of the times the algorithm will not converge before the number of maximum iterations allowed). You can choose different methods to scale the data (z-normalization, min-max scale, etc\ldots{}). I chose to use the min-max method and scale the data in the interval {[}0,1{]}. Usually scaling in the intervals {[}0,1{]} or {[}-1,1{]} tends to give better results.
We therefore scale and split the data before moving on:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{maxs <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(data, }\DecValTok{2}\NormalTok{, max) }
\NormalTok{mins <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(data, }\DecValTok{2}\NormalTok{, min)}

\NormalTok{scaled <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(}\KeywordTok{scale}\NormalTok{(data, }\DataTypeTok{center =}\NormalTok{ mins, }\DataTypeTok{scale =}\NormalTok{ maxs }\OperatorTok{-}\StringTok{ }\NormalTok{mins))}

\NormalTok{train_ <-}\StringTok{ }\NormalTok{scaled[index,]}
\NormalTok{test_ <-}\StringTok{ }\NormalTok{scaled[}\OperatorTok{-}\NormalTok{index,]}
\end{Highlighting}
\end{Shaded}

Note that scale returns a matrix that needs to be coerced into a data.frame.

\hypertarget{parameters}{%
\section{Parameters}\label{parameters}}

As far as I know there is no fixed rule as to how many layers and neurons to use although there are several more or less accepted rules of thumb. Usually, if at all necessary, one hidden layer is enough for a vast numbers of applications. As far as the number of neurons is concerned, it should be between the input layer size and the output layer size, usually 2/3 of the input size. At least in my brief experience testing again and again is the best solution since there is no guarantee that any of these rules will fit your model best.
Since this is a toy example, we are going to use 2 hidden layers with this configuration: 13:5:3:1. The input layer has 13 inputs, the two hidden layers have 5 and 3 neurons and the output layer has, of course, a single output since we are doing regression.
Let's fit the net:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(neuralnet)}
\NormalTok{n <-}\StringTok{ }\KeywordTok{names}\NormalTok{(train_)}
\NormalTok{f <-}\StringTok{ }\KeywordTok{as.formula}\NormalTok{(}\KeywordTok{paste}\NormalTok{(}\StringTok{"medv ~"}\NormalTok{, }\KeywordTok{paste}\NormalTok{(n[}\OperatorTok{!}\NormalTok{n }\OperatorTok{%in%}\StringTok{ "medv"}\NormalTok{], }\DataTypeTok{collapse =} \StringTok{" + "}\NormalTok{)))}
\NormalTok{nn <-}\StringTok{ }\KeywordTok{neuralnet}\NormalTok{(f,}\DataTypeTok{data=}\NormalTok{train_,}\DataTypeTok{hidden=}\KeywordTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{,}\DecValTok{3}\NormalTok{),}\DataTypeTok{linear.output=}\NormalTok{T)}
\end{Highlighting}
\end{Shaded}

\textbf{A couple of notes:}

\begin{itemize}
\item
  For some reason the formula y\textasciitilde{}. is not accepted in the neuralnet() function. You need to first write the formula and then pass it as an argument in the fitting function.
\item
  The hidden argument accepts a vector with the number of neurons for each hidden layer, while the argument linear.output is used to specify whether we want to do regression linear.output=TRUE or classification linear.output=FALSE
\end{itemize}

The \texttt{neuralnet} package provides a nice tool to plot the model:

This is the graphical representation of the model with the weights on each connection:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(nn)}
\end{Highlighting}
\end{Shaded}

The black lines show the connections between each layer and the weights on each connection while the blue lines show the bias term added in each step. The bias can be thought as the intercept of a linear model.
The net is essentially a black box so we cannot say that much about the fitting, the weights and the model. Suffice to say that the training algorithm has converged and therefore the model is ready to be used.

\hypertarget{predicting-medv-using-the-neural-network}{%
\section{Predicting medv using the neural network}\label{predicting-medv-using-the-neural-network}}

Now we can try to predict the values for the test set and calculate the MSE. Remember that the net will output a normalized prediction, so we need to scale it back in order to make a meaningful comparison (or just a simple prediction).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pr.nn <-}\StringTok{ }\KeywordTok{compute}\NormalTok{(nn,test_[,}\DecValTok{1}\OperatorTok{:}\DecValTok{13}\NormalTok{])}

\NormalTok{pr.nn_ <-}\StringTok{ }\NormalTok{pr.nn}\OperatorTok{$}\NormalTok{net.result}\OperatorTok{*}\NormalTok{(}\KeywordTok{max}\NormalTok{(data}\OperatorTok{$}\NormalTok{medv)}\OperatorTok{-}\KeywordTok{min}\NormalTok{(data}\OperatorTok{$}\NormalTok{medv))}\OperatorTok{+}\KeywordTok{min}\NormalTok{(data}\OperatorTok{$}\NormalTok{medv)}
\NormalTok{test.r <-}\StringTok{ }\NormalTok{(test_}\OperatorTok{$}\NormalTok{medv)}\OperatorTok{*}\NormalTok{(}\KeywordTok{max}\NormalTok{(data}\OperatorTok{$}\NormalTok{medv)}\OperatorTok{-}\KeywordTok{min}\NormalTok{(data}\OperatorTok{$}\NormalTok{medv))}\OperatorTok{+}\KeywordTok{min}\NormalTok{(data}\OperatorTok{$}\NormalTok{medv)}

\NormalTok{MSE.nn <-}\StringTok{ }\KeywordTok{sum}\NormalTok{((test.r }\OperatorTok{-}\StringTok{ }\NormalTok{pr.nn_)}\OperatorTok{^}\DecValTok{2}\NormalTok{)}\OperatorTok{/}\KeywordTok{nrow}\NormalTok{(test_)}
\end{Highlighting}
\end{Shaded}

we then compare the two MSEs

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{print}\NormalTok{(}\KeywordTok{paste}\NormalTok{(MSE.lm,MSE.nn))}
\CommentTok{#> [1] "31.2630222372615 16.4595537665717"}
\end{Highlighting}
\end{Shaded}

Apparently, the net is doing a better work than the linear model at predicting medv. Once again, be careful because this result depends on the train-test split performed above. Below, after the visual plot, we are going to perform a fast cross validation in order to be more confident about the results.

A first visual approach to the performance of the network and the linear model on the test set is plotted below

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{))}

\KeywordTok{plot}\NormalTok{(test}\OperatorTok{$}\NormalTok{medv,pr.nn_,}\DataTypeTok{col=}\StringTok{'red'}\NormalTok{,}\DataTypeTok{main=}\StringTok{'Real vs predicted NN'}\NormalTok{,}\DataTypeTok{pch=}\DecValTok{18}\NormalTok{,}\DataTypeTok{cex=}\FloatTok{0.7}\NormalTok{)}
\KeywordTok{abline}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DataTypeTok{lwd=}\DecValTok{2}\NormalTok{)}
\KeywordTok{legend}\NormalTok{(}\StringTok{'bottomright'}\NormalTok{,}\DataTypeTok{legend=}\StringTok{'NN'}\NormalTok{,}\DataTypeTok{pch=}\DecValTok{18}\NormalTok{,}\DataTypeTok{col=}\StringTok{'red'}\NormalTok{, }\DataTypeTok{bty=}\StringTok{'n'}\NormalTok{)}

\KeywordTok{plot}\NormalTok{(test}\OperatorTok{$}\NormalTok{medv,pr.lm,}\DataTypeTok{col=}\StringTok{'blue'}\NormalTok{,}\DataTypeTok{main=}\StringTok{'Real vs predicted lm'}\NormalTok{,}\DataTypeTok{pch=}\DecValTok{18}\NormalTok{, }\DataTypeTok{cex=}\FloatTok{0.7}\NormalTok{)}
\KeywordTok{abline}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DataTypeTok{lwd=}\DecValTok{2}\NormalTok{)}
\KeywordTok{legend}\NormalTok{(}\StringTok{'bottomright'}\NormalTok{,}\DataTypeTok{legend=}\StringTok{'LM'}\NormalTok{,}\DataTypeTok{pch=}\DecValTok{18}\NormalTok{,}\DataTypeTok{col=}\StringTok{'blue'}\NormalTok{, }\DataTypeTok{bty=}\StringTok{'n'}\NormalTok{, }\DataTypeTok{cex=}\NormalTok{.}\DecValTok{95}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{regression_902-fitting_neural_network_files/figure-latex/unnamed-chunk-11-1} \end{center}

By visually inspecting the plot we can see that the predictions made by the neural network are (in general) more concetrated around the line (a perfect alignment with the line would indicate a MSE of 0 and thus an ideal perfect prediction) than those made by the linear model.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(test}\OperatorTok{$}\NormalTok{medv,pr.nn_,}\DataTypeTok{col=}\StringTok{'red'}\NormalTok{,}\DataTypeTok{main=}\StringTok{'Real vs predicted NN'}\NormalTok{,}\DataTypeTok{pch=}\DecValTok{18}\NormalTok{,}\DataTypeTok{cex=}\FloatTok{0.7}\NormalTok{)}
\KeywordTok{points}\NormalTok{(test}\OperatorTok{$}\NormalTok{medv,pr.lm,}\DataTypeTok{col=}\StringTok{'blue'}\NormalTok{,}\DataTypeTok{pch=}\DecValTok{18}\NormalTok{,}\DataTypeTok{cex=}\FloatTok{0.7}\NormalTok{)}
\KeywordTok{abline}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DataTypeTok{lwd=}\DecValTok{2}\NormalTok{)}
\KeywordTok{legend}\NormalTok{(}\StringTok{'bottomright'}\NormalTok{,}\DataTypeTok{legend=}\KeywordTok{c}\NormalTok{(}\StringTok{'NN'}\NormalTok{,}\StringTok{'LM'}\NormalTok{),}\DataTypeTok{pch=}\DecValTok{18}\NormalTok{,}\DataTypeTok{col=}\KeywordTok{c}\NormalTok{(}\StringTok{'red'}\NormalTok{,}\StringTok{'blue'}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{regression_902-fitting_neural_network_files/figure-latex/unnamed-chunk-12-1} \end{center}

\hypertarget{a-fast-cross-validation}{%
\section{A (fast) cross validation}\label{a-fast-cross-validation}}

Cross validation is another very important step of building predictive models. While there are different kind of cross validation methods, the basic idea is repeating the following process a number of time:

\textbf{train-test split}

\begin{itemize}
\tightlist
\item
  Do the train-test split
\item
  Fit the model to the train set
\item
  Test the model on the test set
\item
  Calculate the prediction error
\item
  Repeat the process K times
\end{itemize}

Then by calculating the average error we can get a grasp of how the model is doing.

We are going to implement a fast cross validation using a for loop for the neural network and the cv.glm() function in the boot package for the linear model.
As far as I know, there is no built-in function in R to perform cross-validation on this kind of neural network, if you do know such a function, please let me know in the comments. Here is the 10 fold cross-validated MSE for the linear model:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(boot)}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{200}\NormalTok{)}
\NormalTok{lm.fit <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(medv}\OperatorTok{~}\NormalTok{.,}\DataTypeTok{data=}\NormalTok{data)}
\KeywordTok{cv.glm}\NormalTok{(data,lm.fit,}\DataTypeTok{K=}\DecValTok{10}\NormalTok{)}\OperatorTok{$}\NormalTok{delta[}\DecValTok{1}\NormalTok{]}
\CommentTok{#> [1] 23.2}
\end{Highlighting}
\end{Shaded}

Now the net. Note that I am splitting the data in this way: 90\% train set and 10\% test set in a random way for 10 times. I am also initializing a progress bar using the plyr library because I want to keep an eye on the status of the process since the fitting of the neural network may take a while.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{450}\NormalTok{)}
\NormalTok{cv.error <-}\StringTok{ }\OtherTok{NULL}
\NormalTok{k <-}\StringTok{ }\DecValTok{10}

\KeywordTok{library}\NormalTok{(plyr) }
\NormalTok{pbar <-}\StringTok{ }\KeywordTok{create_progress_bar}\NormalTok{(}\StringTok{'text'}\NormalTok{)}
\NormalTok{pbar}\OperatorTok{$}\KeywordTok{init}\NormalTok{(k)}
\CommentTok{#> }
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|}\StringTok{                                                                 }\ErrorTok{|}\StringTok{   }\DecValTok{0}\NormalTok{%}

\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{k)\{}
\NormalTok{    index <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\KeywordTok{nrow}\NormalTok{(data),}\KeywordTok{round}\NormalTok{(}\FloatTok{0.9}\OperatorTok{*}\KeywordTok{nrow}\NormalTok{(data)))}
\NormalTok{    train.cv <-}\StringTok{ }\NormalTok{scaled[index,]}
\NormalTok{    test.cv <-}\StringTok{ }\NormalTok{scaled[}\OperatorTok{-}\NormalTok{index,]}
    
\NormalTok{    nn <-}\StringTok{ }\KeywordTok{neuralnet}\NormalTok{(f,}\DataTypeTok{data=}\NormalTok{train.cv,}\DataTypeTok{hidden=}\KeywordTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{,}\DecValTok{2}\NormalTok{),}\DataTypeTok{linear.output=}\NormalTok{T)}
    
\NormalTok{    pr.nn <-}\StringTok{ }\KeywordTok{compute}\NormalTok{(nn,test.cv[,}\DecValTok{1}\OperatorTok{:}\DecValTok{13}\NormalTok{])}
\NormalTok{    pr.nn <-}\StringTok{ }\NormalTok{pr.nn}\OperatorTok{$}\NormalTok{net.result}\OperatorTok{*}\NormalTok{(}\KeywordTok{max}\NormalTok{(data}\OperatorTok{$}\NormalTok{medv)}\OperatorTok{-}\KeywordTok{min}\NormalTok{(data}\OperatorTok{$}\NormalTok{medv))}\OperatorTok{+}\KeywordTok{min}\NormalTok{(data}\OperatorTok{$}\NormalTok{medv)}
    
\NormalTok{    test.cv.r <-}\StringTok{ }\NormalTok{(test.cv}\OperatorTok{$}\NormalTok{medv)}\OperatorTok{*}\NormalTok{(}\KeywordTok{max}\NormalTok{(data}\OperatorTok{$}\NormalTok{medv)}\OperatorTok{-}\KeywordTok{min}\NormalTok{(data}\OperatorTok{$}\NormalTok{medv))}\OperatorTok{+}\KeywordTok{min}\NormalTok{(data}\OperatorTok{$}\NormalTok{medv)}
    
\NormalTok{    cv.error[i] <-}\StringTok{ }\KeywordTok{sum}\NormalTok{((test.cv.r }\OperatorTok{-}\StringTok{ }\NormalTok{pr.nn)}\OperatorTok{^}\DecValTok{2}\NormalTok{)}\OperatorTok{/}\KeywordTok{nrow}\NormalTok{(test.cv)}
    
\NormalTok{    pbar}\OperatorTok{$}\KeywordTok{step}\NormalTok{()}
\NormalTok{\}}
\CommentTok{#> }
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|======}\StringTok{                                                           }\ErrorTok{|}\StringTok{  }\DecValTok{10}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=============}\StringTok{                                                    }\ErrorTok{|}\StringTok{  }\DecValTok{20}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|====================}\StringTok{                                             }\ErrorTok{|}\StringTok{  }\DecValTok{30}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==========================}\StringTok{                                       }\ErrorTok{|}\StringTok{  }\DecValTok{40}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|================================}\StringTok{                                 }\ErrorTok{|}\StringTok{  }\DecValTok{50}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=======================================}\StringTok{                          }\ErrorTok{|}\StringTok{  }\DecValTok{60}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==============================================}\StringTok{                   }\ErrorTok{|}\StringTok{  }\DecValTok{70}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|====================================================}\StringTok{             }\ErrorTok{|}\StringTok{  }\DecValTok{80}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==========================================================}\StringTok{       }\ErrorTok{|}\StringTok{  }\DecValTok{90}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=================================================================|}\StringTok{ }\DecValTok{100}\NormalTok{%}
\end{Highlighting}
\end{Shaded}

After a while, the process is done, we calculate the average MSE and plot the results as a boxplot

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(cv.error)}
\CommentTok{#> [1] 7.64}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cv.error}
\CommentTok{#>  [1] 13.33  7.10  6.58  5.70  6.84  5.77 10.75  5.38  9.45  5.50}
\end{Highlighting}
\end{Shaded}

The code for the box plot:
The code above outputs the following boxplot:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{boxplot}\NormalTok{(cv.error,}\DataTypeTok{xlab=}\StringTok{'MSE CV'}\NormalTok{,}\DataTypeTok{col=}\StringTok{'cyan'}\NormalTok{,}
        \DataTypeTok{border=}\StringTok{'blue'}\NormalTok{,}\DataTypeTok{names=}\StringTok{'CV error (MSE)'}\NormalTok{,}
        \DataTypeTok{main=}\StringTok{'CV error (MSE) for NN'}\NormalTok{,}\DataTypeTok{horizontal=}\OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{regression_902-fitting_neural_network_files/figure-latex/unnamed-chunk-17-1} \end{center}

As you can see, the average MSE for the neural network (10.33) is lower than the one of the linear model although there seems to be a certain degree of variation in the MSEs of the cross validation. This may depend on the splitting of the data or the random initialization of the weights in the net. By running the simulation different times with different seeds you can get a more precise point estimate for the average MSE.

\hypertarget{a-final-note-on-model-interpretability}{%
\section{A final note on model interpretability}\label{a-final-note-on-model-interpretability}}

Neural networks resemble black boxes a lot: explaining their outcome is much more difficult than explaining the outcome of simpler model such as a linear model. Therefore, depending on the kind of application you need, you might want to take into account this factor too. Furthermore, as you have seen above, extra care is needed to fit a neural network and small changes can lead to different results.

A gist with the full code for this post can be found here.

Thank you for reading this post, leave a comment below if you have any question.

\hypertarget{visualization-of-neural-networks}{%
\chapter{Visualization of neural networks}\label{visualization-of-neural-networks}}

\url{https://beckmw.wordpress.com/tag/neuralnet/}

In my last post I said I wasn't going to write anymore about neural networks (i.e., multilayer feedforward perceptron, supervised ANN, etc.). That was a lie. I've received several requests to update the neural network plotting function described in the original post. As previously explained, R does not provide a lot of options for visualizing neural networks. The only option I know of is a plotting method for objects from the neuralnet package. This may be my opinion, but I think this plot leaves much to be desired (see below). Also, no plotting methods exist for neural networks created in other packages, i.e., nnet and RSNNS. These packages are the only ones listed on the CRAN task view, so I've updated my original plotting function to work with all three. Additionally, I've added a new option for plotting a raw weight vector to allow use with neural networks created elsewhere. This blog describes these changes, as well as some new arguments added to the original function.

As usual, I'll simulate some data to use for creating the neural networks. The dataset contains eight input variables and two output variables. The final dataset is a data frame with all variables, as well as separate data frames for the input and output variables. I've retained separate datasets based on the syntax for each package.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(clusterGeneration)}
\CommentTok{#> Loading required package: MASS}
\KeywordTok{library}\NormalTok{(tictoc)}
 
\NormalTok{seed.val<-}\StringTok{ }\DecValTok{12345}
\KeywordTok{set.seed}\NormalTok{(seed.val)}
 
\NormalTok{num.vars<-}\DecValTok{8}
\NormalTok{num.obs<-}\DecValTok{1000}
 
\CommentTok{# input variables}
\NormalTok{cov.mat <-}\KeywordTok{genPositiveDefMat}\NormalTok{(num.vars,}\DataTypeTok{covMethod=}\KeywordTok{c}\NormalTok{(}\StringTok{"unifcorrmat"}\NormalTok{))}\OperatorTok{$}\NormalTok{Sigma}
\NormalTok{rand.vars <-}\KeywordTok{mvrnorm}\NormalTok{(num.obs,}\KeywordTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{,num.vars),}\DataTypeTok{Sigma=}\NormalTok{cov.mat)}
 
\CommentTok{# output variables}
\NormalTok{parms <-}\KeywordTok{runif}\NormalTok{(num.vars,}\OperatorTok{-}\DecValTok{10}\NormalTok{,}\DecValTok{10}\NormalTok{)}
\NormalTok{y1 <-}\StringTok{ }\NormalTok{rand.vars }\OperatorTok{%*%}\StringTok{ }\KeywordTok{matrix}\NormalTok{(parms) }\OperatorTok{+}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(num.obs,}\DataTypeTok{sd=}\DecValTok{20}\NormalTok{)}
\NormalTok{parms2 <-}\StringTok{ }\KeywordTok{runif}\NormalTok{(num.vars,}\OperatorTok{-}\DecValTok{10}\NormalTok{,}\DecValTok{10}\NormalTok{)}
\NormalTok{y2 <-}\StringTok{ }\NormalTok{rand.vars }\OperatorTok{%*%}\StringTok{ }\KeywordTok{matrix}\NormalTok{(parms2) }\OperatorTok{+}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(num.obs,}\DataTypeTok{sd=}\DecValTok{20}\NormalTok{)}
 
\CommentTok{# final datasets}
\NormalTok{rand.vars <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(rand.vars)}
\NormalTok{resp <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(y1,y2)}
\KeywordTok{names}\NormalTok{(resp) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{'Y1'}\NormalTok{,}\StringTok{'Y2'}\NormalTok{)}
\NormalTok{dat.in <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(resp, rand.vars)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{glimpse}\NormalTok{(dat.in)}
\CommentTok{#> Observations: 1,000}
\CommentTok{#> Variables: 10}
\CommentTok{#> $ Y1 <dbl> 25.442, -14.578, -36.214, 15.216, -6.393, -20.849, -28.665,...}
\CommentTok{#> $ Y2 <dbl> 16.9, 38.8, 31.2, -31.2, 93.3, 11.7, 59.7, -103.5, -49.8, 5...}
\CommentTok{#> $ X1 <dbl> 3.138, -0.705, -4.373, 0.837, 0.787, 1.923, -1.419, 1.121, ...}
\CommentTok{#> $ X2 <dbl> 0.195, -0.302, 0.773, 1.311, 3.506, 1.245, 3.800, -0.165, 0...}
\CommentTok{#> $ X3 <dbl> -1.795, -2.596, 2.308, 4.081, -3.921, 1.473, -0.926, 7.101,...}
\CommentTok{#> $ X4 <dbl> -2.7216, 3.0589, 1.2455, 3.4607, 2.3775, -2.9833, 2.6669, -...}
\CommentTok{#> $ X5 <dbl> 0.0407, 0.7602, -3.0217, -4.2799, 2.0859, 1.4765, 0.0561, 2...}
\CommentTok{#> $ X6 <dbl> -1.4820, -0.5014, 0.0603, -1.8551, 2.2817, 1.7386, 1.7450, ...}
\CommentTok{#> $ X7 <dbl> -0.7169, -0.3618, -1.5283, 4.2026, -6.1548, -0.3545, -6.028...}
\CommentTok{#> $ X8 <dbl> 1.152, 1.810, -1.357, 0.598, -1.425, -1.210, -1.004, 2.494,...}
\end{Highlighting}
\end{Shaded}

The various neural network packages are used to create separate models for plotting.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# first model with nnet}
\CommentTok{#nnet function from nnet package}
\KeywordTok{library}\NormalTok{(nnet)}
\KeywordTok{set.seed}\NormalTok{(seed.val)}
\KeywordTok{tic}\NormalTok{()}
\NormalTok{mod1 <-}\StringTok{ }\KeywordTok{nnet}\NormalTok{(rand.vars, resp, }\DataTypeTok{data =}\NormalTok{ dat.in, }\DataTypeTok{size =} \DecValTok{10}\NormalTok{, }\DataTypeTok{linout =}\NormalTok{ T)}
\CommentTok{#> # weights:  112}
\CommentTok{#> initial  value 4784162.893260 }
\CommentTok{#> iter  10 value 1794537.980652}
\CommentTok{#> iter  20 value 1577753.498759}
\CommentTok{#> iter  30 value 1485254.945755}
\CommentTok{#> iter  40 value 1449238.248788}
\CommentTok{#> iter  50 value 1427720.291804}
\CommentTok{#> iter  60 value 1416977.236373}
\CommentTok{#> iter  70 value 1405167.753521}
\CommentTok{#> iter  80 value 1395046.792257}
\CommentTok{#> iter  90 value 1370522.267277}
\CommentTok{#> iter 100 value 1363709.540981}
\CommentTok{#> final  value 1363709.540981 }
\CommentTok{#> stopped after 100 iterations}
\KeywordTok{toc}\NormalTok{()}
\CommentTok{#> 0.201 sec elapsed}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# nn <- neuralnet(form.in,}
\CommentTok{#                 data = dat.sc,}
\CommentTok{#                 # hidden = c(13, 10, 3),}
\CommentTok{#                 hidden = c(5),}
\CommentTok{#                 act.fct = "tanh",}
\CommentTok{#                 linear.output = FALSE,}
\CommentTok{#                 lifesign = "minimal")}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# 2nd model with neuralnet}
\CommentTok{# neuralnet function from neuralnet package, notice use of only one response}
\KeywordTok{library}\NormalTok{(neuralnet)}

\NormalTok{softplus <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{log}\NormalTok{(}\DecValTok{1} \OperatorTok{+}\StringTok{ }\KeywordTok{exp}\NormalTok{(x))}
\NormalTok{sigmoid  <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{log}\NormalTok{(}\DecValTok{1} \OperatorTok{+}\StringTok{ }\KeywordTok{exp}\NormalTok{(}\OperatorTok{-}\NormalTok{x))}

\NormalTok{dat.sc <-}\StringTok{ }\KeywordTok{scale}\NormalTok{(dat.in)}
\NormalTok{form.in <-}\StringTok{ }\KeywordTok{as.formula}\NormalTok{(}\StringTok{'Y1 ~ X1+X2+X3+X4+X5+X6+X7+X8'}\NormalTok{)}
\KeywordTok{set.seed}\NormalTok{(seed.val)}
\KeywordTok{tic}\NormalTok{()}
\NormalTok{mod2 <-}\StringTok{ }\KeywordTok{neuralnet}\NormalTok{(form.in, }\DataTypeTok{data =}\NormalTok{ dat.sc, }\DataTypeTok{hidden =} \DecValTok{10}\NormalTok{, }\DataTypeTok{lifesign =} \StringTok{"minimal"}\NormalTok{,}
                  \DataTypeTok{linear.output =} \OtherTok{FALSE}\NormalTok{,}
                  \DataTypeTok{act.fct =} \StringTok{"tanh"}\NormalTok{)}
\CommentTok{#> hidden: 10    thresh: 0.01    rep: 1/1    steps:   26361 error: 160.06372    time: 59.35 secs}
\KeywordTok{toc}\NormalTok{()}
\CommentTok{#> 59.356 sec elapsed}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# third model with RSNNS}
\CommentTok{# mlp function from RSNNS package}
\KeywordTok{library}\NormalTok{(RSNNS)}
\CommentTok{#> Loading required package: Rcpp}
\KeywordTok{set.seed}\NormalTok{(seed.val)}
\KeywordTok{tic}\NormalTok{()}
\NormalTok{mod3 <-}\StringTok{ }\KeywordTok{mlp}\NormalTok{(rand.vars, resp, }\DataTypeTok{size =} \DecValTok{10}\NormalTok{, }\DataTypeTok{linOut =}\NormalTok{ T)}
\KeywordTok{toc}\NormalTok{()}
\CommentTok{#> 0.339 sec elapsed}
\end{Highlighting}
\end{Shaded}

I've noticed some differences between the functions that could lead to some confusion. For simplicity, the above code represents my interpretation of the most direct way to create a neural network in each package. Be very aware that direct comparison of results is not advised given that the default arguments differ between the packages. A few key differences are as follows, although many others should be noted. First, the functions differ in the methods for passing the primary input variables.

The \texttt{nnet} function can take separate (or combined) x and y inputs as data frames or as a formula, the \texttt{neuralnet} function can only use a formula as input, and the mlp function can only take a data frame as combined or separate variables as input. As far as I know, the neuralnet function is not capable of modelling multiple response variables, unless the response is a categorical variable that uses one node for each outcome. Additionally, the default output for the neuralnet function is linear, whereas the opposite is true for the other two functions.

Specifics aside, here's how to use the updated plot function. Note that the same syntax is used to plot each model

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# import the function from Github}
\KeywordTok{library}\NormalTok{(devtools)}
\KeywordTok{source_url}\NormalTok{(}\StringTok{'https://gist.githubusercontent.com/fawda123/7471137/raw/466c1474d0a505ff044412703516c34f1a4684a5/nnet_plot_update.r'}\NormalTok{)}
\CommentTok{#> SHA-1 hash of file is 74c80bd5ddbc17ab3ae5ece9c0ed9beb612e87ef}
 
\CommentTok{# plot each model}
\KeywordTok{plot.nnet}\NormalTok{(mod1)}
\CommentTok{#> Loading required package: scales}
\CommentTok{#> Loading required package: reshape}
\KeywordTok{plot.nnet}\NormalTok{(mod2) }
\KeywordTok{plot.nnet}\NormalTok{(mod3)}
\CommentTok{#> Warning in plot.nnet(mod3): Bias layer not applicable for rsnns object}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{regression_903-visualization_nn_files/figure-latex/unnamed-chunk-5-1} \includegraphics[width=0.7\linewidth]{regression_903-visualization_nn_files/figure-latex/unnamed-chunk-5-2} \includegraphics[width=0.7\linewidth]{regression_903-visualization_nn_files/figure-latex/unnamed-chunk-5-3} \end{center}

The plotting function can also now be used with an arbitrary weight vector, rather than a specific model object. The struct argument must also be included if this option is used. I thought the easiest way to use the plotting function with your own weights was to have the input weights as a numeric vector, including bias layers. I've shown how this can be done using the weights directly from mod1 for simplicity.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{wts.in <-}\StringTok{ }\NormalTok{mod1}\OperatorTok{$}\NormalTok{wts}
\NormalTok{struct <-}\StringTok{ }\NormalTok{mod1}\OperatorTok{$}\NormalTok{n}
\KeywordTok{plot.nnet}\NormalTok{(wts.in,}\DataTypeTok{struct=}\NormalTok{struct)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{regression_903-visualization_nn_files/figure-latex/unnamed-chunk-6-1} \end{center}

Note that \texttt{wts.in} is a numeric vector with length equal to the expected given the architecture (i.e., for 8 10 2 network, 100 connection weights plus 12 bias weights). The plot should look the same as the plot for the neural network from nnet.

The weights in the input vector need to be in a specific order for correct plotting. I realize this is not clear by looking directly at wt.in but this was the simplest approach I could think of. The weight vector shows the weights for each hidden node in sequence, starting with the bias input for each node, then the weights for each output node in sequence, starting with the bias input for each output node. Note that the bias layer has to be included even if the network was not created with biases. If this is the case, simply input a random number where the bias values should go and use the argument bias=F. I'll show the correct order of the weights using an example with plot.nn from the neuralnet package since the weights are included directly on the plot.

If we pretend that the above figure wasn't created in R, we would input the mod.in argument for the updated plotting function as follows. Also note that struct must be included if using this approach.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mod.in<-}\KeywordTok{c}\NormalTok{(}\FloatTok{13.12}\NormalTok{,}\FloatTok{1.49}\NormalTok{,}\FloatTok{0.16}\NormalTok{,}\OperatorTok{-}\FloatTok{0.11}\NormalTok{,}\OperatorTok{-}\FloatTok{0.19}\NormalTok{,}\OperatorTok{-}\FloatTok{0.16}\NormalTok{,}\FloatTok{0.56}\NormalTok{,}\OperatorTok{-}\FloatTok{0.52}\NormalTok{,}\FloatTok{0.81}\NormalTok{)}
\NormalTok{struct<-}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{) }\CommentTok{#two inputs, two hidden, one output }
\KeywordTok{plot.nnet}\NormalTok{(mod.in, }\DataTypeTok{struct=}\NormalTok{struct)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{regression_903-visualization_nn_files/figure-latex/unnamed-chunk-7-1} \end{center}

Note the comparability with the figure created using the neuralnet package. That is, larger weights have thicker lines and color indicates sign (+ black, -- grey).

One of these days I'll actually put these functions in a package. In the meantime, please let me know if any bugs are encountered.

\hypertarget{caret-and-plot-nn}{%
\section{caret and plot NN}\label{caret-and-plot-nn}}

I've changed the function to work with neural networks created using the train function from the caret package. The link above is updated but you can also grab it here.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(caret)}
\CommentTok{#> Loading required package: lattice}
\CommentTok{#> Loading required package: ggplot2}
\CommentTok{#> Registered S3 methods overwritten by 'ggplot2':}
\CommentTok{#>   method         from }
\CommentTok{#>   [.quosures     rlang}
\CommentTok{#>   c.quosures     rlang}
\CommentTok{#>   print.quosures rlang}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'caret'}
\CommentTok{#> The following objects are masked from 'package:RSNNS':}
\CommentTok{#> }
\CommentTok{#>     confusionMatrix, train}
\NormalTok{mod4 <-}\StringTok{ }\KeywordTok{train}\NormalTok{(Y1 }\OperatorTok{~}\NormalTok{., }\DataTypeTok{method=}\StringTok{'nnet'}\NormalTok{, }\DataTypeTok{data=}\NormalTok{dat.in, }\DataTypeTok{linout=}\NormalTok{T)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot.nnet}\NormalTok{(mod4,}\DataTypeTok{nid=}\NormalTok{T)}
\CommentTok{#> Warning in plot.nnet(mod4, nid = T): Using best nnet model from train}
\CommentTok{#> output}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{regression_903-visualization_nn_files/figure-latex/unnamed-chunk-8-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fact<-}\KeywordTok{factor}\NormalTok{(}\KeywordTok{sample}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{'a'}\NormalTok{,}\StringTok{'b'}\NormalTok{,}\StringTok{'c'}\NormalTok{),}\DataTypeTok{size=}\NormalTok{num.obs,}\DataTypeTok{replace=}\NormalTok{T))}
\NormalTok{form.in<-}\KeywordTok{formula}\NormalTok{(}\StringTok{'cbind(Y2,Y1)~X1+X2+X3+fact'}\NormalTok{)}
\NormalTok{mod5<-}\KeywordTok{nnet}\NormalTok{(form.in,}\DataTypeTok{data=}\KeywordTok{cbind}\NormalTok{(dat.in,fact),}\DataTypeTok{size=}\DecValTok{10}\NormalTok{,}\DataTypeTok{linout=}\NormalTok{T)}
\CommentTok{#> # weights:  82}
\CommentTok{#> initial  value 4799569.423556 }
\CommentTok{#> iter  10 value 2864553.218126}
\CommentTok{#> iter  20 value 2595828.194160}
\CommentTok{#> iter  30 value 2517965.483941}
\CommentTok{#> iter  40 value 2464882.178217}
\CommentTok{#> iter  50 value 2444238.700834}
\CommentTok{#> iter  60 value 2424302.290643}
\CommentTok{#> iter  70 value 2395226.949866}
\CommentTok{#> iter  80 value 2375558.751266}
\CommentTok{#> iter  90 value 2343011.050867}
\CommentTok{#> iter 100 value 2298860.593948}
\CommentTok{#> final  value 2298860.593948 }
\CommentTok{#> stopped after 100 iterations}
\KeywordTok{plot.nnet}\NormalTok{(mod5,}\DataTypeTok{nid=}\NormalTok{T)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{regression_903-visualization_nn_files/figure-latex/unnamed-chunk-9-1} \end{center}

\hypertarget{multiple-hidden-layers}{%
\section{Multiple hidden layers}\label{multiple-hidden-layers}}

More updates\ldots{} I've now modified the function to plot multiple hidden layers for networks created using the mlp function in the \texttt{RSNNS} package and neuralnet in the neuralnet package. As far as I know, these are the only neural network functions in R that can create multiple hidden layers. All others use a single hidden layer. I have not tested the plotting function using manual input for the weight vectors with multiple hidden layers. My guess is it won't work but I can't be bothered to change the function unless it's specifically requested. The updated function can be grabbed here (all above links to the function have also been changed).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(RSNNS)}
 
\CommentTok{# neural net with three hidden layers, 9, 11, and 8 nodes in each}
\KeywordTok{tic}\NormalTok{()}
\NormalTok{mod <-}\KeywordTok{mlp}\NormalTok{(rand.vars, resp, }
          \DataTypeTok{size =} \KeywordTok{c}\NormalTok{(}\DecValTok{9}\NormalTok{,}\DecValTok{11}\NormalTok{,}\DecValTok{8}\NormalTok{), }
          \DataTypeTok{linOut =}\NormalTok{ T)}
\KeywordTok{toc}\NormalTok{()}
\CommentTok{#> 0.381 sec elapsed}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mar=}\KeywordTok{numeric}\NormalTok{(}\DecValTok{4}\NormalTok{),}\DataTypeTok{family=}\StringTok{'serif'}\NormalTok{)}
\KeywordTok{plot.nnet}\NormalTok{(mod)}
\CommentTok{#> Warning in plot.nnet(mod): Bias layer not applicable for rsnns object}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{regression_903-visualization_nn_files/figure-latex/run-rsnns-hidden_layers-1} \end{center}

\hypertarget{binary-predictors}{%
\section{Binary predictors}\label{binary-predictors}}

Here's an example using the \texttt{neuralnet} function with binary predictors and categorical outputs (credit to Tao Ma for the model code).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(neuralnet)}
 
\CommentTok{#response}
\NormalTok{AND<-}\KeywordTok{c}\NormalTok{(}\KeywordTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{7}\NormalTok{),}\DecValTok{1}\NormalTok{)}
\NormalTok{OR<-}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\KeywordTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{7}\NormalTok{))}
 
\CommentTok{# response with predictors}
\NormalTok{binary.data <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\KeywordTok{expand.grid}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{), }\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{), }\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{)), AND, OR)}
 
\CommentTok{#model}
\KeywordTok{tic}\NormalTok{()}
\NormalTok{net <-}\StringTok{ }\KeywordTok{neuralnet}\NormalTok{(AND}\OperatorTok{+}\NormalTok{OR }\OperatorTok{~}\StringTok{ }\NormalTok{Var1}\OperatorTok{+}\NormalTok{Var2}\OperatorTok{+}\NormalTok{Var3,}
\NormalTok{                 binary.data, }\DataTypeTok{hidden =}\KeywordTok{c}\NormalTok{(}\DecValTok{6}\NormalTok{,}\DecValTok{12}\NormalTok{,}\DecValTok{8}\NormalTok{), }
                 \DataTypeTok{rep =} \DecValTok{10}\NormalTok{, }
                 \DataTypeTok{err.fct=}\StringTok{"ce"}\NormalTok{, }
                 \DataTypeTok{linear.output=}\OtherTok{FALSE}\NormalTok{)}
\KeywordTok{toc}\NormalTok{()}
\CommentTok{#> 0.107 sec elapsed}
\CommentTok{#plot ouput}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mar=}\KeywordTok{numeric}\NormalTok{(}\DecValTok{4}\NormalTok{),}\DataTypeTok{family=}\StringTok{'serif'}\NormalTok{)}
\KeywordTok{plot.nnet}\NormalTok{(net)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{regression_903-visualization_nn_files/figure-latex/unnamed-chunk-10-1} \end{center}

\hypertarget{color-coding-the-input-layer}{%
\section{color coding the input layer}\label{color-coding-the-input-layer}}

The color vector argument (circle.col) for the nodes was changed to allow a separate color vector for the input layer.

The following example shows how this can be done using relative importance of the input variables to color-code the first layer.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# example showing use of separate colors for input layer}
\CommentTok{# color based on relative importance using 'gar.fun'}
 
\CommentTok{##}
\CommentTok{#create input data}
\NormalTok{seed.val<-}\DecValTok{3}
\KeywordTok{set.seed}\NormalTok{(seed.val)}
  
\NormalTok{num.vars<-}\DecValTok{8}
\NormalTok{num.obs<-}\DecValTok{1000}
  
\CommentTok{#input variables}
\KeywordTok{library}\NormalTok{(clusterGeneration)}
\NormalTok{cov.mat<-}\KeywordTok{genPositiveDefMat}\NormalTok{(num.vars,}\DataTypeTok{covMethod=}\KeywordTok{c}\NormalTok{(}\StringTok{"unifcorrmat"}\NormalTok{))}\OperatorTok{$}\NormalTok{Sigma}
\NormalTok{rand.vars<-}\KeywordTok{mvrnorm}\NormalTok{(num.obs,}\KeywordTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{,num.vars),}\DataTypeTok{Sigma=}\NormalTok{cov.mat)}
  
\CommentTok{# output variables}
\NormalTok{parms<-}\KeywordTok{runif}\NormalTok{(num.vars,}\OperatorTok{-}\DecValTok{10}\NormalTok{,}\DecValTok{10}\NormalTok{)}
\NormalTok{y1<-rand.vars }\OperatorTok{%*%}\StringTok{ }\KeywordTok{matrix}\NormalTok{(parms) }\OperatorTok{+}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(num.obs,}\DataTypeTok{sd=}\DecValTok{20}\NormalTok{)}
 
\CommentTok{# final datasets}
\NormalTok{rand.vars<-}\KeywordTok{data.frame}\NormalTok{(rand.vars)}
\NormalTok{resp<-}\KeywordTok{data.frame}\NormalTok{(y1)}
\KeywordTok{names}\NormalTok{(resp)<-}\StringTok{'Y1'}
\NormalTok{dat.in <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(resp,rand.vars)}
 
\CommentTok{##}
\CommentTok{# create model}
\KeywordTok{library}\NormalTok{(nnet)}
\NormalTok{mod1 <-}\StringTok{ }\KeywordTok{nnet}\NormalTok{(rand.vars,resp,}\DataTypeTok{data=}\NormalTok{dat.in,}\DataTypeTok{size=}\DecValTok{10}\NormalTok{,}\DataTypeTok{linout=}\NormalTok{T)}
\CommentTok{#> # weights:  101}
\CommentTok{#> initial  value 844959.580478 }
\CommentTok{#> iter  10 value 543616.101824}
\CommentTok{#> iter  20 value 479986.887846}
\CommentTok{#> iter  30 value 465607.784054}
\CommentTok{#> iter  40 value 454237.073298}
\CommentTok{#> iter  50 value 445032.412421}
\CommentTok{#> iter  60 value 433191.158624}
\CommentTok{#> iter  70 value 426321.161292}
\CommentTok{#> iter  80 value 424900.966883}
\CommentTok{#> iter  90 value 423816.437605}
\CommentTok{#> iter 100 value 422064.114812}
\CommentTok{#> final  value 422064.114812 }
\CommentTok{#> stopped after 100 iterations}
 
\CommentTok{##}
\CommentTok{# relative importance function}
\KeywordTok{library}\NormalTok{(devtools)}
\KeywordTok{source_url}\NormalTok{(}\StringTok{'https://gist.github.com/fawda123/6206737/raw/2e1bc9cbc48d1a56d2a79dd1d33f414213f5f1b1/gar_fun.r'}\NormalTok{)}
\CommentTok{#> SHA-1 hash of file is 9faa58824c46956c3ff78081696290d9b32d845f}
 
\CommentTok{# relative importance of input variables for Y1}
\NormalTok{rel.imp <-}\StringTok{ }\KeywordTok{gar.fun}\NormalTok{(}\StringTok{'Y1'}\NormalTok{,mod1,}\DataTypeTok{bar.plot=}\NormalTok{F)}\OperatorTok{$}\NormalTok{rel.imp}
 
\CommentTok{#color vector based on relative importance of input values}
\NormalTok{cols<-}\KeywordTok{colorRampPalette}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{'green'}\NormalTok{,}\StringTok{'red'}\NormalTok{))(num.vars)[}\KeywordTok{rank}\NormalTok{(rel.imp)]}
 
\CommentTok{##}
\CommentTok{#plotting function}
\KeywordTok{source_url}\NormalTok{(}\StringTok{'https://gist.githubusercontent.com/fawda123/7471137/raw/466c1474d0a505ff044412703516c34f1a4684a5/nnet_plot_update.r'}\NormalTok{)}
\CommentTok{#> SHA-1 hash of file is 74c80bd5ddbc17ab3ae5ece9c0ed9beb612e87ef}
  
\CommentTok{#plot model with new color vector}
\CommentTok{#separate colors for input vectors using a list for 'circle.col'}
\KeywordTok{plot}\NormalTok{(mod1,}\DataTypeTok{circle.col=}\KeywordTok{list}\NormalTok{(cols,}\StringTok{'lightblue'}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{regression_903-visualization_nn_files/figure-latex/unnamed-chunk-11-1} \end{center}

\bibliography{book.bib,packages.bib}


\end{document}
