# Neural Network from scratch: Examples

## Iris

```{r}
head(iris)
```

```{r train_iris}
train_test_split <- train_test_from_df(df = iris, predict_col_index = 5, train_ratio = 0.7)
training_data <- train_test_split[[1]]
testing_data <- train_test_split[[2]]

in_n <- length(training_data[[1]][[1]])
out_n <- length(training_data[[1]][[-1]])

# [4, 40, 3] 
trained_net <- neuralnetwork(
    c(in_n, 40, out_n),
    training_data=training_data,
    epochs=30, 
    mini_batch_size=10,
    lr=0.5,
    C='ce',
    verbose=TRUE,
    validation_data=testing_data
)
```

```{r iris_evaluate}
# Trained matricies:
biases <- trained_net[[1]]
weights <- trained_net[[-1]]

# Accuracy (train)
evaluate(training_data, biases, weights)  #0.971
# Accuracy (test)
evaluate(testing_data, biases, weights)  #0.956
```

##  MNIST

```{r}
library(grid)
```

```{r echo=FALSE}
# mnist_training <- mnist
# save(mnist_training, file = file.path(data_raw_dir, "mnist_training.rda"))
# 
# mnist_testing <- mnist
# save(mnist_testing, file = file.path(data_raw_dir, "mnist_testing.rda"))
```


```{r load_mnist, cache=TRUE}
# Here we have splits for train-test already (may take a minute to download)
# Train
if (!file.exists(file.path(data_raw_dir, "mnist_training.rda"))) {
  cat("reading from web ...\n")
  mnist <- read.table('https://iliadl.blob.core.windows.net/nnet/mnist_train.csv', 
                    sep=",", header = FALSE) 
  } else {
    cat("reading from disk ...\n")
    load(file = file.path(data_raw_dir, "mnist_training.rda"))
}

mnist_training$V1 <- factor(mnist_training$V1)
training_data <- train_test_from_df(df = mnist_training, predict_col_index = 1, 
                                    train_ratio = 1)[[1]]

# Test
if (!file.exists(file.path(data_raw_dir, "mnist_testing.rda"))) {
  cat("reading from web ...\n")
  mnist <- read.table('https://iliadl.blob.core.windows.net/nnet/mnist_test.csv', 
                    sep=",", header = FALSE) 
  } else {
    cat("reading from disk ...\n")
    load(file = file.path(data_raw_dir, "mnist_testing.rda"))
}

mnist_testing$V1 <- factor(mnist_testing$V1)
testing_data <- train_test_from_df(df = mnist_testing, predict_col_index = 1, 
                                   train_ratio = 1)[[1]]
```

```{r}
if (interactive()) { 
  write.csv(mnist_train, file.path(data_raw_dir, "mnist_train.csv"))
  write.csv(mnist_test, file.path(data_raw_dir, "mnist_test.csv"))
}  
```


### What does the data exactly look like?

```{r how_data_looks}
example_entry <- training_data[[1]]
example_x <- example_entry[[1]]
example_y <- example_entry[[2]]

# Y-vector looks like this:
print(example_y)
# It corresponds to digit:
print(which.max(example_y)-1)

# X-vector has length
print(length(example_x))

# We can think of it as a 28x28 matrix where entries are a shade of gray
grid.raster(matrix(example_x, nrow=28, byrow=TRUE))
```

Let's train a neural net with one 100-neuron hidden-layer to predict (given 784 vector of gray intensity) the digit (from 0 to 9)

```{r nnet_100_neurons}
# Input and output neurons
in_n <- length(training_data[[1]][[1]])
out_n <- length(training_data[[1]][[-1]])

# MNIST: 784, 100, 10 (one hidden-layer)
print("THIS WILL TAKE 20-30 MINUTES...")
trained_net <- neuralnetwork(sizes = c(in_n, 100, out_n), 
                             training_data = training_data,
                             epochs = 3,           # 30
                             mini_batch_size = 2,  # 10
                             lr = 3,
                             C = 'ce',
                             verbose=  TRUE,
                             validation_data = testing_data)
```

```{r confusion_matrices}
# Trained matricies:
biases <- trained_net[[1]]
weights <- trained_net[[-1]]

# CONFUSION TRAIN MATRIX
evaluate(training_data, biases, weights)  #0.98
# CONFUSION TEST MATRIX
evaluate(testing_data, biases, weights)  #0.97
```

```{r test_one_example}
# Test this out with one example
# Do some machine-learning
test_entry <- testing_data[[2]]
test_x <- test_entry[[1]]
test_y <- test_entry[[2]]

# Input
grid.raster(matrix(test_x, nrow=28, byrow=TRUE))

# Output
which.max(feedforward(test_x, biases, weights))-1
# Truth
cat("Truth: ", which.max(test_y)-1)
```

### Example: Visualizations of boundary lines
```{r load_mlbench}
# Load some spiral data
library(mlbench)
data_df <- as.data.frame(mlbench.spirals(10000))

# Looks pretty cool
plot(x=data_df[,1], y=data_df[,2], cex = 1, col=data_df[,3],
     main = "Spirals", 
     xlab = "X1", ylab = "X2")
```

```{r spiral_train}
# Let's use all of this data for training
training_data <- train_test_from_df(df = data_df, predict_col_index = 3, 
                                    train_ratio = 1, scale_input = TRUE)[[1]]
in_n <- length(training_data[[1]][[1]])
out_n <- length(training_data[[1]][[-1]])
```

```{r function-plotBoundaryLinePerNeuron}
# Wrap a function that trains a network and plots the classification
plotBoundaryLinePerNeuron <- function(neurons, epochs)
{
  trained_net <- neuralnetwork(c(in_n, neurons, out_n), training_data, 
                               epochs, 500, 3, 'ce')
  b <- trained_net[[1]]
  w <- trained_net[[-1]]
  # Evaluate
  confusion <- evaluate(training_data, b, w)
  # Generate new-data
  nd <- as.data.frame(matrix(runif(100000, min=-1, max=1), ncol=2))
  # Get predictions
  nd$res <- as.factor(unlist(get_predictions(as.list(as.data.frame(t(nd))), b, w)))
  # Plot predictions
  plot(x=nd[,1], y=nd[,2], pch=19, col=nd[,3],
       main = paste0("Spiral Boundary Line - ", neurons, " neurons"), 
       xlab = "X1", ylab = "X2")
  # Original points
  points(x=data_df[,1], y=data_df[,2], pch=19, col=ifelse(data_df[,3]==2, 
                                                          "orange", "grey"))
}
```

```{r spiral_n50_e80}
 plotBoundaryLinePerNeuron(50, 80)
```

```{r spiral_n10_e80}
 plotBoundaryLinePerNeuron(10, 80)
```

```{r spiral_n5_e100}
 plotBoundaryLinePerNeuron(5, 100)
```

```{r spiral_n4_e100}
 plotBoundaryLinePerNeuron(4, 100)
```

It doesn't seem possible to draw fewer than 4 curves that split the classes, hence below 4 neurons we should start to see performance degrade - because the network doesn't enough params to fit the data. This can be a useful to exercise when trying to figure out how many neurons ones needs  over-fitting

```{r spiral_n3_e120}
 plotBoundaryLinePerNeuron(3, 120)
```

```{r spiral_n2_e200}
 plotBoundaryLinePerNeuron(2, 200)
```


```{r spiral_n1_e120}
 plotBoundaryLinePerNeuron(1, 120)
```

```{r spiral_n40_40_40_e200}
# plotBoundaryLinePerNeuron(c(40,40,40), 200)
```

## What is a neural network?
A neural network can be thought of as a series of logistic regressions stacked on top of each other. This means we could say that a logistic regression is a neural-network (with sigmoid activations) with no hidden-layer.

This hidden-layer lets a neural-network generate non-linearities and leads to the Universal approximation theorem, which states that a network with just one hidden layer can approximate any linear or non-linear function. The number of hidden-layers can go into the hundreds.

It can be useful to think of a neural-network as a combination of two things: 1) many logistic regressions stacked on top of each other that are 'feature-generators' and 2) one read-out-layer which is just a softmax regression. The recent successes in deep-learning can arguable be attributed to the 'feature-generators'. For example; previously with computer vision, we had to painfully state that we wanted to find triangles, circles, colours, and in what combination (similar to how economists decide which interaction-terms they need in a linear regression). Now, the hidden-layers are basically an optimisation to decide which features (which 'interaction-terms') to extract. A lot of deep-learning (transfer learning) is actually done by generating features using a trained-model with the head (read-out layer) cut-off, and then training a logistic regression (or boosted decision-trees) using those features as inputs.

The hidden-layer also means that our loss function is not convex in parameters and we can't roll down a smooth-hill to get to the bottom. Instead of using Gradient Descent (which we did for the case of a logistic-regression) we will use Stochastic Gradient Descent (SGD), which basically shuffles the observations (random/stochastic) and updates the gradient after each mini-batch (generally much less than total number of observations) has been propagated through the network. There are many alternatives to SGD that Sebastian Ruder does a great job of summarising here. I think this is a fascinating topic to go through, but outside the scope of this blog-post. Briefly, however, the vast majority of the optimisation methods are first-order (including SGD, Adam, RMSprop, and Adagrad) because calculating the second-order is too computionally difficult. However, some of these first-order methods have a fixed learning-rate (SGD) and some have an adaptive learning-rate (Adam), which means that the 'amount' we update our weights by becomes a function of the loss - we may make big jumps in the beginning but then take smaller steps as we get closer to the target.

It should be clear, however that minimising the loss on training data is not the main goal - in theory we want to minimise the loss on 'unseen'/test data; hence all the opimisation methods proxy for that under the assumption that a low lost on training data will generalise to 'new' data from the same distribution. This means we may prefer a neural-network with a higher training-loss; because it has a lower validation-loss (on data it hasn't been trained on) - we would typically say that the network has 'overfit' in this case. There have been some recent papers that claim that adaptive optimisation methods do not generalise as well as SGD because they find very sharp minima points.

Previously we only had to back-propagate the gradient one layer, now we also have to back-propagate it through all the hidden-layers. Explaining the back-propagation algorithm is beyond the scope of this post, however it is crucial to understand. Many good resources exist online to help.

We can now create a neural-network from scratch in R using four functions.

First, we initialise our weights:

```{r function-neuralnetwork, eval=FALSE}
neuralnetwork <- function(sizes, training_data, epochs, 
  mini_batch_size, lr, C, verbose=FALSE, 
  validation_data=training_data)
```

Since we now have a complex combination of parameters we can't just initialise them to be 1 or 0, like before - the network may get stuck. To help, we use the gaussian distribution (however, just like with the opimisation, there are many other methods):

```{r biases_weights, eval=FALSE}
    biases <- lapply(seq_along(listb), function(idx){
    r <- listb[[idx]]
    matrix(rnorm(n=r), nrow=r, ncol=1)
    })

    weights <- lapply(seq_along(listb), function(idx){
    c <- listw[[idx]]
    r <- listb[[idx]]
    matrix(rnorm(n=r*c), nrow=r, ncol=c)
    })
```    

Second, we use stochastic gradient descent as our optimisation method:

```{r function-SGD, eval=FALSE}
SGD <- function(training_data, epochs, mini_batch_size, lr, C, sizes, 
                num_layers, biases, weights,
                 verbose=FALSE, validation_data)
 {
   # Every epoch
   for (j in 1:epochs){
     # Stochastic mini-batch (shuffle data)
     training_data <- sample(training_data)
     # Partition set into mini-batches
     mini_batches <- split(training_data, 
                           ceiling(seq_along(training_data)/mini_batch_size))
     # Feed forward (and back) all mini-batches
     for (k in 1:length(mini_batches)) {
       # Update biases and weights
       res <- update_mini_batch(mini_batches[[k]], lr, C, sizes, num_layers, biases, weights)
       biases <- res[[1]]
       weights <- res[[-1]]
     }
   }
   # Return trained biases and weights
   list(biases, weights)
 }
```

Third, as part of the SGD method, we update the weights after each mini-batch has been forward and backwards-propagated:

```{r function-update_mini_batch, eval=FALSE}
update_mini_batch <- function(mini_batch, lr, C, sizes, num_layers, biases, weights)
 {
   nmb <- length(mini_batch)
   listw <- sizes[1:length(sizes)-1] 
   listb <-  sizes[-1]  

   # Initialise updates with zero vectors (for EACH mini-batch)
   nabla_b <- lapply(seq_along(listb), function(idx){
     r <- listb[[idx]]
     matrix(0, nrow=r, ncol=1)
   })
   nabla_w <- lapply(seq_along(listb), function(idx){
     c <- listw[[idx]]
     r <- listb[[idx]]
     matrix(0, nrow=r, ncol=c)
   })  

   # Go through mini_batch
   for (i in 1:nmb){
     x <- mini_batch[[i]][[1]]
     y <- mini_batch[[i]][[-1]]
     # Back propogation will return delta
     # Backprop for each observation in mini-batch
     delta_nablas <- backprop(x, y, C, sizes, num_layers, biases, weights)
     delta_nabla_b <- delta_nablas[[1]]
     delta_nabla_w <- delta_nablas[[-1]]
     # Add on deltas to nabla
     nabla_b <- lapply(seq_along(biases),function(j)
       unlist(nabla_b[[j]])+unlist(delta_nabla_b[[j]]))
     nabla_w <- lapply(seq_along(weights),function(j)
       unlist(nabla_w[[j]])+unlist(delta_nabla_w[[j]]))
   }
   # After mini-batch has finished update biases and weights:
   # i.e. weights = weights - (learning-rate/numbr in batch)*nabla_weights
   # Opposite direction of gradient
   weights <- lapply(seq_along(weights), function(j)
     unlist(weights[[j]])-(lr/nmb)*unlist(nabla_w[[j]]))
   biases <- lapply(seq_along(biases), function(j)
     unlist(biases[[j]])-(lr/nmb)*unlist(nabla_b[[j]]))
   # Return
   list(biases, weights)
 }
```

Fourth, the algorithm we use to calculate the deltas is the back-propagation algorithm.

In this example we use the cross-entropy loss function, which produces the following gradient:

```{r function-cost_delta, eval=FALSE}
cost_delta <- function(method, z, a, y) {
  if (method=='ce'){return (a-y)}
}
```

Also, to be consistent with our logistic regression example we use the sigmoid activation for the hidden layers and for the read-out layer:

```{r function-sigmoids, eval=FALSE}
# Calculate activation function
sigmoid <- function(z){1.0/(1.0+exp(-z))}
# Partial derivative of activation function
sigmoid_prime <- function(z){sigmoid(z)*(1-sigmoid(z))}
```

As mentioned previously, usually the softmax activation is used for the read-out layer. For the hidden layers, ReLU is more common, which is just the max function (negative weights get flattened to 0). The activation function for the hidden layers can be imagined as a race to carry a baton/flame (gradient) without it dying. The sigmoid function flattens out at 0 and at 1, resulting in a flat gradient which is equivalent to the flame dying out (we have lost our signal). The ReLU function helps preserve this gradient.

The back-propagation function is defined as:

```{r eval=FALSE}
backprop <- function(x, y, C, sizes, num_layers, biases, weights)

```

Check out the notebook for the full code — however the principle remains the same: we have a forward-pass where we generate our prediction by propagating the weights through all the layers of the network. We then plug this into the cost gradient and update the weights through all of our layers.

This concludes the creation of a neural network (with as many hidden layers as you desire). It can be a good exercise to replace the hidden-layer activation with ReLU and read-out to be softmax, and also add L1 and L2 regularization. Running this on the iris dataset in the notebook (which contains 4 explanatory variables with 3 possible outomes), with just one hidden-layer containing 40 neurons we get an accuracy of 96% after 30 rounds/epochs of training.

The notebook also runs a 100-neuron handwriting-recognition example to predict the digit corresponding to a 28x28 pixel image.
