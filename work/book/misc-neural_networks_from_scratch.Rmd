# Neural Networks from Scratch, in R

```{r setup, include=F, error=T, message=F, warning=F}
knitr::opts_chunk$set(echo = TRUE, 
                      comment ="#>", 
                      error = TRUE, 
                      message = FALSE, 
                      warning = FALSE,
                      fig.align = 'center')
```

```{r echo=FALSE}
load(file.path(rprojroot::find_rstudio_root_file(), "workspace.RData"))
.libPaths(c(normalizePath(sbox_path), normalizePath(lib_path), .libPaths()))
```

## Introduction

https://blog.revolutionanalytics.com/2017/07/nnets-from-scratch.html

This post is for those of you with a statistics/econometrics background but not necessarily a machine-learning one and for those of you who want some guidance in building a neural-network from scratch in R to better understand how everything fits (and how it doesn't).

Andrej Karpathy wrote that when CS231n (Deep Learning at Stanford) was offered:

> "we intentionally designed the programming assignments to include explicit calculations involved in backpropagation on the lowest level. The students had to implement the forward and the backward pass of each layer in raw numpy. Inevitably, some students complained on the class message boards".

Why bother with backpropagation when all frameworks do it for you automatically and there are more interesting deep-learning problems to consider?

Nowadays we can literally train a full neural-network (on a GPU) in 5 lines.

Karpathy, abstracts away from the "intellectual curiosity" or "you might want to improve on the core algorithm later" argument. His argument is that the calculations are a leaky abstraction:

> “it is easy to fall into the trap of abstracting away the learning process—believing that you can simply stack arbitrary layers together and backprop will 'magically make them work' on your data”

Hence, my motivation for this post is two-fold:

Understanding (by writing from scratch) the leaky abstractions behind neural-networks dramatically shifted my focus to elements whose importance I initially overlooked. If my model is not learning I have a better idea of what to address rather than blindly wasting time switching optimisers (or even frameworks).

A deep-neural-network (DNN), once taken apart into lego blocks, is no longer a black-box that is inaccessible to other disciplines outside of AI. It's a combination of many topics that are very familiar to most people with a basic knowledge of statistics. I believe they need to cover very little (just the glue that holds the blocks together) to get an insight into a whole new realm.

Starting from a linear regression we will work through the maths and the code all the way to a deep-neural-network (DNN) in the accompanying R-notebooks. Hopefully to show that very little is actually new information.

## Step 1 - Linear Regression (See Notebook)
https://github.com/ilkarman/DemoNeuralNet/blob/master/01_LinearRegression.ipynb

```{r echo=FALSE}
knitr::include_graphics(file.path(assets_dir, "linear_regression.jpg"))
```


```{r}
# Reproduce results
set.seed(1234567)
```


```{r}
# Random data in which y is a noisy function of x
X <- runif(100, -5, 5)
y <- X + rnorm(100) + 3
```


### 1. Linear Regression in R
A linear regression assumes that the true function (we are trying to replicate) can be written as:

$$
\begin{aligned}
y = X\beta + \epsilon
\end{aligned}
$$

In other words the true function is a linear combination of its parameters.


#### Vector of residuals
The vector of residuals given an estimate of $\beta$ is thus:

$$\begin{aligned} 
   e = y - X\beta^{OLS}
\end{aligned}$$

Where OLS indicates that this is the Ordinary Least Squares estimate of $\beta$

The OLS estimator by definition minimises the sum of squared residuals:

$$\begin{aligned} 
   e'e = (y - X\beta^{OLS})'(y - X\beta^{OLS})
\end{aligned}$$

$$\begin{aligned} 
   e'e = y'y - y'X\beta^{OLS} - \beta^{OLS'}X'y + \beta^{OLS'}X'X\beta^{OLS'}
\end{aligned}$$

Since the tranpose of a scalar is a scalar: $y'X\beta^{OLS} = (y'X\beta^{OLS})' = \beta^{OLS'}X'y$ we get the Residual Sum of Squares (RSS):

$$\begin{aligned} 
   RSS = e'e = y'y - 2\beta^{OLS'}X'y + \beta^{OLS'}X'X\beta^{OLS'}
\end{aligned}$$

#### Taking the derivative
We take the derivative of this w.r.t to beta-hat and set it equal to 0:

$$\begin{aligned} 
   0 = -2X'y + 2X'X\beta^{OLS}
\end{aligned}$$

The chart below shows an example of such a function; for the example we assume that wages are a linear function of height:

The line-of-best-fit here is a line that minimises the squared sum of residuals, it has a slope of 0.95 and an intercept of 2.95.


```{r fig.width=5, fig.height=5}
# Fit a model (regress weight on height)
fit <- lm(y ~ X)
print(fit)
# Coefficients:
#(Intercept)            X  
#     2.9455       0.9519 

# beta-hat
fit_params <- fit$coefficients

# Plot
plot(x=X, y=y, cex = 1, col = "grey",
     main = "Explain Wages with Height", xlab = "Height", ylab = "Wages")

# Draw the regression line (intercept, slope)
abline(a=fit_params[[1]], b=fit_params[[2]], col="blue")
```



### 2. Linear Regression from Scratch
Assuming that $X'X$ is a positive definite matrix (our variables are not a perfect linear combination of each other & we have more observations than variables) we can find a closed-form solution for $\beta^{OLS}$:

```{r fig.width=5, fig.height=5}
# Matrix of predictors (we only have one in this example)
X_mat <- as.matrix(X)
# Add column of 1s for intercept coefficient
intcpt <- rep(1, length(y))

# Combine predictors with intercept
X_mat <- cbind(intcpt, X_mat)

# OLS (closed-form solution)
beta_hat <- solve(t(X_mat) %*% X_mat) %*% t(X_mat) %*% y
print(beta_hat)
# 2.945535, 0.951942

# Plot
plot(x=X, y=y, cex = 1, col = "grey",
     main = "Explain Wages with Height", xlab = "Height", ylab = "Wages")

# Draw the previous regression line
abline(a=fit_params[[1]], b=fit_params[[2]], col="blue")
# Current regression line
abline(a=beta_hat[[1]], b=beta_hat[[2]], col="green")

# To get y-hat:
y_hat <- X_mat %*% beta_hat
points(x=X, y=y_hat, pch = 2, col='yellow')
```

Above we mentioned the assumption that the matrix $X'X$ is not-singular; below are two examples when this will fail and we cannot calculate the inverse:

```{r}
dim(X_mat)  # 100 by 2
inv <- solve(t(X_mat) %*% X_mat)
dim(inv)  # Possible to invert

# 1. We have a column that is a (perfect) linear combo of another
X_mat_fail <- cbind((10*X_mat[,1])+8, X_mat)
inv <- try(solve(t(X_mat_fail) %*% X_mat_fail), silent = TRUE)
dim(inv)  # Couldn't invert

# 2. We have more variables than observations
X_mat_fail <- matrix(runif(100, -5, 5), nrow=10, ncol=20)
inv <- try(solve(t(X_mat_fail) %*% X_mat_fail), silent = TRUE)
dim(inv)  # Couldn't invert
```


### 3. Linear Regression with Gradient Descent
However, we can also use an interative method known as Gradient Descent, this is a generic method for continuous optimisation. With GD we randomly initialise $\beta^{GD}$ and then calculate the residual (error) and move in the opposite direction to the gradient by a small amount proportional to a parameter we call the learning-rate. GD is a bit like rolling a ball down a hill - it will gradually converge to a stationary-point. If the function is convex with a small enough step-size (learning-rate) and high-enough number of iterations we are guaranteed to find a global minimiser. Stochastic Gradient Descent is usually used for neural-networks to avoid getting stuck in a local minimum due to a non-convex cost function (along with other methods).

The general-formula for GD:

Find a cost-function
Randomly initialise your $\beta$ vector
Get the derivative of the cost-function given $\beta$
Move the $\beta$ vector in the opposite direction to the gradient
In the case of this linear-regression:

Our cost-function is the Mean Squared Error (MSE) which is:

$MSE = \frac{RSS}{N}$

and:

$RSS: y'y - 2\beta^{OLS'}X'y + \beta^{OLS'}X'X\beta^{OLS'}$

The derivative of the RSS is: $-2X'y + 2X'X\beta^{OLS}$

This can be simplified to: $2X'(X\beta^{OLS} - y)$

So, we can write our 'delta' as:

$$\begin{aligned} 
    \frac{dLoss}{d\beta} = \frac{2}{N}\sum_ix_i(x_i\beta^{OLS} - y)
\end{aligned}$$

This means our equation for $\beta^{OLS}$ becomes:

$$\begin{aligned} 
    \beta^{OLS} = \beta^{OLS} - \frac{lr}{N}\sum_ix_i(x_i\beta^{OLS} - y)
\end{aligned}$$


    for (j in 1:epochs)
    {
        residual <- (X_mat %*% beta_hat) - y
        delta <- (t(X_mat) %*% residual) * (1/nrow(X_mat))
        beta_hat <- beta_hat - (lr*delta)
    }


```{r}
gradient_descent <- function(X, y, lr, epochs)
{
  X_mat <- cbind(1, X)
  # Initialise beta_hat matrix
  beta_hat <- matrix(0.1, nrow=ncol(X_mat))
  for (j in 1:epochs)
  {
    residual <- (X_mat %*% beta_hat) - y
    delta <- (t(X_mat) %*% residual) * (1/nrow(X_mat))
    beta_hat <- beta_hat - (lr*delta)
    # Draw the regression line each epoch
    abline(a=beta_hat[[1]], b=beta_hat[[2]], col="grey")
  }
  # Return 
  beta_hat
}
```

With learning-rate set to 0.1 and epochs set to 200 we converge to the same result: 2.95, 0.95. We can track how the line-of-best has been gradually fitted with this method by plotting it at each iteration:

```{r}
# Plot
plot(x=X, y=y, cex = 1, col = "grey",
     main = "Explain Wages with Height", xlab = "Height", ylab = "Wages")

beta_hat <- gradient_descent(X, y, 0.1, 200)
print(beta_hat)
# 2.945535, 0.951942

# Draw the regression line
abline(a=beta_hat[[1]], b=beta_hat[[2]], col="red")

# To get y-hat:
y_hat <- X_mat %*% beta_hat
points(x=X, y=y_hat, pch = 2, col='yellow')
```

Implementing the closed-form solution for the Ordinary Least Squares estimator in R requires just a few lines:

```{r}
# Matrix of explanatory variables
X <- as.matrix(X)
# Add column of 1s for intercept coefficient
intcpt <- rep(1, length(y))
# Combine predictors with intercept
X <- cbind(intcpt, X)
# OLS (closed-form solution)
beta_hat <- solve(t(X) %*% X) %*% t(X) %*% y
```

The vector of values in the variable beta_hat define our "machine-learning model". A linear regression is used to predict a continuous variable (e.g. how many minutes will this plane be delayed by). In the case of predicting a category (e.g. will this plane be delayed - yes/no) we want our prediction to fall between 0 and 1 so that we can interpret it as the probability of observing the respective category (given the data).

When we have just two mutually-exclusive outcomes we would use a binomial logistic regression. With more than two outcomes (or "classes"), which are mutually-exclusive (e.g. this plane will be delayed by less than 5 minutes, 5-10 minutes, or more than 10 minutes), we would use a multinomial logistic regression (or "softmax"). In the case of many (n) classes that are not mutually-exclusive (e.g. this post references "R" and "neural-networks" and "statistics"), we can fit n-binomial logistic regressions.

An alternative approach to the closed-form solution we found above is to use an iterative method, called Gradient Descent (GD). The procedure may look like so:

Start with a random guess for the weights
Plug guess into loss function
Move guess in the opposite direction of the gradient at that point by a small amount (something we call the `learning-rate')
Repeat above for N steps
GD only uses the Jacobian matrix (not the Hessian), however we know that when we have a convex loss, all local minima are global minima and thus GD is guaranteed to converge to the global minimum.

The loss-function used for a linear-regression is the Mean Squared Error:




To use GD we only need to find the partial derivative of this with respect to beta_hat (the 'delta'/gradient).

This can be implemented in R, like so:

```{r}
N <-  200
lr <-  0.1
# Start with a random guess
beta_hat <- matrix(0.1, nrow=ncol(X_mat))
  # Repeat below for N-iterations
  for (j in 1:N)
  {
    # Calculate the cost/error (y_guess - y_truth)
    residual <- (X_mat %*% beta_hat) - y
    # Calculate the gradient at that point
    delta <- (t(X_mat) %*% residual) * (1/nrow(X_mat))
    # Move guess in opposite direction of gradient
    beta_hat <- beta_hat - (lr*delta)
  }
```

## Logistic Regression
https://github.com/ilkarman/DemoNeuralNet/blob/master/02_LogisticRegression.ipynb

```{r}
knitr::include_graphics(file.path(assets_dir, "logistic_regression.jpg"))
```


```{r}
# Reproduce results
set.seed(1234567)

# Two possible outcomes -> binomial
data_df <- as.data.frame(iris)
idx <- data_df$Species %in% c("virginica", "versicolor")
data_df <- data_df[idx,]
y <- ifelse(data_df$Species=="virginica", 1, 0)

# For faster convergence let's rescale X
# So that we can plot this consider only 2 variables
X <- data_df[c(1,3)]
X <- as.matrix(X/max(X))

# Resulting data-set
head(X)
head(y)

```

### 1. Fit a binomial logit in R
A logistic regression is a linear regression that outputs a number bounded between 0 and 1. This means it is useful for classification problems, where we want to predict the probability of something happening. A binomial logistic regression is used when there are just two-classes, to extend beyound two-classes we would typically use a multi-nomial logistic regression (softmax).

Consider the iris-dataset where we try to predict whether a flower is "virginica" or "versicolor" by only looking at petal-length and sepal-length. We fit a linear line to 'best' split the categories:


```{r}
# Fit model
model <- glm(y ~ X, family=binomial(link='logit'))

# Params
print(coef(model))
# Coefficients:
# (Intercept) XSepal.Length XPetal.Length 
# -39.83851     -31.73243     105.16992 
#summary(model)

# Visualise the decision boundary
intcp <- coef(model)[1]/-(coef(model)[3])
slope <- coef(model)[2]/-(coef(model)[3])

# Our points
plot(x=X[,1], y=X[,2], cex = 1, col=data_df$Species,
     main = "Iris type by length and width", 
     xlab = "Sepal Length", ylab = "Petal Length")
legend(x='topright', legend=unique(data_df$Species),col=unique(data_df$Species), pch=1)
# Decision boundary
abline(intcp , slope, col='blue')
```

The above line has an intercept of -39.84, and coefficient of -31.73 for sepal-length and 105.17 for petal-length. These estimates are obtained by maximising the likelihood.

Because the log function is monotone, maximizing the likelihood is the same as maximizing the log-likelihood (or minimising the negative of the log-likelihood)

$$\begin{aligned} 
   l_x(\theta) = \log L_x(\theta)
\end{aligned}$$

For many reasons it is more convenient to use log likelihood rather than likelihood:

$$\begin{aligned}
   \log L_x
   =
   \sum_{i=1}^{N} y_i\beta^Tx_i - \log(1+e^{\beta^Tx_i})  
\end{aligned}$$

    log_likelihood <- function(X_mat, y, beta_hat)
    {
      scores <- X_mat %*% beta_hat
      ll <- (y * scores) - log(1+exp(scores))
      sum(ll)
    }
    
The log-likelihood in this example is -11.92.

```{r}
logLik(model)  # Log-likelihood

# Calculate log-likelihood ourself
log_likelihood <- function(X_mat, y, beta_hat)
{
  scores <- X_mat %*% beta_hat
  # Need to broadcast (y %*% scores)
  ll <- (y * scores) - log(1+exp(scores))
  sum(ll)
}

log_likelihood(cbind(1, X), y, coef(model))  # Match at -11.925
```

### 2. Fit a binomial logit ourselves (GD)
Typically BFGS or other numerical optimisation procedures are used to minimise the cost/max log-likelihood instead of GD, because the parameter space is pretty smaller (compared to neural-networks).

The logistic loss is sometimes called the cross-entropy loss. Let us first examine what would happen if we simply tried to calculate the least-squares loss, like before (for a regression rather than a classification):

$$\begin{aligned} 
   C = \frac{1}{2n}\sum_x\|(y(x) - a(x)\|^2
\end{aligned}$$

This loss will not be convex (in parameters) because $a(x)$ (the activation/link function) that transforms our score into a probability is defined as: $\sigma(z)=\frac{1}{1+e^-z}$ and thus $a(x)=\frac{1}{1+e^-\beta^Tx_i}$

We can construct a convex-loss function. This is binary classification and so we can define the loss for both of the classes. When $y=1$ we want the loss to be large when $\sigma(z)$ is close to zero and for it to be small when it is close to 1. Similarly, when $y=0$ we want the loss to be large when $\sigma(z)$ is close to one and for it be small when it is close to 0. The following loss function satisifies those conditions:

$$\begin{aligned} 
   C = -\frac{1}{n}\sum_xy(x)\ln(a(x)) + (1 - y(x))\ln(1-a(x))
\end{aligned}$$

For example, when $y(x)=1$ our loss-function becomes $C = -\frac{1}{n}\sum_x\ln(a(x))$, which is equal to 0 when $a(x)=1$, otherwise it becomes very high.

Taking the derivative of this loss-function w.r.t to the parameters, it can be shown:

$$\begin{aligned} 
    \frac{dC}{d\beta_i} = \frac{1}{n}\sum_xx_i(a(x) - y)
\end{aligned}$$

Note that the cross-entropy loss is more generally defined as:

$$\begin{aligned} 
   C = -\frac{1}{n}\sum_x\sum_j y_j\ln(a_j)
\end{aligned}$$

For binary classifications where $j=2$, under the condition that the categories are mutually-exclusive $\sum_ja_j=1$ and that y is one-hot so that $y1+y2=1$, we can re-write it as:

$$\begin{aligned} 
   C = -\frac{1}{n}\sum_xy_1\ln(a_1) + (1 - y_1)\ln(1-a_1)
\end{aligned}$$

Which is the same equation we first started with.

The process for using GD for a logistic regression is similar to that of a simple linear-regression. Since our loss is convex we can use either gradient-descent or stochastic-gradient descent; for now we will stick with the former.

```{r}
# Calculate activation function (sigmoid for logit)
sigmoid <- function(z){1.0/(1.0+exp(-z))}

logistic_reg <- function(X, y, epochs, lr)
{
  X_mat <- cbind(1, X)
  beta_hat <- matrix(1, nrow=ncol(X_mat))
  for (j in 1:epochs)
  {
    residual <- sigmoid(X_mat %*% beta_hat) - y
    # Update weights with gradient descent
    delta <- t(X_mat) %*% as.matrix(residual, ncol=nrow(X_mat)) *  (1/nrow(X_mat))
    beta_hat <- beta_hat - (lr*delta)
  }
  # Print log-likliehood
  print(log_likelihood(X_mat, y, beta_hat))
  # Return
  beta_hat
}
```

The only major difference is that we apply a sigmoid function to our prediction - to turn it into a probability. Below we can see why: the output is bounded between 0 and 1.

The shape of the sigmoid curve also means that we can increase the speed of convergence by scaling the variables to be closer to 0 - where the gradient is high. Imagine our inputs have a value of 100 - this can create a very high error, however the gradient is nearly flat and thus the update to the coefficients will be tiny.

We run the below to optimise our logistic regression using GD:

    beta_hat <- logistic_reg(X, y, 300000, 5)

We match the original results with the coefficients: -38.84, -31.73, 105.17

```{r}
# Why did scaling before help with convergence?
# Vanishing gradient
curve(sigmoid, -10, 10)
```

```{r logistic_gradient_descent}
# Takes a while to converge with GD!
beta_hat <- logistic_reg(X, y, 300000, 5)
print(beta_hat)

# Intercept    -39.83848
# Sepal.Length -31.73240
# Petal.Length 105.16983

# Visualise the decision boundary
plot(x=X[,1], y=X[,2], cex = 1, col=data_df$Species,
     main = "Iris type by length and width", 
     xlab = "Sepal Length", ylab = "Petal Length")
legend(x='topright', legend=unique(data_df$Species),col=unique(data_df$Species), pch=1)

# Visualise the decision boundary
intcp <- beta_hat[1]/-(beta_hat[3])
slope <- beta_hat[2]/-(beta_hat[3])

abline(intcp , slope, col='purple')
```


### What is logistic regression?
A logistic regression is a linear regression for binary classification problems. The two main differences to a standard linear regression are:

We use an 'activation'/link function called the logistic-sigmoid to squash the output to a probability bounded by 0 and 1
Instead of minimising the quadratic loss we minimise the negative log-likelihood of the Bernoulli distribution
Everything else remains the same.

We can calcuate our activation function like so:

```{r}
sigmoid <- function(z){1.0/(1.0+exp(-z))}

```

We can create our log-likelihood function in R:

```{r}
log_likelihood <- function(X_mat, y, beta_hat)
{
  scores <- X_mat %*% beta_hat
  ll <- (y * scores) - log(1+exp(scores))
  sum(ll)
}
```


This loss function (the logistic loss or the log-loss) is also called the cross-entropy loss. The cross-entropy loss is basically a measure of 'surprise' and will be the foundation for all the following models, so it is worth examining a bit more.

If we simply constructed the least-squares loss like before, because we now have a non-linear activation function (the sigmoid), the loss will no longer be convex which will make optimisation hard.

$$\begin{aligned} 
   C = \frac{1}{2n}\sum_x ((y(x) - a(x))^2
\end{aligned}$$

We could construct our own loss function for the two classes. When $y=1$, we want our loss function to be very high if our prediction is close to 0, and very low when it is close to 1. When $y=0$, we want our loss function to be very high if our prediction is close to 1, and very low when it is close to 0. This leads us to the following loss function:

$$\begin{aligned} 
   C = -\frac{1}{n}\sum_xy(x)\ln(a(x)) + (1 - y(x))\ln(1-a(x))
\end{aligned}$$

The delta for this loss function is pretty much the same as the one we had earlier for a linear-regression. The only difference is that we apply our sigmoid function to the prediction. This means that the GD function for a logistic regression will also look very similar:

```{r}
logistic_reg <- function(X, y, epochs, lr)
{
  X_mat <- cbind(1, X)
  beta_hat <- matrix(1, nrow=ncol(X_mat))
  for (j in 1:epochs)
  {
    # For a linear regression this was:
    # 1*(X_mat %*% beta_hat) - y
    residual <- sigmoid(X_mat %*% beta_hat) - y
    # Update weights with gradient descent
    delta <- t(X_mat) %*% as.matrix(residual, ncol=nrow(X_mat))*(1/nrow(X_mat))
    beta_hat <- beta_hat - (lr*delta)
  }
  # Print log-likliehood
  print(log_likelihood(X_mat, y, beta_hat))
  # Return
  beta_hat
}
```


## Softmax Regression

```{r}
knitr::include_graphics(file.path(assets_dir, "softmax_regression.jpg"))
```

A generalisation of the logistic regression is the multinomial logistic regression (also called 'softmax'), which is used when there are more than two classes to predict. I haven't created this example in R, because the neural-network in the next step can reduce to something similar, however for completeness I wanted to highlight the main differences if you wanted to create it.

First, instead of using the sigmoid function to squash our (one) value between $\theta$ and 1:

$$\sigma(z) = \frac {1}{1+e^{-z}}$$

We use the softmax function to squash the sum of our $n$ values (for $n$ classes) to 1:

$$\phi(z) = \frac {e^z_j}{\sum_k e^z_k}$$

This means the value supplied for each class can be interpreted as the probability of that class, given the evidence. This also means that when we see the target class and increase the weights to increase the probability of observing it, the probability of the other classes will fall. The implicit assumption is that our classes are mutually exclusive.

Second, we use a more general version of the cross-entropy loss function:

$$\begin{aligned} 
   C = -\frac{1}{n}\sum_x\sum_j y_j\ln(a_j)
\end{aligned}$$

To see why, remember that for binary classifications (previous example) we had two classes: 
$j=2$, under the condition that the categories are mutually-exclusive $\sum_ja_j = 1$ and that $y$ is one-hot so that $y_1+y_2=1$, we can re-write the general formula as:

$$\begin{aligned} 
   C = -\frac{1}{n}\sum_xy_1\ln(a_1) + (1 - y_1)\ln(1-a_1)
\end{aligned}$$

Which is the same equation we first started with. However, now we relax the constraint that 
$j=2$. It can be shown that the cross-entropy loss here has the same gradient as for the case of the binary/two-class cross-entropy on logistic outputs.

$$\begin{aligned} 
   \frac {\partial C} {\partial \beta_i} = \frac {1}{n} \sum x_i(a(x) -y)
\end{aligned}$$


However, although the gradient has the same formula it will be different because the activation here takes on a different value (softmax instead of logistic-sigmoid).

In most deep-learning frameworks you have the choice of 'binary-crossentropy' or 'categorical-crossentropy' loss. Depending on whether your last layer contains sigmoid or softmax activation you would want to choose binary or categorical cross-entropy (respectively). The training of the network should not be affected, since the gradient is the same, however the reported loss (for evaluation) would be wrong if these are mixed up.

The motivation to go through softmax is that most neural-networks will use a softmax layer as the final/'read-out' layer, with a multinomial/categorical cross-entropy loss instead of using sigmoids with a binary cross-entropy loss — when the categories are mutually exclusive. Although multiple sigmoids for multiple classes can also be used (and will be used in the next example), this is generally only used for the case of non-mutually-exclusive labels (i.e. we can have multiple labels). With a softmax output, since the sum of the outputs is constrained to equal 1, we have the advantage of interpreting the outputs as class probabilities.


## Neural Networks
https://github.com/ilkarman/DemoNeuralNet/blob/master/03_NeuralNet.ipynb

```{r echo=FALSE}
knitr::include_graphics(file.path(assets_dir, "neural_networks.jpg"))
```


```{r}
# Reproduce results
set.seed(1234567)
```

### 1. Neural Net in R (5 short functions to care about)
In the previous scenarios we used the mean-squared-error to represent our cost-function:

$$\begin{aligned} 
   C = \frac{1}{2n}\sum_x\|(y(x) - a(x)\|^2
\end{aligned}$$

For classification problems with neural-networks we will now use the cross-entropy cost:

$$\begin{aligned} 
   C = -\frac{1}{n}\sum_xy(x)\ln(a(x)) + (1 - y(x))\ln(1-a(x))
\end{aligned}$$

Where $a=\sigma(\sum_iw_ix_i + b) = \sigma(z)$

We can show that:

$$\begin{aligned} 
   \frac{dC}{dw_i} = \frac{1}{N}\sum_jx_j(\sigma(z)-y)
\end{aligned}$$

    cost_delta <- function(method, z, a, y) {if (method=='ce'){return (a-y)}}

This means that the bigger the error, the faster our weight will learn.

Our main neural-network functions are:

    neuralnetwork 
    SGD   
    update_mini_batch 
    backprop

The `neuralnetwork` function's main job is to initialise the weight and bias matricies given a list of sizes. For example, if we have 10 variables to predict 4 possible classes and we want a hidden-layer with 20 neurons we would pass: c(10,20,4) to this function. It passes these matricies to the SGD function to commence training.

The SGD function splits the training-data into random mini-batches and sends them off to the update_mini_batch function, which calculates the deltas for a batch (using backprop) and then updates the weights and bias matricies - so these are held constant within a batch:

```{r eval=FALSE}
# After mini-batch has finished update biases and weights: 
# Opposite direction of gradient
weights <- lapply(seq_along(weights), function(j)
  unlist(weights[[j]])-(lr/nmb)*unlist(nabla_w[[j]]))
biases <- lapply(seq_along(biases), function(j)
  unlist(biases[[j]])-(lr/nmb)*unlist(nabla_b[[j]]))
```

In other words: $weights = weights - (learning rate/number in batch)*nabla weights$

The backprop function applies the backpropogation algorithm to calculate the partial derivatives (to update mini-batch).

The forward step, goes through the network layer-by-layer and calculates the output of the activation function to calculate the delta (cost gradient given the prediction). For example, the activations in layer l are:

$$\begin{aligned} 
   a^l = \sigma(w^la^{l-1}+b^l)
\end{aligned}$$

The **backward step** propogates the partial derivative (deltas) across all the neurons so that they get a share proportional to their contribution to the output.

First, we initialise neural network bias and weights matrices

```{r initi_neural_network}
neuralnetwork <- function(sizes, training_data, epochs, mini_batch_size, lr, C,
                          verbose=FALSE, validation_data=training_data)
{
  num_layers <- length(sizes)
  listw <- sizes[1:length(sizes)-1] # Skip last (weights from 1st to 2nd-to-last)
  listb <-  sizes[-1]  # Skip first element (biases from 2nd to last)
  
  # Initialise with gaussian distribution for biases and weights
  biases <- lapply(seq_along(listb), function(idx){
    r <- listb[[idx]]
    matrix(rnorm(n=r), nrow=r, ncol=1)
  })
    
  weights <- lapply(seq_along(listb), function(idx){
    c <- listw[[idx]]
    r <- listb[[idx]]
    matrix(rnorm(n=r*c), nrow=r, ncol=c)
  })
    
  SGD(training_data, epochs, mini_batch_size, lr, C, 
      sizes, num_layers, biases, weights, verbose, validation_data)
}
```

Return the derivative of the cost function (quadratic or cross-entropy).

Quadratic cost:

$$\begin{aligned} 
   C = \frac{1}{2n}\sum_x\|(y(x) - a(x)\|^2
\end{aligned}$$

Cross-entropy cost:

$$\begin{aligned} 
   C = -\frac{1}{n}\sum_xy(x)\ln(a(x)) + (1 - y(x))\ln(1-a(x))
\end{aligned}$$

```{r}
cost_delta <- function(method, z, a, y) {if (method=='ce'){return (a-y)}}
```

Perform stochastic-gradient descent to minimise cost function

```{r}
SGD <- function(training_data, epochs, mini_batch_size, lr, C, sizes, num_layers, biases, weights,
                verbose=FALSE, validation_data)
{
  start.time <- Sys.time()
  # Every epoch
  for (j in 1:epochs){
    # Stochastic mini-batch (shuffle data)
    training_data <- sample(training_data)
    # Partition set into mini-batches
    mini_batches <- split(training_data, 
                          ceiling(seq_along(training_data)/mini_batch_size))
    # Feed forward (and back) all mini-batches
    for (k in 1:length(mini_batches)) {
      # Update biases and weights
      res <- update_mini_batch(mini_batches[[k]], lr, C, sizes, num_layers, biases, weights)
      biases <- res[[1]]
      weights <- res[[-1]]
    }
    # Logging
    if(verbose){if(j %% 1 == 0){
      cat("Epoch: ", j, " complete")
      # Print acc and hide confusion matrix
      confusion <- evaluate(validation_data, biases, weights)
      }}
  }
  time.taken <- Sys.time() - start.time
  if(verbose){cat("Training complete in: ", time.taken)}
  cat("Training complete")
  # Return trained biases and weights
  list(biases, weights)
}
```


Update the biase and weights matricies for each mini-batch

```{r}
update_mini_batch <- function(mini_batch, lr, C, sizes, num_layers, biases, weights)
{
  nmb <- length(mini_batch)
  listw <- sizes[1:length(sizes)-1] 
  listb <-  sizes[-1]  
  
  # Initialise updates with zero vectors (for EACH mini-batch)
  nabla_b <- lapply(seq_along(listb), function(idx){
    r <- listb[[idx]]
    matrix(0, nrow=r, ncol=1)
  })
  nabla_w <- lapply(seq_along(listb), function(idx){
    c <- listw[[idx]]
    r <- listb[[idx]]
    matrix(0, nrow=r, ncol=c)
  })  
  
  # Go through mini_batch
  for (i in 1:nmb){
    x <- mini_batch[[i]][[1]]
    y <- mini_batch[[i]][[-1]]
    # Back propogation will return delta
    # Backprop for each obeservation in mini-batch
    delta_nablas <- backprop(x, y, C, sizes, num_layers, biases, weights)
    delta_nabla_b <- delta_nablas[[1]]
    delta_nabla_w <- delta_nablas[[-1]]
    # Add on deltas to nabla
    nabla_b <- lapply(seq_along(biases),function(j)
      unlist(nabla_b[[j]])+unlist(delta_nabla_b[[j]]))
    nabla_w <- lapply(seq_along(weights),function(j)
      unlist(nabla_w[[j]])+unlist(delta_nabla_w[[j]]))
  }
  # After mini-batch has finished update biases and weights:
  # i.e. weights = weights - (learning-rate/numbr in batch)*nabla_weights
  # Opposite direction of gradient
  weights <- lapply(seq_along(weights), function(j)
    unlist(weights[[j]])-(lr/nmb)*unlist(nabla_w[[j]]))
  biases <- lapply(seq_along(biases), function(j)
    unlist(biases[[j]])-(lr/nmb)*unlist(nabla_b[[j]]))
  # Return
  list(biases, weights)
}
```

Backpropogation algorithm to calculate partial derivatives using chain-rule (to update mini-batch).

The **forward step**, goes through the network layer-by-layer and calculates the output of the activation function. For example, the activations in layer $l$ are:

$$\begin{aligned} 
   a^l = \sigma(w^la^{l-1}+b^l)
\end{aligned}$$

```{r}
backprop <- function(x, y, C, sizes, num_layers, biases, weights)
{
  # Initialise updates with zero vectors
  listw <- sizes[1:length(sizes)-1] 
  listb <-  sizes[-1]  
  
  # Initialise updates with zero vectors (for EACH mini-batch)
  nabla_b_backprop <- lapply(seq_along(listb), function(idx){
    r <- listb[[idx]]
    matrix(0, nrow=r, ncol=1)
  })
  nabla_w_backprop <- lapply(seq_along(listb), function(idx){
    c <- listw[[idx]]
    r <- listb[[idx]]
    matrix(0, nrow=r, ncol=c)
  })  
  
  # First:
  # Feed-forward (get predictions)
  activation <- matrix(x, nrow=length(x), ncol=1)
  activations <- list(matrix(x, nrow=length(x), ncol=1))
  # z = f(w.x + b)
  # So need zs to store all z-vectors
  zs <- list()
  for (f in 1:length(biases)){
    b <- biases[[f]]
    w <- weights[[f]]
    w_a <- w%*%activation
    b_broadcast <- matrix(b, nrow=dim(w_a)[1], ncol=dim(w_a)[-1])
    z <- w_a + b
    zs[[f]] <- z
    activation <- sigmoid(z)
    activations[[f+1]] <- activation  # Activations already contain one element
  }
  # Second:
  # Backwards (update gradient using errors)
  # Last layer
  delta <- cost_delta(method=C, z=zs[[length(zs)]], a=activations[[length(activations)]], y=y)
  nabla_b_backprop[[length(nabla_b_backprop)]] <- delta
  nabla_w_backprop[[length(nabla_w_backprop)]] <- delta %*% t(activations[[length(activations)-1]])
  # Second to second-to-last-layer
  # If no hidden-layer reduces to multinomial logit
  if (num_layers > 2) {
      for (k in 2:(num_layers-1)) {
        sp <- sigmoid_prime(zs[[length(zs)-(k-1)]])
        delta <- (t(weights[[length(weights)-(k-2)]]) %*% delta) * sp
        nabla_b_backprop[[length(nabla_b_backprop)-(k-1)]] <- delta
        testyy <- t(activations[[length(activations)-k]])
        nabla_w_backprop[[length(nabla_w_backprop)-(k-1)]] <- delta %*% testyy
      }
  }
  return_nabla <- list(nabla_b_backprop, nabla_w_backprop)
  return_nabla
}
```

#### These run a prediction on test-data and evaluate


```{r}
feedforward <- function(a, biases, weights)
{
  for (f in 1:length(biases)){
    a <- matrix(a, nrow=length(a), ncol=1)
    b <- biases[[f]]
    w <- weights[[f]]
    # (py) a = sigmoid(np.dot(w, a) + b)
    # Equivalent of python np.dot(w,a)
    w_a <- w%*%a
    # Need to manually broadcast b to conform to np.dot(w,a)
    b_broadcast <- matrix(b, nrow=dim(w_a)[1], ncol=dim(w_a)[-1])
    a <- sigmoid(w_a + b_broadcast)
  }
  a
}

get_predictions <- function(test_X, biases, weights)
{
  lapply(c(1:length(test_X)), function(i) {
    which.max(feedforward(test_X[[i]], biases, weights))}
  )
}

evaluate <- function(testing_data, biases, weights)
{
  test_X <- lapply(testing_data, function(x) x[[1]])
  test_y <- lapply(testing_data, function(x) x[[2]])
  pred <- get_predictions(test_X, biases, weights)
  truths <- lapply(test_y, function(x) which.max(x))
  # Accuracy
  correct <- sum(mapply(function(x,y) x==y, pred, truths))
  total <- length(testing_data)
  print(correct/total)
  # Confusion
  res <- as.data.frame(cbind(t(as.data.frame(pred)), t(as.data.frame(truths))))
  colnames(res) <- c("Prediction", "Truth")
  table(as.vector(res$Prediction), as.vector(res$Truth))
}
```

#### Math helpers
```{r}
# Calculate activation function
sigmoid <- function(z){1.0/(1.0+exp(-z))}

# Partial derivative of activation function
sigmoid_prime <- function(z){sigmoid(z)*(1-sigmoid(z))}
```


## This loads data into a format net accepts

```{r}
train_test_from_df <- function(df, predict_col_index, train_ratio, 
                               shuffle_input = TRUE, scale_input=TRUE)
{
  # Helper functions
  # Function to encode factor column as N-dummies
  dmy <- function(df)
  {
    # Select only factor columns
    factor_columns <- which(sapply(df, is.factor))
    if (length(factor_columns) > 0)
    {
      # Split factors into dummies
      dmy_enc <- model.matrix(~. + 0, data=df[factor_columns], 
                              contrasts.arg = lapply(df[factor_columns], contrasts, contrasts=FALSE))
      dmy_enc <- as.data.frame(dmy_enc)
      # Attach factors to df
      df <- cbind(df, dmy_enc)
      # Delete original columns
      df[c(factor_columns)] <- NULL
    }
    df
  }
  
  # Function to standarise inputs to range(0, 1)
  scalemax <- function(df)
  {
    numeric_columns <- which(sapply(df, is.numeric))
    if (length(numeric_columns)){df[numeric_columns] <- lapply(df[numeric_columns], function(x){
      denom <- ifelse(max(x)==0, 1, max(x))
      x/denom
    })}
    df
  }

  # Function to convert df to list of rows
  listfromdf <- function(df){as.list(as.data.frame(t(df)))}
  
  # Omit NAs (allow other options later)
  df <- na.omit(df)
  # Get list for X-data
  if (scale_input){
    X_data <- listfromdf(dmy(scalemax(df[-c(predict_col_index)])))
  } else {
    X_data <- listfromdf(dmy(df[-c(predict_col_index)]))
  }
  # Get list for y-data
  y_data <- listfromdf(dmy(df[c(predict_col_index)]))
  # Combine X,y
  all_data <- list()
  for (i in 1:length(X_data)){
    all_data[[i]] <- c(X_data[i], y_data[i])
  }
  # Shuffle before splitting
  if (shuffle_input) {all_data <- sample(all_data)}
  # Split to training and test
  tr_n <- round(length(all_data)*train_ratio)
  # Return (training, testing)
  list(all_data[c(1:tr_n)], all_data[-c(1:tr_n)])
}
```


### Examples

#### Iris

```{r}
head(iris)
```

```{r train_iris}
train_test_split <- train_test_from_df(df = iris, predict_col_index = 5, train_ratio = 0.7)
training_data <- train_test_split[[1]]
testing_data <- train_test_split[[2]]

in_n <- length(training_data[[1]][[1]])
out_n <- length(training_data[[1]][[-1]])

# [4, 40, 3] 
trained_net <- neuralnetwork(
    c(in_n, 40, out_n),
    training_data=training_data,
    epochs=30, 
    mini_batch_size=10,
    lr=0.5,
    C='ce',
    verbose=TRUE,
    validation_data=testing_data
)
```

```{r iris_evaluate}
# Trained matricies:
biases <- trained_net[[1]]
weights <- trained_net[[-1]]

# Accuracy (train)
evaluate(training_data, biases, weights)  #0.971
# Accuracy (test)
evaluate(testing_data, biases, weights)  #0.956
```

### Example MNIST

```{r}
library(grid)
```

```{r echo=FALSE}
# mnist_training <- mnist
# save(mnist_training, file = file.path(data_raw_dir, "mnist_training.rda"))
# 
# mnist_testing <- mnist
# save(mnist_testing, file = file.path(data_raw_dir, "mnist_testing.rda"))
```


```{r load_mnist, cache=TRUE}
# Here we have splits for train-test already (may take a minute to download)
# Train
if (!file.exists(file.path(data_raw_dir, "mnist_training.rda"))) {
  cat("reading from web ...\n")
  mnist <- read.table('https://iliadl.blob.core.windows.net/nnet/mnist_train.csv', 
                    sep=",", header = FALSE) 
  } else {
    cat("reading from disk ...\n")
    load(file = file.path(data_raw_dir, "mnist_training.rda"))
}

mnist_training$V1 <- factor(mnist_training$V1)
training_data <- train_test_from_df(df = mnist_training, predict_col_index = 1, 
                                    train_ratio = 1)[[1]]

# Test
if (!file.exists(file.path(data_raw_dir, "mnist_testing.rda"))) {
  cat("reading from web ...\n")
  mnist <- read.table('https://iliadl.blob.core.windows.net/nnet/mnist_test.csv', 
                    sep=",", header = FALSE) 
  } else {
    cat("reading from disk ...\n")
    load(file = file.path(data_raw_dir, "mnist_testing.rda"))
}

mnist_testing$V1 <- factor(mnist_testing$V1)
testing_data <- train_test_from_df(df = mnist_testing, predict_col_index = 1, 
                                   train_ratio = 1)[[1]]
```

```{r}
if (interactive()) { 
  write.csv(mnist_train, file.path(data_raw_dir, "mnist_train.csv"))
  write.csv(mnist_test, file.path(data_raw_dir, "mnist_test.csv"))
}  
```


### What does the data exactly look like?

```{r how_data_looks}
example_entry <- training_data[[1]]
example_x <- example_entry[[1]]
example_y <- example_entry[[2]]

# Y-vector looks like this:
print(example_y)
# It corresponds to digit:
print(which.max(example_y)-1)

# X-vector has length
print(length(example_x))

# We can think of it as a 28x28 matrix where entries are a shade of gray
grid.raster(matrix(example_x, nrow=28, byrow=TRUE))
```

Let's train a neural net with one 100-neuron hidden-layer to predict (given 784 vector of gray intensity) the digit (from 0 to 9)

```{r nnet_100_neurons}
# Input and output neurons
in_n <- length(training_data[[1]][[1]])
out_n <- length(training_data[[1]][[-1]])

# MNIST: 784, 100, 10 (one hidden-layer)
print("THIS WILL TAKE 20-30 MINUTES...")
trained_net <- neuralnetwork(sizes = c(in_n, 100, out_n), 
                             training_data = training_data,
                             epochs = 3,           # 30
                             mini_batch_size = 2,  # 10
                             lr = 3,
                             C = 'ce',
                             verbose=  TRUE,
                             validation_data = testing_data)
```

```{r confusion_matrices}
# Trained matricies:
biases <- trained_net[[1]]
weights <- trained_net[[-1]]

# CONFUSION TRAIN MATRIX
evaluate(training_data, biases, weights)  #0.98
# CONFUSION TEST MATRIX
evaluate(testing_data, biases, weights)  #0.97
```

```{r test_one_example}
# Test this out with one example
# Do some machine-learning
test_entry <- testing_data[[2]]
test_x <- test_entry[[1]]
test_y <- test_entry[[2]]

# Input
grid.raster(matrix(test_x, nrow=28, byrow=TRUE))

# Output
which.max(feedforward(test_x, biases, weights))-1
# Truth
cat("Truth: ", which.max(test_y)-1)
```

### Example: Visualizations of boundary lines
```{r load_mlbench}
# Load some spiral data
library(mlbench)
data_df <- as.data.frame(mlbench.spirals(10000))

# Looks pretty cool
plot(x=data_df[,1], y=data_df[,2], cex = 1, col=data_df[,3],
     main = "Spirals", 
     xlab = "X1", ylab = "X2")
```

```{r spiral_train}
# Let's use all of this data for training
training_data <- train_test_from_df(df = data_df, predict_col_index = 3, 
                                    train_ratio = 1, scale_input = TRUE)[[1]]
in_n <- length(training_data[[1]][[1]])
out_n <- length(training_data[[1]][[-1]])
```

```{r function-plotBoundaryLinePerNeuron}
# Wrap a function that trains a network and plots the classification
plotBoundaryLinePerNeuron <- function(neurons, epochs)
{
  trained_net <- neuralnetwork(c(in_n, neurons, out_n), training_data, 
                               epochs, 500, 3, 'ce')
  b <- trained_net[[1]]
  w <- trained_net[[-1]]
  # Evaluate
  confusion <- evaluate(training_data, b, w)
  # Generate new-data
  nd <- as.data.frame(matrix(runif(100000, min=-1, max=1), ncol=2))
  # Get predictions
  nd$res <- as.factor(unlist(get_predictions(as.list(as.data.frame(t(nd))), b, w)))
  # Plot predictions
  plot(x=nd[,1], y=nd[,2], pch=19, col=nd[,3],
       main = paste0("Spiral Boundary Line - ", neurons, " neurons"), 
       xlab = "X1", ylab = "X2")
  # Original points
  points(x=data_df[,1], y=data_df[,2], pch=19, col=ifelse(data_df[,3]==2, 
                                                          "orange", "grey"))
}
```

```{r spiral_n50_e80}
 plotBoundaryLinePerNeuron(50, 80)
```

```{r spiral_n10_e80}
 plotBoundaryLinePerNeuron(10, 80)
```

```{r spiral_n5_e100}
 plotBoundaryLinePerNeuron(5, 100)
```

```{r spiral_n4_e100}
 plotBoundaryLinePerNeuron(4, 100)
```

It doesn't seem possible to draw fewer than 4 curves that split the classes, hence below 4 neurons we should start to see performance degrade - because the network doesn't enough params to fit the data. This can be a useful to exercise when trying to figure out how many neurons ones needs  over-fitting

```{r spiral_n3_e120}
 plotBoundaryLinePerNeuron(3, 120)
```

```{r spiral_n2_e200}
 plotBoundaryLinePerNeuron(2, 200)
```


```{r spiral_n1_e120}
 plotBoundaryLinePerNeuron(1, 120)
```

```{r spiral_n40_40_40_e200}
# plotBoundaryLinePerNeuron(c(40,40,40), 200)
```

### What is a neural network?
A neural network can be thought of as a series of logistic regressions stacked on top of each other. This means we could say that a logistic regression is a neural-network (with sigmoid activations) with no hidden-layer.

This hidden-layer lets a neural-network generate non-linearities and leads to the Universal approximation theorem, which states that a network with just one hidden layer can approximate any linear or non-linear function. The number of hidden-layers can go into the hundreds.

It can be useful to think of a neural-network as a combination of two things: 1) many logistic regressions stacked on top of each other that are 'feature-generators' and 2) one read-out-layer which is just a softmax regression. The recent successes in deep-learning can arguable be attributed to the 'feature-generators'. For example; previously with computer vision, we had to painfully state that we wanted to find triangles, circles, colours, and in what combination (similar to how economists decide which interaction-terms they need in a linear regression). Now, the hidden-layers are basically an optimisation to decide which features (which 'interaction-terms') to extract. A lot of deep-learning (transfer learning) is actually done by generating features using a trained-model with the head (read-out layer) cut-off, and then training a logistic regression (or boosted decision-trees) using those features as inputs.

The hidden-layer also means that our loss function is not convex in parameters and we can't roll down a smooth-hill to get to the bottom. Instead of using Gradient Descent (which we did for the case of a logistic-regression) we will use Stochastic Gradient Descent (SGD), which basically shuffles the observations (random/stochastic) and updates the gradient after each mini-batch (generally much less than total number of observations) has been propagated through the network. There are many alternatives to SGD that Sebastian Ruder does a great job of summarising here. I think this is a fascinating topic to go through, but outside the scope of this blog-post. Briefly, however, the vast majority of the optimisation methods are first-order (including SGD, Adam, RMSprop, and Adagrad) because calculating the second-order is too computionally difficult. However, some of these first-order methods have a fixed learning-rate (SGD) and some have an adaptive learning-rate (Adam), which means that the 'amount' we update our weights by becomes a function of the loss - we may make big jumps in the beginning but then take smaller steps as we get closer to the target.

It should be clear, however that minimising the loss on training data is not the main goal - in theory we want to minimise the loss on 'unseen'/test data; hence all the opimisation methods proxy for that under the assumption that a low lost on training data will generalise to 'new' data from the same distribution. This means we may prefer a neural-network with a higher training-loss; because it has a lower validation-loss (on data it hasn't been trained on) - we would typically say that the network has 'overfit' in this case. There have been some recent papers that claim that adaptive optimisation methods do not generalise as well as SGD because they find very sharp minima points.

Previously we only had to back-propagate the gradient one layer, now we also have to back-propagate it through all the hidden-layers. Explaining the back-propagation algorithm is beyond the scope of this post, however it is crucial to understand. Many good resources exist online to help.

We can now create a neural-network from scratch in R using four functions.

First, we initialise our weights:

```{r function-neuralnetwork, eval=FALSE}
neuralnetwork <- function(sizes, training_data, epochs, 
  mini_batch_size, lr, C, verbose=FALSE, 
  validation_data=training_data)
```

Since we now have a complex combination of parameters we can't just initialise them to be 1 or 0, like before - the network may get stuck. To help, we use the gaussian distribution (however, just like with the opimisation, there are many other methods):

```{r biases_weights, eval=FALSE}
    biases <- lapply(seq_along(listb), function(idx){
    r <- listb[[idx]]
    matrix(rnorm(n=r), nrow=r, ncol=1)
    })

    weights <- lapply(seq_along(listb), function(idx){
    c <- listw[[idx]]
    r <- listb[[idx]]
    matrix(rnorm(n=r*c), nrow=r, ncol=c)
    })
```    

Second, we use stochastic gradient descent as our optimisation method:

```{r function-SGD, eval=FALSE}
SGD <- function(training_data, epochs, mini_batch_size, lr, C, sizes, 
                num_layers, biases, weights,
                 verbose=FALSE, validation_data)
 {
   # Every epoch
   for (j in 1:epochs){
     # Stochastic mini-batch (shuffle data)
     training_data <- sample(training_data)
     # Partition set into mini-batches
     mini_batches <- split(training_data, 
                           ceiling(seq_along(training_data)/mini_batch_size))
     # Feed forward (and back) all mini-batches
     for (k in 1:length(mini_batches)) {
       # Update biases and weights
       res <- update_mini_batch(mini_batches[[k]], lr, C, sizes, num_layers, biases, weights)
       biases <- res[[1]]
       weights <- res[[-1]]
     }
   }
   # Return trained biases and weights
   list(biases, weights)
 }
```

Third, as part of the SGD method, we update the weights after each mini-batch has been forward and backwards-propagated:

```{r function-update_mini_batch, eval=FALSE}
update_mini_batch <- function(mini_batch, lr, C, sizes, num_layers, biases, weights)
 {
   nmb <- length(mini_batch)
   listw <- sizes[1:length(sizes)-1] 
   listb <-  sizes[-1]  

   # Initialise updates with zero vectors (for EACH mini-batch)
   nabla_b <- lapply(seq_along(listb), function(idx){
     r <- listb[[idx]]
     matrix(0, nrow=r, ncol=1)
   })
   nabla_w <- lapply(seq_along(listb), function(idx){
     c <- listw[[idx]]
     r <- listb[[idx]]
     matrix(0, nrow=r, ncol=c)
   })  

   # Go through mini_batch
   for (i in 1:nmb){
     x <- mini_batch[[i]][[1]]
     y <- mini_batch[[i]][[-1]]
     # Back propogation will return delta
     # Backprop for each observation in mini-batch
     delta_nablas <- backprop(x, y, C, sizes, num_layers, biases, weights)
     delta_nabla_b <- delta_nablas[[1]]
     delta_nabla_w <- delta_nablas[[-1]]
     # Add on deltas to nabla
     nabla_b <- lapply(seq_along(biases),function(j)
       unlist(nabla_b[[j]])+unlist(delta_nabla_b[[j]]))
     nabla_w <- lapply(seq_along(weights),function(j)
       unlist(nabla_w[[j]])+unlist(delta_nabla_w[[j]]))
   }
   # After mini-batch has finished update biases and weights:
   # i.e. weights = weights - (learning-rate/numbr in batch)*nabla_weights
   # Opposite direction of gradient
   weights <- lapply(seq_along(weights), function(j)
     unlist(weights[[j]])-(lr/nmb)*unlist(nabla_w[[j]]))
   biases <- lapply(seq_along(biases), function(j)
     unlist(biases[[j]])-(lr/nmb)*unlist(nabla_b[[j]]))
   # Return
   list(biases, weights)
 }
```

Fourth, the algorithm we use to calculate the deltas is the back-propagation algorithm.

In this example we use the cross-entropy loss function, which produces the following gradient:

```{r function-cost_delta, eval=FALSE}
cost_delta <- function(method, z, a, y) {
  if (method=='ce'){return (a-y)}
}
```

Also, to be consistent with our logistic regression example we use the sigmoid activation for the hidden layers and for the read-out layer:

```{r function-sigmoids, eval=FALSE}
# Calculate activation function
sigmoid <- function(z){1.0/(1.0+exp(-z))}
# Partial derivative of activation function
sigmoid_prime <- function(z){sigmoid(z)*(1-sigmoid(z))}
```
As mentioned previously, usually the softmax activation is used for the read-out layer. For the hidden layers, ReLU is more common, which is just the max function (negative weights get flattened to 0). The activation function for the hidden layers can be imagined as a race to carry a baton/flame (gradient) without it dying. The sigmoid function flattens out at 0 and at 1, resulting in a flat gradient which is equivalent to the flame dying out (we have lost our signal). The ReLU function helps preserve this gradient.

The back-propagation function is defined as:

```{r eval=FALSE}
backprop <- function(x, y, C, sizes, num_layers, biases, weights)

```

Check out the notebook for the full code — however the principle remains the same: we have a forward-pass where we generate our prediction by propagating the weights through all the layers of the network. We then plug this into the cost gradient and update the weights through all of our layers.

This concludes the creation of a neural network (with as many hidden layers as you desire). It can be a good exercise to replace the hidden-layer activation with ReLU and read-out to be softmax, and also add L1 and L2 regularization. Running this on the iris dataset in the notebook (which contains 4 explanatory variables with 3 possible outomes), with just one hidden-layer containing 40 neurons we get an accuracy of 96% after 30 rounds/epochs of training.

The notebook also runs a 100-neuron handwriting-recognition example to predict the digit corresponding to a 28x28 pixel image.

