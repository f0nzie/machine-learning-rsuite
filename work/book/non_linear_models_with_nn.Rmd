# Multivariate Regression with Neural Networks: Unique, Exact and Generic Models

Source: https://www.datasciencecentral.com/profiles/blogs/6448529:BlogPost:735264?xg_source=activity

Reference: http://neuralnetworksanddeeplearning.com/chap1.html

## The Many Degrees of Freedom {-}
Neural networks are characterized by having a large number of parameters – the many degrees of freedom. So it is natural to expect that many combinations of these parameters can explain the outputs, given the inputs. But this does not always have to be the case. In some situations, even while we have many more parameters than constraints, there is only one possible solution to those parameters. Let us look at how this can happen in the context of neural networks.

### Multiple exact generic models {-}
First the case of multiple exact models all of which are generally valid. That is, irrespective of the training data range used to obtain these models, they predict the exact output for any input. Consider a simple neural net in Figure (\@ref(fig:fig1)) that uses one hidden layer with one neuron. We want to see if we can train this neural net to add any two inputs $x_1$ and $x_2$. What model(s) will it come up with in its attempt to minimize the error/cost is the question.

(ref:fig1) **Multiple Exact Models** are possible even in the linear case when we have a hidden layer.

```{r fig1, fig.cap="(ref:fig1)", out.width="400px", echo=FALSE}
knitr::include_graphics(file.path(assets_dir, "nn_1h_layer.png"))
```

The input to a neuron in any non-input layer is taken to be a linearly weighted function of the outputs (i.e. activations) of the neurons in the previous layer. We use identity as the activation function here, so the output of a neuron is the same as the input it gets. With the notation $b$ as the bias, $w$ the weights, $z$ the input, and $a$ the activation – the equations shown in Figure \@ref(fig:fig1) follow directly. Requiring that the output activation $a_3$ be equal to $y$, i.e. $x_1 + x_2$, we get:


\begin{equation}
  x_1 + x_2 = w_3 w_1 x_1 + w_3 w_2 x_2 + (w_3b_2 + b_3)
  (\#eq:eq1)
\end{equation}


For Equation \@ref(eq:eq1) to be true for all inputs, $x_1$ and $x_2$, we would need:


\begin{equation}
  w_1 w_3 = w_2 w_3 \equiv 1, \qquad w_3 b_2 + b_3 \equiv 0
  (\#eq:eq2)
\end{equation}



With five unknowns and three equations we have two degrees of freedom. Clearly, we are going to get multiple solutions. Choosing $b_1$ and $b_2$, (both $\neq 0$), as the independent variables, we get:

\begin{equation} 
w_1 = w_2 = -\frac{b_2}{b_3} \qquad \qquad w_3 = -\frac{b_3}{b_2} 
(\#eq:eq3)
\end{equation}


Table 1 shows results from the neural network (Figure \@ref(fig:fig1)) that has been trained with identical data, but with different initial values for $w$ and $b$. Each run drives the  cost (sum of the squared errors) to near zero, but yields a different final model. We see that the converged model in each case closely obeys Equation \@ref(eq:eq3), so that the model has generic validity for any and all inputs – not just the training data range.


### Unique exact generic model {-}
Let us now remove the hidden layer so the neural network is as shown in Figure \@ref(fig:fig2).

(ref:fig2) **Unique Exact Model**. The only possible solution is obtained for any initial guess.

```{r fig2, fig.cap="(ref:fig2)", out.width="400px", echo=FALSE}
knitr::include_graphics(file.path(assets_dir, "nn_no_hidden_layer.png"))
```

Requiring again that the output activation $a_3$ be equal to $x_1 + x_2$, we get:

\begin{equation} 
    y = x_1 + x_2 = w_1 x_1 + w_2 x_2 + b_3
    (\#eq:eq4)
\end{equation}


The only possible solution to Equation  \@ref(eq:eq4) that works for all $x_1$ and $x_2$ is:


\begin{equation} 
    w_1 = w_2 \equiv 1 \qquad \text{ and, } \qquad b_3 \equiv 0 
    (\#eq:eq5)
\end{equation}


This is unlike the situation when we used the hidden layer. Given that there is only one solution, the neural net has to obtain it if it is going to converge. Table 2 below bears out this result from simulating the above neural network with different initial guesses for $b$ and $w$. We do in fact obtain the only possible solution in all cases trying to minimize the cost function.


##  Nonlinear Models {-}
The requirement that the outputs be a linear function of the inputs for obtaining exact models is limiting. But we can accommodate the cases when the outputs can reasonably be approximated as *polynomials* in terms of the inputs.

### Single input and response {-}
A simple example is a single output y being a polynomial of order r in a single input x.

\begin{equation}
  y = w_0 + w_1 x + w_2 x^2 + \cdots + w_r x^r 
  (\#eq:eq6)
\end{equation}


Given $n$ measurements of $x$ and $\hat{y}$ we have:

\begin{equation}
\begin{aligned}
\begin{bmatrix} 
    \hat{y}_1 \\ 
    \hat{y}_2 \\ 
    \vdots \\ 
    \hat{y}_n 
\end{bmatrix} & =
\begin{bmatrix} 1 & x_1 & x_1^2 & x_1^3 & \cdots & x_1^r  \\
    1 & x_2 & x_2^2 & x_2^3 & \cdots & x_1^r  \\
    \vdots \\
    1 & x_n & x_n^2 & x_n^3 & \cdots & x_n^r  \\
\end{bmatrix} \cdot
\begin{bmatrix} {w}_0 \\ 
    {w}_1 \\ 
    \vdots \\ 
    {w}_r 
\end{bmatrix}
\end{aligned}
(\#eq:eq7)
\end{equation}


A least squares estimate $\underline{\hat{w}}$, that minimizes $\left(\underline{y} \cdot  \underline{\hat{y}}\right)^T \cdot \left(\underline{y}  - \underline{\hat{y}}\right)$, based on these measurements is known.


\begin{equation} \underline{\hat{w}} = \left( {\underline{\underline{X}}}^T {\underline{\underline{X}}} \right)^{-1} {\underline{\underline{X}}}^T \underline{\widehat{y}}
(\#eq:eq8)
\end{equation}



### Multiple inputs and responses {-}
Extending the above to multiple inputs and outputs (the multivariate case) is straightforward. Say we have $m$ outputs/responses, and $q$ actual inputs/predictors. Each measurement for a response has a form like Equation 6 but extended to include all the $q$ predictors. It is a polynomial of degree $r$ in each predictor so we will have $qr + 1$ coefficients in the equation. In compact matrix notation:

\begin{equation}
\underbrace{\underline{\underline{Y}}}_{n \times m} = \underbrace{\underline{\underline{X}}}_{n \times (qr+1)} \cdot \underbrace{\underline{\underline{W}}}_{ (qr+1) \times m } 
(\#eq:eq9)
\end{equation}


Appealing to the single response/input case in Equations \@ref(eq:eq6) and \@ref(eq:eq7) it is easy to understand the following about Equation \@ref(eq:eq9).

* $\underline{\underline{Y}}$ above is simply the $m$ response vectors (of length $n$, the number of measurements in Equation \@ref(eq:eq9) stacked side-by-side.

* The $k^{th}$ row of $\underline{\underline{Y}}$ represents the $k^{th}$ measurement of all $m$ responses and the $j^{th}$ column of $\underline{\underline{Y}}$ has the $n$ measurements for the $j^{th}$ response.

* Each column of $\underline{\underline{W}}$ has the length $qr +1$, the number of coefficients in the polynomial expression for the corresponding response.

* $\underline{X_0} \equiv \underline{1}$ is the unit column vector. The remaining $qr$ columns are formed from the actual $q$ predictors contributing $r$ columns each. That is, each predictor $z$ contributes $r$ columns with values $\left\{z, z^2, \cdots , z^r \right\}$.


Given the actual measurements $\underline{\underline{\widehat{Y}}}$, the least squares estimate $\underline{\underline{\widehat{W}}}$ is similar to  Equation \@ref(eq:eq8).


\begin{equation} \underline{\underline{\widehat{W}}} = \left( {\underline{\underline{X}}}^T {\underline{\underline{X}}} \right)^{-1} {\underline{\underline{X}}}^T \underline{\underline{\widehat{Y}}}
(\#eq:eq10)
\end{equation}


### The neural net {-}
Now we are ready to build a neural net that will obtain the unique exact model representing a polynomial relationship between inputs and outputs. We have to use $r-1$ extra inputs $\left\{ x_1^2, \cdots , x_1^r\right\}$ for each actual input measurement $x_1$, as we are targeting an $r^{th}$ degree polynomial for the outputs in each predictor variable. This is the price we have to pay in order to make the outputs a linear function of the inputs so we can use our hidden layer free neural network to obtain the unique exact model.

(ref:fig3) **Unique and exact** polynomial representation with a neural net model

```{r fig3, fig.cap="(ref:fig3)", out.width="500px", echo=FALSE}
knitr::include_graphics(file.path(assets_dir, "nn_polynomial.png"))
```

Having gotten all this down we will henceforth  simply use the symbol $p$ for the number of predictors, instead of $qr$. This is for ease of notation. The net will naturally have $p+1$ input neurons (with input $x_0 \equiv 1$), $m$ output neurons, no hidden layers, and employs linear input summation, and identity as the activation function, as shown in Figure \@ref(fig:fig3).

Using the sum of squares of differences at the output layer as the cost function we have:


\begin{equation}
\begin{aligned}
C = & \frac{1}{2} \sum_{i=1}^{m} \left( y_i - \hat{y}_i \right)^2 =\frac{1}{2} \sum_{i=1}^{m} \left( \sum_{k=0}^{p} W_{ik} x_k - \hat{y}_i \right)^2 \\ 
  \frac{dC}{dW_{ij}} = & x_j \left( y_i - \hat{y}_i \right) = x_j \left( \sum_{k=0}^{p} W_{ik} x_k - \hat{y}_i \right) \\ 
  \frac{d^2C}{dW_{ij}^{2}} = & x_j^2 > 0 \qquad \forall x_j \neq 0 
  (\#eq:eq11)
\end{aligned}
\end{equation}


It follows from the second derivative above that the cost function is convex in $\left\{W_{ij}\right\}$ for all input data $\underline{x}$.  So we are going to march towards a model achieving the global minimum no matter what training data we use.


## Conclusions {-}
We have gone over some of the basics of the problem set up with neural networks to obtain unique, exact, and generalized target models. Building and training the network, code snippets, simulations, convergence, stability, etc., will make this post too long so will be covered in an upcoming blog.





