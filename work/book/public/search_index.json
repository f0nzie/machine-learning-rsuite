[
["index.html", "A Machine Learning Book Prerequisites", " A Machine Learning Book Alfonso R. Reyes 2019-09-20 Prerequisites This is a sample book written in Markdown. You can use anything that Pandoc’s Markdown supports, e.g., a math equation \\(a^2 + b^2 = c^2\\). The bookdown package can be installed from CRAN or Github: install.packages(&quot;bookdown&quot;) # or the development version # devtools::install_github(&quot;rstudio/bookdown&quot;) "],
["introduction-to-h2o-.html", "Chapter 1 Introduction to h2o. 1.1 Bad Loans dataset. (GLM, RF, GBM, DL, NB) 1.2 Introduction 1.3 Algorithms 1.4 GLM 1.5 Random Forest 1.6 Gradient Boosting Machine (GBM) 1.7 Deep Learning 1.8 Naive Bayes model", " Chapter 1 Introduction to h2o. 1.1 Bad Loans dataset. (GLM, RF, GBM, DL, NB) Source: https://github.com/h2oai/h2o-tutorials/blob/master/h2o-open-tour-2016/chicago/intro-to-h2o.R 1.2 Introduction Introductory H2O Machine Learning Tutorial Prepared for H2O Open Chicago 2016: http://open.h2o.ai/chicago.html 1.2.1 Install and download h2o First step is to download &amp; install the h2o R library The latest version is available by clicking on the R tab here: http://h2o-release.s3.amazonaws.com/h2o/latest_stable.html Load the H2O library and start up the H2O cluster locally on your machine: # Load the H2O library and start up the H2O cluster locally on your machine library(h2o) h2o.init(nthreads = -1, #Number of threads -1 means use all cores on your machine max_mem_size = &quot;8G&quot;) #max mem size is the maximum memory to allocate to H2O #&gt; #&gt; H2O is not running yet, starting it now... #&gt; #&gt; Note: In case of errors look at the following log files: #&gt; /tmp/RtmpHWWDtc/h2o_datascience_started_from_r.out #&gt; /tmp/RtmpHWWDtc/h2o_datascience_started_from_r.err #&gt; #&gt; #&gt; Starting H2O JVM and connecting: . Connection successful! #&gt; #&gt; R is connected to the H2O cluster: #&gt; H2O cluster uptime: 1 seconds 457 milliseconds #&gt; H2O cluster timezone: America/Chicago #&gt; H2O data parsing timezone: UTC #&gt; H2O cluster version: 3.22.1.1 #&gt; H2O cluster version age: 8 months and 22 days !!! #&gt; H2O cluster name: H2O_started_from_R_datascience_mwl453 #&gt; H2O cluster total nodes: 1 #&gt; H2O cluster total memory: 7.11 GB #&gt; H2O cluster total cores: 8 #&gt; H2O cluster allowed cores: 8 #&gt; H2O cluster healthy: TRUE #&gt; H2O Connection ip: localhost #&gt; H2O Connection port: 54321 #&gt; H2O Connection proxy: NA #&gt; H2O Internal Security: FALSE #&gt; H2O API Extensions: XGBoost, Algos, AutoML, Core V3, Core V4 #&gt; R Version: R version 3.6.0 (2019-04-26) 1.2.2 Load the dataset Next we will import a cleaned up version of the Lending Club “Bad Loans” dataset The purpose here is to predict whether a loan will be bad (not repaid to the lender). The response column, bad_loan, is 1 if the loan was bad, and 0 otherwise Import the data loan_csv &lt;- &quot;/Volumes/H2OTOUR/loan.csv&quot; Alternatively, you can import the data directly from a URL # modify this for your machine loan_csv &lt;- &quot;https://raw.githubusercontent.com/h2oai/app-consumer-loan/master/data/loan.csv&quot; data &lt;- h2o.importFile(loan_csv) # 163,987 rows x 15 columns #&gt; | | | 0% | |================ | 25% | |========================== | 41% | |======================================= | 59% | |======================================================= | 84% | |=================================================================| 100% dim(data) #&gt; [1] 163987 15 # [1] 163987 15 url &lt;- &quot;https://raw.githubusercontent.com/h2oai/app-consumer-loan/master/data/loan.csv&quot; loans &lt;- read.csv(url) write.csv(loans, file = file.path(data_raw_dir, &quot;loan.csv&quot;)) 1.2.3 Feature Engineering Since we want to train a binary classification model, we must ensure that the response is coded as a factor If the response is 0/1, H2O will assume it’s numeric, which means that H2O will train a regression model instead data$bad_loan &lt;- as.factor(data$bad_loan) #encode the binary repsonse as a factor h2o.levels(data$bad_loan) #optional: after encoding, this shows the two factor levels, &#39;0&#39; and &#39;1&#39; #&gt; [1] &quot;0&quot; &quot;1&quot; # [1] &quot;0&quot; &quot;1&quot; 1.2.4 Partition data Partition the data into training, validation and test sets # Partition the data into training, validation and test sets splits &lt;- h2o.splitFrame(data = data, ratios = c(0.7, 0.15), #partition data into 70%, 15%, 15% chunks seed = 1) #setting a seed will guarantee reproducibility train &lt;- splits[[1]] valid &lt;- splits[[2]] test &lt;- splits[[3]] Take a look at the size of each partition Notice that h2o.splitFrame uses approximate splitting not exact splitting (for efficiency) so these are not exactly 70%, 15% and 15% of the total rows nrow(train) # 114908 #&gt; [1] 114908 nrow(valid) # 24498 #&gt; [1] 24498 nrow(test) # 24581 #&gt; [1] 24581 1.2.5 Identify response and predictor variables # Identify response and predictor variables y &lt;- &quot;bad_loan&quot; x &lt;- setdiff(names(data), c(y, &quot;int_rate&quot;)) # remove the interest rate column because it&#39;s correlated with the outcome print(x) #&gt; [1] &quot;loan_amnt&quot; &quot;term&quot; #&gt; [3] &quot;emp_length&quot; &quot;home_ownership&quot; #&gt; [5] &quot;annual_inc&quot; &quot;purpose&quot; #&gt; [7] &quot;addr_state&quot; &quot;dti&quot; #&gt; [9] &quot;delinq_2yrs&quot; &quot;revol_util&quot; #&gt; [11] &quot;total_acc&quot; &quot;longest_credit_length&quot; #&gt; [13] &quot;verification_status&quot; # [1] &quot;loan_amnt&quot; &quot;term&quot; # [3] &quot;emp_length&quot; &quot;home_ownership&quot; # [5] &quot;annual_inc&quot; &quot;verification_status&quot; # [7] &quot;purpose&quot; &quot;addr_state&quot; # [9] &quot;dti&quot; &quot;delinq_2yrs&quot; # [11] &quot;revol_util&quot; &quot;total_acc&quot; # [13] &quot;longest_credit_length&quot; 1.3 Algorithms Now that we have prepared the data, we can train some models We will start by training a single model from each of the H2O supervised algos: Generalized Linear Model (GLM) Random Forest (RF) Gradient Boosting Machine (GBM) Deep Learning (DL) Naive Bayes (NB) 1.4 GLM Let’s start with a basic binomial Generalized Linear Model By default, h2o.glm uses a regularized, elastic net model glm_fit1 &lt;- h2o.glm(x = x, y = y, training_frame = train, model_id = &quot;glm_fit1&quot;, family = &quot;binomial&quot;) #similar to R&#39;s glm, h2o.glm has the family argument #&gt; | | | 0% | |======== | 12% | |=================================================================| 100% Next we will do some automatic tuning by passing in a validation frame and setting lambda_search = True. Since we are training a GLM with regularization, we should try to find the right amount of regularization (to avoid overfitting). The model parameter, lambda, controls the amount of regularization in a GLM model and we can find the optimal value for lambda automatically by setting lambda_search = TRUE and passing in a validation frame (which is used to evaluate model performance using a particular value of lambda). glm_fit2 &lt;- h2o.glm(x = x, y = y, training_frame = train, model_id = &quot;glm_fit2&quot;, validation_frame = valid, family = &quot;binomial&quot;, lambda_search = TRUE) #&gt; | | | 0% | |================== | 27% | |==================================== | 56% | |=================================================================| 100% Let’s compare the performance of the two GLMs # Let&#39;s compare the performance of the two GLMs glm_perf1 &lt;- h2o.performance(model = glm_fit1, newdata = test) glm_perf2 &lt;- h2o.performance(model = glm_fit2, newdata = test) # Print model performance glm_perf1 #&gt; H2OBinomialMetrics: glm #&gt; #&gt; MSE: 0.142 #&gt; RMSE: 0.377 #&gt; LogLoss: 0.451 #&gt; Mean Per-Class Error: 0.37 #&gt; AUC: 0.677 #&gt; pr_auc: 0.327 #&gt; Gini: 0.355 #&gt; R^2: 0.0639 #&gt; Residual Deviance: 22176 #&gt; AIC: 22280 #&gt; #&gt; Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold: #&gt; 0 1 Error Rate #&gt; 0 13647 6344 0.317343 =6344/19991 #&gt; 1 1939 2651 0.422440 =1939/4590 #&gt; Totals 15586 8995 0.336968 =8283/24581 #&gt; #&gt; Maximum Metrics: Maximum metrics at their respective thresholds #&gt; metric threshold value idx #&gt; 1 max f1 0.193323 0.390283 222 #&gt; 2 max f2 0.118600 0.556655 307 #&gt; 3 max f0point5 0.276272 0.354086 146 #&gt; 4 max accuracy 0.494244 0.814410 29 #&gt; 5 max precision 0.744500 1.000000 0 #&gt; 6 max recall 0.001225 1.000000 399 #&gt; 7 max specificity 0.744500 1.000000 0 #&gt; 8 max absolute_mcc 0.198334 0.210606 216 #&gt; 9 max min_per_class_accuracy 0.180070 0.627783 236 #&gt; 10 max mean_per_class_accuracy 0.193323 0.630109 222 #&gt; #&gt; Gains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)` glm_perf2 #&gt; H2OBinomialMetrics: glm #&gt; #&gt; MSE: 0.142 #&gt; RMSE: 0.377 #&gt; LogLoss: 0.451 #&gt; Mean Per-Class Error: 0.372 #&gt; AUC: 0.677 #&gt; pr_auc: 0.326 #&gt; Gini: 0.354 #&gt; R^2: 0.0635 #&gt; Residual Deviance: 22186 #&gt; AIC: 22282 #&gt; #&gt; Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold: #&gt; 0 1 Error Rate #&gt; 0 13699 6292 0.314742 =6292/19991 #&gt; 1 1968 2622 0.428758 =1968/4590 #&gt; Totals 15667 8914 0.336032 =8260/24581 #&gt; #&gt; Maximum Metrics: Maximum metrics at their respective thresholds #&gt; metric threshold value idx #&gt; 1 max f1 0.194171 0.388329 216 #&gt; 2 max f2 0.119200 0.555998 306 #&gt; 3 max f0point5 0.256488 0.351893 153 #&gt; 4 max accuracy 0.474001 0.814654 32 #&gt; 5 max precision 0.736186 1.000000 0 #&gt; 6 max recall 0.001255 1.000000 399 #&gt; 7 max specificity 0.736186 1.000000 0 #&gt; 8 max absolute_mcc 0.198114 0.208337 212 #&gt; 9 max min_per_class_accuracy 0.180131 0.625181 231 #&gt; 10 max mean_per_class_accuracy 0.194171 0.628250 216 #&gt; #&gt; Gains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)` Instead of printing the entire model performance metrics object, it is probably easier to print just the metric that you are interested in comparing. Retreive test set AUC h2o.auc(glm_perf1) #0.677449084114 #&gt; [1] 0.677 h2o.auc(glm_perf2) #0.677675858276 #&gt; [1] 0.677 Compare test AUC to the training AUC and validation AUC # Compare test AUC to the training AUC and validation AUC h2o.auc(glm_fit2, train = TRUE) #0.674306164325 #&gt; [1] 0.673 h2o.auc(glm_fit2, valid = TRUE) #0.675512216705 #&gt; [1] 0.675 glm_fit2@model$validation_metrics #0.675512216705 #&gt; H2OBinomialMetrics: glm #&gt; ** Reported on validation data. ** #&gt; #&gt; MSE: 0.142 #&gt; RMSE: 0.377 #&gt; LogLoss: 0.451 #&gt; Mean Per-Class Error: 0.37 #&gt; AUC: 0.675 #&gt; pr_auc: 0.316 #&gt; Gini: 0.351 #&gt; R^2: 0.0597 #&gt; Residual Deviance: 22101 #&gt; AIC: 22197 #&gt; #&gt; Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold: #&gt; 0 1 Error Rate #&gt; 0 13591 6365 0.318952 =6365/19956 #&gt; 1 1916 2626 0.421841 =1916/4542 #&gt; Totals 15507 8991 0.338028 =8281/24498 #&gt; #&gt; Maximum Metrics: Maximum metrics at their respective thresholds #&gt; metric threshold value idx #&gt; 1 max f1 0.193519 0.388088 217 #&gt; 2 max f2 0.116436 0.555055 308 #&gt; 3 max f0point5 0.288405 0.343386 132 #&gt; 4 max accuracy 0.487882 0.815250 29 #&gt; 5 max precision 0.576333 0.681818 9 #&gt; 6 max recall 0.004789 1.000000 398 #&gt; 7 max specificity 0.715719 0.999950 0 #&gt; 8 max absolute_mcc 0.195417 0.209494 215 #&gt; 9 max min_per_class_accuracy 0.180760 0.627731 230 #&gt; 10 max mean_per_class_accuracy 0.192639 0.629672 218 #&gt; #&gt; Gains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)` 1.5 Random Forest H2O’s Random Forest (RF) implements a distributed version of the standard Random Forest algorithm and variable importance measures. First we will train a basic Random Forest model with default parameters. The Random Forest model will infer the response distribution from the response encoding. A seed is required for reproducibility. rf_fit1 &lt;- h2o.randomForest(x = x, y = y, training_frame = train, model_id = &quot;rf_fit1&quot;, seed = 1) #&gt; | | | 0% | |=== | 4% | |======== | 12% | |============== | 22% | |===================== | 32% | |============================== | 46% | |======================================== | 62% | |================================================= | 76% | |========================================================== | 90% | |=================================================================| 100% Next we will increase the number of trees used in the forest by setting ntrees = 100. The default number of trees in an H2O Random Forest is 50, so this RF will be twice as big as the default. Usually increasing the number of trees in a RF will increase performance as well. Unlike Gradient Boosting Machines (GBMs), Random Forests are fairly resistant (although not free from) overfitting. See the GBM example below for additional guidance on preventing overfitting using H2O’s early stopping functionality. rf_fit2 &lt;- h2o.randomForest(x = x, y = y, training_frame = train, model_id = &quot;rf_fit2&quot;, #validation_frame = valid, #only used if stopping_rounds &gt; 0 ntrees = 100, seed = 1) #&gt; | | | 0% | |=== | 5% | |======= | 11% | |========== | 16% | |============== | 21% | |================== | 28% | |======================= | 35% | |=========================== | 42% | |================================ | 49% | |==================================== | 56% | |========================================= | 63% | |============================================== | 71% | |=================================================== | 78% | |======================================================= | 85% | |============================================================ | 92% | |=================================================================| 100% Let’s compare the performance of the two RFs # Let&#39;s compare the performance of the two RFs rf_perf1 &lt;- h2o.performance(model = rf_fit1, newdata = test) rf_perf2 &lt;- h2o.performance(model = rf_fit2, newdata = test) # Print model performance rf_perf1 #&gt; H2OBinomialMetrics: drf #&gt; #&gt; MSE: 0.144 #&gt; RMSE: 0.379 #&gt; LogLoss: 0.459 #&gt; Mean Per-Class Error: 0.379 #&gt; AUC: 0.663 #&gt; pr_auc: 0.311 #&gt; Gini: 0.327 #&gt; #&gt; Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold: #&gt; 0 1 Error Rate #&gt; 0 12842 7149 0.357611 =7149/19991 #&gt; 1 1839 2751 0.400654 =1839/4590 #&gt; Totals 14681 9900 0.365648 =8988/24581 #&gt; #&gt; Maximum Metrics: Maximum metrics at their respective thresholds #&gt; metric threshold value idx #&gt; 1 max f1 0.193397 0.379710 229 #&gt; 2 max f2 0.077797 0.547982 344 #&gt; 3 max f0point5 0.277524 0.344251 158 #&gt; 4 max accuracy 0.543639 0.813799 29 #&gt; 5 max precision 0.817454 1.000000 0 #&gt; 6 max recall 0.001181 1.000000 399 #&gt; 7 max specificity 0.817454 1.000000 0 #&gt; 8 max absolute_mcc 0.252480 0.195090 178 #&gt; 9 max min_per_class_accuracy 0.186549 0.619826 235 #&gt; 10 max mean_per_class_accuracy 0.192603 0.620890 230 #&gt; #&gt; Gains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)` rf_perf2 #&gt; H2OBinomialMetrics: drf #&gt; #&gt; MSE: 0.143 #&gt; RMSE: 0.378 #&gt; LogLoss: 0.454 #&gt; Mean Per-Class Error: 0.377 #&gt; AUC: 0.669 #&gt; pr_auc: 0.32 #&gt; Gini: 0.339 #&gt; #&gt; Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold: #&gt; 0 1 Error Rate #&gt; 0 13172 6819 0.341103 =6819/19991 #&gt; 1 1891 2699 0.411983 =1891/4590 #&gt; Totals 15063 9518 0.354339 =8710/24581 #&gt; #&gt; Maximum Metrics: Maximum metrics at their respective thresholds #&gt; metric threshold value idx #&gt; 1 max f1 0.196407 0.382620 225 #&gt; 2 max f2 0.092270 0.549691 331 #&gt; 3 max f0point5 0.291396 0.349342 144 #&gt; 4 max accuracy 0.555908 0.813840 20 #&gt; 5 max precision 0.651522 0.785714 5 #&gt; 6 max recall 0.004212 1.000000 398 #&gt; 7 max specificity 0.711667 0.999950 0 #&gt; 8 max absolute_mcc 0.229251 0.204236 194 #&gt; 9 max min_per_class_accuracy 0.184594 0.619829 235 #&gt; 10 max mean_per_class_accuracy 0.196407 0.623457 225 #&gt; #&gt; Gains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)` # Retreive test set AUC h2o.auc(rf_perf1) # 0.662266990734 #&gt; [1] 0.663 h2o.auc(rf_perf2) # 0.66525468051 #&gt; [1] 0.669 1.5.1 Cross-validate performance Rather than using held-out test set to evaluate model performance, a user may wish to estimate model performance using cross-validation. Using the RF algorithm (with default model parameters) as an example, we demonstrate how to perform k-fold cross-validation using H2O. No custom code or loops are required, you simply specify the number of desired folds in the nfolds argument. Since we are not going to use a test set here, we can use the original (full) dataset, which we called data rather than the subsampled train dataset. Note that this will take approximately k (nfolds) times longer than training a single RF model, since it will train k models in the cross-validation process (trained on n(k-1)/k rows), in addition to the final model trained on the full training_frame dataset with n rows. rf_fit3 &lt;- h2o.randomForest(x = x, y = y, training_frame = train, model_id = &quot;rf_fit3&quot;, seed = 1, nfolds = 5) #&gt; | | | 0% | |= | 1% | |== | 2% | |== | 4% | |=== | 5% | |===== | 7% | |====== | 10% | |======== | 12% | |========= | 14% | |========== | 16% | |=========== | 17% | |============ | 19% | |============= | 20% | |============== | 21% | |=============== | 23% | |================ | 25% | |================== | 28% | |==================== | 30% | |===================== | 32% | |====================== | 34% | |======================= | 35% | |======================== | 37% | |======================== | 38% | |========================== | 39% | |=========================== | 42% | |============================= | 44% | |============================== | 46% | |=============================== | 48% | |================================ | 50% | |================================== | 52% | |================================== | 53% | |=================================== | 54% | |==================================== | 55% | |====================================== | 58% | |======================================= | 60% | |========================================= | 63% | |========================================== | 65% | |=========================================== | 67% | |============================================ | 68% | |============================================= | 69% | |============================================== | 71% | |=============================================== | 72% | |================================================ | 74% | |================================================== | 77% | |=================================================== | 79% | |===================================================== | 81% | |====================================================== | 83% | |====================================================== | 84% | |======================================================== | 86% | |========================================================= | 88% | |========================================================== | 89% | |============================================================ | 92% | |============================================================= | 94% | |=============================================================== | 96% | |================================================================ | 99% | |=================================================================| 100% To evaluate the cross-validated AUC, do the following: # To evaluate the cross-validated AUC, do the following: h2o.auc(rf_fit3, xval = TRUE) # 0.661201482614 #&gt; [1] 0.659 1.6 Gradient Boosting Machine (GBM) H2O’s Gradient Boosting Machine (GBM) offers a Stochastic GBM, which can increase performance quite a bit compared to the original GBM implementation. Now we will train a basic GBM model The GBM model will infer the response distribution from the response encoding if not specified explicitly through the distribution argument. A seed is required for reproducibility. gbm_fit1 &lt;- h2o.gbm(x = x, y = y, training_frame = train, model_id = &quot;gbm_fit1&quot;, seed = 1) #&gt; | | | 0% | |============== | 22% | |============================= | 44% | |============================================== | 70% | |============================================================== | 96% | |=================================================================| 100% Next we will increase the number of trees used in the GBM by setting ntrees=500. The default number of trees in an H2O GBM is 50, so this GBM will trained using ten times the default. Increasing the number of trees in a GBM is one way to increase performance of the model, however, you have to be careful not to overfit your model to the training data by using too many trees. To automatically find the optimal number of trees, you must use H2O’s early stopping functionality. This example will not do that, however, the following # example will. gbm_fit2 &lt;- h2o.gbm(x = x, y = y, training_frame = train, model_id = &quot;gbm_fit2&quot;, #validation_frame = valid, #only used if stopping_rounds &gt; 0 ntrees = 500, seed = 1) #&gt; | | | 0% | |== | 3% | |==== | 6% | |====== | 9% | |======= | 11% | |=========== | 17% | |=============== | 23% | |======================= | 35% | |==================================== | 55% | |================================================== | 77% | |================================================================ | 98% | |=================================================================| 100% We will again set ntrees = 500, however, this time we will use early stopping in order to prevent overfitting (from too many trees). All of H2O’s algorithms have early stopping available, however early stopping is not enabled by default (with the exception of Deep Learning). There are several parameters that should be used to control early stopping. The three that are common to all the algorithms are: stopping_rounds, stopping_metric and stopping_tolerance. The stopping metric is the metric by which you’d like to measure performance, and so we will choose AUC here. The score_tree_interval is a parameter specific to the Random Forest model and the GBM. Setting score_tree_interval = 5 will score the model after every five trees. The parameters we have set below specify that the model will stop training after there have been three scoring intervals where the AUC has not increased more than 0.0005. Since we have specified a validation frame, the stopping tolerance will be computed on validation AUC rather than training AUC. gbm_fit3 &lt;- h2o.gbm(x = x, y = y, training_frame = train, model_id = &quot;gbm_fit3&quot;, validation_frame = valid, #only used if stopping_rounds &gt; 0 ntrees = 500, score_tree_interval = 5, #used for early stopping stopping_rounds = 3, #used for early stopping stopping_metric = &quot;AUC&quot;, #used for early stopping stopping_tolerance = 0.0005, #used for early stopping seed = 1) #&gt; | | | 0% | |=== | 4% | |===== | 8% | |======== | 13% | |=========== | 17% | |=================================================================| 100% Let’s compare the performance of the two GBMs # Let&#39;s compare the performance of the two GBMs gbm_perf1 &lt;- h2o.performance(model = gbm_fit1, newdata = test) gbm_perf2 &lt;- h2o.performance(model = gbm_fit2, newdata = test) gbm_perf3 &lt;- h2o.performance(model = gbm_fit3, newdata = test) # Print model performance gbm_perf1 #&gt; H2OBinomialMetrics: gbm #&gt; #&gt; MSE: 0.141 #&gt; RMSE: 0.376 #&gt; LogLoss: 0.448 #&gt; Mean Per-Class Error: 0.367 #&gt; AUC: 0.684 #&gt; pr_auc: 0.332 #&gt; Gini: 0.368 #&gt; #&gt; Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold: #&gt; 0 1 Error Rate #&gt; 0 12143 7848 0.392577 =7848/19991 #&gt; 1 1571 3019 0.342266 =1571/4590 #&gt; Totals 13714 10867 0.383182 =9419/24581 #&gt; #&gt; Maximum Metrics: Maximum metrics at their respective thresholds #&gt; metric threshold value idx #&gt; 1 max f1 0.171139 0.390632 251 #&gt; 2 max f2 0.108885 0.560103 328 #&gt; 3 max f0point5 0.285149 0.356430 145 #&gt; 4 max accuracy 0.510077 0.814410 27 #&gt; 5 max precision 0.601699 0.636364 8 #&gt; 6 max recall 0.037543 1.000000 398 #&gt; 7 max specificity 0.719189 0.999950 0 #&gt; 8 max absolute_mcc 0.220471 0.215401 199 #&gt; 9 max min_per_class_accuracy 0.176689 0.628540 244 #&gt; 10 max mean_per_class_accuracy 0.170225 0.632624 252 #&gt; #&gt; Gains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)` gbm_perf2 #&gt; H2OBinomialMetrics: gbm #&gt; #&gt; MSE: 0.142 #&gt; RMSE: 0.376 #&gt; LogLoss: 0.449 #&gt; Mean Per-Class Error: 0.367 #&gt; AUC: 0.684 #&gt; pr_auc: 0.329 #&gt; Gini: 0.368 #&gt; #&gt; Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold: #&gt; 0 1 Error Rate #&gt; 0 13661 6330 0.316642 =6330/19991 #&gt; 1 1918 2672 0.417865 =1918/4590 #&gt; Totals 15579 9002 0.335544 =8248/24581 #&gt; #&gt; Maximum Metrics: Maximum metrics at their respective thresholds #&gt; metric threshold value idx #&gt; 1 max f1 0.189615 0.393172 234 #&gt; 2 max f2 0.096969 0.558918 333 #&gt; 3 max f0point5 0.278776 0.359560 160 #&gt; 4 max accuracy 0.521287 0.814287 37 #&gt; 5 max precision 0.901295 1.000000 0 #&gt; 6 max recall 0.018504 1.000000 398 #&gt; 7 max specificity 0.901295 1.000000 0 #&gt; 8 max absolute_mcc 0.231089 0.217255 196 #&gt; 9 max min_per_class_accuracy 0.174914 0.630834 249 #&gt; 10 max mean_per_class_accuracy 0.156944 0.633792 267 #&gt; #&gt; Gains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)` gbm_perf3 #&gt; H2OBinomialMetrics: gbm #&gt; #&gt; MSE: 0.141 #&gt; RMSE: 0.376 #&gt; LogLoss: 0.448 #&gt; Mean Per-Class Error: 0.367 #&gt; AUC: 0.684 #&gt; pr_auc: 0.331 #&gt; Gini: 0.369 #&gt; #&gt; Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold: #&gt; 0 1 Error Rate #&gt; 0 13652 6339 0.317093 =6339/19991 #&gt; 1 1917 2673 0.417647 =1917/4590 #&gt; Totals 15569 9012 0.335869 =8256/24581 #&gt; #&gt; Maximum Metrics: Maximum metrics at their respective thresholds #&gt; metric threshold value idx #&gt; 1 max f1 0.189870 0.393030 234 #&gt; 2 max f2 0.109837 0.559580 321 #&gt; 3 max f0point5 0.294571 0.356508 148 #&gt; 4 max accuracy 0.510869 0.814572 40 #&gt; 5 max precision 0.797620 1.000000 0 #&gt; 6 max recall 0.026373 1.000000 397 #&gt; 7 max specificity 0.797620 1.000000 0 #&gt; 8 max absolute_mcc 0.231530 0.218255 197 #&gt; 9 max min_per_class_accuracy 0.176566 0.631808 248 #&gt; 10 max mean_per_class_accuracy 0.175468 0.633400 249 #&gt; #&gt; Gains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)` # Retreive test set AUC h2o.auc(gbm_perf1) # 0.682765594191 #&gt; [1] 0.684 h2o.auc(gbm_perf2) # 0.671854616713 #&gt; [1] 0.684 h2o.auc(gbm_perf3) # 0.68309902855 #&gt; [1] 0.684 To examine the scoring history, use the scoring_history method on a trained model. If score_tree_interval is not specified, it will score at various intervals, as we can see for h2o.scoreHistory() below. However, regular 5-tree intervals are used for h2o.scoreHistory(). The gbm_fit2 was trained only using a training set (no validation set), so the scoring history is calculated for training set performance metrics only. h2o.scoreHistory(gbm_fit2) #&gt; Scoring History: #&gt; timestamp duration number_of_trees training_rmse #&gt; 1 2019-09-20 13:16:06 0.001 sec 0 0.38563 #&gt; 2 2019-09-20 13:16:06 0.130 sec 1 0.38370 #&gt; 3 2019-09-20 13:16:06 0.181 sec 2 0.38206 #&gt; 4 2019-09-20 13:16:06 0.235 sec 3 0.38069 #&gt; 5 2019-09-20 13:16:06 0.281 sec 4 0.37954 #&gt; training_logloss training_auc training_pr_auc training_lift #&gt; 1 0.47403 0.50000 0.00000 1.00000 #&gt; 2 0.46913 0.65779 0.30174 2.68330 #&gt; 3 0.46512 0.66583 0.31164 2.79399 #&gt; 4 0.46184 0.66851 0.31500 2.97100 #&gt; 5 0.45912 0.67011 0.31822 2.97544 #&gt; training_classification_error #&gt; 1 0.81825 #&gt; 2 0.40069 #&gt; 3 0.33325 #&gt; 4 0.34475 #&gt; 5 0.33180 #&gt; #&gt; --- #&gt; timestamp duration number_of_trees training_rmse #&gt; 52 2019-09-20 13:16:10 3.756 sec 51 0.36691 #&gt; 53 2019-09-20 13:16:10 3.855 sec 52 0.36680 #&gt; 54 2019-09-20 13:16:10 3.927 sec 53 0.36668 #&gt; 55 2019-09-20 13:16:10 4.004 sec 54 0.36659 #&gt; 56 2019-09-20 13:16:14 8.015 sec 263 0.36129 #&gt; 57 2019-09-20 13:16:16 10.304 sec 500 0.36129 #&gt; training_logloss training_auc training_pr_auc training_lift #&gt; 52 0.42985 0.71600 0.37577 3.67930 #&gt; 53 0.42960 0.71647 0.37653 3.66973 #&gt; 54 0.42934 0.71694 0.37734 3.70322 #&gt; 55 0.42915 0.71734 0.37776 3.71279 #&gt; 56 0.41770 0.74024 0.41866 4.25823 #&gt; 57 0.41770 0.74024 0.41866 4.25823 #&gt; training_classification_error #&gt; 52 0.29229 #&gt; 53 0.30064 #&gt; 54 0.29613 #&gt; 55 0.30007 #&gt; 56 0.25719 #&gt; 57 0.25719 When early stopping is used, we see that training stopped at 105 trees instead of the full 500. Since we used a validation set in gbm_fit3, both training and validation performance metrics are stored in the scoring history object. Take a look at the validation AUC to observe that the correct stopping tolerance was enforced. h2o.scoreHistory(gbm_fit3) #&gt; Scoring History: #&gt; timestamp duration number_of_trees training_rmse #&gt; 1 2019-09-20 13:16:17 0.005 sec 0 0.38563 #&gt; 2 2019-09-20 13:16:18 0.262 sec 5 0.37853 #&gt; 3 2019-09-20 13:16:18 0.504 sec 10 0.37508 #&gt; 4 2019-09-20 13:16:18 0.739 sec 15 0.37307 #&gt; 5 2019-09-20 13:16:18 0.966 sec 20 0.37152 #&gt; 6 2019-09-20 13:16:19 1.211 sec 25 0.37041 #&gt; 7 2019-09-20 13:16:19 1.469 sec 30 0.36947 #&gt; 8 2019-09-20 13:16:19 1.713 sec 35 0.36877 #&gt; 9 2019-09-20 13:16:19 1.947 sec 40 0.36808 #&gt; 10 2019-09-20 13:16:20 2.182 sec 45 0.36748 #&gt; 11 2019-09-20 13:16:20 2.429 sec 50 0.36699 #&gt; 12 2019-09-20 13:16:20 2.663 sec 55 0.36651 #&gt; 13 2019-09-20 13:16:20 2.897 sec 60 0.36607 #&gt; 14 2019-09-20 13:16:21 3.139 sec 65 0.36574 #&gt; 15 2019-09-20 13:16:21 3.376 sec 70 0.36531 #&gt; 16 2019-09-20 13:16:21 3.616 sec 75 0.36503 #&gt; 17 2019-09-20 13:16:21 3.851 sec 80 0.36460 #&gt; 18 2019-09-20 13:16:22 4.116 sec 85 0.36426 #&gt; 19 2019-09-20 13:16:22 4.361 sec 90 0.36394 #&gt; 20 2019-09-20 13:16:22 4.589 sec 95 0.36362 #&gt; training_logloss training_auc training_pr_auc training_lift #&gt; 1 0.47403 0.50000 0.00000 1.00000 #&gt; 2 0.45676 0.67362 0.32194 3.04518 #&gt; 3 0.44884 0.68117 0.33330 3.19128 #&gt; 4 0.44424 0.68708 0.34105 3.30132 #&gt; 5 0.44060 0.69498 0.34930 3.46878 #&gt; 6 0.43800 0.69983 0.35479 3.47356 #&gt; 7 0.43578 0.70425 0.36014 3.48792 #&gt; 8 0.43410 0.70746 0.36373 3.53576 #&gt; 9 0.43252 0.71082 0.36837 3.61710 #&gt; 10 0.43116 0.71346 0.37174 3.66495 #&gt; 11 0.43002 0.71568 0.37493 3.66016 #&gt; 12 0.42899 0.71765 0.37852 3.70801 #&gt; 13 0.42801 0.71953 0.38157 3.73193 #&gt; 14 0.42726 0.72093 0.38411 3.78934 #&gt; 15 0.42632 0.72277 0.38733 3.82762 #&gt; 16 0.42572 0.72396 0.38955 3.85154 #&gt; 17 0.42477 0.72596 0.39282 3.95202 #&gt; 18 0.42407 0.72713 0.39550 3.99508 #&gt; 19 0.42338 0.72852 0.39819 4.02857 #&gt; 20 0.42272 0.72984 0.40069 4.02378 #&gt; training_classification_error validation_rmse validation_logloss #&gt; 1 0.81825 0.38864 0.47953 #&gt; 2 0.32117 0.38233 0.46398 #&gt; 3 0.32202 0.37958 0.45742 #&gt; 4 0.32027 0.37828 0.45428 #&gt; 5 0.33371 0.37739 0.45210 #&gt; 6 0.32537 0.37676 0.45053 #&gt; 7 0.29722 0.37636 0.44949 #&gt; 8 0.29544 0.37604 0.44866 #&gt; 9 0.28871 0.37587 0.44818 #&gt; 10 0.30181 0.37574 0.44781 #&gt; 11 0.30179 0.37560 0.44744 #&gt; 12 0.29464 0.37552 0.44718 #&gt; 13 0.30343 0.37547 0.44703 #&gt; 14 0.28692 0.37543 0.44694 #&gt; 15 0.28579 0.37536 0.44676 #&gt; 16 0.26903 0.37536 0.44673 #&gt; 17 0.28476 0.37534 0.44664 #&gt; 18 0.26950 0.37537 0.44671 #&gt; 19 0.27036 0.37538 0.44671 #&gt; 20 0.26573 0.37540 0.44673 #&gt; validation_auc validation_pr_auc validation_lift #&gt; 1 0.50000 0.00000 1.00000 #&gt; 2 0.66168 0.30421 2.75098 #&gt; 3 0.66767 0.30959 2.68582 #&gt; 4 0.67061 0.31287 2.70784 #&gt; 5 0.67426 0.31757 2.79590 #&gt; 6 0.67685 0.32121 2.99403 #&gt; 7 0.67866 0.32312 2.97202 #&gt; 8 0.68006 0.32477 3.03806 #&gt; 9 0.68113 0.32493 2.90597 #&gt; 10 0.68183 0.32549 2.88396 #&gt; 11 0.68251 0.32659 2.86194 #&gt; 12 0.68326 0.32626 2.86194 #&gt; 13 0.68354 0.32686 2.88396 #&gt; 14 0.68364 0.32705 2.83993 #&gt; 15 0.68405 0.32754 2.97202 #&gt; 16 0.68423 0.32747 2.88396 #&gt; 17 0.68454 0.32735 2.86194 #&gt; 18 0.68431 0.32727 2.83993 #&gt; 19 0.68434 0.32718 2.95000 #&gt; 20 0.68436 0.32678 2.92799 #&gt; validation_classification_error #&gt; 1 0.81460 #&gt; 2 0.35387 #&gt; 3 0.35285 #&gt; 4 0.39028 #&gt; 5 0.36770 #&gt; 6 0.35240 #&gt; 7 0.34848 #&gt; 8 0.34386 #&gt; 9 0.34807 #&gt; 10 0.38681 #&gt; 11 0.33774 #&gt; 12 0.34215 #&gt; 13 0.34431 #&gt; 14 0.34146 #&gt; 15 0.34329 #&gt; 16 0.33872 #&gt; 17 0.34276 #&gt; 18 0.34256 #&gt; 19 0.34289 #&gt; 20 0.35097 Look at scoring history for third GBM model # Look at scoring history for third GBM model plot(gbm_fit3, timestep = &quot;number_of_trees&quot;, metric = &quot;AUC&quot;) plot(gbm_fit3, timestep = &quot;number_of_trees&quot;, metric = &quot;logloss&quot;) 1.7 Deep Learning H2O’s Deep Learning algorithm is a multilayer feed-forward artificial neural network. It can also be used to train an autoencoder. In this example we will train a standard supervised prediction model. 1.7.1 Train a default DL First we will train a basic DL model with default parameters. The DL model will infer the response distribution from the response encoding if it is not specified explicitly through the distribution argument. H2O’s DL will not be reproducible if it is run on more than a single core, so in this example, the performance metrics below may vary slightly from what you see on your machine. In H2O’s DL, early stopping is enabled by default, so below, it will use the training set and default stopping parameters to perform early stopping. dl_fit1 &lt;- h2o.deeplearning(x = x, y = y, training_frame = train, model_id = &quot;dl_fit1&quot;, seed = 1) #&gt; | | | 0% | |====== | 9% | |=========== | 17% | |================= | 26% | |======================= | 35% | |============================ | 44% | |================================== | 52% | |======================================== | 61% | |============================================= | 70% | |=================================================== | 78% | |========================================================= | 87% | |============================================================== | 96% | |=================================================================| 100% 1.7.2 Train a DL with new architecture and more epochs. Next we will increase the number of epochs used in the GBM by setting epochs=20 (the default is 10). Increasing the number of epochs in a deep neural net may increase performance of the model, however, you have to be careful not to overfit your model to your training data. To automatically find the optimal number of epochs, you must use H2O’s early stopping functionality. Unlike the rest of the H2O algorithms, H2O’s DL will use early stopping by default, so for comparison we will first turn off early stopping. We do this in the next example by setting stopping_rounds=0. dl_fit2 &lt;- h2o.deeplearning(x = x, y = y, training_frame = train, model_id = &quot;dl_fit2&quot;, #validation_frame = valid, #only used if stopping_rounds &gt; 0 epochs = 20, hidden= c(10,10), stopping_rounds = 0, # disable early stopping seed = 1) #&gt; | | | 0% | |============== | 22% | |=============================== | 48% | |================================================ | 74% | |=================================================================| 100% 1.7.3 Train a DL with early stopping This example will use the same model parameters as dl_fit2. This time, we will turn on early stopping and specify the stopping criterion. We will also pass a validation set, as is recommended for early stopping. dl_fit3 &lt;- h2o.deeplearning(x = x, y = y, training_frame = train, model_id = &quot;dl_fit3&quot;, validation_frame = valid, #in DL, early stopping is on by default epochs = 20, hidden = c(10,10), score_interval = 1, #used for early stopping stopping_rounds = 3, #used for early stopping stopping_metric = &quot;AUC&quot;, #used for early stopping stopping_tolerance = 0.0005, #used for early stopping seed = 1) #&gt; | | | 0% | |=========== | 17% | |============================ | 44% | |============================================= | 70% | |============================================================== | 96% | |=================================================================| 100% Let’s compare the performance of the three DL models # Let&#39;s compare the performance of the three DL models dl_perf1 &lt;- h2o.performance(model = dl_fit1, newdata = test) dl_perf2 &lt;- h2o.performance(model = dl_fit2, newdata = test) dl_perf3 &lt;- h2o.performance(model = dl_fit3, newdata = test) # Print model performance dl_perf1 #&gt; H2OBinomialMetrics: deeplearning #&gt; #&gt; MSE: 0.142 #&gt; RMSE: 0.377 #&gt; LogLoss: 0.451 #&gt; Mean Per-Class Error: 0.368 #&gt; AUC: 0.679 #&gt; pr_auc: 0.328 #&gt; Gini: 0.358 #&gt; #&gt; Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold: #&gt; 0 1 Error Rate #&gt; 0 12315 7676 0.383973 =7676/19991 #&gt; 1 1617 2973 0.352288 =1617/4590 #&gt; Totals 13932 10649 0.378056 =9293/24581 #&gt; #&gt; Maximum Metrics: Maximum metrics at their respective thresholds #&gt; metric threshold value idx #&gt; 1 max f1 0.203830 0.390183 226 #&gt; 2 max f2 0.114305 0.557179 318 #&gt; 3 max f0point5 0.291873 0.355524 145 #&gt; 4 max accuracy 0.499597 0.814735 33 #&gt; 5 max precision 0.852920 1.000000 0 #&gt; 6 max recall 0.011283 1.000000 397 #&gt; 7 max specificity 0.852920 1.000000 0 #&gt; 8 max absolute_mcc 0.261090 0.209133 172 #&gt; 9 max min_per_class_accuracy 0.208269 0.630501 221 #&gt; 10 max mean_per_class_accuracy 0.202916 0.631896 227 #&gt; #&gt; Gains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)` dl_perf2 #&gt; H2OBinomialMetrics: deeplearning #&gt; #&gt; MSE: 0.142 #&gt; RMSE: 0.377 #&gt; LogLoss: 0.452 #&gt; Mean Per-Class Error: 0.368 #&gt; AUC: 0.678 #&gt; pr_auc: 0.327 #&gt; Gini: 0.356 #&gt; #&gt; Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold: #&gt; 0 1 Error Rate #&gt; 0 12677 7314 0.365865 =7314/19991 #&gt; 1 1699 2891 0.370153 =1699/4590 #&gt; Totals 14376 10205 0.366665 =9013/24581 #&gt; #&gt; Maximum Metrics: Maximum metrics at their respective thresholds #&gt; metric threshold value idx #&gt; 1 max f1 0.177318 0.390808 237 #&gt; 2 max f2 0.108436 0.553939 309 #&gt; 3 max f0point5 0.344448 0.350937 117 #&gt; 4 max accuracy 0.542031 0.814084 24 #&gt; 5 max precision 0.758606 1.000000 0 #&gt; 6 max recall 0.007360 1.000000 397 #&gt; 7 max specificity 0.758606 1.000000 0 #&gt; 8 max absolute_mcc 0.198214 0.209518 217 #&gt; 9 max min_per_class_accuracy 0.176592 0.630234 238 #&gt; 10 max mean_per_class_accuracy 0.177318 0.631991 237 #&gt; #&gt; Gains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)` dl_perf3 #&gt; H2OBinomialMetrics: deeplearning #&gt; #&gt; MSE: 0.142 #&gt; RMSE: 0.377 #&gt; LogLoss: 0.45 #&gt; Mean Per-Class Error: 0.368 #&gt; AUC: 0.679 #&gt; pr_auc: 0.328 #&gt; Gini: 0.359 #&gt; #&gt; Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold: #&gt; 0 1 Error Rate #&gt; 0 13645 6346 0.317443 =6346/19991 #&gt; 1 1919 2671 0.418083 =1919/4590 #&gt; Totals 15564 9017 0.336235 =8265/24581 #&gt; #&gt; Maximum Metrics: Maximum metrics at their respective thresholds #&gt; metric threshold value idx #&gt; 1 max f1 0.206469 0.392592 215 #&gt; 2 max f2 0.114563 0.556877 310 #&gt; 3 max f0point5 0.301729 0.355131 135 #&gt; 4 max accuracy 0.471461 0.814328 34 #&gt; 5 max precision 0.610998 0.888889 3 #&gt; 6 max recall 0.010050 1.000000 397 #&gt; 7 max specificity 0.658454 0.999950 0 #&gt; 8 max absolute_mcc 0.242030 0.213907 183 #&gt; 9 max min_per_class_accuracy 0.189677 0.630934 231 #&gt; 10 max mean_per_class_accuracy 0.197147 0.632592 224 #&gt; #&gt; Gains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)` # Retreive test set AUC h2o.auc(dl_perf1) # 0.6774335 #&gt; [1] 0.679 h2o.auc(dl_perf2) # 0.678446 #&gt; [1] 0.678 h2o.auc(dl_perf3) # 0.6770498 #&gt; [1] 0.679 Look at scoring history for third DL model # Look at scoring history for third DL model plot(dl_fit3, timestep = &quot;epochs&quot;, metric = &quot;AUC&quot;) 1.7.4 Scoring history # Scoring history h2o.scoreHistory(dl_fit3) #&gt; Scoring History: #&gt; timestamp duration training_speed epochs iterations #&gt; 1 2019-09-20 13:17:00 0.000 sec NA 0.00000 0 #&gt; 2 2019-09-20 13:17:00 0.335 sec 403193 obs/sec 0.87019 1 #&gt; 3 2019-09-20 13:17:01 1.353 sec 501722 obs/sec 5.22209 6 #&gt; 4 2019-09-20 13:17:02 2.392 sec 552705 obs/sec 10.44248 12 #&gt; 5 2019-09-20 13:17:03 3.519 sec 588137 obs/sec 16.53733 19 #&gt; 6 2019-09-20 13:17:04 4.163 sec 604863 obs/sec 20.01859 23 #&gt; 7 2019-09-20 13:17:04 4.246 sec 604545 obs/sec 20.01859 23 #&gt; samples training_rmse training_logloss training_r2 training_auc #&gt; 1 0.000000 NA NA NA NA #&gt; 2 99992.000000 0.38408 0.47737 0.02396 0.66062 #&gt; 3 600060.000000 0.38390 0.47629 0.02490 0.67676 #&gt; 4 1199924.000000 0.37663 0.44944 0.06147 0.67872 #&gt; 5 1900272.000000 0.37625 0.44869 0.06337 0.68175 #&gt; 6 2300296.000000 0.37735 0.45269 0.05788 0.68475 #&gt; 7 2300296.000000 0.37663 0.44944 0.06147 0.67872 #&gt; training_pr_auc training_lift training_classification_error #&gt; 1 NA NA NA #&gt; 2 0.30495 2.66706 0.41616 #&gt; 3 0.31653 2.50377 0.37247 #&gt; 4 0.31834 2.28605 0.35862 #&gt; 5 0.32264 2.77592 0.32292 #&gt; 6 0.32348 2.61263 0.36731 #&gt; 7 0.31834 2.28605 0.35862 #&gt; validation_rmse validation_logloss validation_r2 validation_auc #&gt; 1 NA NA NA NA #&gt; 2 0.38358 0.47517 0.02578 0.66689 #&gt; 3 0.38363 0.47642 0.02553 0.67580 #&gt; 4 0.37599 0.44875 0.06398 0.67987 #&gt; 5 0.37669 0.44979 0.06048 0.67884 #&gt; 6 0.37774 0.45472 0.05523 0.67984 #&gt; 7 0.37599 0.44875 0.06398 0.67987 #&gt; validation_pr_auc validation_lift validation_classification_error #&gt; 1 NA NA NA #&gt; 2 0.30772 2.75187 0.37587 #&gt; 3 0.31883 2.77388 0.36248 #&gt; 4 0.32374 2.79590 0.34966 #&gt; 5 0.31831 2.48769 0.39244 #&gt; 6 0.32072 2.72985 0.33648 #&gt; 7 0.32374 2.79590 0.34966 # Scoring History: # timestamp duration training_speed epochs # 1 2016-05-03 10:33:29 0.000 sec 0.00000 # 2 2016-05-03 10:33:29 0.347 sec 424697 rows/sec 0.86851 # 3 2016-05-03 10:33:30 1.356 sec 601925 rows/sec 6.09185 # 4 2016-05-03 10:33:31 2.348 sec 717617 rows/sec 13.05168 # 5 2016-05-03 10:33:32 3.281 sec 777538 rows/sec 20.00783 # 6 2016-05-03 10:33:32 3.345 sec 777275 rows/sec 20.00783 # iterations samples training_MSE training_r2 # 1 0 0.000000 # 2 1 99804.000000 0.14402 0.03691 # 3 7 700039.000000 0.14157 0.05333 # 4 15 1499821.000000 0.14033 0.06159 # 5 23 2299180.000000 0.14079 0.05853 # 6 23 2299180.000000 0.14157 0.05333 # training_logloss training_AUC training_lift # 1 # 2 0.45930 0.66685 2.20727 # 3 0.45220 0.68133 2.59354 # 4 0.44710 0.67993 2.70390 # 5 0.45100 0.68192 2.81426 # 6 0.45220 0.68133 2.59354 # training_classification_error validation_MSE validation_r2 # 1 # 2 0.36145 0.14682 0.03426 # 3 0.33647 0.14500 0.04619 # 4 0.37126 0.14411 0.05204 # 5 0.32868 0.14474 0.04793 # 6 0.33647 0.14500 0.04619 # validation_logloss validation_AUC validation_lift # 1 # 2 0.46692 0.66582 2.53209 # 3 0.46256 0.67354 2.64124 # 4 0.45789 0.66986 2.44478 # 5 0.46292 0.67117 2.70672 # 6 0.46256 0.67354 2.64124 # validation_classification_error # 1 # 2 0.37197 # 3 0.34716 # 4 0.34385 # 5 0.36544 # 6 0.34716 1.8 Naive Bayes model The Naive Bayes (NB) algorithm does not usually beat an algorithm like a Random Forest or GBM, however it is still a popular algorithm, especially in the text domain (when your input is text encoded as “Bag of Words”, for example). The Naive Bayes algorithm is for binary or multiclass classification problems only, not regression. Therefore, your response must be a factor instead of a numeric. # First we will train a basic NB model with default parameters. nb_fit1 &lt;- h2o.naiveBayes(x = x, y = y, training_frame = train, model_id = &quot;nb_fit1&quot;) #&gt; | | | 0% | |=========== | 17% | |=================================================================| 100% 1.8.1 Train a NB model with Laplace Smoothing One of the few tunable model parameters for the Naive Bayes algorithm is the amount of Laplace smoothing. The H2O Naive Bayes model will not use any Laplace smoothing by default. nb_fit2 &lt;- h2o.naiveBayes(x = x, y = y, training_frame = train, model_id = &quot;nb_fit2&quot;, laplace = 6) #&gt; | | | 0% | |====================================================== | 83% | |=================================================================| 100% # Let&#39;s compare the performance of the two NB models nb_perf1 &lt;- h2o.performance(model = nb_fit1, newdata = test) nb_perf2 &lt;- h2o.performance(model = nb_fit2, newdata = test) # Print model performance nb_perf1 #&gt; H2OBinomialMetrics: naivebayes #&gt; #&gt; MSE: 0.15 #&gt; RMSE: 0.387 #&gt; LogLoss: 0.489 #&gt; Mean Per-Class Error: 0.39 #&gt; AUC: 0.651 #&gt; pr_auc: 0.297 #&gt; Gini: 0.303 #&gt; #&gt; Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold: #&gt; 0 1 Error Rate #&gt; 0 13184 6807 0.340503 =6807/19991 #&gt; 1 2021 2569 0.440305 =2021/4590 #&gt; Totals 15205 9376 0.359139 =8828/24581 #&gt; #&gt; Maximum Metrics: Maximum metrics at their respective thresholds #&gt; metric threshold value idx #&gt; 1 max f1 0.225948 0.367893 236 #&gt; 2 max f2 0.090634 0.545538 346 #&gt; 3 max f0point5 0.340471 0.335807 166 #&gt; 4 max accuracy 0.999554 0.812945 0 #&gt; 5 max precision 0.559607 0.428747 70 #&gt; 6 max recall 0.000196 1.000000 399 #&gt; 7 max specificity 0.999554 0.999550 0 #&gt; 8 max absolute_mcc 0.287231 0.188641 196 #&gt; 9 max min_per_class_accuracy 0.208724 0.602832 250 #&gt; 10 max mean_per_class_accuracy 0.225948 0.609596 236 #&gt; #&gt; Gains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)` nb_perf2 #&gt; H2OBinomialMetrics: naivebayes #&gt; #&gt; MSE: 0.15 #&gt; RMSE: 0.387 #&gt; LogLoss: 0.489 #&gt; Mean Per-Class Error: 0.39 #&gt; AUC: 0.651 #&gt; pr_auc: 0.297 #&gt; Gini: 0.303 #&gt; #&gt; Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold: #&gt; 0 1 Error Rate #&gt; 0 14002 5989 0.299585 =5989/19991 #&gt; 1 2207 2383 0.480828 =2207/4590 #&gt; Totals 16209 8372 0.333428 =8196/24581 #&gt; #&gt; Maximum Metrics: Maximum metrics at their respective thresholds #&gt; metric threshold value idx #&gt; 1 max f1 0.242206 0.367690 222 #&gt; 2 max f2 0.088660 0.545677 347 #&gt; 3 max f0point5 0.362995 0.336012 152 #&gt; 4 max accuracy 0.999564 0.812945 0 #&gt; 5 max precision 0.574610 0.428775 63 #&gt; 6 max recall 0.000207 1.000000 399 #&gt; 7 max specificity 0.999564 0.999550 0 #&gt; 8 max absolute_mcc 0.286635 0.189479 192 #&gt; 9 max min_per_class_accuracy 0.207609 0.604357 248 #&gt; 10 max mean_per_class_accuracy 0.247906 0.609878 218 #&gt; #&gt; Gains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)` # Retreive test set AUC h2o.auc(nb_perf1) # 0.6488014 #&gt; [1] 0.651 h2o.auc(nb_perf2) # 0.6490678 #&gt; [1] 0.651 "],
["classification-algorithms-comparison-diabetes-datset-cart-lda-svm-knn-rf.html", "Chapter 2 Classification algorithms comparison. Diabetes datset. (CART, LDA, SVM, KNN, RF) 2.1 PimaIndiansDiabetes dataset 2.2 Introduction 2.3 Workflow 2.4 Train the models using cross-validation 2.5 Compare models 2.6 Plot comparison", " Chapter 2 Classification algorithms comparison. Diabetes datset. (CART, LDA, SVM, KNN, RF) 2.1 PimaIndiansDiabetes dataset 2.2 Introduction We compare the following clasification algorithms: CART LDA SVM KNN RF 2.3 Workflow Load dataset Create the train dataset Train the models Collect resamples Plot comparison Summarize p-values # load packages library(mlbench) library(caret) # load the dataset data(PimaIndiansDiabetes) dplyr::glimpse(PimaIndiansDiabetes) #&gt; Observations: 768 #&gt; Variables: 9 #&gt; $ pregnant &lt;dbl&gt; 6, 1, 8, 1, 0, 5, 3, 10, 2, 8, 4, 10, 10, 1, 5, 7, 0, 7… #&gt; $ glucose &lt;dbl&gt; 148, 85, 183, 89, 137, 116, 78, 115, 197, 125, 110, 168… #&gt; $ pressure &lt;dbl&gt; 72, 66, 64, 66, 40, 74, 50, 0, 70, 96, 92, 74, 80, 60, … #&gt; $ triceps &lt;dbl&gt; 35, 29, 0, 23, 35, 0, 32, 0, 45, 0, 0, 0, 0, 23, 19, 0,… #&gt; $ insulin &lt;dbl&gt; 0, 0, 0, 94, 168, 0, 88, 0, 543, 0, 0, 0, 0, 846, 175, … #&gt; $ mass &lt;dbl&gt; 33.6, 26.6, 23.3, 28.1, 43.1, 25.6, 31.0, 35.3, 30.5, 0… #&gt; $ pedigree &lt;dbl&gt; 0.627, 0.351, 0.672, 0.167, 2.288, 0.201, 0.248, 0.134,… #&gt; $ age &lt;dbl&gt; 50, 31, 32, 21, 33, 30, 26, 29, 53, 54, 30, 34, 57, 59,… #&gt; $ diabetes &lt;fct&gt; pos, neg, pos, neg, pos, neg, pos, neg, pos, pos, neg, … tibble::as_tibble(PimaIndiansDiabetes) #&gt; # A tibble: 768 x 9 #&gt; pregnant glucose pressure triceps insulin mass pedigree age diabetes #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; #&gt; 1 6 148 72 35 0 33.6 0.627 50 pos #&gt; 2 1 85 66 29 0 26.6 0.351 31 neg #&gt; 3 8 183 64 0 0 23.3 0.672 32 pos #&gt; 4 1 89 66 23 94 28.1 0.167 21 neg #&gt; 5 0 137 40 35 168 43.1 2.29 33 pos #&gt; 6 5 116 74 0 0 25.6 0.201 30 neg #&gt; # … with 762 more rows 2.4 Train the models using cross-validation # prepare training scheme trainControl &lt;- trainControl(method = &quot;repeatedcv&quot;, number=10, repeats=3) # CART set.seed(7) fit.cart &lt;- train(diabetes~., data=PimaIndiansDiabetes, method = &quot;rpart&quot;, trControl=trainControl) # LDA: Linear Discriminant Analysis set.seed(7) fit.lda &lt;- train(diabetes~., data=PimaIndiansDiabetes, method=&quot;lda&quot;, trControl=trainControl) # SVM set.seed(7) fit.svm &lt;- train(diabetes~., data=PimaIndiansDiabetes, method=&quot;svmRadial&quot;, trControl=trainControl) # KNN set.seed(7) fit.knn &lt;- train(diabetes~., data=PimaIndiansDiabetes, method=&quot;knn&quot;, trControl=trainControl) # Random Forest set.seed(7) fit.rf &lt;- train(diabetes~., data=PimaIndiansDiabetes, method=&quot;rf&quot;, trControl=trainControl) # collect resamples results &lt;- resamples(list(CART=fit.cart, LDA=fit.lda, SVM=fit.svm, KNN=fit.knn, RF=fit.rf)) 2.5 Compare models # summarize differences between models summary(results) #&gt; #&gt; Call: #&gt; summary.resamples(object = results) #&gt; #&gt; Models: CART, LDA, SVM, KNN, RF #&gt; Number of resamples: 30 #&gt; #&gt; Accuracy #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s #&gt; CART 0.675 0.727 0.753 0.747 0.766 0.792 0 #&gt; LDA 0.714 0.751 0.766 0.779 0.800 0.908 0 #&gt; SVM 0.724 0.751 0.763 0.771 0.792 0.895 0 #&gt; KNN 0.675 0.704 0.727 0.737 0.766 0.831 0 #&gt; RF 0.684 0.731 0.760 0.764 0.802 0.842 0 #&gt; #&gt; Kappa #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s #&gt; CART 0.276 0.362 0.424 0.415 0.486 0.525 0 #&gt; LDA 0.301 0.419 0.466 0.486 0.531 0.781 0 #&gt; SVM 0.339 0.400 0.446 0.462 0.523 0.748 0 #&gt; KNN 0.255 0.341 0.384 0.398 0.454 0.620 0 #&gt; RF 0.295 0.378 0.464 0.463 0.545 0.643 0 2.6 Plot comparison # box and whisker plots to compare models scales &lt;- list(x=list(relation=&quot;free&quot;), y=list(relation=&quot;free&quot;)) bwplot(results, scales=scales) # density plots of accuracy scales &lt;- list(x=list(relation=&quot;free&quot;), y=list(relation=&quot;free&quot;)) densityplot(results, scales=scales, pch = &quot;|&quot;) # dot plots of accuracy scales &lt;- list(x=list(relation=&quot;free&quot;), y=list(relation=&quot;free&quot;)) dotplot(results, scales=scales) # parallel plots to compare models parallelplot(results) # pairwise scatter plots of predictions to compare models splom(results) # xyplot plots to compare models xyplot(results, models=c(&quot;LDA&quot;, &quot;SVM&quot;)) # difference in model predictions diffs &lt;- diff(results) # summarize p-values for pairwise comparisons summary(diffs) #&gt; #&gt; Call: #&gt; summary.diff.resamples(object = diffs) #&gt; #&gt; p-value adjustment: bonferroni #&gt; Upper diagonal: estimates of the difference #&gt; Lower diagonal: p-value for H0: difference = 0 #&gt; #&gt; Accuracy #&gt; CART LDA SVM KNN RF #&gt; CART -0.03214 -0.02432 0.01002 -0.01688 #&gt; LDA 0.001186 0.00781 0.04216 0.01525 #&gt; SVM 0.011640 0.915689 0.03434 0.00744 #&gt; KNN 1.000000 6.68e-05 0.000294 -0.02690 #&gt; RF 0.272754 0.449062 1.000000 0.018379 #&gt; #&gt; Kappa #&gt; CART LDA SVM KNN RF #&gt; CART -0.071016 -0.046972 0.016687 -0.047894 #&gt; LDA 0.000809 0.024044 0.087703 0.023122 #&gt; SVM 0.025808 0.356273 0.063659 -0.000922 #&gt; KNN 1.000000 0.000386 0.004082 -0.064581 #&gt; RF 0.021176 1.000000 1.000000 0.015897 "],
["multiclass-classification-comparison-diabetes-dataset-lda-cart-knn-svm-rf.html", "Chapter 3 Multiclass classification comparison. Diabetes dataset. (LDA, CART, KNN, SVM, RF) 3.1 iris dataset 3.2 Introduction 3.3 Workflow 3.4 Peek at the dataset 3.5 Levels of the class 3.6 class distribution 3.7 Visualize the dataset 3.8 Evaluate algorithms 3.9 Make predictions", " Chapter 3 Multiclass classification comparison. Diabetes dataset. (LDA, CART, KNN, SVM, RF) 3.1 iris dataset 3.2 Introduction These are the algorithms used: LDA CART KNN SVM RF # load the caret package library(caret) #&gt; Loading required package: lattice #&gt; Loading required package: ggplot2 #&gt; Registered S3 methods overwritten by &#39;ggplot2&#39;: #&gt; method from #&gt; [.quosures rlang #&gt; c.quosures rlang #&gt; print.quosures rlang # attach the iris dataset to the environment data(iris) # rename the dataset dataset &lt;- iris 3.3 Workflow Load dataset Create train and test datasets, 80/20 Inspect dataset Visualize features Set the train control to 10 cross-validations Metric: accuracy Train the models Compare accuracy of models Visual comparison Make predictions on validation set We will split the loaded dataset into two, 80% of which we will use to train our models and 20% that we will hold back as a validation dataset. # create a list of 80% of the rows in the original dataset we can use for training validationIndex &lt;- createDataPartition(dataset$Species, p=0.80, list=FALSE) # select 20% of the data for validation validation &lt;- dataset[-validationIndex,] # use the remaining 80% of data to training and testing the models dataset &lt;- dataset[validationIndex,] # dimensions of dataset dim(dataset) #&gt; [1] 120 5 # list types for each attribute sapply(dataset, class) #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width Species #&gt; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;factor&quot; 3.4 Peek at the dataset # take a peek at the first 5 rows of the data head(dataset) #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width Species #&gt; 1 5.1 3.5 1.4 0.2 setosa #&gt; 2 4.9 3.0 1.4 0.2 setosa #&gt; 3 4.7 3.2 1.3 0.2 setosa #&gt; 4 4.6 3.1 1.5 0.2 setosa #&gt; 5 5.0 3.6 1.4 0.2 setosa #&gt; 6 5.4 3.9 1.7 0.4 setosa library(dplyr) #&gt; #&gt; Attaching package: &#39;dplyr&#39; #&gt; The following objects are masked from &#39;package:stats&#39;: #&gt; #&gt; filter, lag #&gt; The following objects are masked from &#39;package:base&#39;: #&gt; #&gt; intersect, setdiff, setequal, union glimpse(dataset) #&gt; Observations: 120 #&gt; Variables: 5 #&gt; $ Sepal.Length &lt;dbl&gt; 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5… #&gt; $ Sepal.Width &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3… #&gt; $ Petal.Length &lt;dbl&gt; 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1… #&gt; $ Petal.Width &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0… #&gt; $ Species &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, set… library(skimr) #&gt; #&gt; Attaching package: &#39;skimr&#39; #&gt; The following object is masked from &#39;package:stats&#39;: #&gt; #&gt; filter skim(dataset) #&gt; Skim summary statistics #&gt; n obs: 120 #&gt; n variables: 5 #&gt; #&gt; ── Variable type:factor ────────────────────────────────────────────────── #&gt; variable missing complete n n_unique top_counts #&gt; Species 0 120 120 3 set: 40, ver: 40, vir: 40, NA: 0 #&gt; ordered #&gt; FALSE #&gt; #&gt; ── Variable type:numeric ───────────────────────────────────────────────── #&gt; variable missing complete n mean sd p0 p25 p50 p75 p100 #&gt; Petal.Length 0 120 120 3.76 1.78 1 1.58 4.35 5.1 6.9 #&gt; Petal.Width 0 120 120 1.2 0.76 0.1 0.3 1.3 1.8 2.5 #&gt; Sepal.Length 0 120 120 5.86 0.84 4.3 5.1 5.8 6.4 7.9 #&gt; Sepal.Width 0 120 120 3.06 0.44 2 2.8 3 3.32 4.4 #&gt; hist #&gt; ▇▁▁▂▅▅▃▁ #&gt; ▇▁▁▅▃▃▃▂ #&gt; ▂▇▅▆▆▅▁▂ #&gt; ▁▃▅▇▅▂▁▁ 3.5 Levels of the class # list the levels for the class levels(dataset$Species) #&gt; [1] &quot;setosa&quot; &quot;versicolor&quot; &quot;virginica&quot; 3.6 class distribution # summarize the class distribution percentage &lt;- prop.table(table(dataset$Species)) * 100 cbind(freq=table(dataset$Species), percentage=percentage) #&gt; freq percentage #&gt; setosa 40 33.3 #&gt; versicolor 40 33.3 #&gt; virginica 40 33.3 3.7 Visualize the dataset # split input and output x &lt;- dataset[,1:4] y &lt;- dataset[,5] # boxplot for each attribute on one image par(mfrow=c(1,4)) for(i in 1:4) { boxplot(x[,i], main=names(dataset)[i]) } # barplot for class breakdown plot(y) # scatter plot matrix featurePlot(x=x, y=y, plot=&quot;ellipse&quot;) # box and whisker plots for each attribute featurePlot(x=x, y=y, plot=&quot;box&quot;) # density plots for each attribute by class value scales &lt;- list(x=list(relation=&quot;free&quot;), y=list(relation=&quot;free&quot;)) featurePlot(x=x, y=y, plot=&quot;density&quot;, scales=scales) 3.8 Evaluate algorithms 3.8.1 split and metrics # Run algorithms using 10-fold cross-validation trainControl &lt;- trainControl(method=&quot;cv&quot;, number=10) metric &lt;- &quot;Accuracy&quot; 3.8.2 build models # LDA set.seed(7) fit.lda &lt;- train(Species~., data=dataset, method = &quot;lda&quot;, metric=metric, trControl=trainControl) # CART set.seed(7) fit.cart &lt;- train(Species~., data=dataset, method = &quot;rpart&quot;, metric=metric, trControl=trainControl) # KNN set.seed(7) fit.knn &lt;- train(Species~., data=dataset, method = &quot;knn&quot;, metric=metric, trControl=trainControl) # SVM set.seed(7) fit.svm &lt;- train(Species~., data=dataset, method = &quot;svmRadial&quot;, metric=metric, trControl=trainControl) # Random Forest set.seed(7) fit.rf &lt;- train(Species~., data=dataset, method = &quot;rf&quot;, metric=metric, trControl=trainControl) 3.8.3 compare #summarize accuracy of models results &lt;- resamples(list(lda = fit.lda, cart = fit.cart, knn = fit.knn, svm = fit.svm, rf = fit.rf)) summary(results) #&gt; #&gt; Call: #&gt; summary.resamples(object = results) #&gt; #&gt; Models: lda, cart, knn, svm, rf #&gt; Number of resamples: 10 #&gt; #&gt; Accuracy #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s #&gt; lda 0.917 1.000 1 0.992 1 1 0 #&gt; cart 0.917 0.917 1 0.967 1 1 0 #&gt; knn 0.917 0.938 1 0.975 1 1 0 #&gt; svm 0.833 0.917 1 0.958 1 1 0 #&gt; rf 0.917 0.917 1 0.967 1 1 0 #&gt; #&gt; Kappa #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s #&gt; lda 0.875 1.000 1 0.987 1 1 0 #&gt; cart 0.875 0.875 1 0.950 1 1 0 #&gt; knn 0.875 0.906 1 0.962 1 1 0 #&gt; svm 0.750 0.875 1 0.937 1 1 0 #&gt; rf 0.875 0.875 1 0.950 1 1 0 # compare accuracy of models dotplot(results) # summarize Best Model print(fit.lda) #&gt; Linear Discriminant Analysis #&gt; #&gt; 120 samples #&gt; 4 predictor #&gt; 3 classes: &#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39; #&gt; #&gt; No pre-processing #&gt; Resampling: Cross-Validated (10 fold) #&gt; Summary of sample sizes: 108, 108, 108, 108, 108, 108, ... #&gt; Resampling results: #&gt; #&gt; Accuracy Kappa #&gt; 0.992 0.987 3.9 Make predictions # estimate skill of LDA on the validation dataset predictions &lt;- predict(fit.lda, validation) confusionMatrix(predictions, validation$Species) #&gt; Confusion Matrix and Statistics #&gt; #&gt; Reference #&gt; Prediction setosa versicolor virginica #&gt; setosa 10 0 0 #&gt; versicolor 0 9 1 #&gt; virginica 0 1 9 #&gt; #&gt; Overall Statistics #&gt; #&gt; Accuracy : 0.933 #&gt; 95% CI : (0.779, 0.992) #&gt; No Information Rate : 0.333 #&gt; P-Value [Acc &gt; NIR] : 8.75e-12 #&gt; #&gt; Kappa : 0.9 #&gt; #&gt; Mcnemar&#39;s Test P-Value : NA #&gt; #&gt; Statistics by Class: #&gt; #&gt; Class: setosa Class: versicolor Class: virginica #&gt; Sensitivity 1.000 0.900 0.900 #&gt; Specificity 1.000 0.950 0.950 #&gt; Pos Pred Value 1.000 0.900 0.900 #&gt; Neg Pred Value 1.000 0.950 0.950 #&gt; Prevalence 0.333 0.333 0.333 #&gt; Detection Rate 0.333 0.300 0.300 #&gt; Detection Prevalence 0.333 0.333 0.333 #&gt; Balanced Accuracy 1.000 0.925 0.925 "],
["regression-algorithms-comparison-boston-dataset-lm-gkm-glmnet-svm-cart-knn.html", "Chapter 4 Regression algorithms comparison. Boston dataset. (LM, GKM, GLMNET, SVM, CART, KNN) 4.1 Boston dataset 4.2 Introduction 4.3 Workflow 4.4 Evaluation 4.5 Feature selection 4.6 Evaluate Algorithms: Box-Cox Transform 4.7 Tune SVM 4.8 Ensembling 4.9 Finalize the model", " Chapter 4 Regression algorithms comparison. Boston dataset. (LM, GKM, GLMNET, SVM, CART, KNN) 4.1 Boston dataset Comparison of various algorithms. 4.2 Introduction These are the algorithms used: LM GLM GLMNET SVM CART KNN # load packages library(mlbench) library(caret) #&gt; Loading required package: lattice #&gt; Loading required package: ggplot2 #&gt; Registered S3 methods overwritten by &#39;ggplot2&#39;: #&gt; method from #&gt; [.quosures rlang #&gt; c.quosures rlang #&gt; print.quosures rlang library(corrplot) #&gt; corrplot 0.84 loaded # attach the BostonHousing dataset data(BostonHousing) dplyr::glimpse(BostonHousing) #&gt; Observations: 506 #&gt; Variables: 14 #&gt; $ crim &lt;dbl&gt; 0.00632, 0.02731, 0.02729, 0.03237, 0.06905, 0.02985, 0.… #&gt; $ zn &lt;dbl&gt; 18.0, 0.0, 0.0, 0.0, 0.0, 0.0, 12.5, 12.5, 12.5, 12.5, 1… #&gt; $ indus &lt;dbl&gt; 2.31, 7.07, 7.07, 2.18, 2.18, 2.18, 7.87, 7.87, 7.87, 7.… #&gt; $ chas &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… #&gt; $ nox &lt;dbl&gt; 0.538, 0.469, 0.469, 0.458, 0.458, 0.458, 0.524, 0.524, … #&gt; $ rm &lt;dbl&gt; 6.58, 6.42, 7.18, 7.00, 7.15, 6.43, 6.01, 6.17, 5.63, 6.… #&gt; $ age &lt;dbl&gt; 65.2, 78.9, 61.1, 45.8, 54.2, 58.7, 66.6, 96.1, 100.0, 8… #&gt; $ dis &lt;dbl&gt; 4.09, 4.97, 4.97, 6.06, 6.06, 6.06, 5.56, 5.95, 6.08, 6.… #&gt; $ rad &lt;dbl&gt; 1, 2, 2, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 4, 4, 4, 4, 4, 4,… #&gt; $ tax &lt;dbl&gt; 296, 242, 242, 222, 222, 222, 311, 311, 311, 311, 311, 3… #&gt; $ ptratio &lt;dbl&gt; 15.3, 17.8, 17.8, 18.7, 18.7, 18.7, 15.2, 15.2, 15.2, 15… #&gt; $ b &lt;dbl&gt; 397, 397, 393, 395, 397, 394, 396, 397, 387, 387, 393, 3… #&gt; $ lstat &lt;dbl&gt; 4.98, 9.14, 4.03, 2.94, 5.33, 5.21, 12.43, 19.15, 29.93,… #&gt; $ medv &lt;dbl&gt; 24.0, 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18… tibble::as_tibble(BostonHousing) #&gt; # A tibble: 506 x 14 #&gt; crim zn indus chas nox rm age dis rad tax ptratio #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.00632 18 2.31 0 0.538 6.58 65.2 4.09 1 296 15.3 #&gt; 2 0.0273 0 7.07 0 0.469 6.42 78.9 4.97 2 242 17.8 #&gt; 3 0.0273 0 7.07 0 0.469 7.18 61.1 4.97 2 242 17.8 #&gt; 4 0.0324 0 2.18 0 0.458 7.00 45.8 6.06 3 222 18.7 #&gt; 5 0.0690 0 2.18 0 0.458 7.15 54.2 6.06 3 222 18.7 #&gt; 6 0.0298 0 2.18 0 0.458 6.43 58.7 6.06 3 222 18.7 #&gt; # … with 500 more rows, and 3 more variables: b &lt;dbl&gt;, lstat &lt;dbl&gt;, #&gt; # medv &lt;dbl&gt; 4.3 Workflow Load dataset Create train and test datasets, 80/20 Inspect dataset: Dimension classes skimr Analyze features correlation Visualize features histograms density plots pairwise correlogram Train as-is Set the train control to 10 cross-validations 3 repetitions Metric: RMSE Train the models Compare accuracy of models Visual comparison dot plot Train with Feature selection Feature selection findCorrelation generate new dataset Train models again Compare RMSE again Visual comparison dot plot Train with dataset transformation data transformatiom Center Scale BoxCox Train models Compare RMSE Visual comparison dot plot Tune the best model Set the train control to 10 cross-validations 3 repetitions Metric: RMSE Train the models Radial SVM Sigma vector .C BoxCox 9, Ensembling Select the algorithms Random Forest Stochastic Gradient Boosting Cubist Numeric comparison resample summary Visual comparison dot plot Tune the best model: Cubist Set the train control to 10 cross-validations 3 repetitions Metric: RMSE Train the models Cubist .committees .neighbors BoxCox Evaluate the tuning parameters Numeric comparison print tuned model Visual comparison scatter plot Finalize the model Back transformation Summary Apply model to validation set Transform the dataset Make prediction Calculate the RMSE # Split out validation dataset # create a list of 80% of the rows in the original dataset we can use for training set.seed(7) validationIndex &lt;- createDataPartition(BostonHousing$medv, p=0.80, list=FALSE) # select 20% of the data for validation validation &lt;- BostonHousing[-validationIndex,] # use the remaining 80% of data to training and testing the models dataset &lt;- BostonHousing[validationIndex,] # dimensions of dataset dim(validation) #&gt; [1] 99 14 dim(dataset) #&gt; [1] 407 14 # list types for each attribute sapply(dataset, class) #&gt; crim zn indus chas nox rm age #&gt; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;factor&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; #&gt; dis rad tax ptratio b lstat medv #&gt; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; # take a peek at the first 20 rows of the data head(dataset, n=20) #&gt; crim zn indus chas nox rm age dis rad tax ptratio b lstat #&gt; 1 0.00632 18.0 2.31 0 0.538 6.58 65.2 4.09 1 296 15.3 397 4.98 #&gt; 2 0.02731 0.0 7.07 0 0.469 6.42 78.9 4.97 2 242 17.8 397 9.14 #&gt; 3 0.02729 0.0 7.07 0 0.469 7.18 61.1 4.97 2 242 17.8 393 4.03 #&gt; 4 0.03237 0.0 2.18 0 0.458 7.00 45.8 6.06 3 222 18.7 395 2.94 #&gt; 5 0.06905 0.0 2.18 0 0.458 7.15 54.2 6.06 3 222 18.7 397 5.33 #&gt; 6 0.02985 0.0 2.18 0 0.458 6.43 58.7 6.06 3 222 18.7 394 5.21 #&gt; 7 0.08829 12.5 7.87 0 0.524 6.01 66.6 5.56 5 311 15.2 396 12.43 #&gt; 10 0.17004 12.5 7.87 0 0.524 6.00 85.9 6.59 5 311 15.2 387 17.10 #&gt; 11 0.22489 12.5 7.87 0 0.524 6.38 94.3 6.35 5 311 15.2 393 20.45 #&gt; 12 0.11747 12.5 7.87 0 0.524 6.01 82.9 6.23 5 311 15.2 397 13.27 #&gt; 13 0.09378 12.5 7.87 0 0.524 5.89 39.0 5.45 5 311 15.2 390 15.71 #&gt; 14 0.62976 0.0 8.14 0 0.538 5.95 61.8 4.71 4 307 21.0 397 8.26 #&gt; 15 0.63796 0.0 8.14 0 0.538 6.10 84.5 4.46 4 307 21.0 380 10.26 #&gt; 17 1.05393 0.0 8.14 0 0.538 5.93 29.3 4.50 4 307 21.0 387 6.58 #&gt; 20 0.72580 0.0 8.14 0 0.538 5.73 69.5 3.80 4 307 21.0 391 11.28 #&gt; 21 1.25179 0.0 8.14 0 0.538 5.57 98.1 3.80 4 307 21.0 377 21.02 #&gt; 22 0.85204 0.0 8.14 0 0.538 5.96 89.2 4.01 4 307 21.0 393 13.83 #&gt; 23 1.23247 0.0 8.14 0 0.538 6.14 91.7 3.98 4 307 21.0 397 18.72 #&gt; 24 0.98843 0.0 8.14 0 0.538 5.81 100.0 4.10 4 307 21.0 395 19.88 #&gt; 25 0.75026 0.0 8.14 0 0.538 5.92 94.1 4.40 4 307 21.0 394 16.30 #&gt; medv #&gt; 1 24.0 #&gt; 2 21.6 #&gt; 3 34.7 #&gt; 4 33.4 #&gt; 5 36.2 #&gt; 6 28.7 #&gt; 7 22.9 #&gt; 10 18.9 #&gt; 11 15.0 #&gt; 12 18.9 #&gt; 13 21.7 #&gt; 14 20.4 #&gt; 15 18.2 #&gt; 17 23.1 #&gt; 20 18.2 #&gt; 21 13.6 #&gt; 22 19.6 #&gt; 23 15.2 #&gt; 24 14.5 #&gt; 25 15.6 library(skimr) #&gt; #&gt; Attaching package: &#39;skimr&#39; #&gt; The following object is masked from &#39;package:stats&#39;: #&gt; #&gt; filter skim_with(numeric = list(hist = NULL)) skim(dataset) #&gt; Skim summary statistics #&gt; n obs: 407 #&gt; n variables: 14 #&gt; #&gt; ── Variable type:factor ────────────────────────────────────────────────── #&gt; variable missing complete n n_unique top_counts ordered #&gt; chas 0 407 407 2 0: 378, 1: 29, NA: 0 FALSE #&gt; #&gt; ── Variable type:numeric ───────────────────────────────────────────────── #&gt; variable missing complete n mean sd p0 p25 p50 p75 #&gt; age 0 407 407 68.38 28.16 6.2 42.7 77.3 94.2 #&gt; b 0 407 407 357.19 89.67 0.32 373.81 391.27 396.02 #&gt; crim 0 407 407 3.64 8.8 0.0063 0.08 0.27 3.69 #&gt; dis 0 407 407 3.82 2.12 1.13 2.11 3.15 5.21 #&gt; indus 0 407 407 11 6.87 0.74 4.93 8.56 18.1 #&gt; lstat 0 407 407 12.56 7.03 1.92 7.06 11.32 16.45 #&gt; medv 0 407 407 22.52 8.96 5 17.05 21.2 25 #&gt; nox 0 407 407 0.55 0.12 0.39 0.45 0.54 0.63 #&gt; ptratio 0 407 407 18.42 2.18 12.6 17 19 20.2 #&gt; rad 0 407 407 9.59 8.77 1 4 5 24 #&gt; rm 0 407 407 6.29 0.7 3.56 5.89 6.21 6.62 #&gt; tax 0 407 407 408.75 168.72 187 280.5 334 666 #&gt; zn 0 407 407 11.92 24.19 0 0 0 15 #&gt; p100 #&gt; 100 #&gt; 396.9 #&gt; 88.98 #&gt; 12.13 #&gt; 27.74 #&gt; 37.97 #&gt; 50 #&gt; 0.87 #&gt; 22 #&gt; 24 #&gt; 8.78 #&gt; 711 #&gt; 100 dataset[,4] &lt;- as.numeric(as.character(dataset[,4])) skim(dataset) #&gt; Skim summary statistics #&gt; n obs: 407 #&gt; n variables: 14 #&gt; #&gt; ── Variable type:numeric ───────────────────────────────────────────────── #&gt; variable missing complete n mean sd p0 p25 p50 #&gt; age 0 407 407 68.38 28.16 6.2 42.7 77.3 #&gt; b 0 407 407 357.19 89.67 0.32 373.81 391.27 #&gt; chas 0 407 407 0.071 0.26 0 0 0 #&gt; crim 0 407 407 3.64 8.8 0.0063 0.08 0.27 #&gt; dis 0 407 407 3.82 2.12 1.13 2.11 3.15 #&gt; indus 0 407 407 11 6.87 0.74 4.93 8.56 #&gt; lstat 0 407 407 12.56 7.03 1.92 7.06 11.32 #&gt; medv 0 407 407 22.52 8.96 5 17.05 21.2 #&gt; nox 0 407 407 0.55 0.12 0.39 0.45 0.54 #&gt; ptratio 0 407 407 18.42 2.18 12.6 17 19 #&gt; rad 0 407 407 9.59 8.77 1 4 5 #&gt; rm 0 407 407 6.29 0.7 3.56 5.89 6.21 #&gt; tax 0 407 407 408.75 168.72 187 280.5 334 #&gt; zn 0 407 407 11.92 24.19 0 0 0 #&gt; p75 p100 #&gt; 94.2 100 #&gt; 396.02 396.9 #&gt; 0 1 #&gt; 3.69 88.98 #&gt; 5.21 12.13 #&gt; 18.1 27.74 #&gt; 16.45 37.97 #&gt; 25 50 #&gt; 0.63 0.87 #&gt; 20.2 22 #&gt; 24 24 #&gt; 6.62 8.78 #&gt; 666 711 #&gt; 15 100 no more factors or character variables # find correlation between variables cor(dataset[,1:13]) #&gt; crim zn indus chas nox rm age dis #&gt; crim 1.0000 -0.1996 0.4076 -0.05507 0.4099 -0.194 0.3524 -0.376 #&gt; zn -0.1996 1.0000 -0.5314 -0.02987 -0.5202 0.311 -0.5845 0.680 #&gt; indus 0.4076 -0.5314 1.0000 0.06583 0.7733 -0.383 0.6512 -0.711 #&gt; chas -0.0551 -0.0299 0.0658 1.00000 0.0934 0.127 0.0735 -0.099 #&gt; nox 0.4099 -0.5202 0.7733 0.09340 1.0000 -0.296 0.7338 -0.769 #&gt; rm -0.1940 0.3111 -0.3826 0.12677 -0.2961 1.000 -0.2262 0.207 #&gt; age 0.3524 -0.5845 0.6512 0.07350 0.7338 -0.226 1.0000 -0.749 #&gt; dis -0.3756 0.6799 -0.7113 -0.09905 -0.7693 0.207 -0.7492 1.000 #&gt; rad 0.6083 -0.3227 0.6200 -0.00245 0.6276 -0.221 0.4690 -0.504 #&gt; tax 0.5711 -0.3184 0.7185 -0.03064 0.6758 -0.295 0.5058 -0.526 #&gt; ptratio 0.2897 -0.3888 0.3782 -0.12283 0.1888 -0.365 0.2709 -0.228 #&gt; b -0.3442 0.1747 -0.3644 0.03782 -0.3684 0.126 -0.2742 0.284 #&gt; lstat 0.4229 -0.4219 0.6136 -0.08430 0.5839 -0.612 0.6066 -0.501 #&gt; rad tax ptratio b lstat #&gt; crim 0.60834 0.5711 0.290 -0.3442 0.4229 #&gt; zn -0.32273 -0.3184 -0.389 0.1747 -0.4219 #&gt; indus 0.61998 0.7185 0.378 -0.3644 0.6136 #&gt; chas -0.00245 -0.0306 -0.123 0.0378 -0.0843 #&gt; nox 0.62760 0.6758 0.189 -0.3684 0.5839 #&gt; rm -0.22126 -0.2953 -0.365 0.1260 -0.6120 #&gt; age 0.46896 0.5058 0.271 -0.2742 0.6066 #&gt; dis -0.50372 -0.5264 -0.228 0.2843 -0.5013 #&gt; rad 1.00000 0.9201 0.480 -0.4231 0.5025 #&gt; tax 0.92005 1.0000 0.469 -0.4303 0.5538 #&gt; ptratio 0.47971 0.4691 1.000 -0.1700 0.4093 #&gt; b -0.42314 -0.4303 -0.170 1.0000 -0.3509 #&gt; lstat 0.50251 0.5538 0.409 -0.3509 1.0000 library(dplyr) #&gt; #&gt; Attaching package: &#39;dplyr&#39; #&gt; The following objects are masked from &#39;package:stats&#39;: #&gt; #&gt; filter, lag #&gt; The following objects are masked from &#39;package:base&#39;: #&gt; #&gt; intersect, setdiff, setequal, union m &lt;- cor(dataset[,1:13]) diag(m) &lt;- 0 # select variables with correlation 0.7 and above threshold &lt;- 0.7 ok &lt;- apply(abs(m) &gt;= threshold, 1, any) m[ok, ok] #&gt; indus nox age dis rad tax #&gt; indus 0.000 0.773 0.651 -0.711 0.620 0.719 #&gt; nox 0.773 0.000 0.734 -0.769 0.628 0.676 #&gt; age 0.651 0.734 0.000 -0.749 0.469 0.506 #&gt; dis -0.711 -0.769 -0.749 0.000 -0.504 -0.526 #&gt; rad 0.620 0.628 0.469 -0.504 0.000 0.920 #&gt; tax 0.719 0.676 0.506 -0.526 0.920 0.000 # values of correlation &gt;= 0.7 ind &lt;- sapply(1:13, function(x) abs(m[, x]) &gt; 0.7) m[ind] #&gt; [1] 0.773 -0.711 0.719 0.773 0.734 -0.769 0.734 -0.749 -0.711 -0.769 #&gt; [11] -0.749 0.920 0.719 0.920 # defining a index for selecting if the condition is met cind &lt;- apply(m, 2, function(x) any(abs(x) &gt; 0.7)) cm &lt;- m[, cind] # since col6 only has values less than 0.5 it is not taken cm #&gt; indus nox age dis rad tax #&gt; crim 0.4076 0.4099 0.3524 -0.376 0.60834 0.5711 #&gt; zn -0.5314 -0.5202 -0.5845 0.680 -0.32273 -0.3184 #&gt; indus 0.0000 0.7733 0.6512 -0.711 0.61998 0.7185 #&gt; chas 0.0658 0.0934 0.0735 -0.099 -0.00245 -0.0306 #&gt; nox 0.7733 0.0000 0.7338 -0.769 0.62760 0.6758 #&gt; rm -0.3826 -0.2961 -0.2262 0.207 -0.22126 -0.2953 #&gt; age 0.6512 0.7338 0.0000 -0.749 0.46896 0.5058 #&gt; dis -0.7113 -0.7693 -0.7492 0.000 -0.50372 -0.5264 #&gt; rad 0.6200 0.6276 0.4690 -0.504 0.00000 0.9201 #&gt; tax 0.7185 0.6758 0.5058 -0.526 0.92005 0.0000 #&gt; ptratio 0.3782 0.1888 0.2709 -0.228 0.47971 0.4691 #&gt; b -0.3644 -0.3684 -0.2742 0.284 -0.42314 -0.4303 #&gt; lstat 0.6136 0.5839 0.6066 -0.501 0.50251 0.5538 rind &lt;- apply(cm, 1, function(x) any(abs(x) &gt; 0.7)) rm &lt;- cm[rind, ] rm #&gt; indus nox age dis rad tax #&gt; indus 0.000 0.773 0.651 -0.711 0.620 0.719 #&gt; nox 0.773 0.000 0.734 -0.769 0.628 0.676 #&gt; age 0.651 0.734 0.000 -0.749 0.469 0.506 #&gt; dis -0.711 -0.769 -0.749 0.000 -0.504 -0.526 #&gt; rad 0.620 0.628 0.469 -0.504 0.000 0.920 #&gt; tax 0.719 0.676 0.506 -0.526 0.920 0.000 # histograms for each attribute par(mfrow=c(3,5)) for(i in 1:13) { hist(dataset[,i], main=names(dataset)[i]) } # density plot for each attribute par(mfrow=c(3,5)) for(i in 1:13) { plot(density(dataset[,i]), main=names(dataset)[i]) } # boxplots for each attribute par(mfrow=c(3,5)) for(i in 1:13) { boxplot(dataset[,i], main=names(dataset)[i]) } # scatter plot matrix pairs(dataset[,1:13]) # correlation plot correlations &lt;- cor(dataset[,1:13]) corrplot(correlations, method=&quot;circle&quot;) 4.4 Evaluation # Run algorithms using 10-fold cross-validation trainControl &lt;- trainControl(method=&quot;repeatedcv&quot;, number=10, repeats=3) metric &lt;- &quot;RMSE&quot; # LM set.seed(7) fit.lm &lt;- train(medv~., data=dataset, method=&quot;lm&quot;, metric=metric, preProc=c(&quot;center&quot;, &quot;scale&quot;), trControl=trainControl) # GLM set.seed(7) fit.glm &lt;- train(medv~., data=dataset, method=&quot;glm&quot;, metric=metric, preProc=c(&quot;center&quot;, &quot;scale&quot;), trControl=trainControl) # GLMNET set.seed(7) fit.glmnet &lt;- train(medv~., data=dataset, method=&quot;glmnet&quot;, metric=metric, preProc=c(&quot;center&quot;, &quot;scale&quot;), trControl=trainControl) # SVM set.seed(7) fit.svm &lt;- train(medv~., data=dataset, method=&quot;svmRadial&quot;, metric=metric, preProc=c(&quot;center&quot;, &quot;scale&quot;), trControl=trainControl) # CART set.seed(7) grid &lt;- expand.grid(.cp=c(0, 0.05, 0.1)) fit.cart &lt;- train(medv~., data=dataset, method=&quot;rpart&quot;, metric=metric, tuneGrid=grid, preProc=c(&quot;center&quot;, &quot;scale&quot;), trControl=trainControl) # KNN set.seed(7) fit.knn &lt;- train(medv~., data=dataset, method=&quot;knn&quot;, metric=metric, preProc=c(&quot;center&quot;, &quot;scale&quot;), trControl=trainControl) # Compare algorithms results &lt;- resamples(list(LM = fit.lm, GLM = fit.glm, GLMNET = fit.glmnet, SVM = fit.svm, CART = fit.cart, KNN = fit.knn)) summary(results) #&gt; #&gt; Call: #&gt; summary.resamples(object = results) #&gt; #&gt; Models: LM, GLM, GLMNET, SVM, CART, KNN #&gt; Number of resamples: 30 #&gt; #&gt; MAE #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s #&gt; LM 2.30 2.90 3.37 3.32 3.70 4.64 0 #&gt; GLM 2.30 2.90 3.37 3.32 3.70 4.64 0 #&gt; GLMNET 2.30 2.88 3.34 3.30 3.70 4.63 0 #&gt; SVM 1.42 1.99 2.52 2.39 2.65 3.35 0 #&gt; CART 2.22 2.62 2.88 2.93 3.08 4.16 0 #&gt; KNN 1.98 2.69 2.87 2.95 3.24 4.00 0 #&gt; #&gt; RMSE #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s #&gt; LM 2.99 3.87 4.63 4.63 5.32 6.69 0 #&gt; GLM 2.99 3.87 4.63 4.63 5.32 6.69 0 #&gt; GLMNET 2.99 3.88 4.62 4.62 5.32 6.69 0 #&gt; SVM 2.05 2.95 3.81 3.91 4.46 6.98 0 #&gt; CART 2.77 3.38 4.00 4.20 4.60 7.09 0 #&gt; KNN 2.65 3.74 4.42 4.48 5.06 6.98 0 #&gt; #&gt; Rsquared #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s #&gt; LM 0.505 0.674 0.747 0.740 0.813 0.900 0 #&gt; GLM 0.505 0.674 0.747 0.740 0.813 0.900 0 #&gt; GLMNET 0.503 0.673 0.747 0.741 0.816 0.904 0 #&gt; SVM 0.519 0.762 0.845 0.810 0.896 0.970 0 #&gt; CART 0.514 0.737 0.816 0.778 0.842 0.899 0 #&gt; KNN 0.519 0.748 0.804 0.770 0.829 0.931 0 dotplot(results) 4.5 Feature selection # remove correlated attributes # find attributes that are highly correlated set.seed(7) cutoff &lt;- 0.70 correlations &lt;- cor(dataset[,1:13]) highlyCorrelated &lt;- findCorrelation(correlations, cutoff=cutoff) for (value in highlyCorrelated) { print(names(dataset)[value]) } #&gt; [1] &quot;indus&quot; #&gt; [1] &quot;nox&quot; #&gt; [1] &quot;tax&quot; #&gt; [1] &quot;dis&quot; # create a new dataset without highly correlated features datasetFeatures &lt;- dataset[,-highlyCorrelated] dim(datasetFeatures) #&gt; [1] 407 10 # Run algorithms using 10-fold cross-validation trainControl &lt;- trainControl(method=&quot;repeatedcv&quot;, number=10, repeats=3) metric &lt;- &quot;RMSE&quot; # LM set.seed(7) fit.lm &lt;- train(medv~., data=dataset, method=&quot;lm&quot;, metric=metric, preProc=c(&quot;center&quot;, &quot;scale&quot;), trControl=trainControl) # GLM set.seed(7) fit.glm &lt;- train(medv~., data=dataset, method=&quot;glm&quot;, metric=metric, preProc=c(&quot;center&quot;, &quot;scale&quot;), trControl=trainControl) # GLMNET set.seed(7) fit.glmnet &lt;- train(medv~., data=dataset, method=&quot;glmnet&quot;, metric=metric, preProc=c(&quot;center&quot;, &quot;scale&quot;), trControl=trainControl) # SVM set.seed(7) fit.svm &lt;- train(medv~., data=dataset, method=&quot;svmRadial&quot;, metric=metric, preProc=c(&quot;center&quot;, &quot;scale&quot;), trControl=trainControl) # CART set.seed(7) grid &lt;- expand.grid(.cp=c(0, 0.05, 0.1)) fit.cart &lt;- train(medv~., data=dataset, method=&quot;rpart&quot;, metric=metric, tuneGrid=grid, preProc=c(&quot;center&quot;, &quot;scale&quot;), trControl=trainControl) # KNN set.seed(7) fit.knn &lt;- train(medv~., data=dataset, method=&quot;knn&quot;, metric=metric, preProc=c(&quot;center&quot;, &quot;scale&quot;), trControl=trainControl) # Compare algorithms feature_results &lt;- resamples(list(LM = fit.lm, GLM = fit.glm, GLMNET = fit.glmnet, SVM = fit.svm, CART = fit.cart, KNN = fit.knn)) summary(feature_results) #&gt; #&gt; Call: #&gt; summary.resamples(object = feature_results) #&gt; #&gt; Models: LM, GLM, GLMNET, SVM, CART, KNN #&gt; Number of resamples: 30 #&gt; #&gt; MAE #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s #&gt; LM 2.30 2.90 3.37 3.32 3.70 4.64 0 #&gt; GLM 2.30 2.90 3.37 3.32 3.70 4.64 0 #&gt; GLMNET 2.30 2.88 3.34 3.30 3.70 4.63 0 #&gt; SVM 1.42 1.99 2.52 2.39 2.65 3.35 0 #&gt; CART 2.22 2.62 2.88 2.93 3.08 4.16 0 #&gt; KNN 1.98 2.69 2.87 2.95 3.24 4.00 0 #&gt; #&gt; RMSE #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s #&gt; LM 2.99 3.87 4.63 4.63 5.32 6.69 0 #&gt; GLM 2.99 3.87 4.63 4.63 5.32 6.69 0 #&gt; GLMNET 2.99 3.88 4.62 4.62 5.32 6.69 0 #&gt; SVM 2.05 2.95 3.81 3.91 4.46 6.98 0 #&gt; CART 2.77 3.38 4.00 4.20 4.60 7.09 0 #&gt; KNN 2.65 3.74 4.42 4.48 5.06 6.98 0 #&gt; #&gt; Rsquared #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s #&gt; LM 0.505 0.674 0.747 0.740 0.813 0.900 0 #&gt; GLM 0.505 0.674 0.747 0.740 0.813 0.900 0 #&gt; GLMNET 0.503 0.673 0.747 0.741 0.816 0.904 0 #&gt; SVM 0.519 0.762 0.845 0.810 0.896 0.970 0 #&gt; CART 0.514 0.737 0.816 0.778 0.842 0.899 0 #&gt; KNN 0.519 0.748 0.804 0.770 0.829 0.931 0 dotplot(feature_results) Comparing the results, we can see that this has made the RMSE worse for the linear and the nonlinear algorithms. The correlated attributes we removed are contributing to the accuracy of the models. 4.6 Evaluate Algorithms: Box-Cox Transform We know that some of the attributes have a skew and others perhaps have an exponential distribution. One option would be to explore squaring and log transforms respectively (you could try this!). Another approach would be to use a power transform and let it figure out the amount to correct each attribute. One example is the Box-Cox power transform. Let’s try using this transform to rescale the original data and evaluate the effect on the same 6 algorithms. We will also leave in the centering and scaling for the benefit of the instance-based methods. # Run algorithms using 10-fold cross-validation trainControl &lt;- trainControl(method=&quot;repeatedcv&quot;, number=10, repeats=3) metric &lt;- &quot;RMSE&quot; # lm set.seed(7) fit.lm &lt;- train(medv~., data=dataset, method=&quot;lm&quot;, metric=metric, preProc=c(&quot;center&quot;, &quot;scale&quot;, &quot;BoxCox&quot;), trControl=trainControl) # GLM set.seed(7) fit.glm &lt;- train(medv~., data=dataset, method=&quot;glm&quot;, metric=metric, preProc=c(&quot;center&quot;, &quot;scale&quot;, &quot;BoxCox&quot;), trControl=trainControl) # GLMNET set.seed(7) fit.glmnet &lt;- train(medv~., data=dataset, method=&quot;glmnet&quot;, metric=metric, preProc=c(&quot;center&quot;, &quot;scale&quot;, &quot;BoxCox&quot;), trControl=trainControl) # SVM set.seed(7) fit.svm &lt;- train(medv~., data=dataset, method=&quot;svmRadial&quot;, metric=metric, preProc=c(&quot;center&quot;, &quot;scale&quot;, &quot;BoxCox&quot;), trControl=trainControl) # CART set.seed(7) grid &lt;- expand.grid(.cp=c(0, 0.05, 0.1)) fit.cart &lt;- train(medv~., data=dataset, method=&quot;rpart&quot;, metric=metric, tuneGrid=grid, preProc=c(&quot;center&quot;, &quot;scale&quot;, &quot;BoxCox&quot;), trControl=trainControl) # KNN set.seed(7) fit.knn &lt;- train(medv~., data=dataset, method=&quot;knn&quot;, metric=metric, preProc=c(&quot;center&quot;, &quot;scale&quot;, &quot;BoxCox&quot;), trControl=trainControl) # Compare algorithms transformResults &lt;- resamples(list(LM = fit.lm, GLM = fit.glm, GLMNET = fit.glmnet, SVM = fit.svm, CART = fit.cart, KNN = fit.knn)) summary(transformResults) #&gt; #&gt; Call: #&gt; summary.resamples(object = transformResults) #&gt; #&gt; Models: LM, GLM, GLMNET, SVM, CART, KNN #&gt; Number of resamples: 30 #&gt; #&gt; MAE #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s #&gt; LM 2.10 2.80 3.21 3.18 3.45 4.44 0 #&gt; GLM 2.10 2.80 3.21 3.18 3.45 4.44 0 #&gt; GLMNET 2.11 2.80 3.21 3.17 3.45 4.43 0 #&gt; SVM 1.30 1.95 2.25 2.26 2.48 3.19 0 #&gt; CART 2.22 2.62 2.89 2.94 3.11 4.16 0 #&gt; KNN 2.33 2.66 2.82 2.97 3.27 3.96 0 #&gt; #&gt; RMSE #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s #&gt; LM 2.82 3.81 4.43 4.37 5.00 6.16 0 #&gt; GLM 2.82 3.81 4.43 4.37 5.00 6.16 0 #&gt; GLMNET 2.83 3.79 4.42 4.36 5.00 6.18 0 #&gt; SVM 1.80 2.73 3.41 3.70 4.24 6.73 0 #&gt; CART 2.77 3.38 4.00 4.23 4.83 7.09 0 #&gt; KNN 3.01 3.73 4.37 4.52 5.02 7.30 0 #&gt; #&gt; Rsquared #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s #&gt; LM 0.560 0.712 0.775 0.766 0.825 0.910 0 #&gt; GLM 0.560 0.712 0.775 0.766 0.825 0.910 0 #&gt; GLMNET 0.556 0.713 0.775 0.766 0.826 0.910 0 #&gt; SVM 0.524 0.778 0.854 0.827 0.907 0.979 0 #&gt; CART 0.514 0.727 0.816 0.774 0.843 0.899 0 #&gt; KNN 0.492 0.723 0.792 0.762 0.842 0.937 0 dotplot(transformResults) 4.7 Tune SVM print(fit.svm) #&gt; Support Vector Machines with Radial Basis Function Kernel #&gt; #&gt; 407 samples #&gt; 13 predictor #&gt; #&gt; Pre-processing: centered (13), scaled (13), Box-Cox transformation (11) #&gt; Resampling: Cross-Validated (10 fold, repeated 3 times) #&gt; Summary of sample sizes: 365, 366, 366, 367, 366, 366, ... #&gt; Resampling results across tuning parameters: #&gt; #&gt; C RMSE Rsquared MAE #&gt; 0.25 4.54 0.772 2.73 #&gt; 0.50 4.07 0.802 2.46 #&gt; 1.00 3.70 0.827 2.26 #&gt; #&gt; Tuning parameter &#39;sigma&#39; was held constant at a value of 0.116 #&gt; RMSE was used to select the optimal model using the smallest value. #&gt; The final values used for the model were sigma = 0.116 and C = 1. Let’s design a grid search around a C value of 1. We might see a small trend of decreasing RMSE with increasing C, so let’s try all integer C values between 1 and 10. Another parameter that caret let us tune is the sigma parameter. This is a smoothing parameter. Good sigma values often start around 0.1, so we will try numbers before and after. # tune SVM sigma and C parametres trainControl &lt;- trainControl(method=&quot;repeatedcv&quot;, number=10, repeats=3) metric &lt;- &quot;RMSE&quot; set.seed(7) grid &lt;- expand.grid(.sigma = c(0.025, 0.05, 0.1, 0.15), .C = seq(1, 10, by=1)) fit.svm &lt;- train(medv~., data=dataset, method=&quot;svmRadial&quot;, metric=metric, tuneGrid=grid, preProc=c(&quot;BoxCox&quot;), trControl=trainControl) print(fit.svm) #&gt; Support Vector Machines with Radial Basis Function Kernel #&gt; #&gt; 407 samples #&gt; 13 predictor #&gt; #&gt; Pre-processing: Box-Cox transformation (11) #&gt; Resampling: Cross-Validated (10 fold, repeated 3 times) #&gt; Summary of sample sizes: 365, 366, 366, 367, 366, 366, ... #&gt; Resampling results across tuning parameters: #&gt; #&gt; sigma C RMSE Rsquared MAE #&gt; 0.025 1 3.67 0.830 2.34 #&gt; 0.025 2 3.49 0.840 2.21 #&gt; 0.025 3 3.45 0.842 2.17 #&gt; 0.025 4 3.42 0.844 2.14 #&gt; 0.025 5 3.41 0.845 2.13 #&gt; 0.025 6 3.40 0.846 2.12 #&gt; 0.025 7 3.39 0.846 2.11 #&gt; 0.025 8 3.39 0.846 2.11 #&gt; 0.025 9 3.38 0.846 2.11 #&gt; 0.025 10 3.37 0.847 2.10 #&gt; 0.050 1 3.61 0.833 2.25 #&gt; 0.050 2 3.44 0.843 2.17 #&gt; 0.050 3 3.36 0.848 2.11 #&gt; 0.050 4 3.30 0.852 2.08 #&gt; 0.050 5 3.25 0.856 2.05 #&gt; 0.050 6 3.20 0.860 2.03 #&gt; 0.050 7 3.16 0.862 2.02 #&gt; 0.050 8 3.13 0.865 2.02 #&gt; 0.050 9 3.11 0.866 2.01 #&gt; 0.050 10 3.10 0.867 2.01 #&gt; 0.100 1 3.68 0.829 2.26 #&gt; 0.100 2 3.37 0.848 2.12 #&gt; 0.100 3 3.23 0.858 2.06 #&gt; 0.100 4 3.17 0.862 2.04 #&gt; 0.100 5 3.14 0.865 2.04 #&gt; 0.100 6 3.11 0.866 2.04 #&gt; 0.100 7 3.09 0.868 2.04 #&gt; 0.100 8 3.09 0.868 2.04 #&gt; 0.100 9 3.08 0.868 2.04 #&gt; 0.100 10 3.08 0.868 2.05 #&gt; 0.150 1 3.79 0.822 2.30 #&gt; 0.150 2 3.42 0.846 2.14 #&gt; 0.150 3 3.30 0.854 2.09 #&gt; 0.150 4 3.26 0.857 2.09 #&gt; 0.150 5 3.24 0.858 2.09 #&gt; 0.150 6 3.23 0.858 2.10 #&gt; 0.150 7 3.23 0.857 2.12 #&gt; 0.150 8 3.24 0.856 2.13 #&gt; 0.150 9 3.26 0.855 2.15 #&gt; 0.150 10 3.27 0.854 2.17 #&gt; #&gt; RMSE was used to select the optimal model using the smallest value. #&gt; The final values used for the model were sigma = 0.1 and C = 9. plot(fit.svm) 4.8 Ensembling We can try some ensemble methods on the problem and see if we can get a further decrease in our RMSE. Random Forest, bagging (RF). Gradient Boosting Machines (GBM). Cubist, boosting (CUBIST). # try ensembles seed &lt;- 7 trainControl &lt;- trainControl(method=&quot;repeatedcv&quot;, number=10, repeats=3) metric &lt;- &quot;RMSE&quot; # Random Forest set.seed(seed) fit.rf &lt;- train(medv~., data=dataset, method=&quot;rf&quot;, metric=metric, preProc=c(&quot;BoxCox&quot;), trControl=trainControl) # Stochastic Gradient Boosting set.seed(seed) fit.gbm &lt;- train(medv~., data=dataset, method=&quot;gbm&quot;, metric=metric, preProc=c(&quot;BoxCox&quot;), trControl=trainControl, verbose=FALSE) # Cubist set.seed(seed) fit.cubist &lt;- train(medv~., data=dataset, method=&quot;cubist&quot;, metric=metric, preProc=c(&quot;BoxCox&quot;), trControl=trainControl) # Compare algorithms ensembleResults &lt;- resamples(list(RF = fit.rf, GBM = fit.gbm, CUBIST = fit.cubist)) summary(ensembleResults) #&gt; #&gt; Call: #&gt; summary.resamples(object = ensembleResults) #&gt; #&gt; Models: RF, GBM, CUBIST #&gt; Number of resamples: 30 #&gt; #&gt; MAE #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s #&gt; RF 1.64 1.98 2.20 2.21 2.30 3.22 0 #&gt; GBM 1.65 1.98 2.27 2.33 2.55 3.75 0 #&gt; CUBIST 1.31 1.75 1.95 2.00 2.17 2.89 0 #&gt; #&gt; RMSE #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s #&gt; RF 2.11 2.58 3.08 3.22 3.70 6.52 0 #&gt; GBM 1.92 2.54 3.31 3.38 3.67 6.85 0 #&gt; CUBIST 1.79 2.38 2.74 3.09 3.78 5.79 0 #&gt; #&gt; Rsquared #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s #&gt; RF 0.597 0.852 0.900 0.869 0.919 0.972 0 #&gt; GBM 0.558 0.815 0.889 0.851 0.912 0.964 0 #&gt; CUBIST 0.681 0.818 0.909 0.875 0.932 0.970 0 dotplot(ensembleResults) Let’s dive deeper into Cubist and see if we can tune it further and get more skill out of it. Cubist has two parameters that are tunable with caret: committees which is the number of boosting operations and neighbors which is used during prediction and is the number of instances used to correct the rule-based prediction (although the documentation is perhaps a little ambiguous on this). # look at parameters used for Cubist print(fit.cubist) #&gt; Cubist #&gt; #&gt; 407 samples #&gt; 13 predictor #&gt; #&gt; Pre-processing: Box-Cox transformation (11) #&gt; Resampling: Cross-Validated (10 fold, repeated 3 times) #&gt; Summary of sample sizes: 365, 366, 366, 367, 366, 366, ... #&gt; Resampling results across tuning parameters: #&gt; #&gt; committees neighbors RMSE Rsquared MAE #&gt; 1 0 3.94 0.805 2.50 #&gt; 1 5 3.66 0.828 2.24 #&gt; 1 9 3.69 0.825 2.26 #&gt; 10 0 3.45 0.848 2.29 #&gt; 10 5 3.19 0.868 2.04 #&gt; 10 9 3.23 0.864 2.07 #&gt; 20 0 3.34 0.858 2.25 #&gt; 20 5 3.09 0.875 2.00 #&gt; 20 9 3.12 0.872 2.03 #&gt; #&gt; RMSE was used to select the optimal model using the smallest value. #&gt; The final values used for the model were committees = 20 and neighbors = 5. Let’s use a grid search to tune around those values. We’ll try all committees between 15 and 25 and spot-check a neighbors value above and below 5. library(Cubist) # Tune the Cubist algorithm trainControl &lt;- trainControl(method=&quot;repeatedcv&quot;, number=10, repeats=3) metric &lt;- &quot;RMSE&quot; set.seed(7) grid &lt;- expand.grid(.committees = seq(15, 25, by=1), .neighbors = c(3, 5, 7)) tune.cubist &lt;- train(medv~., data=dataset, method = &quot;cubist&quot;, metric=metric, preProc=c(&quot;BoxCox&quot;), tuneGrid=grid, trControl=trainControl) print(tune.cubist) #&gt; Cubist #&gt; #&gt; 407 samples #&gt; 13 predictor #&gt; #&gt; Pre-processing: Box-Cox transformation (11) #&gt; Resampling: Cross-Validated (10 fold, repeated 3 times) #&gt; Summary of sample sizes: 365, 366, 366, 367, 366, 366, ... #&gt; Resampling results across tuning parameters: #&gt; #&gt; committees neighbors RMSE Rsquared MAE #&gt; 15 3 3.07 0.877 2.00 #&gt; 15 5 3.13 0.873 2.02 #&gt; 15 7 3.14 0.871 2.03 #&gt; 16 3 3.05 0.878 1.99 #&gt; 16 5 3.11 0.874 2.01 #&gt; 16 7 3.12 0.872 2.02 #&gt; 17 3 3.04 0.879 1.98 #&gt; 17 5 3.09 0.875 2.00 #&gt; 17 7 3.11 0.873 2.01 #&gt; 18 3 3.03 0.880 1.97 #&gt; 18 5 3.08 0.876 2.00 #&gt; 18 7 3.10 0.874 2.01 #&gt; 19 3 3.03 0.880 1.97 #&gt; 19 5 3.08 0.876 1.99 #&gt; 19 7 3.10 0.874 2.01 #&gt; 20 3 3.03 0.879 1.98 #&gt; 20 5 3.09 0.875 2.00 #&gt; 20 7 3.10 0.874 2.01 #&gt; 21 3 3.03 0.879 1.98 #&gt; 21 5 3.09 0.876 2.00 #&gt; 21 7 3.10 0.874 2.02 #&gt; 22 3 3.03 0.879 1.98 #&gt; 22 5 3.09 0.875 2.00 #&gt; 22 7 3.10 0.874 2.02 #&gt; 23 3 3.03 0.880 1.98 #&gt; 23 5 3.09 0.876 2.01 #&gt; 23 7 3.10 0.874 2.02 #&gt; 24 3 3.03 0.879 1.98 #&gt; 24 5 3.09 0.875 2.01 #&gt; 24 7 3.11 0.873 2.02 #&gt; 25 3 3.03 0.880 1.98 #&gt; 25 5 3.09 0.876 2.01 #&gt; 25 7 3.10 0.874 2.02 #&gt; #&gt; RMSE was used to select the optimal model using the smallest value. #&gt; The final values used for the model were committees = 25 and neighbors = 3. plot(tune.cubist) We can see that we have achieved a more accurate model again with an RMSE of 2.822 using committees = 18 and neighbors = 3. It looks like the results for the Cubist algorithm are the most accurate. Let’s finalize it by creating a new standalone Cubist model with the parameters above trained using the whole dataset. We must also use the Box-Cox power transform. 4.9 Finalize the model # prepare the data transform using training data set.seed(7) x &lt;- dataset[,1:13] y &lt;- dataset[,14] # transform preprocessParams &lt;- preProcess(x, method=c(&quot;BoxCox&quot;)) transX &lt;- predict(preprocessParams, x) # train the final model finalModel &lt;- cubist(x = transX, y=y, committees=18) summary(finalModel) #&gt; #&gt; Call: #&gt; cubist.default(x = transX, y = y, committees = 18) #&gt; #&gt; #&gt; Cubist [Release 2.07 GPL Edition] Fri Sep 20 13:24:12 2019 #&gt; --------------------------------- #&gt; #&gt; Target attribute `outcome&#39; #&gt; #&gt; Read 407 cases (14 attributes) from undefined.data #&gt; #&gt; Model 1: #&gt; #&gt; Rule 1/1: [84 cases, mean 14.29, range 5 to 27.5, est err 1.97] #&gt; #&gt; if #&gt; nox &gt; -0.4864544 #&gt; then #&gt; outcome = 35.08 - 2.45 crim - 4.31 lstat + 2.1e-05 b #&gt; #&gt; Rule 1/2: [163 cases, mean 19.37, range 7 to 31, est err 2.10] #&gt; #&gt; if #&gt; nox &lt;= -0.4864544 #&gt; lstat &gt; 2.848535 #&gt; then #&gt; outcome = 186.8 - 2.34 lstat - 3.3 dis - 88 tax + 2 rad + 4.4 rm #&gt; - 0.033 ptratio - 0.0116 age + 3.3e-05 b #&gt; #&gt; Rule 1/3: [24 cases, mean 21.65, range 18.2 to 25.3, est err 1.19] #&gt; #&gt; if #&gt; rm &lt;= 3.326479 #&gt; dis &gt; 1.345056 #&gt; lstat &lt;= 2.848535 #&gt; then #&gt; outcome = 43.83 + 14.5 rm - 2.29 lstat - 3.8 dis - 30 tax #&gt; - 0.014 ptratio - 1.4 nox + 0.017 zn + 0.4 rad + 0.15 crim #&gt; - 0.0025 age + 8e-06 b #&gt; #&gt; Rule 1/4: [7 cases, mean 27.66, range 20.7 to 50, est err 7.89] #&gt; #&gt; if #&gt; rm &gt; 3.326479 #&gt; ptratio &gt; 193.545 #&gt; lstat &lt;= 2.848535 #&gt; then #&gt; outcome = 19.64 + 7.8 rm - 3.4 dis - 1.62 lstat + 0.27 crim - 0.006 age #&gt; + 0.023 zn - 7 tax - 0.003 ptratio #&gt; #&gt; Rule 1/5: [141 cases, mean 30.60, range 15 to 50, est err 2.09] #&gt; #&gt; if #&gt; rm &gt; 3.326479 #&gt; ptratio &lt;= 193.545 #&gt; then #&gt; outcome = 137.95 + 21.7 rm - 3.43 lstat - 4.9 dis - 87 tax - 0.0162 age #&gt; - 0.039 ptratio + 0.06 crim + 0.005 zn #&gt; #&gt; Rule 1/6: [8 cases, mean 32.16, range 22.1 to 50, est err 8.67] #&gt; #&gt; if #&gt; rm &lt;= 3.326479 #&gt; dis &lt;= 1.345056 #&gt; lstat &lt;= 2.848535 #&gt; then #&gt; outcome = -19.71 + 18.58 lstat - 15.9 dis + 5.6 rm #&gt; #&gt; Model 2: #&gt; #&gt; Rule 2/1: [23 cases, mean 10.57, range 5 to 15, est err 3.06] #&gt; #&gt; if #&gt; crim &gt; 2.086391 #&gt; dis &lt;= 0.6604174 #&gt; b &gt; 67032.41 #&gt; then #&gt; outcome = 37.22 - 4.83 crim - 7 dis - 1.9 lstat - 1.9e-05 b - 0.7 rm #&gt; #&gt; Rule 2/2: [70 cases, mean 14.82, range 5 to 50, est err 3.90] #&gt; #&gt; if #&gt; rm &lt;= 3.620525 #&gt; dis &lt;= 0.6604174 #&gt; then #&gt; outcome = 74.6 - 21 dis - 5.09 lstat - 15 tax - 0.0017 age + 6e-06 b #&gt; #&gt; Rule 2/3: [18 cases, mean 18.03, range 7.5 to 50, est err 6.81] #&gt; #&gt; if #&gt; crim &gt; 2.086391 #&gt; dis &lt;= 0.6604174 #&gt; b &lt;= 67032.41 #&gt; then #&gt; outcome = 94.95 - 40.1 dis - 8.15 crim - 7.14 lstat - 3.5e-05 b - 1.3 rm #&gt; #&gt; Rule 2/4: [258 cases, mean 20.74, range 9.5 to 36.2, est err 1.92] #&gt; #&gt; if #&gt; rm &lt;= 3.620525 #&gt; dis &gt; 0.6604174 #&gt; lstat &gt; 1.805082 #&gt; then #&gt; outcome = 61.89 - 2.56 lstat + 5.5 rm - 2.8 dis + 7.3e-05 b - 0.0132 age #&gt; - 26 tax - 0.11 indus - 0.004 ptratio + 0.05 crim #&gt; #&gt; Rule 2/5: [37 cases, mean 31.66, range 10.4 to 50, est err 3.70] #&gt; #&gt; if #&gt; rm &gt; 3.620525 #&gt; lstat &gt; 1.805082 #&gt; then #&gt; outcome = 370.03 - 180 tax - 2.19 lstat - 1.7 dis + 2.6 rm #&gt; - 0.016 ptratio - 0.25 indus + 0.12 crim - 0.0021 age #&gt; + 9e-06 b - 0.5 nox #&gt; #&gt; Rule 2/6: [42 cases, mean 38.23, range 22.8 to 50, est err 3.70] #&gt; #&gt; if #&gt; lstat &lt;= 1.805082 #&gt; then #&gt; outcome = -73.87 + 32.4 rm - 9.4e-05 b - 1.8 dis + 0.028 zn #&gt; - 0.013 ptratio #&gt; #&gt; Rule 2/7: [4 cases, mean 40.20, range 37.6 to 42.8, est err 7.33] #&gt; #&gt; if #&gt; rm &gt; 4.151791 #&gt; dis &gt; 1.114486 #&gt; then #&gt; outcome = 35.8 #&gt; #&gt; Rule 2/8: [8 cases, mean 47.45, range 41.3 to 50, est err 10.01] #&gt; #&gt; if #&gt; dis &lt;= 1.114486 #&gt; lstat &lt;= 1.805082 #&gt; then #&gt; outcome = 48.96 + 7.53 crim - 4.1e-05 b - 0.8 dis + 1.2 rm + 0.008 zn #&gt; #&gt; Model 3: #&gt; #&gt; Rule 3/1: [81 cases, mean 13.93, range 5 to 23.2, est err 2.24] #&gt; #&gt; if #&gt; nox &gt; -0.4864544 #&gt; lstat &gt; 2.848535 #&gt; then #&gt; outcome = 55.03 - 0.0631 age - 2.11 crim + 12 nox - 4.16 lstat #&gt; + 3.2e-05 b #&gt; #&gt; Rule 3/2: [163 cases, mean 19.37, range 7 to 31, est err 2.29] #&gt; #&gt; if #&gt; nox &lt;= -0.4864544 #&gt; lstat &gt; 2.848535 #&gt; then #&gt; outcome = 77.73 - 0.059 ptratio + 5.8 rm - 3.2 dis - 0.0139 age #&gt; - 1.15 lstat - 30 tax - 1.1 nox + 0.4 rad #&gt; #&gt; Rule 3/3: [62 cases, mean 24.01, range 18.2 to 50, est err 3.56] #&gt; #&gt; if #&gt; rm &lt;= 3.448196 #&gt; lstat &lt;= 2.848535 #&gt; then #&gt; outcome = 94.86 + 18.2 rm + 0.63 crim - 68 tax - 2.3 dis - 3 nox #&gt; - 0.0098 age - 0.41 indus - 0.011 ptratio #&gt; #&gt; Rule 3/4: [143 cases, mean 28.76, range 16.5 to 50, est err 2.53] #&gt; #&gt; if #&gt; dis &gt; 0.9547035 #&gt; lstat &lt;= 2.848535 #&gt; then #&gt; outcome = 269.46 + 17.9 rm - 6.1 dis - 153 tax + 0.96 crim - 0.0217 age #&gt; - 5.5 nox - 0.62 indus - 0.028 ptratio - 0.89 lstat + 0.4 rad #&gt; + 0.004 zn #&gt; #&gt; Rule 3/5: [10 cases, mean 35.13, range 21.9 to 50, est err 9.31] #&gt; #&gt; if #&gt; dis &lt;= 0.6492998 #&gt; lstat &lt;= 2.848535 #&gt; then #&gt; outcome = 58.69 - 56.8 dis - 8.4 nox #&gt; #&gt; Rule 3/6: [10 cases, mean 41.67, range 22 to 50, est err 9.89] #&gt; #&gt; if #&gt; dis &gt; 0.6492998 #&gt; dis &lt;= 0.9547035 #&gt; lstat &lt;= 2.848535 #&gt; then #&gt; outcome = 47.93 #&gt; #&gt; Model 4: #&gt; #&gt; Rule 4/1: [69 cases, mean 12.69, range 5 to 27.5, est err 2.55] #&gt; #&gt; if #&gt; dis &lt;= 0.719156 #&gt; lstat &gt; 3.508535 #&gt; then #&gt; outcome = 180.13 - 7.2 dis + 0.039 age - 3.78 lstat - 83 tax #&gt; #&gt; Rule 4/2: [164 cases, mean 19.42, range 12 to 31, est err 1.96] #&gt; #&gt; if #&gt; dis &gt; 0.719156 #&gt; lstat &gt; 2.848535 #&gt; then #&gt; outcome = 52.75 + 7.1 rm - 2.05 lstat - 3.6 dis + 8.2e-05 b - 0.0152 age #&gt; - 25 tax + 0.5 rad - 1.2 nox - 0.008 ptratio #&gt; #&gt; Rule 4/3: [11 cases, mean 20.39, range 15 to 27.9, est err 3.51] #&gt; #&gt; if #&gt; dis &lt;= 0.719156 #&gt; lstat &gt; 2.848535 #&gt; lstat &lt;= 3.508535 #&gt; then #&gt; outcome = 21.69 #&gt; #&gt; Rule 4/4: [63 cases, mean 23.22, range 16.5 to 31.5, est err 1.67] #&gt; #&gt; if #&gt; rm &lt;= 3.483629 #&gt; dis &gt; 0.9731624 #&gt; lstat &lt;= 2.848535 #&gt; then #&gt; outcome = 59.35 - 3.96 lstat - 3.1 dis + 1 rm - 14 tax + 0.3 rad #&gt; - 0.7 nox - 0.005 ptratio + 6e-06 b #&gt; #&gt; Rule 4/5: [8 cases, mean 33.08, range 22 to 50, est err 23.91] #&gt; #&gt; if #&gt; rm &gt; 3.369183 #&gt; dis &lt;= 0.9731624 #&gt; lstat &gt; 2.254579 #&gt; lstat &lt;= 2.848535 #&gt; then #&gt; outcome = -322.28 + 64.9 lstat + 56.8 rm - 30.2 dis #&gt; #&gt; Rule 4/6: [7 cases, mean 33.87, range 22.1 to 50, est err 13.21] #&gt; #&gt; if #&gt; rm &lt;= 3.369183 #&gt; dis &lt;= 0.9731624 #&gt; lstat &lt;= 2.848535 #&gt; then #&gt; outcome = -52.11 + 43.45 lstat - 30.8 dis #&gt; #&gt; Rule 4/7: [91 cases, mean 34.43, range 21.9 to 50, est err 3.32] #&gt; #&gt; if #&gt; rm &gt; 3.483629 #&gt; lstat &lt;= 2.848535 #&gt; then #&gt; outcome = -33.09 + 22 rm - 5.02 lstat - 0.038 ptratio - 0.9 dis #&gt; + 0.005 zn #&gt; #&gt; Rule 4/8: [22 cases, mean 36.99, range 21.9 to 50, est err 13.21] #&gt; #&gt; if #&gt; dis &lt;= 0.9731624 #&gt; lstat &lt;= 2.848535 #&gt; then #&gt; outcome = 80.3 - 17.43 lstat - 0.134 ptratio + 2.5 rm - 1.2 dis #&gt; + 0.008 zn #&gt; #&gt; Model 5: #&gt; #&gt; Rule 5/1: [84 cases, mean 14.29, range 5 to 27.5, est err 2.81] #&gt; #&gt; if #&gt; nox &gt; -0.4864544 #&gt; then #&gt; outcome = 56.48 + 28.5 nox - 0.0875 age - 3.58 crim - 5.9 dis #&gt; - 2.96 lstat + 0.073 ptratio + 1.7e-05 b #&gt; #&gt; Rule 5/2: [163 cases, mean 19.37, range 7 to 31, est err 2.38] #&gt; #&gt; if #&gt; nox &lt;= -0.4864544 #&gt; lstat &gt; 2.848535 #&gt; then #&gt; outcome = 61.59 - 0.064 ptratio + 5.9 rm - 3.1 dis - 0.0142 age #&gt; - 0.77 lstat - 21 tax #&gt; #&gt; Rule 5/3: [163 cases, mean 29.94, range 16.5 to 50, est err 3.65] #&gt; #&gt; if #&gt; lstat &lt;= 2.848535 #&gt; then #&gt; outcome = 264.17 + 21.9 rm - 8 dis - 155 tax - 0.0317 age #&gt; - 0.032 ptratio + 0.29 crim - 1.6 nox - 0.25 indus #&gt; #&gt; Rule 5/4: [10 cases, mean 35.13, range 21.9 to 50, est err 11.79] #&gt; #&gt; if #&gt; dis &lt;= 0.6492998 #&gt; lstat &lt;= 2.848535 #&gt; then #&gt; outcome = 68.19 - 73.4 dis + 1.1 rm + 0.11 crim - 0.6 nox - 0.1 indus #&gt; - 0.0017 age - 0.12 lstat #&gt; #&gt; Model 6: #&gt; #&gt; Rule 6/1: [71 cases, mean 15.57, range 5 to 50, est err 4.42] #&gt; #&gt; if #&gt; dis &lt;= 0.6443245 #&gt; lstat &gt; 1.793385 #&gt; then #&gt; outcome = 45.7 - 20.6 dis - 5.38 lstat #&gt; #&gt; Rule 6/2: [159 cases, mean 19.53, range 8.3 to 36.2, est err 2.08] #&gt; #&gt; if #&gt; rm &lt;= 3.329365 #&gt; dis &gt; 0.6443245 #&gt; then #&gt; outcome = 24.33 + 8.8 rm + 0.000118 b - 0.0146 age - 2.5 dis #&gt; - 0.95 lstat + 0.37 crim - 0.32 indus + 0.02 zn - 16 tax #&gt; + 0.2 rad - 0.5 nox - 0.004 ptratio #&gt; #&gt; Rule 6/3: [175 cases, mean 27.80, range 9.5 to 50, est err 2.95] #&gt; #&gt; if #&gt; rm &gt; 3.329365 #&gt; dis &gt; 0.6443245 #&gt; then #&gt; outcome = 0.11 + 18.7 rm - 3.11 lstat + 8.1e-05 b - 1.1 dis + 0.19 crim #&gt; - 20 tax - 0.19 indus + 0.3 rad - 0.7 nox - 0.005 ptratio #&gt; + 0.006 zn #&gt; #&gt; Rule 6/4: [8 cases, mean 32.50, range 21.9 to 50, est err 10.34] #&gt; #&gt; if #&gt; dis &lt;= 0.6443245 #&gt; lstat &gt; 1.793385 #&gt; lstat &lt;= 2.894121 #&gt; then #&gt; outcome = 69.38 - 71.2 dis - 0.14 lstat #&gt; #&gt; Rule 6/5: [34 cases, mean 37.55, range 22.8 to 50, est err 3.55] #&gt; #&gt; if #&gt; rm &lt;= 4.151791 #&gt; lstat &lt;= 1.793385 #&gt; then #&gt; outcome = -125.14 + 41.7 rm + 4.3 rad + 1.48 indus - 0.014 ptratio #&gt; #&gt; Rule 6/6: [7 cases, mean 43.66, range 37.6 to 50, est err 3.12] #&gt; #&gt; if #&gt; rm &gt; 4.151791 #&gt; lstat &lt;= 1.793385 #&gt; then #&gt; outcome = -137.67 + 44.6 rm - 0.064 ptratio #&gt; #&gt; Model 7: #&gt; #&gt; Rule 7/1: [84 cases, mean 14.29, range 5 to 27.5, est err 2.91] #&gt; #&gt; if #&gt; nox &gt; -0.4864544 #&gt; then #&gt; outcome = 46.85 - 3.45 crim - 0.0621 age + 14.2 nox + 4.4 dis #&gt; - 2.01 lstat + 2.5e-05 b #&gt; #&gt; Rule 7/2: [323 cases, mean 24.66, range 7 to 50, est err 3.68] #&gt; #&gt; if #&gt; nox &lt;= -0.4864544 #&gt; then #&gt; outcome = 57.59 - 0.065 ptratio - 4.4 dis + 6.8 rm - 0.0143 age #&gt; - 1.36 lstat - 19 tax - 0.8 nox - 0.12 crim + 0.09 indus #&gt; #&gt; Rule 7/3: [132 cases, mean 28.24, range 16.5 to 50, est err 2.55] #&gt; #&gt; if #&gt; dis &gt; 1.063503 #&gt; lstat &lt;= 2.848535 #&gt; then #&gt; outcome = 270.92 + 24.5 rm - 0.0418 age - 165 tax - 5.7 dis #&gt; - 0.028 ptratio + 0.26 crim + 0.017 zn #&gt; #&gt; Rule 7/4: [7 cases, mean 36.01, range 23.3 to 50, est err 3.87] #&gt; #&gt; if #&gt; dis &lt;= 0.6002641 #&gt; lstat &lt;= 2.848535 #&gt; then #&gt; outcome = 57.18 - 69.5 dis - 6.5 nox + 1.9 rm - 0.015 ptratio #&gt; #&gt; Rule 7/5: [24 cases, mean 37.55, range 21.9 to 50, est err 8.66] #&gt; #&gt; if #&gt; dis &gt; 0.6002641 #&gt; dis &lt;= 1.063503 #&gt; lstat &lt;= 2.848535 #&gt; then #&gt; outcome = -3.76 - 14.8 dis - 2.93 crim - 0.16 ptratio + 17.5 rm - 15 nox #&gt; #&gt; Model 8: #&gt; #&gt; Rule 8/1: [80 cases, mean 13.75, range 5 to 27.9, est err 3.51] #&gt; #&gt; if #&gt; dis &lt;= 0.719156 #&gt; lstat &gt; 2.848535 #&gt; then #&gt; outcome = 123.46 - 11.3 dis - 5.06 lstat - 45 tax + 0.9 rad + 1.7e-05 b #&gt; #&gt; Rule 8/2: [164 cases, mean 19.42, range 12 to 31, est err 2.05] #&gt; #&gt; if #&gt; dis &gt; 0.719156 #&gt; lstat &gt; 2.848535 #&gt; then #&gt; outcome = 227.11 - 120 tax + 6.4 rm + 9.3e-05 b - 3.3 dis + 2 rad #&gt; - 0.0183 age - 0.93 lstat + 0.05 crim - 0.3 nox #&gt; #&gt; Rule 8/3: [163 cases, mean 29.94, range 16.5 to 50, est err 3.54] #&gt; #&gt; if #&gt; lstat &lt;= 2.848535 #&gt; then #&gt; outcome = 158.14 - 5.73 lstat + 10.8 rm - 4 dis - 83 tax - 4.1 nox #&gt; + 0.61 crim - 0.54 indus + 1 rad + 3.6e-05 b #&gt; #&gt; Rule 8/4: [7 cases, mean 36.01, range 23.3 to 50, est err 11.44] #&gt; #&gt; if #&gt; dis &lt;= 0.6002641 #&gt; lstat &lt;= 2.848535 #&gt; then #&gt; outcome = 72.89 - 87.2 dis + 0.6 rm - 0.13 lstat #&gt; #&gt; Rule 8/5: [47 cases, mean 38.44, range 15 to 50, est err 5.71] #&gt; #&gt; if #&gt; rm &gt; 3.726352 #&gt; then #&gt; outcome = 602.95 - 10.4 lstat + 21 rm - 326 tax - 0.093 ptratio #&gt; #&gt; Model 9: #&gt; #&gt; Rule 9/1: [81 cases, mean 13.93, range 5 to 23.2, est err 2.91] #&gt; #&gt; if #&gt; nox &gt; -0.4864544 #&gt; lstat &gt; 2.848535 #&gt; then #&gt; outcome = 41.11 - 3.98 crim - 4.42 lstat + 6.7 nox #&gt; #&gt; Rule 9/2: [163 cases, mean 19.37, range 7 to 31, est err 2.49] #&gt; #&gt; if #&gt; nox &lt;= -0.4864544 #&gt; lstat &gt; 2.848535 #&gt; then #&gt; outcome = 44.98 - 0.068 ptratio - 4.4 dis + 6.6 rm - 1.25 lstat #&gt; - 0.0118 age - 0.9 nox - 12 tax - 0.08 crim + 0.06 indus #&gt; #&gt; Rule 9/3: [132 cases, mean 28.24, range 16.5 to 50, est err 2.35] #&gt; #&gt; if #&gt; dis &gt; 1.063503 #&gt; lstat &lt;= 2.848535 #&gt; then #&gt; outcome = 157.67 + 22.2 rm - 0.0383 age - 104 tax - 0.033 ptratio #&gt; - 2.2 dis #&gt; #&gt; Rule 9/4: [7 cases, mean 30.76, range 21.9 to 50, est err 6.77] #&gt; #&gt; if #&gt; dis &lt;= 1.063503 #&gt; b &lt;= 66469.73 #&gt; lstat &lt;= 2.848535 #&gt; then #&gt; outcome = 48.52 - 56.1 dis - 12.9 nox - 0.032 ptratio + 2.7 rm #&gt; #&gt; Rule 9/5: [24 cases, mean 39.09, range 22 to 50, est err 6.20] #&gt; #&gt; if #&gt; dis &lt;= 1.063503 #&gt; b &gt; 66469.73 #&gt; lstat &lt;= 2.848535 #&gt; then #&gt; outcome = -5.49 - 34.8 dis - 20.7 nox + 18.2 rm - 0.051 ptratio #&gt; #&gt; Model 10: #&gt; #&gt; Rule 10/1: [327 cases, mean 19.45, range 5 to 50, est err 2.77] #&gt; #&gt; if #&gt; rm &lt;= 3.617282 #&gt; lstat &gt; 1.805082 #&gt; then #&gt; outcome = 270.78 - 4.09 lstat - 131 tax + 2.9 rad + 5.3e-05 b - 0.6 dis #&gt; - 0.16 indus + 0.7 rm - 0.3 nox #&gt; #&gt; Rule 10/2: [38 cases, mean 31.57, range 10.4 to 50, est err 4.71] #&gt; #&gt; if #&gt; rm &gt; 3.617282 #&gt; lstat &gt; 1.805082 #&gt; then #&gt; outcome = 308.44 - 150 tax - 2.63 lstat + 1.6 rad - 1.9 dis - 0.49 indus #&gt; + 2.5 rm + 3e-05 b - 1.2 nox + 0.14 crim - 0.005 ptratio #&gt; #&gt; Rule 10/3: [35 cases, mean 37.15, range 22.8 to 50, est err 2.76] #&gt; #&gt; if #&gt; rm &lt;= 4.151791 #&gt; lstat &lt;= 1.805082 #&gt; then #&gt; outcome = -71.65 + 33.4 rm - 0.017 ptratio - 0.34 lstat + 0.2 rad #&gt; - 0.3 dis - 7 tax - 0.4 nox #&gt; #&gt; Rule 10/4: [10 cases, mean 42.63, range 21.9 to 50, est err 7.11] #&gt; #&gt; if #&gt; rm &gt; 4.151791 #&gt; then #&gt; outcome = -92.51 + 32.8 rm - 0.03 ptratio #&gt; #&gt; Model 11: #&gt; #&gt; Rule 11/1: [84 cases, mean 14.29, range 5 to 27.5, est err 4.13] #&gt; #&gt; if #&gt; nox &gt; -0.4864544 #&gt; then #&gt; outcome = 42.75 - 4.12 crim + 18.1 nox - 0.045 age + 6.8 dis #&gt; - 1.86 lstat #&gt; #&gt; Rule 11/2: [244 cases, mean 17.56, range 5 to 31, est err 4.29] #&gt; #&gt; if #&gt; lstat &gt; 2.848535 #&gt; then #&gt; outcome = 34.83 - 5.2 dis - 0.058 ptratio - 0.0228 age + 5.8 rm #&gt; - 0.56 lstat - 0.07 crim - 0.4 nox - 5 tax #&gt; #&gt; Rule 11/3: [163 cases, mean 29.94, range 16.5 to 50, est err 3.49] #&gt; #&gt; if #&gt; lstat &lt;= 2.848535 #&gt; then #&gt; outcome = 151.5 + 23.3 rm - 5.5 dis + 1.01 crim - 0.0211 age #&gt; - 0.052 ptratio - 98 tax + 0.031 zn #&gt; #&gt; Rule 11/4: [10 cases, mean 35.13, range 21.9 to 50, est err 25.19] #&gt; #&gt; if #&gt; dis &lt;= 0.6492998 #&gt; lstat &lt;= 2.848535 #&gt; then #&gt; outcome = 130.87 - 157.1 dis - 15.76 crim #&gt; #&gt; Model 12: #&gt; #&gt; Rule 12/1: [80 cases, mean 13.75, range 5 to 27.9, est err 4.76] #&gt; #&gt; if #&gt; dis &lt;= 0.719156 #&gt; lstat &gt; 2.894121 #&gt; then #&gt; outcome = 182.68 - 6.03 lstat - 7.6 dis - 76 tax + 1.3 rad - 0.52 indus #&gt; + 2.6e-05 b #&gt; #&gt; Rule 12/2: [300 cases, mean 19.10, range 5 to 50, est err 2.76] #&gt; #&gt; if #&gt; rm &lt;= 3.50716 #&gt; lstat &gt; 1.793385 #&gt; then #&gt; outcome = 83.61 - 3 lstat + 9.6e-05 b - 0.0072 age - 33 tax + 0.7 rad #&gt; + 0.32 indus #&gt; #&gt; Rule 12/3: [10 cases, mean 24.25, range 15.7 to 36.2, est err 13.88] #&gt; #&gt; if #&gt; rm &lt;= 3.50716 #&gt; tax &lt;= 1.865769 #&gt; then #&gt; outcome = 35.46 #&gt; #&gt; Rule 12/4: [10 cases, mean 32.66, range 21.9 to 50, est err 6.28] #&gt; #&gt; if #&gt; dis &lt;= 0.719156 #&gt; lstat &gt; 1.793385 #&gt; lstat &lt;= 2.894121 #&gt; then #&gt; outcome = 82.78 - 69.5 dis - 3.66 indus #&gt; #&gt; Rule 12/5: [89 cases, mean 32.75, range 13.4 to 50, est err 3.39] #&gt; #&gt; if #&gt; rm &gt; 3.50716 #&gt; dis &gt; 0.719156 #&gt; then #&gt; outcome = 313.22 + 13.7 rm - 174 tax - 3.06 lstat + 4.8e-05 b - 1.5 dis #&gt; - 0.41 indus + 0.7 rad - 0.0055 age + 0.22 crim #&gt; #&gt; Rule 12/6: [34 cases, mean 37.55, range 22.8 to 50, est err 3.25] #&gt; #&gt; if #&gt; rm &lt;= 4.151791 #&gt; lstat &lt;= 1.793385 #&gt; then #&gt; outcome = -86.8 + 36 rm - 0.3 lstat - 5 tax #&gt; #&gt; Rule 12/7: [7 cases, mean 43.66, range 37.6 to 50, est err 5.79] #&gt; #&gt; if #&gt; rm &gt; 4.151791 #&gt; lstat &lt;= 1.793385 #&gt; then #&gt; outcome = -158.68 + 47.4 rm - 0.02 ptratio #&gt; #&gt; Model 13: #&gt; #&gt; Rule 13/1: [84 cases, mean 14.29, range 5 to 27.5, est err 2.87] #&gt; #&gt; if #&gt; nox &gt; -0.4864544 #&gt; then #&gt; outcome = 54.69 - 3.79 crim - 0.0644 age + 11.4 nox - 2.53 lstat #&gt; #&gt; Rule 13/2: [8 cases, mean 17.76, range 7 to 27.9, est err 13.69] #&gt; #&gt; if #&gt; nox &lt;= -0.4864544 #&gt; age &gt; 296.3423 #&gt; b &lt;= 60875.57 #&gt; then #&gt; outcome = -899.55 + 3.0551 age #&gt; #&gt; Rule 13/3: [31 cases, mean 17.94, range 7 to 27.9, est err 5.15] #&gt; #&gt; if #&gt; nox &lt;= -0.4864544 #&gt; b &lt;= 60875.57 #&gt; lstat &gt; 2.848535 #&gt; then #&gt; outcome = 44.43 - 3.51 lstat - 0.054 ptratio - 1.4 dis - 0.26 crim #&gt; - 0.0042 age - 0.21 indus + 0.9 rm #&gt; #&gt; Rule 13/4: [163 cases, mean 19.37, range 7 to 31, est err 3.37] #&gt; #&gt; if #&gt; nox &lt;= -0.4864544 #&gt; lstat &gt; 2.848535 #&gt; then #&gt; outcome = -5.76 + 0.000242 b + 8.9 rm - 5.2 dis - 0.0209 age #&gt; - 0.042 ptratio - 0.63 indus #&gt; #&gt; Rule 13/5: [163 cases, mean 29.94, range 16.5 to 50, est err 3.45] #&gt; #&gt; if #&gt; lstat &lt;= 2.848535 #&gt; then #&gt; outcome = 178.84 + 23.8 rm - 0.0343 age - 4.5 dis - 114 tax + 0.88 crim #&gt; - 0.048 ptratio + 0.026 zn #&gt; #&gt; Rule 13/6: [7 cases, mean 36.01, range 23.3 to 50, est err 14.09] #&gt; #&gt; if #&gt; dis &lt;= 0.6002641 #&gt; lstat &lt;= 2.848535 #&gt; then #&gt; outcome = 45.82 - 70.3 dis - 9.9 nox + 5.1 rm + 1.5 rad #&gt; #&gt; Rule 13/7: [31 cases, mean 37.21, range 21.9 to 50, est err 7.73] #&gt; #&gt; if #&gt; dis &lt;= 1.063503 #&gt; lstat &lt;= 2.848535 #&gt; then #&gt; outcome = 95.05 - 4.52 lstat - 7.5 dis + 8.8 rm - 0.064 ptratio #&gt; - 6.2 nox - 36 tax #&gt; #&gt; Model 14: #&gt; #&gt; Rule 14/1: [49 cases, mean 16.06, range 8.4 to 22.7, est err 3.17] #&gt; #&gt; if #&gt; nox &gt; -0.4205732 #&gt; lstat &gt; 2.848535 #&gt; then #&gt; outcome = 12.83 + 42.3 nox - 4.77 lstat + 9.7 rm + 7.8e-05 b #&gt; #&gt; Rule 14/2: [78 cases, mean 16.36, range 5 to 50, est err 5.17] #&gt; #&gt; if #&gt; dis &lt;= 0.6604174 #&gt; then #&gt; outcome = 110.6 - 10.4 dis - 4.85 lstat + 0.0446 age - 46 tax + 0.8 rad #&gt; #&gt; Rule 14/3: [57 cases, mean 18.40, range 9.5 to 31, est err 2.43] #&gt; #&gt; if #&gt; nox &gt; -0.9365134 #&gt; nox &lt;= -0.4205732 #&gt; age &gt; 245.2507 #&gt; dis &gt; 0.6604174 #&gt; lstat &gt; 2.848535 #&gt; then #&gt; outcome = 206.69 - 0.1012 age - 7.05 lstat + 12.2 nox - 67 tax + 0.3 rad #&gt; + 0.5 rm - 0.3 dis #&gt; #&gt; Rule 14/4: [230 cases, mean 20.19, range 9.5 to 36.2, est err 2.09] #&gt; #&gt; if #&gt; rm &lt;= 3.483629 #&gt; dis &gt; 0.6492998 #&gt; then #&gt; outcome = 119.15 - 2.61 lstat + 5.2 rm - 57 tax - 1.8 dis - 2.4 nox #&gt; + 0.7 rad + 0.24 crim + 0.003 age - 0.007 ptratio + 9e-06 b #&gt; #&gt; Rule 14/5: [48 cases, mean 20.28, range 10.2 to 24.5, est err 2.13] #&gt; #&gt; if #&gt; nox &gt; -0.9365134 #&gt; nox &lt;= -0.4205732 #&gt; age &lt;= 245.2507 #&gt; dis &gt; 0.6604174 #&gt; lstat &gt; 2.848535 #&gt; then #&gt; outcome = 19.4 - 1.91 lstat + 1.02 indus - 0.013 age + 2.7 rm + 2.6 nox #&gt; - 0.009 ptratio #&gt; #&gt; Rule 14/6: [44 cases, mean 20.69, range 14.4 to 29.6, est err 2.26] #&gt; #&gt; if #&gt; nox &lt;= -0.9365134 #&gt; lstat &gt; 2.848535 #&gt; then #&gt; outcome = 87.55 - 0.000315 b - 6.5 dis + 2.6 rad - 0.59 lstat - 18 tax #&gt; #&gt; Rule 14/7: [102 cases, mean 32.44, range 13.4 to 50, est err 3.35] #&gt; #&gt; if #&gt; rm &gt; 3.483629 #&gt; dis &gt; 0.6492998 #&gt; then #&gt; outcome = 126.92 + 22.7 rm - 4.68 lstat - 85 tax - 0.036 ptratio #&gt; - 1.1 dis + 0.007 zn #&gt; #&gt; Rule 14/8: [84 cases, mean 33.40, range 21 to 50, est err 2.44] #&gt; #&gt; if #&gt; rm &gt; 3.483629 #&gt; tax &lt;= 1.896025 #&gt; then #&gt; outcome = 347.12 + 25.2 rm - 213 tax - 3.5 lstat - 0.013 ptratio #&gt; #&gt; Rule 14/9: [10 cases, mean 35.13, range 21.9 to 50, est err 12.13] #&gt; #&gt; if #&gt; dis &lt;= 0.6492998 #&gt; lstat &lt;= 2.848535 #&gt; then #&gt; outcome = 72.65 - 77.8 dis #&gt; #&gt; Model 15: #&gt; #&gt; Rule 15/1: [28 cases, mean 12.35, range 5 to 27.9, est err 4.09] #&gt; #&gt; if #&gt; crim &gt; 2.405809 #&gt; b &gt; 16084.5 #&gt; then #&gt; outcome = 53.45 - 7.8 crim - 3.5 lstat - 0.0189 age #&gt; #&gt; Rule 15/2: [11 cases, mean 13.56, range 8.3 to 27.5, est err 5.99] #&gt; #&gt; if #&gt; crim &gt; 2.405809 #&gt; b &lt;= 16084.5 #&gt; then #&gt; outcome = 8.73 + 0.001756 b #&gt; #&gt; Rule 15/3: [244 cases, mean 17.56, range 5 to 31, est err 2.73] #&gt; #&gt; if #&gt; lstat &gt; 2.848535 #&gt; then #&gt; outcome = 103.02 - 0.0251 age - 2.37 lstat - 3.5 dis + 6.8e-05 b + 4 rm #&gt; - 0.035 ptratio - 41 tax - 0.25 crim #&gt; #&gt; Rule 15/4: [131 cases, mean 28.22, range 16.5 to 50, est err 2.59] #&gt; #&gt; if #&gt; dis &gt; 1.086337 #&gt; lstat &lt;= 2.848535 #&gt; then #&gt; outcome = 267.07 + 17.7 rm - 0.0421 age - 150 tax - 5.5 dis + 0.88 crim #&gt; - 0.035 ptratio + 0.031 zn - 0.12 lstat - 0.3 nox #&gt; #&gt; Rule 15/5: [13 cases, mean 33.08, range 22 to 50, est err 4.44] #&gt; #&gt; if #&gt; nox &lt;= -0.7229691 #&gt; dis &lt;= 1.086337 #&gt; lstat &lt;= 2.848535 #&gt; then #&gt; outcome = 148.52 - 0.002365 b - 85.9 nox - 1 dis + 0.16 crim + 0.8 rm #&gt; + 0.007 zn - 0.0016 age - 7 tax - 0.003 ptratio #&gt; #&gt; Rule 15/6: [7 cases, mean 36.01, range 23.3 to 50, est err 7.00] #&gt; #&gt; if #&gt; dis &lt;= 0.6002641 #&gt; lstat &lt;= 2.848535 #&gt; then #&gt; outcome = 50.55 - 68.1 dis - 11.4 nox + 0.00012 b + 1 rm - 0.008 ptratio #&gt; #&gt; Rule 15/7: [12 cases, mean 41.77, range 21.9 to 50, est err 9.73] #&gt; #&gt; if #&gt; nox &gt; -0.7229691 #&gt; dis &gt; 0.6002641 #&gt; lstat &lt;= 2.848535 #&gt; then #&gt; outcome = 13.74 - 92 nox - 40.5 dis - 0.023 ptratio + 2.6 rm #&gt; #&gt; Model 16: #&gt; #&gt; Rule 16/1: [60 cases, mean 15.95, range 7.2 to 27.5, est err 3.16] #&gt; #&gt; if #&gt; nox &gt; -0.4344906 #&gt; then #&gt; outcome = 46.98 - 6.53 lstat - 6.9 dis - 1.1 rm #&gt; #&gt; Rule 16/2: [45 cases, mean 16.89, range 5 to 50, est err 5.45] #&gt; #&gt; if #&gt; nox &lt;= -0.4344906 #&gt; dis &lt;= 0.6557049 #&gt; then #&gt; outcome = 35.33 - 37 dis - 51.7 nox - 7.38 lstat - 0.4 rm #&gt; #&gt; Rule 16/3: [128 cases, mean 19.97, range 9.5 to 36.2, est err 2.52] #&gt; #&gt; if #&gt; rm &lt;= 3.626081 #&gt; dis &gt; 0.6557049 #&gt; dis &lt;= 1.298828 #&gt; lstat &gt; 2.133251 #&gt; then #&gt; outcome = 61.65 - 3.35 lstat + 4.9 dis + 1.6 rm - 1.3 nox - 22 tax #&gt; + 0.5 rad + 1.8e-05 b + 0.09 crim - 0.004 ptratio #&gt; #&gt; Rule 16/4: [140 cases, mean 21.93, range 12.7 to 35.1, est err 2.19] #&gt; #&gt; if #&gt; rm &lt;= 3.626081 #&gt; dis &gt; 1.298828 #&gt; then #&gt; outcome = 54.16 - 3.58 lstat + 2.2 rad - 1.6 dis - 1.9 nox + 1.8 rm #&gt; - 17 tax + 1.3e-05 b + 0.06 crim - 0.003 ptratio #&gt; #&gt; Rule 16/5: [30 cases, mean 21.97, range 14.4 to 29.1, est err 2.41] #&gt; #&gt; if #&gt; rm &lt;= 3.626081 #&gt; dis &gt; 1.298828 #&gt; tax &lt;= 1.879832 #&gt; lstat &gt; 2.133251 #&gt; then #&gt; outcome = -1065.35 + 566 tax + 8.7 rm - 0.13 lstat - 0.2 dis - 0.3 nox #&gt; #&gt; Rule 16/6: [22 cases, mean 30.88, range 10.4 to 50, est err 4.51] #&gt; #&gt; if #&gt; rm &gt; 3.626081 #&gt; lstat &gt; 2.133251 #&gt; then #&gt; outcome = 42.24 + 18.7 rm - 1.5 indus - 1.84 lstat - 2.5 nox - 1.6 dis #&gt; - 39 tax + 0.7 rad - 0.012 ptratio + 0.0035 age + 1.2e-05 b #&gt; + 0.11 crim #&gt; #&gt; Rule 16/7: [73 cases, mean 34.52, range 20.6 to 50, est err 3.36] #&gt; #&gt; if #&gt; lstat &lt;= 2.133251 #&gt; then #&gt; outcome = 50.6 + 19.6 rm - 2.77 lstat - 3.2 nox - 1.7 dis - 45 tax #&gt; + 1 rad + 0.007 age - 0.014 ptratio #&gt; #&gt; Model 17: #&gt; #&gt; Rule 17/1: [116 cases, mean 15.37, range 5 to 27.9, est err 2.55] #&gt; #&gt; if #&gt; crim &gt; 0.4779842 #&gt; lstat &gt; 2.944963 #&gt; then #&gt; outcome = 35.96 - 3.68 crim - 3.41 lstat + 0.3 nox #&gt; #&gt; Rule 17/2: [112 cases, mean 19.13, range 7 to 31, est err 2.14] #&gt; #&gt; if #&gt; crim &lt;= 0.4779842 #&gt; lstat &gt; 2.944963 #&gt; then #&gt; outcome = 184.65 - 0.0365 age + 9 rm - 4.1 dis - 97 tax + 8.4e-05 b #&gt; - 0.024 ptratio #&gt; #&gt; Rule 17/3: [9 cases, mean 28.37, range 15 to 50, est err 11.17] #&gt; #&gt; if #&gt; dis &lt;= 0.9547035 #&gt; b &lt;= 66469.73 #&gt; lstat &lt;= 2.944963 #&gt; then #&gt; outcome = -1.12 + 0.000454 b #&gt; #&gt; Rule 17/4: [179 cases, mean 29.28, range 15 to 50, est err 3.35] #&gt; #&gt; if #&gt; lstat &lt;= 2.944963 #&gt; then #&gt; outcome = 278.16 + 20 rm - 7.4 dis - 0.0356 age - 161 tax + 0.051 zn #&gt; - 0.61 lstat + 0.17 crim - 0.008 ptratio #&gt; #&gt; Rule 17/5: [23 cases, mean 36.10, range 15 to 50, est err 10.83] #&gt; #&gt; if #&gt; dis &lt;= 0.9547035 #&gt; lstat &lt;= 2.944963 #&gt; then #&gt; outcome = 233.74 - 8.5 dis + 12.1 rm + 1.15 crim - 2.42 lstat - 113 tax #&gt; - 0.0221 age + 0.068 zn - 0.031 ptratio #&gt; #&gt; Model 18: #&gt; #&gt; Rule 18/1: [84 cases, mean 14.29, range 5 to 27.5, est err 2.44] #&gt; #&gt; if #&gt; nox &gt; -0.4864544 #&gt; then #&gt; outcome = 41.55 - 6.2 lstat + 14.6 nox + 3.8e-05 b #&gt; #&gt; Rule 18/2: [163 cases, mean 19.37, range 7 to 31, est err 2.44] #&gt; #&gt; if #&gt; nox &lt;= -0.4864544 #&gt; lstat &gt; 2.848535 #&gt; then #&gt; outcome = 172.79 - 3.67 lstat + 3.1 rad - 3.5 dis - 72 tax - 0.72 indus #&gt; - 0.033 ptratio - 1.2 nox + 0.0027 age + 0.6 rm + 0.05 crim #&gt; + 5e-06 b #&gt; #&gt; Rule 18/3: [106 cases, mean 25.41, range 16.5 to 50, est err 2.76] #&gt; #&gt; if #&gt; rm &lt;= 3.626081 #&gt; lstat &lt;= 2.848535 #&gt; then #&gt; outcome = 10.71 - 4.6 dis - 2.21 lstat + 2.3 rad + 5.5 rm - 5.3 nox #&gt; - 0.83 indus - 0.003 ptratio #&gt; #&gt; Rule 18/4: [4 cases, mean 33.47, range 30.1 to 36.2, est err 5.61] #&gt; #&gt; if #&gt; rm &lt;= 3.626081 #&gt; tax &lt;= 1.863917 #&gt; lstat &lt;= 2.848535 #&gt; then #&gt; outcome = 36.84 #&gt; #&gt; Rule 18/5: [10 cases, mean 35.13, range 21.9 to 50, est err 17.40] #&gt; #&gt; if #&gt; dis &lt;= 0.6492998 #&gt; lstat &lt;= 2.848535 #&gt; then #&gt; outcome = 84.58 - 94.7 dis - 0.15 lstat #&gt; #&gt; Rule 18/6: [57 cases, mean 38.38, range 21.9 to 50, est err 3.97] #&gt; #&gt; if #&gt; rm &gt; 3.626081 #&gt; lstat &lt;= 2.848535 #&gt; then #&gt; outcome = 100.34 + 22.3 rm - 5.79 lstat - 0.062 ptratio - 69 tax #&gt; + 0.3 rad - 0.5 nox - 0.3 dis + 0.0011 age #&gt; #&gt; #&gt; Evaluation on training data (407 cases): #&gt; #&gt; Average |error| 1.72 #&gt; Relative |error| 0.26 #&gt; Correlation coefficient 0.96 #&gt; #&gt; #&gt; Attribute usage: #&gt; Conds Model #&gt; #&gt; 72% 84% lstat #&gt; 38% 85% dis #&gt; 35% 80% rm #&gt; 27% 55% nox #&gt; 4% 58% crim #&gt; 2% 49% b #&gt; 2% 68% ptratio #&gt; 1% 78% tax #&gt; 1% 67% age #&gt; 41% rad #&gt; 36% indus #&gt; 20% zn #&gt; #&gt; #&gt; Time: 0.2 secs We can now use this model to evaluate our held-out validation dataset. Again, we must prepare the input data using the same Box-Cox transform. # transform the validation dataset set.seed(7) valX &lt;- validation[,1:13] trans_valX &lt;- predict(preprocessParams, valX) valY &lt;- validation[,14] # use final model to make predictions on the validation dataset predictions &lt;- predict(finalModel, newdata = trans_valX, neighbors=3) # calculate RMSE rmse &lt;- RMSE(predictions, valY) r2 &lt;- R2(predictions, valY) print(rmse) #&gt; [1] 3.24 We can see that the estimated RMSE on this unseen data is about 2.666, lower but not too dissimilar from our expected RMSE of 2.822. "],
["classification-algorithms-comparison-breast-cancer-dataset-lg-lda-glmnet-knn-cartm-nb-svm.html", "Chapter 5 Classification algorithms comparison. Breast cancer dataset, (LG, LDA, GLMNET, KNN, CARTM NB, SVM) 5.1 BreastCancer dataset 5.2 Introduction 5.3 Workflow 5.4 Inspect the dataset 5.5 clean up 5.6 Analyze the class variable 5.7 Unimodal visualization 5.8 Multimodal visualization 5.9 Algorithms Evaluation 5.10 Data transform 5.11 Tuning SVM 5.12 Tuning KNN 5.13 Ensemble 5.14 Finalize model 5.15 Prepare the validation set", " Chapter 5 Classification algorithms comparison. Breast cancer dataset, (LG, LDA, GLMNET, KNN, CARTM NB, SVM) 5.1 BreastCancer dataset 5.2 Introduction In this classification problem we apply these algorithms: Linear LG (logistic regression) LDA (linear discriminant analysis) GLMNET (Regularized logistic regression) Non-linear KNN (k-Nearest Neighbors) CART (Classification and Regression Trees) NB (Naive Bayes) SVM (Support Vector Machines) # load packages library(mlbench) library(caret) #&gt; Loading required package: lattice #&gt; Loading required package: ggplot2 #&gt; Registered S3 methods overwritten by &#39;ggplot2&#39;: #&gt; method from #&gt; [.quosures rlang #&gt; c.quosures rlang #&gt; print.quosures rlang library(tictoc) # Load data data(BreastCancer) 5.3 Workflow Load dataset Create train and validation datasets, 80/20 Inspect dataset: dimension class of variables skimr Clean up features Convert character to numeric Frequency table on class remove NAs Visualize features histograms (loop on variables) density plots (loop) boxplot (loop) Pairwise jittered plot Barplots for all features (loop) Train as-is Set the train control to 10 cross-validations 3 repetitions Metric: Accuracy Train the models Numeric comparison of model results Visual comparison dot plot Train with data transformation data transformatiom BoxCox Train models Numeric comparison Visual comparison dot plot Tune the best model: SVM Set the train control to 10 cross-validations 3 repetitions Metric: Accuracy Train the models Radial SVM Sigma vector .C BoxCox Evaluate tuning parameters Tune the best model: KNN Set the train control to 10 cross-validations 3 repetitions Metric: Accuracy Train the models .k BoxCox Evaluate tuning parameters Scatter plot 10, Ensembling Select the algorithms Bagged CART Random Forest Stochastic Gradient Boosting C5.0 Numeric comparison resamples summary Visual comparison dot plot Finalize the model Back transformation preProcess predict Apply model to validation set Prepare validation set Transform the dataset Make prediction knn3Train Calculate accuracy Confusion Matrix 5.4 Inspect the dataset dplyr::glimpse(BreastCancer) #&gt; Observations: 699 #&gt; Variables: 11 #&gt; $ Id &lt;chr&gt; &quot;1000025&quot;, &quot;1002945&quot;, &quot;1015425&quot;, &quot;1016277&quot;, &quot;101… #&gt; $ Cl.thickness &lt;ord&gt; 5, 5, 3, 6, 4, 8, 1, 2, 2, 4, 1, 2, 5, 1, 8, 7, … #&gt; $ Cell.size &lt;ord&gt; 1, 4, 1, 8, 1, 10, 1, 1, 1, 2, 1, 1, 3, 1, 7, 4,… #&gt; $ Cell.shape &lt;ord&gt; 1, 4, 1, 8, 1, 10, 1, 2, 1, 1, 1, 1, 3, 1, 5, 6,… #&gt; $ Marg.adhesion &lt;ord&gt; 1, 5, 1, 1, 3, 8, 1, 1, 1, 1, 1, 1, 3, 1, 10, 4,… #&gt; $ Epith.c.size &lt;ord&gt; 2, 7, 2, 3, 2, 7, 2, 2, 2, 2, 1, 2, 2, 2, 7, 6, … #&gt; $ Bare.nuclei &lt;fct&gt; 1, 10, 2, 4, 1, 10, 10, 1, 1, 1, 1, 1, 3, 3, 9, … #&gt; $ Bl.cromatin &lt;fct&gt; 3, 3, 3, 3, 3, 9, 3, 3, 1, 2, 3, 2, 4, 3, 5, 4, … #&gt; $ Normal.nucleoli &lt;fct&gt; 1, 2, 1, 7, 1, 7, 1, 1, 1, 1, 1, 1, 4, 1, 5, 3, … #&gt; $ Mitoses &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 4, 1, … #&gt; $ Class &lt;fct&gt; benign, benign, benign, benign, benign, malignan… tibble::as_tibble(BreastCancer) #&gt; # A tibble: 699 x 11 #&gt; Id Cl.thickness Cell.size Cell.shape Marg.adhesion Epith.c.size #&gt; &lt;chr&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; #&gt; 1 1000… 5 1 1 1 2 #&gt; 2 1002… 5 4 4 5 7 #&gt; 3 1015… 3 1 1 1 2 #&gt; 4 1016… 6 8 8 1 3 #&gt; 5 1017… 4 1 1 3 2 #&gt; 6 1017… 8 10 10 8 7 #&gt; # … with 693 more rows, and 5 more variables: Bare.nuclei &lt;fct&gt;, #&gt; # Bl.cromatin &lt;fct&gt;, Normal.nucleoli &lt;fct&gt;, Mitoses &lt;fct&gt;, Class &lt;fct&gt; # Split out validation dataset # create a list of 80% of the rows in the original dataset we can use for training set.seed(7) validationIndex &lt;- createDataPartition(BreastCancer$Class, p=0.80, list=FALSE) # select 20% of the data for validation validation &lt;- BreastCancer[-validationIndex,] # use the remaining 80% of data to training and testing the models dataset &lt;- BreastCancer[validationIndex,] # dimensions of dataset dim(validation) #&gt; [1] 139 11 dim(dataset) #&gt; [1] 560 11 # peek head(dataset, n=20) #&gt; Id Cl.thickness Cell.size Cell.shape Marg.adhesion Epith.c.size #&gt; 1 1000025 5 1 1 1 2 #&gt; 2 1002945 5 4 4 5 7 #&gt; 3 1015425 3 1 1 1 2 #&gt; 5 1017023 4 1 1 3 2 #&gt; 6 1017122 8 10 10 8 7 #&gt; 7 1018099 1 1 1 1 2 #&gt; 8 1018561 2 1 2 1 2 #&gt; 9 1033078 2 1 1 1 2 #&gt; 10 1033078 4 2 1 1 2 #&gt; 11 1035283 1 1 1 1 1 #&gt; 13 1041801 5 3 3 3 2 #&gt; 14 1043999 1 1 1 1 2 #&gt; 15 1044572 8 7 5 10 7 #&gt; 16 1047630 7 4 6 4 6 #&gt; 18 1049815 4 1 1 1 2 #&gt; 19 1050670 10 7 7 6 4 #&gt; 21 1054590 7 3 2 10 5 #&gt; 22 1054593 10 5 5 3 6 #&gt; 23 1056784 3 1 1 1 2 #&gt; 24 1057013 8 4 5 1 2 #&gt; Bare.nuclei Bl.cromatin Normal.nucleoli Mitoses Class #&gt; 1 1 3 1 1 benign #&gt; 2 10 3 2 1 benign #&gt; 3 2 3 1 1 benign #&gt; 5 1 3 1 1 benign #&gt; 6 10 9 7 1 malignant #&gt; 7 10 3 1 1 benign #&gt; 8 1 3 1 1 benign #&gt; 9 1 1 1 5 benign #&gt; 10 1 2 1 1 benign #&gt; 11 1 3 1 1 benign #&gt; 13 3 4 4 1 malignant #&gt; 14 3 3 1 1 benign #&gt; 15 9 5 5 4 malignant #&gt; 16 1 4 3 1 malignant #&gt; 18 1 3 1 1 benign #&gt; 19 10 4 1 2 malignant #&gt; 21 10 5 4 4 malignant #&gt; 22 7 7 10 1 malignant #&gt; 23 1 2 1 1 benign #&gt; 24 &lt;NA&gt; 7 3 1 malignant library(skimr) #&gt; #&gt; Attaching package: &#39;skimr&#39; #&gt; The following object is masked from &#39;package:stats&#39;: #&gt; #&gt; filter skim(dataset) #&gt; Skim summary statistics #&gt; n obs: 560 #&gt; n variables: 11 #&gt; #&gt; ── Variable type:character ─────────────────────────────────────────────── #&gt; variable missing complete n min max empty n_unique #&gt; Id 0 560 560 5 8 0 523 #&gt; #&gt; ── Variable type:factor ────────────────────────────────────────────────── #&gt; variable missing complete n n_unique #&gt; Bare.nuclei 12 548 560 10 #&gt; Bl.cromatin 0 560 560 10 #&gt; Cell.shape 0 560 560 10 #&gt; Cell.size 0 560 560 10 #&gt; Cl.thickness 0 560 560 10 #&gt; Class 0 560 560 2 #&gt; Epith.c.size 0 560 560 10 #&gt; Marg.adhesion 0 560 560 10 #&gt; Mitoses 0 560 560 9 #&gt; Normal.nucleoli 0 560 560 10 #&gt; top_counts ordered #&gt; 1: 327, 10: 102, 2: 28, 5: 24 FALSE #&gt; 3: 136, 2: 134, 1: 118, 7: 64 FALSE #&gt; 1: 281, 10: 48, 3: 47, 2: 46 TRUE #&gt; 1: 307, 10: 55, 3: 43, 2: 36 TRUE #&gt; 1: 113, 3: 94, 5: 94, 4: 62 TRUE #&gt; ben: 367, mal: 193, NA: 0 FALSE #&gt; 2: 304, 3: 60, 4: 40, 1: 38 TRUE #&gt; 1: 331, 2: 44, 10: 44, 3: 42 TRUE #&gt; 1: 466, 3: 29, 2: 23, 4: 10 FALSE #&gt; 1: 347, 10: 50, 3: 36, 2: 29 FALSE # types sapply(dataset, class) #&gt; $Id #&gt; [1] &quot;character&quot; #&gt; #&gt; $Cl.thickness #&gt; [1] &quot;ordered&quot; &quot;factor&quot; #&gt; #&gt; $Cell.size #&gt; [1] &quot;ordered&quot; &quot;factor&quot; #&gt; #&gt; $Cell.shape #&gt; [1] &quot;ordered&quot; &quot;factor&quot; #&gt; #&gt; $Marg.adhesion #&gt; [1] &quot;ordered&quot; &quot;factor&quot; #&gt; #&gt; $Epith.c.size #&gt; [1] &quot;ordered&quot; &quot;factor&quot; #&gt; #&gt; $Bare.nuclei #&gt; [1] &quot;factor&quot; #&gt; #&gt; $Bl.cromatin #&gt; [1] &quot;factor&quot; #&gt; #&gt; $Normal.nucleoli #&gt; [1] &quot;factor&quot; #&gt; #&gt; $Mitoses #&gt; [1] &quot;factor&quot; #&gt; #&gt; $Class #&gt; [1] &quot;factor&quot; We can see that besides the Id, the attributes are factors. This makes sense. I think for modeling it may be more useful to work with the data as numbers than factors. Factors might make things easier for decision tree algorithms (or not). Given that there is an ordinal relationship between the levels we can expose that structure to other algorithms better if we work directly with the integer numbers. 5.5 clean up # Remove redundant variable Id dataset &lt;- dataset[,-1] # convert input values to numeric for(i in 1:9) { dataset[,i] &lt;- as.numeric(as.character(dataset[,i])) } # summary summary(dataset) #&gt; Cl.thickness Cell.size Cell.shape Marg.adhesion #&gt; Min. : 1.00 Min. : 1.00 Min. : 1.00 Min. : 1.00 #&gt; 1st Qu.: 2.00 1st Qu.: 1.00 1st Qu.: 1.00 1st Qu.: 1.00 #&gt; Median : 4.00 Median : 1.00 Median : 1.00 Median : 1.00 #&gt; Mean : 4.44 Mean : 3.14 Mean : 3.21 Mean : 2.79 #&gt; 3rd Qu.: 6.00 3rd Qu.: 5.00 3rd Qu.: 5.00 3rd Qu.: 4.00 #&gt; Max. :10.00 Max. :10.00 Max. :10.00 Max. :10.00 #&gt; #&gt; Epith.c.size Bare.nuclei Bl.cromatin Normal.nucleoli #&gt; Min. : 1.00 Min. : 1.00 Min. : 1.00 Min. : 1.00 #&gt; 1st Qu.: 2.00 1st Qu.: 1.00 1st Qu.: 2.00 1st Qu.: 1.00 #&gt; Median : 2.00 Median : 1.00 Median : 3.00 Median : 1.00 #&gt; Mean : 3.23 Mean : 3.48 Mean : 3.45 Mean : 2.94 #&gt; 3rd Qu.: 4.00 3rd Qu.: 5.25 3rd Qu.: 5.00 3rd Qu.: 4.00 #&gt; Max. :10.00 Max. :10.00 Max. :10.00 Max. :10.00 #&gt; NA&#39;s :12 #&gt; Mitoses Class #&gt; Min. : 1.00 benign :367 #&gt; 1st Qu.: 1.00 malignant:193 #&gt; Median : 1.00 #&gt; Mean : 1.59 #&gt; 3rd Qu.: 1.00 #&gt; Max. :10.00 #&gt; skim(dataset) #&gt; Skim summary statistics #&gt; n obs: 560 #&gt; n variables: 10 #&gt; #&gt; ── Variable type:factor ────────────────────────────────────────────────── #&gt; variable missing complete n n_unique top_counts ordered #&gt; Class 0 560 560 2 ben: 367, mal: 193, NA: 0 FALSE #&gt; #&gt; ── Variable type:numeric ───────────────────────────────────────────────── #&gt; variable missing complete n mean sd p0 p25 p50 p75 p100 #&gt; Bare.nuclei 12 548 560 3.48 3.62 1 1 1 5.25 10 #&gt; Bl.cromatin 0 560 560 3.45 2.43 1 2 3 5 10 #&gt; Cell.shape 0 560 560 3.21 2.97 1 1 1 5 10 #&gt; Cell.size 0 560 560 3.14 3.07 1 1 1 5 10 #&gt; Cl.thickness 0 560 560 4.44 2.83 1 2 4 6 10 #&gt; Epith.c.size 0 560 560 3.23 2.22 1 2 2 4 10 #&gt; Marg.adhesion 0 560 560 2.79 2.85 1 1 1 4 10 #&gt; Mitoses 0 560 560 1.59 1.7 1 1 1 1 10 #&gt; Normal.nucleoli 0 560 560 2.94 3.08 1 1 1 4 10 #&gt; hist #&gt; ▇▁▁▁▁▁▁▂ #&gt; ▇▅▁▁▁▂▁▁ #&gt; ▇▁▁▁▁▁▁▁ #&gt; ▇▁▁▁▁▁▁▂ #&gt; ▇▅▃▅▂▁▂▃ #&gt; ▇▂▁▁▁▁▁▁ #&gt; ▇▁▁▁▁▁▁▁ #&gt; ▇▁▁▁▁▁▁▁ #&gt; ▇▁▁▁▁▁▁▂ we can see we have 13 NA values for the Bare.nuclei attribute. This suggests we may need to remove the records (or impute values) with NA values for some analysis and modeling techniques. 5.6 Analyze the class variable # class distribution cbind(freq = table(dataset$Class), percentage = prop.table(table(dataset$Class))*100) #&gt; freq percentage #&gt; benign 367 65.5 #&gt; malignant 193 34.5 There is indeed a 65% to 35% split for benign-malignant in the class values which is imbalanced, but not so much that we need to be thinking about rebalancing the dataset, at least not yet. 5.6.1 remove NAs # summarize correlations between input variables complete_cases &lt;- complete.cases(dataset) cor(dataset[complete_cases,1:9]) #&gt; Cl.thickness Cell.size Cell.shape Marg.adhesion #&gt; Cl.thickness 1.000 0.665 0.667 0.486 #&gt; Cell.size 0.665 1.000 0.904 0.722 #&gt; Cell.shape 0.667 0.904 1.000 0.694 #&gt; Marg.adhesion 0.486 0.722 0.694 1.000 #&gt; Epith.c.size 0.543 0.773 0.739 0.643 #&gt; Bare.nuclei 0.598 0.700 0.721 0.669 #&gt; Bl.cromatin 0.565 0.752 0.739 0.692 #&gt; Normal.nucleoli 0.570 0.737 0.741 0.644 #&gt; Mitoses 0.347 0.453 0.432 0.419 #&gt; Epith.c.size Bare.nuclei Bl.cromatin Normal.nucleoli #&gt; Cl.thickness 0.543 0.598 0.565 0.570 #&gt; Cell.size 0.773 0.700 0.752 0.737 #&gt; Cell.shape 0.739 0.721 0.739 0.741 #&gt; Marg.adhesion 0.643 0.669 0.692 0.644 #&gt; Epith.c.size 1.000 0.614 0.628 0.642 #&gt; Bare.nuclei 0.614 1.000 0.685 0.605 #&gt; Bl.cromatin 0.628 0.685 1.000 0.692 #&gt; Normal.nucleoli 0.642 0.605 0.692 1.000 #&gt; Mitoses 0.485 0.351 0.356 0.432 #&gt; Mitoses #&gt; Cl.thickness 0.347 #&gt; Cell.size 0.453 #&gt; Cell.shape 0.432 #&gt; Marg.adhesion 0.419 #&gt; Epith.c.size 0.485 #&gt; Bare.nuclei 0.351 #&gt; Bl.cromatin 0.356 #&gt; Normal.nucleoli 0.432 #&gt; Mitoses 1.000 We can see some modest to high correlation between some of the attributes. For example between cell shape and cell size at 0.90 correlation. 5.7 Unimodal visualization # histograms each attribute par(mfrow=c(3,3)) for(i in 1:9) { hist(dataset[,i], main=names(dataset)[i]) } We can see that almost all of the distributions have an exponential or bimodal shape to them. # density plot for each attribute par(mfrow=c(3,3)) complete_cases &lt;- complete.cases(dataset) for(i in 1:9) { plot(density(dataset[complete_cases,i]), main=names(dataset)[i]) } These plots add more support to our initial ideas. We can see bimodal distributions (two bumps) and exponential-looking distributions. # boxplots for each attribute par(mfrow=c(3,3)) for(i in 1:9) { boxplot(dataset[,i], main=names(dataset)[i]) } 5.8 Multimodal visualization # scatter plot matrix jittered_x &lt;- sapply(dataset[,1:9], jitter) pairs(jittered_x, names(dataset[,1:9]), col=dataset$Class) We can see that the black (benign) a part to be clustered around the bottom-right corner (smaller values) and red (malignant) are all over the place. # bar plots of each variable by class par(mfrow=c(3,3)) for(i in 1:9) { barplot(table(dataset$Class,dataset[,i]), main=names(dataset)[i], legend.text=unique(dataset$Class)) } 5.9 Algorithms Evaluation Linear Algorithms: Logistic Regression (LG), Linear Discriminate Analysis (LDA) and Regularized Logistic Regression (GLMNET). Nonlinear Algorithms: k-Nearest Neighbors (KNN), Classiﬁcation and Regression Trees (CART), Naive Bayes (NB) and Support Vector Machines with Radial Basis Functions (SVM). For simplicity, we will use Accuracy and Kappa metrics. Given that it is a medical test, we could have gone with the Area Under ROC Curve (AUC) and looked at the sensitivity and speciﬁcity to select the best algorithms. # 10-fold cross-validation with 3 repeats trainControl &lt;- trainControl(method = &quot;repeatedcv&quot;, number=10, repeats=3) metric &lt;- &quot;Accuracy&quot; tic() # LG set.seed(7) fit.glm &lt;- train(Class~., data=dataset, method=&quot;glm&quot;, metric=metric, trControl=trainControl, na.action=na.omit) # LDA set.seed(7) fit.lda &lt;- train(Class~., data=dataset, method=&quot;lda&quot;, metric=metric, trControl=trainControl, na.action=na.omit) # GLMNET set.seed(7) fit.glmnet &lt;- train(Class~., data=dataset, method=&quot;glmnet&quot;, metric=metric, trControl=trainControl, na.action=na.omit) # KNN set.seed(7) fit.knn &lt;- train(Class~., data=dataset, method=&quot;knn&quot;, metric=metric, trControl=trainControl, na.action=na.omit) # CART set.seed(7) fit.cart &lt;- train(Class~., data=dataset, method=&quot;rpart&quot;, metric=metric, trControl=trainControl, na.action=na.omit) # Naive Bayes set.seed(7) fit.nb &lt;- train(Class~., data=dataset, method=&quot;nb&quot;, metric=metric, trControl=trainControl, na.action=na.omit) # SVM set.seed(7) fit.svm &lt;- train(Class~., data=dataset, method=&quot;svmRadial&quot;, metric=metric, trControl=trainControl, na.action=na.omit) # Compare algorithms results &lt;- resamples(list(LG = fit.glm, LDA = fit.lda, GLMNET = fit.glmnet, KNN = fit.knn, CART = fit.cart, NB = fit.nb, SVM = fit.svm)) toc() #&gt; 13.89 sec elapsed summary(results) #&gt; #&gt; Call: #&gt; summary.resamples(object = results) #&gt; #&gt; Models: LG, LDA, GLMNET, KNN, CART, NB, SVM #&gt; Number of resamples: 30 #&gt; #&gt; Accuracy #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s #&gt; LG 0.909 0.945 0.964 0.968 0.995 1 0 #&gt; LDA 0.907 0.945 0.964 0.963 0.982 1 0 #&gt; GLMNET 0.927 0.964 0.964 0.973 0.995 1 0 #&gt; KNN 0.927 0.964 0.982 0.976 1.000 1 0 #&gt; CART 0.833 0.927 0.945 0.943 0.964 1 0 #&gt; NB 0.927 0.963 0.981 0.970 0.982 1 0 #&gt; SVM 0.907 0.945 0.964 0.965 0.982 1 0 #&gt; #&gt; Kappa #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s #&gt; LG 0.806 0.880 0.921 0.930 0.990 1 0 #&gt; LDA 0.786 0.880 0.918 0.917 0.959 1 0 #&gt; GLMNET 0.843 0.918 0.922 0.940 0.990 1 0 #&gt; KNN 0.843 0.920 0.959 0.947 1.000 1 0 #&gt; CART 0.630 0.840 0.879 0.876 0.920 1 0 #&gt; NB 0.835 0.918 0.959 0.934 0.960 1 0 #&gt; SVM 0.804 0.883 0.922 0.926 0.960 1 0 dotplot(results) We can see good accuracy across the board. All algorithms have a mean accuracy above 90%, well above the baseline of 65% if we just predicted benign. The problem is learnable. We can see that KNN (97.08%) and logistic regression (NB was 96.2% and GLMNET was 96.4%) had the highest accuracy on the problem. 5.10 Data transform We know we have some skewed distributions. There are transform methods that we can use to adjust and normalize these distributions. A favorite for positive input attributes (which we have in this case) is the Box-Cox transform. # 10-fold cross-validation with 3 repeats trainControl &lt;- trainControl(method=&quot;repeatedcv&quot;, number=10, repeats=3) metric &lt;- &quot;Accuracy&quot; # LG set.seed(7) fit.glm &lt;- train(Class~., data=dataset, method=&quot;glm&quot;, metric=metric, preProc=c(&quot;BoxCox&quot;), trControl=trainControl, na.action=na.omit) # LDA set.seed(7) fit.lda &lt;- train(Class~., data=dataset, method=&quot;lda&quot;, metric=metric, preProc=c(&quot;BoxCox&quot;), trControl=trainControl, na.action=na.omit) # GLMNET set.seed(7) fit.glmnet &lt;- train(Class~., data=dataset, method=&quot;glmnet&quot;, metric=metric, preProc=c(&quot;BoxCox&quot;), trControl=trainControl, na.action=na.omit) # KNN set.seed(7) fit.knn &lt;- train(Class~., data=dataset, method=&quot;knn&quot;, metric=metric, preProc=c(&quot;BoxCox&quot;), trControl=trainControl, na.action=na.omit) # CART set.seed(7) fit.cart &lt;- train(Class~., data=dataset, method=&quot;rpart&quot;, metric=metric, preProc=c(&quot;BoxCox&quot;), trControl=trainControl, na.action=na.omit) # Naive Bayes set.seed(7) fit.nb &lt;- train(Class~., data=dataset, method=&quot;nb&quot;, metric=metric, preProc=c(&quot;BoxCox&quot;), trControl=trainControl, na.action=na.omit) #&gt; Warning in FUN(X[[i]], ...): Numerical 0 probability for all classes with #&gt; observation 1 #&gt; Warning in FUN(X[[i]], ...): Numerical 0 probability for all classes with #&gt; observation 24 #&gt; Warning in FUN(X[[i]], ...): Numerical 0 probability for all classes with #&gt; observation 28 #&gt; Warning in FUN(X[[i]], ...): Numerical 0 probability for all classes with #&gt; observation 20 #&gt; Warning in FUN(X[[i]], ...): Numerical 0 probability for all classes with #&gt; observation 11 #&gt; Warning in FUN(X[[i]], ...): Numerical 0 probability for all classes with #&gt; observation 18 #&gt; Warning in FUN(X[[i]], ...): Numerical 0 probability for all classes with #&gt; observation 54 #&gt; Warning in FUN(X[[i]], ...): Numerical 0 probability for all classes with #&gt; observation 3 #&gt; Warning in FUN(X[[i]], ...): Numerical 0 probability for all classes with #&gt; observation 23 #&gt; Warning in FUN(X[[i]], ...): Numerical 0 probability for all classes with #&gt; observation 21 #&gt; Warning in FUN(X[[i]], ...): Numerical 0 probability for all classes with #&gt; observation 27 #&gt; Warning in FUN(X[[i]], ...): Numerical 0 probability for all classes with #&gt; observation 53 #&gt; Warning in FUN(X[[i]], ...): Numerical 0 probability for all classes with #&gt; observation 12 #&gt; Warning in FUN(X[[i]], ...): Numerical 0 probability for all classes with #&gt; observation 9 #&gt; Warning in FUN(X[[i]], ...): Numerical 0 probability for all classes with #&gt; observation 2 #&gt; Warning in FUN(X[[i]], ...): Numerical 0 probability for all classes with #&gt; observation 17 #&gt; Warning in FUN(X[[i]], ...): Numerical 0 probability for all classes with #&gt; observation 9 #&gt; Warning in FUN(X[[i]], ...): Numerical 0 probability for all classes with #&gt; observation 55 #&gt; Warning in FUN(X[[i]], ...): Numerical 0 probability for all classes with #&gt; observation 23 #&gt; Warning in FUN(X[[i]], ...): Numerical 0 probability for all classes with #&gt; observation 32 # SVM set.seed(7) fit.svm &lt;- train(Class~., data=dataset, method=&quot;svmRadial&quot;, metric=metric, preProc=c(&quot;BoxCox&quot;), trControl=trainControl, na.action=na.omit) # Compare algorithms transformResults &lt;- resamples(list(LG = fit.glm, LDA = fit.lda, GLMNET = fit.glmnet, KNN = fit.knn, CART = fit.cart, NB = fit.nb, SVM = fit.svm)) summary(transformResults) #&gt; #&gt; Call: #&gt; summary.resamples(object = transformResults) #&gt; #&gt; Models: LG, LDA, GLMNET, KNN, CART, NB, SVM #&gt; Number of resamples: 30 #&gt; #&gt; Accuracy #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s #&gt; LG 0.909 0.963 0.982 0.973 0.996 1 0 #&gt; LDA 0.927 0.964 0.981 0.974 0.982 1 0 #&gt; GLMNET 0.944 0.964 0.982 0.980 1.000 1 0 #&gt; KNN 0.909 0.964 0.981 0.976 0.982 1 0 #&gt; CART 0.833 0.927 0.945 0.943 0.964 1 0 #&gt; NB 0.927 0.964 0.982 0.978 1.000 1 0 #&gt; SVM 0.927 0.964 0.982 0.980 1.000 1 0 #&gt; #&gt; Kappa #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s #&gt; LG 0.806 0.919 0.959 0.941 0.990 1 0 #&gt; LDA 0.847 0.921 0.959 0.945 0.961 1 0 #&gt; GLMNET 0.878 0.922 0.960 0.957 1.000 1 0 #&gt; KNN 0.806 0.922 0.959 0.949 0.961 1 0 #&gt; CART 0.630 0.840 0.879 0.876 0.920 1 0 #&gt; NB 0.843 0.922 0.960 0.953 1.000 1 0 #&gt; SVM 0.847 0.922 0.960 0.957 1.000 1 0 dotplot(transformResults) We can see that the accuracy of the previous best algorithm KNN was elevated to 97.14%. We have a new ranking, showing SVM with the most accurate mean accuracy at 97.20%. 5.11 Tuning SVM # 10-fold cross-validation with 3 repeats trainControl &lt;- trainControl(method=&quot;repeatedcv&quot;, number=10, repeats=3) metric &lt;- &quot;Accuracy&quot; set.seed(7) grid &lt;- expand.grid(.sigma = c(0.025, 0.05, 0.1, 0.15), .C = seq(1, 10, by=1)) fit.svm &lt;- train(Class~., data=dataset, method=&quot;svmRadial&quot;, metric=metric, tuneGrid=grid, preProc=c(&quot;BoxCox&quot;), trControl=trainControl, na.action=na.omit) print(fit.svm) #&gt; Support Vector Machines with Radial Basis Function Kernel #&gt; #&gt; 560 samples #&gt; 9 predictor #&gt; 2 classes: &#39;benign&#39;, &#39;malignant&#39; #&gt; #&gt; Pre-processing: Box-Cox transformation (9) #&gt; Resampling: Cross-Validated (10 fold, repeated 3 times) #&gt; Summary of sample sizes: 493, 492, 493, 493, 493, 494, ... #&gt; Resampling results across tuning parameters: #&gt; #&gt; sigma C Accuracy Kappa #&gt; 0.025 1 0.979 0.956 #&gt; 0.025 2 0.979 0.954 #&gt; 0.025 3 0.979 0.956 #&gt; 0.025 4 0.977 0.950 #&gt; 0.025 5 0.977 0.950 #&gt; 0.025 6 0.978 0.951 #&gt; 0.025 7 0.979 0.954 #&gt; 0.025 8 0.979 0.956 #&gt; 0.025 9 0.979 0.956 #&gt; 0.025 10 0.979 0.956 #&gt; 0.050 1 0.980 0.957 #&gt; 0.050 2 0.980 0.957 #&gt; 0.050 3 0.979 0.956 #&gt; 0.050 4 0.980 0.957 #&gt; 0.050 5 0.980 0.957 #&gt; 0.050 6 0.980 0.957 #&gt; 0.050 7 0.979 0.956 #&gt; 0.050 8 0.979 0.954 #&gt; 0.050 9 0.979 0.954 #&gt; 0.050 10 0.979 0.954 #&gt; 0.100 1 0.980 0.957 #&gt; 0.100 2 0.980 0.957 #&gt; 0.100 3 0.979 0.956 #&gt; 0.100 4 0.978 0.953 #&gt; 0.100 5 0.978 0.952 #&gt; 0.100 6 0.975 0.946 #&gt; 0.100 7 0.976 0.948 #&gt; 0.100 8 0.976 0.948 #&gt; 0.100 9 0.976 0.948 #&gt; 0.100 10 0.975 0.946 #&gt; 0.150 1 0.980 0.957 #&gt; 0.150 2 0.978 0.953 #&gt; 0.150 3 0.978 0.953 #&gt; 0.150 4 0.976 0.949 #&gt; 0.150 5 0.976 0.948 #&gt; 0.150 6 0.975 0.946 #&gt; 0.150 7 0.974 0.944 #&gt; 0.150 8 0.972 0.939 #&gt; 0.150 9 0.970 0.934 #&gt; 0.150 10 0.968 0.930 #&gt; #&gt; Accuracy was used to select the optimal model using the largest value. #&gt; The final values used for the model were sigma = 0.15 and C = 1. plot(fit.svm) We can see that we have made very little diﬀerence to the results. The most accurate model had a score of 97.31% (the same as our previously rounded score of 97.20%) using a sigma = 0.1 and C = 1. We could tune further, but I don’t expect a payoﬀ. 5.12 Tuning KNN # 10-fold cross-validation with 3 repeats trainControl &lt;- trainControl(method=&quot;repeatedcv&quot;, number=10, repeats=3) metric &lt;- &quot;Accuracy&quot; set.seed(7) grid &lt;- expand.grid(.k = seq(1,20, by=1)) fit.knn &lt;- train(Class~., data=dataset, method=&quot;knn&quot;, metric=metric, tuneGrid=grid, preProc=c(&quot;BoxCox&quot;), trControl=trainControl, na.action=na.omit) print(fit.knn) #&gt; k-Nearest Neighbors #&gt; #&gt; 560 samples #&gt; 9 predictor #&gt; 2 classes: &#39;benign&#39;, &#39;malignant&#39; #&gt; #&gt; Pre-processing: Box-Cox transformation (9) #&gt; Resampling: Cross-Validated (10 fold, repeated 3 times) #&gt; Summary of sample sizes: 493, 492, 493, 493, 493, 494, ... #&gt; Resampling results across tuning parameters: #&gt; #&gt; k Accuracy Kappa #&gt; 1 0.958 0.908 #&gt; 2 0.960 0.912 #&gt; 3 0.968 0.931 #&gt; 4 0.970 0.935 #&gt; 5 0.972 0.939 #&gt; 6 0.973 0.942 #&gt; 7 0.976 0.949 #&gt; 8 0.975 0.946 #&gt; 9 0.976 0.947 #&gt; 10 0.976 0.949 #&gt; 11 0.976 0.949 #&gt; 12 0.976 0.947 #&gt; 13 0.974 0.945 #&gt; 14 0.975 0.946 #&gt; 15 0.976 0.947 #&gt; 16 0.975 0.946 #&gt; 17 0.976 0.947 #&gt; 18 0.976 0.947 #&gt; 19 0.978 0.951 #&gt; 20 0.977 0.950 #&gt; #&gt; Accuracy was used to select the optimal model using the largest value. #&gt; The final value used for the model was k = 19. plot(fit.knn) We can see again that tuning has made little diﬀerence, settling on a value of k = 7 with an accuracy of 97.19%. This is higher than the previous 97.14%, but very similar (or perhaps identical!) to the result achieved by the tuned SVM. 5.13 Ensemble # 10-fold cross-validation with 3 repeats trainControl &lt;- trainControl(method=&quot;repeatedcv&quot;, number=10, repeats=3) metric &lt;- &quot;Accuracy&quot; # Bagged CART set.seed(7) fit.treebag &lt;- train(Class~., data=dataset, method=&quot;treebag&quot;, metric=metric, trControl=trainControl, na.action=na.omit) # Random Forest set.seed(7) fit.rf &lt;- train(Class~., data=dataset, method=&quot;rf&quot;, metric=metric, preProc=c(&quot;BoxCox&quot;), trControl=trainControl, na.action=na.omit) # Stochastic Gradient Boosting set.seed(7) fit.gbm &lt;- train(Class~., data=dataset, method=&quot;gbm&quot;, metric=metric, preProc=c(&quot;BoxCox&quot;), trControl=trainControl, verbose=FALSE, na.action=na.omit) # C5.0 set.seed(7) fit.c50 &lt;- train(Class~., data=dataset, method=&quot;C5.0&quot;, metric=metric, preProc=c(&quot;BoxCox&quot;), trControl=trainControl, na.action=na.omit) # Compare results ensembleResults &lt;- resamples(list(BAG = fit.treebag, RF = fit.rf, GBM = fit.gbm, C50 = fit.c50)) summary(ensembleResults) #&gt; #&gt; Call: #&gt; summary.resamples(object = ensembleResults) #&gt; #&gt; Models: BAG, RF, GBM, C50 #&gt; Number of resamples: 30 #&gt; #&gt; Accuracy #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s #&gt; BAG 0.907 0.963 0.982 0.973 0.982 1 0 #&gt; RF 0.926 0.981 0.982 0.979 0.995 1 0 #&gt; GBM 0.929 0.963 0.981 0.973 0.995 1 0 #&gt; C50 0.907 0.964 0.982 0.972 0.982 1 0 #&gt; #&gt; Kappa #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s #&gt; BAG 0.804 0.920 0.959 0.942 0.96 1 0 #&gt; RF 0.841 0.959 0.960 0.955 0.99 1 0 #&gt; GBM 0.844 0.919 0.959 0.941 0.99 1 0 #&gt; C50 0.795 0.918 0.959 0.939 0.96 1 0 dotplot(ensembleResults) We see that Random Forest was the most accurate with a score of 97.26%. Very similar to our tuned models above. We could spend time tuning the parameters of Random Forest (e.g. increasing the number of trees) and the other ensemble methods, but I don’t expect to see better accuracy scores other than random statistical ﬂuctuations. 5.14 Finalize model We now need to ﬁnalize the model, which really means choose which model we would like to use. For simplicity I would probably select the KNN method, at the expense of the memory required to store the training dataset. SVM would be a good choice to trade-oﬀ space and time complexity. I probably would not select the Random Forest algorithm given the complexity of the model. It seems overkill for this dataset, lots of trees with little beneﬁt in Accuracy. Let’s go with the KNN algorithm. This is really simple, as we do not need to store a model. We do need to capture the parameters of the Box-Cox transform though. And we also need to prepare the data by removing the unused Id attribute and converting all of the inputs to numeric format. The implementation of KNN (knn3()) belongs to the caret package and does not support missing values. We will have to remove the rows with missing values from the training dataset as well as the validation dataset. The code below shows the preparation of the pre-processing parameters using the training dataset. # prepare parameters for data transform set.seed(7) datasetNoMissing &lt;- dataset[complete.cases(dataset),] x &lt;- datasetNoMissing[,1:9] # transform preprocessParams &lt;- preProcess(x, method=c(&quot;BoxCox&quot;)) x &lt;- predict(preprocessParams, x) 5.15 Prepare the validation set Next we need to prepare the validation dataset for making a prediction. We must: Remove the Id attribute. Remove those rows with missing data. Convert all input attributes to numeric. Apply the Box-Cox transform to the input attributes using parameters prepared on the training dataset. # prepare the validation dataset set.seed(7) # remove id column validation &lt;- validation[,-1] # remove missing values (not allowed in this implementation of knn) validation &lt;- validation[complete.cases(validation),] # convert to numeric for(i in 1:9) { validation[,i] &lt;- as.numeric(as.character(validation[,i])) } # transform the validation dataset validationX &lt;- predict(preprocessParams, validation[,1:9]) # make predictions set.seed(7) # knn3Train(train, test, cl, k = 1, l = 0, prob = TRUE, use.all = TRUE) # k: number of neighbours considered. predictions &lt;- knn3Train(x, validationX, datasetNoMissing$Class, k = 9, prob = FALSE) # convert confusionMatrix(as.factor(predictions), validation$Class) #&gt; Confusion Matrix and Statistics #&gt; #&gt; Reference #&gt; Prediction benign malignant #&gt; benign 83 1 #&gt; malignant 4 47 #&gt; #&gt; Accuracy : 0.963 #&gt; 95% CI : (0.916, 0.988) #&gt; No Information Rate : 0.644 #&gt; P-Value [Acc &gt; NIR] : &lt;2e-16 #&gt; #&gt; Kappa : 0.92 #&gt; #&gt; Mcnemar&#39;s Test P-Value : 0.371 #&gt; #&gt; Sensitivity : 0.954 #&gt; Specificity : 0.979 #&gt; Pos Pred Value : 0.988 #&gt; Neg Pred Value : 0.922 #&gt; Prevalence : 0.644 #&gt; Detection Rate : 0.615 #&gt; Detection Prevalence : 0.622 #&gt; Balanced Accuracy : 0.967 #&gt; #&gt; &#39;Positive&#39; Class : benign #&gt; We can see that the accuracy of the final model on the validation dataset is 99.26%. This is optimistic because there is only 136 rows, but it does show that we have an accurate standalone model that we could use on other unclassiﬁed data. "],
["classification-algorithms-comparison-outbreaks-dataset-rf-glmnet-knn-pda-lda-nsc-c5-pls.html", "Chapter 6 Classification algorithms comparison. outbreaks dataset. (RF, GLMNET, KNN, PDA, LDA, NSC, C5, PLS) 6.1 Introduction 6.2 The data 6.3 Features 6.4 Test, train and validation data sets 6.5 Comparing Machine Learning algorithms 6.6 Comparing accuracy of models 6.7 Predicting unknown outcomes 6.8 Conclusions", " Chapter 6 Classification algorithms comparison. outbreaks dataset. (RF, GLMNET, KNN, PDA, LDA, NSC, C5, PLS) 6.1 Introduction Among the many nice R packages containing data collections is the outbreaks package. It contains datsets on epidemics and among them is data from the 2013 outbreak of influenza A H7N9 in China as analysed by Kucharski et al. (2014): A. Kucharski, H. Mills, A. Pinsent, C. Fraser, M. Van Kerkhove, C. A. Donnelly, and S. Riley. 2014. Distinguishing between reservoir exposure and human-to-human transmission for emerging pathogens using case onset data. PLOS Currents Outbreaks. Mar 7, edition 1. doi: 10.1371/currents.outbreaks.e1473d9bfc99d080ca242139a06c455f. A. Kucharski, H. Mills, A. Pinsent, C. Fraser, M. Van Kerkhove, C. A. Donnelly, and S. Riley. 2014. Data from: Distinguishing between reservoir exposure and human-to-human transmission for emerging pathogens using case onset data. Dryad Digital Repository. http://dx.doi.org/10.5061/dryad.2g43n. 6.1.1 Algorithms We compare these classification algorithms: Random Forest GLM net k-Nearest Neighbors Penalized Discriminant Analysis Stabilized Linear Discriminant Analysis Nearest Shrunken Centroids Single C5.0 Tree Partial Least Squares 6.1.2 Workflow Load dataset Data wrangling Many dates as one variable: gather Convert group to factor: factor Change dates descriptions: dplyr::mapvalues Set several provinces as Other: mapvalues Convert gender to factor Convert province to factor Features visualization Area density plot of Month vs Age by Province, by date group, by outcome, by gender Number of flu cases by gender, by province Age density plot of flu cases by outcome Days passed from outset by province, by age, by gender, by outcome Feature engineering Generate new features Gender to numeric Provinces to binary classifiers Outcome to binary classifier Impute missing data: mice Split dataset in training, validation and test sets Feature importance with Decision Trees with Random Forest Wrangling dataset Plot Importance vs Variable Impact on Datasets Density plot of training, validation and test datasets Features vs outcome by age, days onset to hospital, day onset to outcome Train models on training validation dataset Numeric and visual Comparison of models Accuracy Kappa Compare predictions on training validation set Create dataset for results New variable for outcome Predicting on unknown outcomes on training data Numeric and visual Comparison of models Accuracy Kappa Compare predictions on training validation set Create dataset for results New variable for outcome Calculate predicted outcome Save to CSV file Calculate recovery cases Summarize outcome Plot month vs log2-ratio of recovery vs death, by gender, by age, by date group 6.1.3 Can we predict flu outcome with Machine Learning in R? I will be using their data as an example to show how to use Machine Learning algorithms for predicting disease outcome. To do so, I selected and extracted features from the raw data, including age, days between onset and outcome, gender, whether the patients were hospitalised, etc. Missing values were imputed and different model algorithms were used to predict outcome (death or recovery). The prediction accuracy, sensitivity and specificity. The thus prepared dataset was devided into training and testing subsets. The test subset contained all cases with an unknown outcome. Before I applied the models to the test data, I further split the training data into validation subsets. The tested modeling algorithms were similarly successful at predicting the outcomes of the validation data. To decide on final classifications, I compared predictions from all models and defined the outcome “Death” or “Recovery” as a function of all models, whereas classifications with a low prediction probability were flagged as “uncertain”. Accounting for this uncertainty led to a 100% correct classification of the validation test set. The training cases with unknown outcome were then classified based on the same algorithms. From 57 unknown cases, 14 were classified as “Recovery”, 10 as “Death” and 33 as uncertain. In a Part 2, I am looking at how extreme gradient boosting performs on this dataset. Disclaimer: I am not an expert in Machine Learning. Everything I know, I tought myself during the last months. So, if you see any mistakes or have tips and tricks for improvement, please don’t hesitate to let me know! Thanks. :-) 6.2 The data The dataset contains case ID, date of onset, date of hospitalisation, date of outcome, gender, age, province and of course the outcome: Death or Recovery. I can already see that there are a couple of missing values in the data, which I will deal with later. options(width = 1000) if (!require(&quot;outbreaks&quot;)) install.packages(&quot;outbreaks&quot;) #&gt; Loading required package: outbreaks library(outbreaks) fluH7N9_china_2013_backup &lt;- fluH7N9_china_2013 fluH7N9_china_2013$age[which(fluH7N9_china_2013$age == &quot;?&quot;)] &lt;- NA fluH7N9_china_2013$case_ID &lt;- paste(&quot;case&quot;, fluH7N9_china_2013$case_id, sep = &quot;_&quot;) head(fluH7N9_china_2013) #&gt; case_id date_of_onset date_of_hospitalisation date_of_outcome outcome gender age province case_ID #&gt; 1 1 2013-02-19 &lt;NA&gt; 2013-03-04 Death m 87 Shanghai case_1 #&gt; 2 2 2013-02-27 2013-03-03 2013-03-10 Death m 27 Shanghai case_2 #&gt; 3 3 2013-03-09 2013-03-19 2013-04-09 Death f 35 Anhui case_3 #&gt; 4 4 2013-03-19 2013-03-27 &lt;NA&gt; &lt;NA&gt; f 45 Jiangsu case_4 #&gt; 5 5 2013-03-19 2013-03-30 2013-05-15 Recover f 48 Jiangsu case_5 #&gt; 6 6 2013-03-21 2013-03-28 2013-04-26 Death f 32 Jiangsu case_6 Before I start preparing the data for Machine Learning, I want to get an idea of the distribution of the data points and their different variables by plotting. Most provinces have only a handful of cases, so I am combining them into the category “other” and keep only Jiangsu, Shanghai and Zhejian and separate provinces. # make tidy data, clean up and simplify library(tidyr) # put different type of dates in one variable (Date) fluH7N9_china_2013_gather &lt;- fluH7N9_china_2013 %&gt;% gather(Group, Date, date_of_onset:date_of_outcome) # convert Group to factor fluH7N9_china_2013_gather$Group &lt;- factor(fluH7N9_china_2013_gather$Group, levels = c(&quot;date_of_onset&quot;, &quot;date_of_hospitalisation&quot;, &quot;date_of_outcome&quot;)) # change the dates description with plyr::mapvalues library(plyr) from &lt;- c(&quot;date_of_onset&quot;, &quot;date_of_hospitalisation&quot;, &quot;date_of_outcome&quot;) to &lt;- c(&quot;Date of onset&quot;, &quot;Date of hospitalisation&quot;, &quot;Date of outcome&quot;) fluH7N9_china_2013_gather$Group &lt;- mapvalues(fluH7N9_china_2013_gather$Group, from = from, to = to) # change additional provinces to Other from &lt;- c(&quot;Anhui&quot;, &quot;Beijing&quot;, &quot;Fujian&quot;, &quot;Guangdong&quot;, &quot;Hebei&quot;, &quot;Henan&quot;, &quot;Hunan&quot;, &quot;Jiangxi&quot;, &quot;Shandong&quot;, &quot;Taiwan&quot;) to &lt;- rep(&quot;Other&quot;, 10) fluH7N9_china_2013_gather$province &lt;- mapvalues(fluH7N9_china_2013_gather$province, from = from, to = to) # convert gender to factor levels(fluH7N9_china_2013_gather$gender) &lt;- c(levels(fluH7N9_china_2013_gather$gender), &quot;unknown&quot;) # replace NA in gender by &quot;unknown is_na &lt;- is.na(fluH7N9_china_2013_gather$gender) fluH7N9_china_2013_gather$gender[is_na] &lt;- &quot;unknown&quot; # convert province to factor fluH7N9_china_2013_gather$province &lt;- factor(fluH7N9_china_2013_gather$province, levels = c(&quot;Jiangsu&quot;, &quot;Shanghai&quot;, &quot;Zhejiang&quot;, &quot;Other&quot;)) library(ggplot2) #&gt; Registered S3 methods overwritten by &#39;ggplot2&#39;: #&gt; method from #&gt; [.quosures rlang #&gt; c.quosures rlang #&gt; print.quosures rlang my_theme &lt;- function(base_size = 12, base_family = &quot;sans&quot;){ theme_minimal(base_size = base_size, base_family = base_family) + theme( axis.text = element_text(size = 12), axis.text.x = element_text(angle = 45, vjust = 0.5, hjust = 0.5), axis.title = element_text(size = 14), panel.grid.major = element_line(color = &quot;grey&quot;), panel.grid.minor = element_blank(), panel.background = element_rect(fill = &quot;aliceblue&quot;), strip.background = element_rect(fill = &quot;lightgrey&quot;, color = &quot;grey&quot;, size = 1), strip.text = element_text(face = &quot;bold&quot;, size = 12, color = &quot;black&quot;), legend.position = &quot;bottom&quot;, legend.justification = &quot;top&quot;, legend.box = &quot;horizontal&quot;, legend.box.background = element_rect(colour = &quot;grey50&quot;), legend.background = element_blank(), panel.border = element_rect(color = &quot;grey&quot;, fill = NA, size = 0.5) ) } ggplot(data = fluH7N9_china_2013_gather, aes(x = Date, y = as.numeric(age), fill = outcome)) + stat_density2d(aes(alpha = ..level..), geom = &quot;polygon&quot;) + geom_jitter(aes(color = outcome, shape = gender), size = 1.5) + geom_rug(aes(color = outcome)) + labs( fill = &quot;Outcome&quot;, color = &quot;Outcome&quot;, alpha = &quot;Level&quot;, shape = &quot;Gender&quot;, x = &quot;Date in 2013&quot;, y = &quot;Age&quot;, title = &quot;2013 Influenza A H7N9 cases in China&quot;, subtitle = &quot;Dataset from &#39;outbreaks&#39; package (Kucharski et al. 2014)&quot;, caption = &quot;&quot; ) + facet_grid(Group ~ province) + my_theme() + scale_shape_manual(values = c(15, 16, 17)) + scale_color_brewer(palette=&quot;Set1&quot;, na.value = &quot;grey50&quot;) + scale_fill_brewer(palette=&quot;Set1&quot;) #&gt; Warning: Removed 149 rows containing non-finite values (stat_density2d). #&gt; Warning: Removed 149 rows containing missing values (geom_point). This plot shows the dates of onset, hospitalisation and outcome (if known) of each data point. Outcome is marked by color and age shown on the y-axis. Gender is marked by point shape. The density distribution of date by age for the cases seems to indicate that older people died more frequently in the Jiangsu and Zhejiang province than in Shanghai and in other provinces. When we look at the distribution of points along the time axis, it suggests that their might be a positive correlation between the likelihood of death and an early onset or early outcome. I also want to know how many cases there are for each gender and province and compare the genders’ age distribution. # more tidy data. first remove age fluH7N9_china_2013_gather_2 &lt;- fluH7N9_china_2013_gather[, -4] %&gt;% gather(group_2, value, gender:province) #&gt; Warning: attributes are not identical across measure variables; #&gt; they will be dropped # change descriptions from &lt;- c(&quot;m&quot;, &quot;f&quot;, &quot;unknown&quot;, &quot;Other&quot;) to &lt;- c(&quot;Male&quot;, &quot;Female&quot;, &quot;Unknown gender&quot;, &quot;Other province&quot;) fluH7N9_china_2013_gather_2$value &lt;- mapvalues(fluH7N9_china_2013_gather_2$value, from = from, to = to) # convert to factor fluH7N9_china_2013_gather_2$value &lt;- factor(fluH7N9_china_2013_gather_2$value, levels = c(&quot;Female&quot;, &quot;Male&quot;, &quot;Unknown gender&quot;, &quot;Jiangsu&quot;, &quot;Shanghai&quot;, &quot;Zhejiang&quot;, &quot;Other province&quot;)) p1 &lt;- ggplot(data = fluH7N9_china_2013_gather_2, aes(x = value, fill = outcome, color = outcome)) + geom_bar(position = &quot;dodge&quot;, alpha = 0.7, size = 1) + my_theme() + scale_fill_brewer(palette=&quot;Set1&quot;, na.value = &quot;grey50&quot;) + scale_color_brewer(palette=&quot;Set1&quot;, na.value = &quot;grey50&quot;) + labs( color = &quot;&quot;, fill = &quot;&quot;, x = &quot;&quot;, y = &quot;Count&quot;, title = &quot;2013 Influenza A H7N9 cases in China&quot;, subtitle = &quot;Gender and Province numbers of flu cases&quot;, caption = &quot;&quot; ) p2 &lt;- ggplot(data = fluH7N9_china_2013_gather, aes(x = as.numeric(age), fill = outcome, color = outcome)) + geom_density(alpha = 0.3, size = 1) + geom_rug() + scale_color_brewer(palette=&quot;Set1&quot;, na.value = &quot;grey50&quot;) + scale_fill_brewer(palette=&quot;Set1&quot;, na.value = &quot;grey50&quot;) + my_theme() + labs( color = &quot;&quot;, fill = &quot;&quot;, x = &quot;Age&quot;, y = &quot;Density&quot;, title = &quot;&quot;, subtitle = &quot;Age distribution of flu cases&quot;, caption = &quot;&quot; ) library(gridExtra) library(grid) grid.arrange(p1, p2, ncol = 2) #&gt; Warning: Removed 6 rows containing non-finite values (stat_density). In the dataset, there are more male than female cases and correspondingly, we see more deaths, recoveries and unknown outcomes in men than in women. This is potentially a problem later on for modeling because the inherent likelihoods for outcome are not directly comparable between the sexes. Most unknown outcomes were recorded in Zhejiang. Similarly to gender, we don’t have an equal distribution of data points across provinces either. When we look at the age distribution it is obvious that people who died tended to be slightly older than those who recovered. The density curve of unknown outcomes is more similar to that of death than of recovery, suggesting that among these people there might have been more deaths than recoveries. And lastly, I want to plot how many days passed between onset, hospitalisation and outcome for each case. ggplot(data = fluH7N9_china_2013_gather, aes(x = Date, y = as.numeric(age), color = outcome)) + geom_point(aes(color = outcome, shape = gender), size = 1.5, alpha = 0.6) + geom_path(aes(group = case_ID)) + facet_wrap( ~ province, ncol = 2) + my_theme() + scale_shape_manual(values = c(15, 16, 17)) + scale_color_brewer(palette=&quot;Set1&quot;, na.value = &quot;grey50&quot;) + scale_fill_brewer(palette=&quot;Set1&quot;) + labs( color = &quot;Outcome&quot;, shape = &quot;Gender&quot;, x = &quot;Date in 2013&quot;, y = &quot;Age&quot;, title = &quot;2013 Influenza A H7N9 cases in China&quot;, subtitle = &quot;Dataset from &#39;outbreaks&#39; package (Kucharski et al. 2014)&quot;, caption = &quot;\\nTime from onset of flu to outcome.&quot; ) #&gt; Warning: Removed 149 rows containing missing values (geom_point). #&gt; Warning: Removed 122 rows containing missing values (geom_path). This plot shows that there are many missing values in the dates, so it is hard to draw a general conclusion. 6.3 Features In Machine Learning-speak features are the variables used for model training. Using the right features dramatically influences the accuracy of the model. Because we don’t have many features, I am keeping age as it is, but I am also generating new features: from the date information I am calculating the days between onset and outcome and between onset and hospitalisation I am converting gender into numeric values with 1 for female and 0 for male similarly, I am converting provinces to binary classifiers (yes == 1, no == 0) for Shanghai, Zhejiang, Jiangsu and other provinces the same binary classification is given for whether a case was hospitalised, and whether they had an early onset or early outcome (earlier than the median date) # convert gender, provinces to discrete and numeric values library(dplyr) #&gt; #&gt; Attaching package: &#39;dplyr&#39; #&gt; The following object is masked from &#39;package:gridExtra&#39;: #&gt; #&gt; combine #&gt; The following objects are masked from &#39;package:plyr&#39;: #&gt; #&gt; arrange, count, desc, failwith, id, mutate, rename, summarise, summarize #&gt; The following objects are masked from &#39;package:stats&#39;: #&gt; #&gt; filter, lag #&gt; The following objects are masked from &#39;package:base&#39;: #&gt; #&gt; intersect, setdiff, setequal, union dataset &lt;- fluH7N9_china_2013 %&gt;% mutate(hospital = as.factor(ifelse(is.na(date_of_hospitalisation), 0, 1)), gender_f = as.factor(ifelse(gender == &quot;f&quot;, 1, 0)), province_Jiangsu = as.factor(ifelse(province == &quot;Jiangsu&quot;, 1, 0)), province_Shanghai = as.factor(ifelse(province == &quot;Shanghai&quot;, 1, 0)), province_Zhejiang = as.factor(ifelse(province == &quot;Zhejiang&quot;, 1, 0)), province_other = as.factor(ifelse(province == &quot;Zhejiang&quot; | province == &quot;Jiangsu&quot; | province == &quot;Shanghai&quot;, 0, 1)), days_onset_to_outcome = as.numeric(as.character(gsub(&quot; days&quot;, &quot;&quot;, as.Date(as.character(date_of_outcome), format = &quot;%Y-%m-%d&quot;) - as.Date(as.character(date_of_onset), format = &quot;%Y-%m-%d&quot;)))), days_onset_to_hospital = as.numeric(as.character(gsub(&quot; days&quot;, &quot;&quot;, as.Date(as.character(date_of_hospitalisation), format = &quot;%Y-%m-%d&quot;) - as.Date(as.character(date_of_onset), format = &quot;%Y-%m-%d&quot;)))), age = as.numeric(as.character(age)), early_onset = as.factor(ifelse(date_of_onset &lt; summary(date_of_onset)[[3]], 1, 0)), early_outcome = as.factor(ifelse(date_of_outcome &lt; summary(date_of_outcome)[[3]], 1, 0))) %&gt;% subset(select = -c(2:4, 6, 8)) # print rownames(dataset) &lt;- dataset$case_ID dataset &lt;- dataset[, -c(1,4)] head(dataset) #&gt; outcome age hospital gender_f province_Jiangsu province_Shanghai province_Zhejiang province_other days_onset_to_outcome days_onset_to_hospital early_onset early_outcome #&gt; case_1 Death 87 0 0 0 1 0 0 13 NA 1 1 #&gt; case_2 Death 27 1 0 0 1 0 0 11 4 1 1 #&gt; case_3 Death 35 1 1 0 0 0 1 31 10 1 1 #&gt; case_4 &lt;NA&gt; 45 1 1 1 0 0 0 NA 8 1 &lt;NA&gt; #&gt; case_5 Recover 48 1 1 1 0 0 0 57 11 1 0 #&gt; case_6 Death 32 1 1 1 0 0 0 36 7 1 1 summary(dataset$outcome) #&gt; Death Recover NA&#39;s #&gt; 32 47 57 6.3.1 Imputing missing values https://www.r-bloggers.com/imputing-missing-data-with-r-mice-package/ When looking at the dataset I created for modeling, it is obvious that we have quite a few missing values. The missing values from the outcome column are what I want to predict but for the rest I would either have to remove the entire row from the data or impute the missing information. I decided to try the latter with the mice package. library(mice) #&gt; Loading required package: lattice #&gt; #&gt; Attaching package: &#39;mice&#39; #&gt; The following object is masked from &#39;package:tidyr&#39;: #&gt; #&gt; complete #&gt; The following objects are masked from &#39;package:base&#39;: #&gt; #&gt; cbind, rbind dataset_impute &lt;- mice(dataset[, -1], print = FALSE) # remove outcome #&gt; Warning: Number of logged events: 150 impute_complete &lt;- mice::complete(dataset_impute, 1) # return 1st imputed dataset rownames(impute_complete) &lt;- row.names(dataset) impute_complete #&gt; age hospital gender_f province_Jiangsu province_Shanghai province_Zhejiang province_other days_onset_to_outcome days_onset_to_hospital early_onset early_outcome #&gt; case_1 87 0 0 0 1 0 0 13 6 1 1 #&gt; case_2 27 1 0 0 1 0 0 11 4 1 1 #&gt; case_3 35 1 1 0 0 0 1 31 10 1 1 #&gt; case_4 45 1 1 1 0 0 0 14 8 1 1 #&gt; case_5 48 1 1 1 0 0 0 57 11 1 0 #&gt; case_6 32 1 1 1 0 0 0 36 7 1 1 #&gt; case_7 83 1 0 1 0 0 0 20 9 1 1 #&gt; case_8 38 1 0 0 0 1 0 20 11 1 1 #&gt; case_9 67 1 0 0 0 1 0 7 0 1 1 #&gt; case_10 48 1 0 0 1 0 0 6 4 1 1 #&gt; case_11 64 1 0 0 0 1 0 6 2 1 1 #&gt; case_12 52 0 1 0 1 0 0 7 7 1 1 #&gt; case_13 67 1 1 0 1 0 0 12 3 1 1 #&gt; case_14 4 0 0 0 1 0 0 10 5 1 1 #&gt; case_15 61 0 1 1 0 0 0 32 4 1 0 #&gt; case_16 79 0 0 1 0 0 0 38 4 1 0 #&gt; case_17 74 1 0 0 1 0 0 14 6 1 1 #&gt; case_18 66 1 0 0 1 0 0 20 4 1 1 #&gt; case_19 59 1 0 0 1 0 0 67 5 1 0 #&gt; case_20 55 1 0 0 0 0 1 22 4 1 1 #&gt; case_21 67 1 0 0 1 0 0 23 1 1 1 #&gt; case_22 85 1 0 1 0 0 0 31 4 1 0 #&gt; case_23 25 1 1 1 0 0 0 46 0 1 0 #&gt; case_24 64 0 0 0 1 0 0 6 5 1 1 #&gt; case_25 62 1 0 0 1 0 0 35 0 1 0 #&gt; case_26 77 1 0 0 1 0 0 11 4 1 1 #&gt; case_27 51 1 1 0 0 1 0 37 27 1 1 #&gt; case_28 79 0 0 0 0 1 0 32 0 1 0 #&gt; case_29 76 1 1 0 1 0 0 32 4 1 0 #&gt; case_30 81 0 1 0 1 0 0 23 0 1 0 #&gt; case_31 70 0 0 1 0 0 0 32 4 1 0 #&gt; case_32 74 0 0 1 0 0 0 38 4 1 0 #&gt; case_33 65 0 0 0 0 1 0 24 8 1 0 #&gt; case_34 74 1 0 0 1 0 0 11 5 1 1 #&gt; case_35 83 1 1 0 1 0 0 38 5 1 0 #&gt; case_36 68 0 0 0 1 0 0 17 4 1 1 #&gt; case_37 31 0 0 1 0 0 0 45 7 1 0 #&gt; case_38 56 0 0 1 0 0 0 23 7 1 0 #&gt; case_39 66 1 0 0 0 1 0 13 0 0 1 #&gt; case_40 74 1 0 0 0 1 0 6 5 1 1 #&gt; case_41 54 1 1 0 0 1 0 23 6 1 1 #&gt; case_42 53 1 0 0 1 0 0 11 7 1 1 #&gt; case_43 86 1 0 0 1 0 0 18 3 1 1 #&gt; case_44 7 1 1 0 0 0 1 6 0 0 1 #&gt; case_45 56 1 0 0 1 0 0 86 3 1 0 #&gt; case_46 77 0 1 1 0 0 0 9 0 1 1 #&gt; case_47 72 0 0 1 0 0 0 32 5 1 0 #&gt; case_48 65 1 0 0 0 1 0 20 6 1 1 #&gt; case_49 38 1 0 0 0 1 0 28 5 1 0 #&gt; case_50 34 1 0 0 0 0 1 17 3 1 0 #&gt; case_51 65 1 0 0 0 0 1 15 2 0 1 #&gt; case_52 64 0 1 0 0 1 0 17 7 1 1 #&gt; case_53 62 0 1 0 0 1 0 22 7 1 1 #&gt; case_54 75 0 0 0 0 1 0 37 7 1 0 #&gt; case_55 79 0 0 0 0 1 0 18 7 0 0 #&gt; case_56 73 1 0 0 1 0 0 11 6 1 1 #&gt; case_57 54 1 0 0 1 0 0 13 4 0 1 #&gt; case_58 78 1 0 0 1 0 0 31 4 1 0 #&gt; case_59 50 0 0 1 0 0 0 37 7 1 0 #&gt; case_60 26 0 0 1 0 0 0 18 4 0 1 #&gt; case_61 60 0 0 1 0 0 0 10 4 1 1 #&gt; case_62 68 0 1 0 0 1 0 11 5 1 1 #&gt; case_63 60 0 0 0 0 0 1 26 3 0 0 #&gt; case_64 56 0 0 1 0 0 0 16 4 0 0 #&gt; case_65 21 0 1 1 0 0 0 37 4 1 0 #&gt; case_66 72 0 0 1 0 0 0 32 3 0 0 #&gt; case_67 56 0 0 0 0 1 0 31 3 0 0 #&gt; case_68 57 0 0 0 0 1 0 6 10 0 1 #&gt; case_69 62 0 0 0 0 1 0 8 7 0 1 #&gt; case_70 58 0 1 0 0 1 0 10 11 0 1 #&gt; case_71 72 0 1 0 0 1 0 13 6 0 0 #&gt; case_72 47 1 0 0 1 0 0 17 5 0 0 #&gt; case_73 69 0 0 0 1 0 0 7 4 0 1 #&gt; case_74 54 0 0 0 1 0 0 17 6 1 0 #&gt; case_75 83 0 0 0 1 0 0 6 4 1 1 #&gt; case_76 55 0 0 0 1 0 0 6 7 1 1 #&gt; case_77 2 0 0 0 1 0 0 13 7 1 0 #&gt; case_78 89 1 0 0 1 0 0 17 4 0 0 #&gt; case_79 37 0 1 0 0 1 0 16 0 0 0 #&gt; case_80 74 0 0 0 0 1 0 22 3 0 0 #&gt; case_81 86 0 0 0 0 1 0 10 6 0 1 #&gt; case_82 41 0 0 0 0 1 0 8 10 0 1 #&gt; case_83 38 0 0 0 0 0 1 23 4 0 0 #&gt; case_84 26 0 1 1 0 0 0 13 1 1 1 #&gt; case_85 80 1 1 0 1 0 0 13 7 0 1 #&gt; case_86 54 0 1 0 0 1 0 10 3 0 1 #&gt; case_87 69 0 0 0 0 1 0 8 3 0 1 #&gt; case_88 4 0 0 0 0 0 1 11 0 0 0 #&gt; case_89 54 1 0 1 0 0 0 36 6 0 0 #&gt; case_90 43 1 0 0 0 1 0 9 3 0 1 #&gt; case_91 48 1 0 0 0 1 0 16 6 0 0 #&gt; case_92 66 1 1 0 0 1 0 13 7 0 1 #&gt; case_93 56 0 0 0 0 1 0 17 0 0 0 #&gt; case_94 35 0 1 0 0 1 0 13 6 0 0 #&gt; case_95 37 0 0 0 0 1 0 7 4 1 1 #&gt; case_96 43 0 0 1 0 0 0 37 4 1 0 #&gt; case_97 75 1 1 0 1 0 0 22 5 0 0 #&gt; case_98 76 0 0 0 0 1 0 13 3 0 1 #&gt; case_99 68 0 1 0 0 1 0 6 7 0 1 #&gt; case_100 58 0 0 0 0 1 0 20 7 0 0 #&gt; case_101 79 0 1 0 0 1 0 13 3 0 1 #&gt; case_102 81 0 0 0 0 1 0 32 6 0 0 #&gt; case_103 68 1 0 1 0 0 0 37 6 0 0 #&gt; case_104 54 0 1 0 0 1 0 20 7 0 0 #&gt; case_105 32 0 0 0 0 1 0 22 6 0 0 #&gt; case_106 36 1 0 0 0 0 1 30 6 0 0 #&gt; case_107 91 0 0 0 0 0 1 21 10 0 0 #&gt; case_108 84 0 0 0 0 1 0 13 3 0 0 #&gt; case_109 62 0 0 0 0 1 0 6 0 0 1 #&gt; case_110 53 1 0 0 0 0 1 9 4 0 0 #&gt; case_111 56 0 0 0 0 0 1 22 5 0 0 #&gt; case_112 69 0 0 0 0 0 1 10 1 0 0 #&gt; case_113 60 0 1 0 0 1 0 13 3 0 0 #&gt; case_114 50 0 1 0 0 1 0 10 6 0 1 #&gt; case_115 38 0 0 0 0 1 0 16 5 0 0 #&gt; case_116 65 1 0 0 0 0 1 21 5 0 0 #&gt; case_117 76 0 1 0 0 0 1 28 5 0 0 #&gt; case_118 49 0 0 1 0 0 0 18 5 0 1 #&gt; case_119 36 0 0 1 0 0 0 26 4 0 0 #&gt; case_120 60 0 0 1 0 0 0 31 7 1 0 #&gt; case_121 64 1 1 0 0 0 1 30 5 0 0 #&gt; case_122 38 0 0 0 0 1 0 2 3 0 1 #&gt; case_123 54 1 0 0 0 0 1 16 7 0 0 #&gt; case_124 80 0 0 0 0 0 1 17 7 0 0 #&gt; case_125 31 0 1 0 0 0 1 20 0 0 0 #&gt; case_126 80 1 0 0 0 0 1 24 10 0 0 #&gt; case_127 4 0 0 0 0 0 1 9 4 0 0 #&gt; case_128 58 1 0 0 0 0 1 22 7 0 0 #&gt; case_129 69 1 0 0 0 0 1 28 4 0 0 #&gt; case_130 69 1 0 0 0 0 1 30 1 0 0 #&gt; case_131 9 1 0 0 0 0 1 11 1 0 0 #&gt; case_132 79 1 1 0 0 0 1 18 0 0 1 #&gt; case_133 6 1 0 0 0 0 1 2 0 0 0 #&gt; case_134 15 1 0 1 0 0 0 7 1 0 0 #&gt; case_135 61 1 1 0 0 0 1 32 3 0 0 #&gt; case_136 51 1 1 0 0 0 1 8 1 0 1 dataset_complete &lt;- merge(dataset[, 1, drop = FALSE], # mice::complete(dataset_impute, 1), impute_complete, by = &quot;row.names&quot;, all = TRUE) rownames(dataset_complete) &lt;- dataset_complete$Row.names dataset_complete &lt;- dataset_complete[, -1] dataset_complete #&gt; outcome age hospital gender_f province_Jiangsu province_Shanghai province_Zhejiang province_other days_onset_to_outcome days_onset_to_hospital early_onset early_outcome #&gt; case_1 Death 87 0 0 0 1 0 0 13 6 1 1 #&gt; case_10 Death 48 1 0 0 1 0 0 6 4 1 1 #&gt; case_100 &lt;NA&gt; 58 0 0 0 0 1 0 20 7 0 0 #&gt; case_101 &lt;NA&gt; 79 0 1 0 0 1 0 13 3 0 1 #&gt; case_102 &lt;NA&gt; 81 0 0 0 0 1 0 32 6 0 0 #&gt; case_103 &lt;NA&gt; 68 1 0 1 0 0 0 37 6 0 0 #&gt; case_104 &lt;NA&gt; 54 0 1 0 0 1 0 20 7 0 0 #&gt; case_105 &lt;NA&gt; 32 0 0 0 0 1 0 22 6 0 0 #&gt; case_106 Recover 36 1 0 0 0 0 1 30 6 0 0 #&gt; case_107 Death 91 0 0 0 0 0 1 21 10 0 0 #&gt; case_108 &lt;NA&gt; 84 0 0 0 0 1 0 13 3 0 0 #&gt; case_109 &lt;NA&gt; 62 0 0 0 0 1 0 6 0 0 1 #&gt; case_11 Death 64 1 0 0 0 1 0 6 2 1 1 #&gt; case_110 &lt;NA&gt; 53 1 0 0 0 0 1 9 4 0 0 #&gt; case_111 Death 56 0 0 0 0 0 1 22 5 0 0 #&gt; case_112 &lt;NA&gt; 69 0 0 0 0 0 1 10 1 0 0 #&gt; case_113 &lt;NA&gt; 60 0 1 0 0 1 0 13 3 0 0 #&gt; case_114 &lt;NA&gt; 50 0 1 0 0 1 0 10 6 0 1 #&gt; case_115 &lt;NA&gt; 38 0 0 0 0 1 0 16 5 0 0 #&gt; case_116 Recover 65 1 0 0 0 0 1 21 5 0 0 #&gt; case_117 Recover 76 0 1 0 0 0 1 28 5 0 0 #&gt; case_118 &lt;NA&gt; 49 0 0 1 0 0 0 18 5 0 1 #&gt; case_119 Recover 36 0 0 1 0 0 0 26 4 0 0 #&gt; case_12 Death 52 0 1 0 1 0 0 7 7 1 1 #&gt; case_120 &lt;NA&gt; 60 0 0 1 0 0 0 31 7 1 0 #&gt; case_121 Death 64 1 1 0 0 0 1 30 5 0 0 #&gt; case_122 &lt;NA&gt; 38 0 0 0 0 1 0 2 3 0 1 #&gt; case_123 Death 54 1 0 0 0 0 1 16 7 0 0 #&gt; case_124 Recover 80 0 0 0 0 0 1 17 7 0 0 #&gt; case_125 Recover 31 0 1 0 0 0 1 20 0 0 0 #&gt; case_126 &lt;NA&gt; 80 1 0 0 0 0 1 24 10 0 0 #&gt; case_127 Recover 4 0 0 0 0 0 1 9 4 0 0 #&gt; case_128 Recover 58 1 0 0 0 0 1 22 7 0 0 #&gt; case_129 Recover 69 1 0 0 0 0 1 28 4 0 0 #&gt; case_13 Death 67 1 1 0 1 0 0 12 3 1 1 #&gt; case_130 &lt;NA&gt; 69 1 0 0 0 0 1 30 1 0 0 #&gt; case_131 Recover 9 1 0 0 0 0 1 11 1 0 0 #&gt; case_132 &lt;NA&gt; 79 1 1 0 0 0 1 18 0 0 1 #&gt; case_133 Recover 6 1 0 0 0 0 1 2 0 0 0 #&gt; case_134 Recover 15 1 0 1 0 0 0 7 1 0 0 #&gt; case_135 Death 61 1 1 0 0 0 1 32 3 0 0 #&gt; case_136 &lt;NA&gt; 51 1 1 0 0 0 1 8 1 0 1 #&gt; case_14 Recover 4 0 0 0 1 0 0 10 5 1 1 #&gt; case_15 &lt;NA&gt; 61 0 1 1 0 0 0 32 4 1 0 #&gt; case_16 &lt;NA&gt; 79 0 0 1 0 0 0 38 4 1 0 #&gt; case_17 Death 74 1 0 0 1 0 0 14 6 1 1 #&gt; case_18 Recover 66 1 0 0 1 0 0 20 4 1 1 #&gt; case_19 Death 59 1 0 0 1 0 0 67 5 1 0 #&gt; case_2 Death 27 1 0 0 1 0 0 11 4 1 1 #&gt; case_20 Recover 55 1 0 0 0 0 1 22 4 1 1 #&gt; case_21 Recover 67 1 0 0 1 0 0 23 1 1 1 #&gt; case_22 &lt;NA&gt; 85 1 0 1 0 0 0 31 4 1 0 #&gt; case_23 Recover 25 1 1 1 0 0 0 46 0 1 0 #&gt; case_24 Death 64 0 0 0 1 0 0 6 5 1 1 #&gt; case_25 Recover 62 1 0 0 1 0 0 35 0 1 0 #&gt; case_26 Death 77 1 0 0 1 0 0 11 4 1 1 #&gt; case_27 Recover 51 1 1 0 0 1 0 37 27 1 1 #&gt; case_28 &lt;NA&gt; 79 0 0 0 0 1 0 32 0 1 0 #&gt; case_29 Recover 76 1 1 0 1 0 0 32 4 1 0 #&gt; case_3 Death 35 1 1 0 0 0 1 31 10 1 1 #&gt; case_30 Recover 81 0 1 0 1 0 0 23 0 1 0 #&gt; case_31 &lt;NA&gt; 70 0 0 1 0 0 0 32 4 1 0 #&gt; case_32 &lt;NA&gt; 74 0 0 1 0 0 0 38 4 1 0 #&gt; case_33 Recover 65 0 0 0 0 1 0 24 8 1 0 #&gt; case_34 Death 74 1 0 0 1 0 0 11 5 1 1 #&gt; case_35 Death 83 1 1 0 1 0 0 38 5 1 0 #&gt; case_36 Recover 68 0 0 0 1 0 0 17 4 1 1 #&gt; case_37 Recover 31 0 0 1 0 0 0 45 7 1 0 #&gt; case_38 &lt;NA&gt; 56 0 0 1 0 0 0 23 7 1 0 #&gt; case_39 &lt;NA&gt; 66 1 0 0 0 1 0 13 0 0 1 #&gt; case_4 &lt;NA&gt; 45 1 1 1 0 0 0 14 8 1 1 #&gt; case_40 &lt;NA&gt; 74 1 0 0 0 1 0 6 5 1 1 #&gt; case_41 &lt;NA&gt; 54 1 1 0 0 1 0 23 6 1 1 #&gt; case_42 &lt;NA&gt; 53 1 0 0 1 0 0 11 7 1 1 #&gt; case_43 Death 86 1 0 0 1 0 0 18 3 1 1 #&gt; case_44 Recover 7 1 1 0 0 0 1 6 0 0 1 #&gt; case_45 Death 56 1 0 0 1 0 0 86 3 1 0 #&gt; case_46 Death 77 0 1 1 0 0 0 9 0 1 1 #&gt; case_47 &lt;NA&gt; 72 0 0 1 0 0 0 32 5 1 0 #&gt; case_48 &lt;NA&gt; 65 1 0 0 0 1 0 20 6 1 1 #&gt; case_49 Recover 38 1 0 0 0 1 0 28 5 1 0 #&gt; case_5 Recover 48 1 1 1 0 0 0 57 11 1 0 #&gt; case_50 Recover 34 1 0 0 0 0 1 17 3 1 0 #&gt; case_51 Recover 65 1 0 0 0 0 1 15 2 0 1 #&gt; case_52 &lt;NA&gt; 64 0 1 0 0 1 0 17 7 1 1 #&gt; case_53 Death 62 0 1 0 0 1 0 22 7 1 1 #&gt; case_54 &lt;NA&gt; 75 0 0 0 0 1 0 37 7 1 0 #&gt; case_55 Recover 79 0 0 0 0 1 0 18 7 0 0 #&gt; case_56 &lt;NA&gt; 73 1 0 0 1 0 0 11 6 1 1 #&gt; case_57 Recover 54 1 0 0 1 0 0 13 4 0 1 #&gt; case_58 Recover 78 1 0 0 1 0 0 31 4 1 0 #&gt; case_59 Recover 50 0 0 1 0 0 0 37 7 1 0 #&gt; case_6 Death 32 1 1 1 0 0 0 36 7 1 1 #&gt; case_60 Recover 26 0 0 1 0 0 0 18 4 0 1 #&gt; case_61 Death 60 0 0 1 0 0 0 10 4 1 1 #&gt; case_62 &lt;NA&gt; 68 0 1 0 0 1 0 11 5 1 1 #&gt; case_63 &lt;NA&gt; 60 0 0 0 0 0 1 26 3 0 0 #&gt; case_64 Recover 56 0 0 1 0 0 0 16 4 0 0 #&gt; case_65 Recover 21 0 1 1 0 0 0 37 4 1 0 #&gt; case_66 &lt;NA&gt; 72 0 0 1 0 0 0 32 3 0 0 #&gt; case_67 &lt;NA&gt; 56 0 0 0 0 1 0 31 3 0 0 #&gt; case_68 &lt;NA&gt; 57 0 0 0 0 1 0 6 10 0 1 #&gt; case_69 &lt;NA&gt; 62 0 0 0 0 1 0 8 7 0 1 #&gt; case_7 Death 83 1 0 1 0 0 0 20 9 1 1 #&gt; case_70 &lt;NA&gt; 58 0 1 0 0 1 0 10 11 0 1 #&gt; case_71 &lt;NA&gt; 72 0 1 0 0 1 0 13 6 0 0 #&gt; case_72 Recover 47 1 0 0 1 0 0 17 5 0 0 #&gt; case_73 Recover 69 0 0 0 1 0 0 7 4 0 1 #&gt; case_74 Recover 54 0 0 0 1 0 0 17 6 1 0 #&gt; case_75 Death 83 0 0 0 1 0 0 6 4 1 1 #&gt; case_76 Death 55 0 0 0 1 0 0 6 7 1 1 #&gt; case_77 Recover 2 0 0 0 1 0 0 13 7 1 0 #&gt; case_78 Death 89 1 0 0 1 0 0 17 4 0 0 #&gt; case_79 Recover 37 0 1 0 0 1 0 16 0 0 0 #&gt; case_8 Death 38 1 0 0 0 1 0 20 11 1 1 #&gt; case_80 &lt;NA&gt; 74 0 0 0 0 1 0 22 3 0 0 #&gt; case_81 Death 86 0 0 0 0 1 0 10 6 0 1 #&gt; case_82 Recover 41 0 0 0 0 1 0 8 10 0 1 #&gt; case_83 Recover 38 0 0 0 0 0 1 23 4 0 0 #&gt; case_84 &lt;NA&gt; 26 0 1 1 0 0 0 13 1 1 1 #&gt; case_85 &lt;NA&gt; 80 1 1 0 1 0 0 13 7 0 1 #&gt; case_86 &lt;NA&gt; 54 0 1 0 0 1 0 10 3 0 1 #&gt; case_87 Death 69 0 0 0 0 1 0 8 3 0 1 #&gt; case_88 &lt;NA&gt; 4 0 0 0 0 0 1 11 0 0 0 #&gt; case_89 Recover 54 1 0 1 0 0 0 36 6 0 0 #&gt; case_9 &lt;NA&gt; 67 1 0 0 0 1 0 7 0 1 1 #&gt; case_90 &lt;NA&gt; 43 1 0 0 0 1 0 9 3 0 1 #&gt; case_91 Recover 48 1 0 0 0 1 0 16 6 0 0 #&gt; case_92 &lt;NA&gt; 66 1 1 0 0 1 0 13 7 0 1 #&gt; case_93 &lt;NA&gt; 56 0 0 0 0 1 0 17 0 0 0 #&gt; case_94 Recover 35 0 1 0 0 1 0 13 6 0 0 #&gt; case_95 &lt;NA&gt; 37 0 0 0 0 1 0 7 4 1 1 #&gt; case_96 &lt;NA&gt; 43 0 0 1 0 0 0 37 4 1 0 #&gt; case_97 Recover 75 1 1 0 1 0 0 22 5 0 0 #&gt; case_98 Death 76 0 0 0 0 1 0 13 3 0 1 #&gt; case_99 &lt;NA&gt; 68 0 1 0 0 1 0 6 7 0 1 cat(&quot;NAs before imput: &quot;, sum(is.na(dataset)), &quot;\\n&quot;) #&gt; NAs before imput: 277 summary(dataset$outcome) #&gt; Death Recover NA&#39;s #&gt; 32 47 57 cat(&quot;NAs after imput: &quot;, sum(is.na(dataset_complete)), &quot;\\n&quot;) #&gt; NAs after imput: 57 summary(dataset_complete$outcome) #&gt; Death Recover NA&#39;s #&gt; 32 47 57 6.4 Test, train and validation data sets For building the model, I am separating the imputed data frame into training and test data. Test data are the 57 cases with unknown outcome. The training data will be further devided for validation of the models: 70% of the training data will be kept for model building and the remaining 30% will be used for model testing. I am using the caret package for modeling. train_index &lt;- which(is.na(dataset_complete$outcome)) train_data &lt;- dataset_complete[-train_index, ] # 79x12 test_data &lt;- dataset_complete[train_index, -1] # 57x11 library(caret) set.seed(27) val_index &lt;- createDataPartition(train_data$outcome, p = 0.7, list=FALSE) val_train_data &lt;- train_data[val_index, ] val_test_data &lt;- train_data[-val_index, ] val_train_X &lt;- val_train_data[,-1] val_test_X &lt;- val_test_data[,-1] 6.4.1 Decision trees To get an idea about how each feature contributes to the prediction of the outcome, I first built a decision tree based on the training data using rpart and rattle. library(rpart) library(rattle) #&gt; Rattle: A free graphical interface for data science with R. #&gt; Version 5.2.0 Copyright (c) 2006-2018 Togaware Pty Ltd. #&gt; Type &#39;rattle()&#39; to shake, rattle, and roll your data. library(rpart.plot) library(RColorBrewer) set.seed(27) fit &lt;- rpart(outcome ~ ., data = train_data, method = &quot;class&quot;, control = rpart.control(xval = 10, minbucket = 2, cp = 0), parms = list(split = &quot;information&quot;)) fancyRpartPlot(fit) This randomly generated decision tree shows that cases with an early outcome were most likely to die when they were 68 or older, when they also had an early onset and when they were sick for fewer than 13 days. If a person was not among the first cases and was younger than 52, they had a good chance of recovering, but if they were 82 or older, they were more likely to die from the flu. 6.4.2 Feature Importance Not all of the features I created will be equally important to the model. The decision tree already gave me an idea of which features might be most important but I also want to estimate feature importance using a Random Forest approach with repeated cross validation. # prepare training scheme control &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 10, repeats = 10) # train the model set.seed(27) model &lt;- train(outcome ~ ., data = train_data, method = &quot;rf&quot;, preProcess = NULL, trControl = control) # estimate variable importance importance &lt;- varImp(model, scale=TRUE) from &lt;- c(&quot;age&quot;, &quot;hospital1&quot;, &quot;gender_f1&quot;, &quot;province_Jiangsu1&quot;, &quot;province_Shanghai1&quot;, &quot;province_Zhejiang1&quot;, &quot;province_other1&quot;, &quot;days_onset_to_outcome&quot;, &quot;days_onset_to_hospital&quot;, &quot;early_onset1&quot;, &quot;early_outcome1&quot; ) to &lt;- c(&quot;Age&quot;, &quot;Hospital&quot;, &quot;Female&quot;, &quot;Jiangsu&quot;, &quot;Shanghai&quot;, &quot;Zhejiang&quot;, &quot;Other province&quot;, &quot;Days onset to outcome&quot;, &quot;Days onset to hospital&quot;, &quot;Early onset&quot;, &quot;Early outcome&quot; ) importance_df_1 &lt;- importance$importance importance_df_1$group &lt;- rownames(importance_df_1) importance_df_1$group &lt;- mapvalues(importance_df_1$group, from = from, to = to) f = importance_df_1[order(importance_df_1$Overall, decreasing = FALSE), &quot;group&quot;] importance_df_2 &lt;- importance_df_1 importance_df_2$Overall &lt;- 0 importance_df &lt;- rbind(importance_df_1, importance_df_2) # setting factor levels importance_df &lt;- within(importance_df, group &lt;- factor(group, levels = f)) importance_df_1 &lt;- within(importance_df_1, group &lt;- factor(group, levels = f)) ggplot() + geom_point(data = importance_df_1, aes(x = Overall, y = group, color = group), size = 2) + geom_path(data = importance_df, aes(x = Overall, y = group, color = group, group = group), size = 1) + scale_color_manual(values = rep(brewer.pal(1, &quot;Set1&quot;)[1], 11)) + my_theme() + theme(legend.position = &quot;none&quot;, axis.text.x = element_text(angle = 0, vjust = 0.5, hjust = 0.5)) + labs( x = &quot;Importance&quot;, y = &quot;&quot;, title = &quot;2013 Influenza A H7N9 cases in China&quot;, subtitle = &quot;Scaled feature importance&quot;, caption = &quot;\\nDetermined with Random Forest and repeated cross validation (10 repeats, 10 times)&quot; ) #&gt; Warning in brewer.pal(1, &quot;Set1&quot;): minimal value for n is 3, returning requested palette with 3 different levels This tells me that age is the most important determining factor for predicting disease outcome, followed by days between onset an outcome, early outcome and days between onset and hospitalisation. 6.4.3 Feature Plot Before I start actually building models, I want to check whether the distribution of feature values is comparable between training, validation and test datasets. # tidy dataframe of 11 variables for plotting features for all datasets # dataset_complete: 136x12. test + val_train + val_test dataset_complete_gather &lt;- dataset_complete %&gt;% mutate(set = ifelse(rownames(dataset_complete) %in% rownames(test_data), &quot;Test Data&quot;, ifelse(rownames(dataset_complete) %in% rownames(val_train_data), &quot;Validation Train Data&quot;, ifelse(rownames(dataset_complete) %in% rownames(val_test_data), &quot;Validation Test Data&quot;, &quot;NA&quot;))), case_ID = rownames(.)) %&gt;% gather(group, value, age:early_outcome) #&gt; Warning: attributes are not identical across measure variables; #&gt; they will be dropped # map values in group to more readable from &lt;- c(&quot;age&quot;, &quot;hospital&quot;, &quot;gender_f&quot;, &quot;province_Jiangsu&quot;, &quot;province_Shanghai&quot;, &quot;province_Zhejiang&quot;, &quot;province_other&quot;, &quot;days_onset_to_outcome&quot;, &quot;days_onset_to_hospital&quot;, &quot;early_onset&quot;, &quot;early_outcome&quot; ) to &lt;- c(&quot;Age&quot;, &quot;Hospital&quot;, &quot;Female&quot;, &quot;Jiangsu&quot;, &quot;Shanghai&quot;, &quot;Zhejiang&quot;, &quot;Other province&quot;, &quot;Days onset to outcome&quot;, &quot;Days onset to hospital&quot;, &quot;Early onset&quot;, &quot;Early outcome&quot; ) dataset_complete_gather$group &lt;- mapvalues(dataset_complete_gather$group, from = from, to = to) 6.4.4 Plot distribution of features in each dataset # plot features all datasets ggplot(data = dataset_complete_gather, aes(x = as.numeric(value), fill = outcome, color = outcome)) + geom_density(alpha = 0.2) + geom_rug() + scale_color_brewer(palette=&quot;Set1&quot;, na.value = &quot;grey50&quot;) + scale_fill_brewer(palette=&quot;Set1&quot;, na.value = &quot;grey50&quot;) + my_theme() + facet_wrap(set ~ group, ncol = 11, scales = &quot;free&quot;) + labs( x = &quot;Value&quot;, y = &quot;Density&quot;, title = &quot;2013 Influenza A H7N9 cases in China&quot;, subtitle = &quot;Features for classifying outcome&quot;, caption = &quot;\\nDensity distribution of all features used for classification of flu outcome.&quot; ) 6.4.5 Plot 3 features vs outcome, all datasets # plot three groups vs outcome ggplot(subset(dataset_complete_gather, group == &quot;Age&quot; | group == &quot;Days onset to hospital&quot; | group == &quot;Days onset to outcome&quot;), aes(x=outcome, y=as.numeric(value), fill=set)) + geom_boxplot() + my_theme() + scale_fill_brewer(palette=&quot;Set1&quot;, type = &quot;div &quot;) + facet_wrap( ~ group, ncol = 3, scales = &quot;free&quot;) + labs( fill = &quot;&quot;, x = &quot;Outcome&quot;, y = &quot;Value&quot;, title = &quot;2013 Influenza A H7N9 cases in China&quot;, subtitle = &quot;Features for classifying outcome&quot;, caption = &quot;\\nBoxplot of the features age, days from onset to hospitalisation and days from onset to outcome.&quot; ) Luckily, the distributions looks reasonably similar between the validation and test data, except for a few outliers. 6.5 Comparing Machine Learning algorithms Before I try to predict the outcome of the unknown cases, I am testing the models’ accuracy with the validation datasets on a couple of algorithms. I have chosen only a few more well known algorithms, but caret implements many more. I have chosen to not do any preprocessing because I was worried that the different data distributions with continuous variables (e.g. age) and binary variables (i.e. 0, 1 classification of e.g. hospitalisation) would lead to problems. Random Forest GLM net k-Nearest Neighbors Penalized Discriminant Analysis Stabilized Linear Discriminant Analysis Nearest Shrunken Centroids Single C5.0 Tree Partial Least Squares train_control &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 10, repeats = 10, verboseIter = FALSE) 6.5.1 Random Forest Random Forests predictions are based on the generation of multiple classification trees. This model classified 14 out of 23 cases correctly. set.seed(27) model_rf &lt;- caret::train(outcome ~ ., data = val_train_data, method = &quot;rf&quot;, preProcess = NULL, trControl = train_control) model_rf #&gt; Random Forest #&gt; #&gt; 56 samples #&gt; 11 predictors #&gt; 2 classes: &#39;Death&#39;, &#39;Recover&#39; #&gt; #&gt; No pre-processing #&gt; Resampling: Cross-Validated (10 fold, repeated 10 times) #&gt; Summary of sample sizes: 51, 49, 50, 51, 49, 51, ... #&gt; Resampling results across tuning parameters: #&gt; #&gt; mtry Accuracy Kappa #&gt; 2 0.687 0.340 #&gt; 6 0.732 0.432 #&gt; 11 0.726 0.423 #&gt; #&gt; Accuracy was used to select the optimal model using the largest value. #&gt; The final value used for the model was mtry = 6. confusionMatrix(predict(model_rf, val_test_data[, -1]), val_test_data$outcome) #&gt; Confusion Matrix and Statistics #&gt; #&gt; Reference #&gt; Prediction Death Recover #&gt; Death 3 0 #&gt; Recover 6 14 #&gt; #&gt; Accuracy : 0.739 #&gt; 95% CI : (0.516, 0.898) #&gt; No Information Rate : 0.609 #&gt; P-Value [Acc &gt; NIR] : 0.1421 #&gt; #&gt; Kappa : 0.378 #&gt; #&gt; Mcnemar&#39;s Test P-Value : 0.0412 #&gt; #&gt; Sensitivity : 0.333 #&gt; Specificity : 1.000 #&gt; Pos Pred Value : 1.000 #&gt; Neg Pred Value : 0.700 #&gt; Prevalence : 0.391 #&gt; Detection Rate : 0.130 #&gt; Detection Prevalence : 0.130 #&gt; Balanced Accuracy : 0.667 #&gt; #&gt; &#39;Positive&#39; Class : Death #&gt; 6.5.2 GLM net Lasso or elastic net regularization for generalized linear model regression are based on linear regression models and is useful when we have feature correlation in our model. This model classified 13 out of 23 cases correctly. set.seed(27) model_glmnet &lt;- caret::train(outcome ~ ., data = val_train_data, method = &quot;glmnet&quot;, preProcess = NULL, trControl = train_control) model_glmnet #&gt; glmnet #&gt; #&gt; 56 samples #&gt; 11 predictors #&gt; 2 classes: &#39;Death&#39;, &#39;Recover&#39; #&gt; #&gt; No pre-processing #&gt; Resampling: Cross-Validated (10 fold, repeated 10 times) #&gt; Summary of sample sizes: 51, 49, 50, 51, 49, 51, ... #&gt; Resampling results across tuning parameters: #&gt; #&gt; alpha lambda Accuracy Kappa #&gt; 0.10 0.000491 0.671 0.324 #&gt; 0.10 0.004909 0.669 0.318 #&gt; 0.10 0.049093 0.680 0.339 #&gt; 0.55 0.000491 0.671 0.324 #&gt; 0.55 0.004909 0.671 0.322 #&gt; 0.55 0.049093 0.695 0.365 #&gt; 1.00 0.000491 0.671 0.324 #&gt; 1.00 0.004909 0.672 0.326 #&gt; 1.00 0.049093 0.714 0.414 #&gt; #&gt; Accuracy was used to select the optimal model using the largest value. #&gt; The final values used for the model were alpha = 1 and lambda = 0.0491. confusionMatrix(predict(model_glmnet, val_test_data[, -1]), val_test_data$outcome) #&gt; Confusion Matrix and Statistics #&gt; #&gt; Reference #&gt; Prediction Death Recover #&gt; Death 3 2 #&gt; Recover 6 12 #&gt; #&gt; Accuracy : 0.652 #&gt; 95% CI : (0.427, 0.836) #&gt; No Information Rate : 0.609 #&gt; P-Value [Acc &gt; NIR] : 0.422 #&gt; #&gt; Kappa : 0.207 #&gt; #&gt; Mcnemar&#39;s Test P-Value : 0.289 #&gt; #&gt; Sensitivity : 0.333 #&gt; Specificity : 0.857 #&gt; Pos Pred Value : 0.600 #&gt; Neg Pred Value : 0.667 #&gt; Prevalence : 0.391 #&gt; Detection Rate : 0.130 #&gt; Detection Prevalence : 0.217 #&gt; Balanced Accuracy : 0.595 #&gt; #&gt; &#39;Positive&#39; Class : Death #&gt; 6.5.3 k-Nearest Neighbors k-nearest neighbors predicts based on point distances with predefined constants. This model classified 14 out of 23 cases correctly. set.seed(27) model_kknn &lt;- caret::train(outcome ~ ., data = val_train_data, method = &quot;kknn&quot;, preProcess = NULL, trControl = train_control) model_kknn #&gt; k-Nearest Neighbors #&gt; #&gt; 56 samples #&gt; 11 predictors #&gt; 2 classes: &#39;Death&#39;, &#39;Recover&#39; #&gt; #&gt; No pre-processing #&gt; Resampling: Cross-Validated (10 fold, repeated 10 times) #&gt; Summary of sample sizes: 51, 49, 50, 51, 49, 51, ... #&gt; Resampling results across tuning parameters: #&gt; #&gt; kmax Accuracy Kappa #&gt; 5 0.666 0.313 #&gt; 7 0.653 0.274 #&gt; 9 0.648 0.263 #&gt; #&gt; Tuning parameter &#39;distance&#39; was held constant at a value of 2 #&gt; Tuning parameter &#39;kernel&#39; was held constant at a value of optimal #&gt; Accuracy was used to select the optimal model using the largest value. #&gt; The final values used for the model were kmax = 5, distance = 2 and kernel = optimal. confusionMatrix(predict(model_kknn, val_test_data[, -1]), val_test_data$outcome) #&gt; Confusion Matrix and Statistics #&gt; #&gt; Reference #&gt; Prediction Death Recover #&gt; Death 5 3 #&gt; Recover 4 11 #&gt; #&gt; Accuracy : 0.696 #&gt; 95% CI : (0.471, 0.868) #&gt; No Information Rate : 0.609 #&gt; P-Value [Acc &gt; NIR] : 0.264 #&gt; #&gt; Kappa : 0.348 #&gt; #&gt; Mcnemar&#39;s Test P-Value : 1.000 #&gt; #&gt; Sensitivity : 0.556 #&gt; Specificity : 0.786 #&gt; Pos Pred Value : 0.625 #&gt; Neg Pred Value : 0.733 #&gt; Prevalence : 0.391 #&gt; Detection Rate : 0.217 #&gt; Detection Prevalence : 0.348 #&gt; Balanced Accuracy : 0.671 #&gt; #&gt; &#39;Positive&#39; Class : Death #&gt; 6.5.4 Penalized Discriminant Analysis Penalized Discriminant Analysis is the penalized linear discriminant analysis and is also useful for when we have highly correlated features. This model classified 14 out of 23 cases correctly. set.seed(27) model_pda &lt;- caret::train(outcome ~ ., data = val_train_data, method = &quot;pda&quot;, preProcess = NULL, trControl = train_control) #&gt; Warning: predictions failed for Fold01.Rep01: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold02.Rep01: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold03.Rep01: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold04.Rep01: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold05.Rep01: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold06.Rep01: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold07.Rep01: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold08.Rep01: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold09.Rep01: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold10.Rep01: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold01.Rep02: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold02.Rep02: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold03.Rep02: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold04.Rep02: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold05.Rep02: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold06.Rep02: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold07.Rep02: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold08.Rep02: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold09.Rep02: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold10.Rep02: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold01.Rep03: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold02.Rep03: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold03.Rep03: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold04.Rep03: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold05.Rep03: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold06.Rep03: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold07.Rep03: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold08.Rep03: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold09.Rep03: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold10.Rep03: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold01.Rep04: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold02.Rep04: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold03.Rep04: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold04.Rep04: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold05.Rep04: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold06.Rep04: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold07.Rep04: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold08.Rep04: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold09.Rep04: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold10.Rep04: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold01.Rep05: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold02.Rep05: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold03.Rep05: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold04.Rep05: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold05.Rep05: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold06.Rep05: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold07.Rep05: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold08.Rep05: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold09.Rep05: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold10.Rep05: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold01.Rep06: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold02.Rep06: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold03.Rep06: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold04.Rep06: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold05.Rep06: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold06.Rep06: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold07.Rep06: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold08.Rep06: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold09.Rep06: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold10.Rep06: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold01.Rep07: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold02.Rep07: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold03.Rep07: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold04.Rep07: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold05.Rep07: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold06.Rep07: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold07.Rep07: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold08.Rep07: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold09.Rep07: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold10.Rep07: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold01.Rep08: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold02.Rep08: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold03.Rep08: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold04.Rep08: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold05.Rep08: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold06.Rep08: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold07.Rep08: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold08.Rep08: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold09.Rep08: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold10.Rep08: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold01.Rep09: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold02.Rep09: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold03.Rep09: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold04.Rep09: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold05.Rep09: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold06.Rep09: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold07.Rep09: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold08.Rep09: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold09.Rep09: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold10.Rep09: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold01.Rep10: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold02.Rep10: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold03.Rep10: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold04.Rep10: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold05.Rep10: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold06.Rep10: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold07.Rep10: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold08.Rep10: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold09.Rep10: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold10.Rep10: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, : There were missing values in resampled performance measures. #&gt; Warning in train.default(x, y, weights = w, ...): missing values found in aggregated results model_pda #&gt; Penalized Discriminant Analysis #&gt; #&gt; 56 samples #&gt; 11 predictors #&gt; 2 classes: &#39;Death&#39;, &#39;Recover&#39; #&gt; #&gt; No pre-processing #&gt; Resampling: Cross-Validated (10 fold, repeated 10 times) #&gt; Summary of sample sizes: 51, 49, 50, 51, 49, 51, ... #&gt; Resampling results across tuning parameters: #&gt; #&gt; lambda Accuracy Kappa #&gt; 0e+00 NaN NaN #&gt; 1e-04 0.681 0.343 #&gt; 1e-01 0.681 0.343 #&gt; #&gt; Accuracy was used to select the optimal model using the largest value. #&gt; The final value used for the model was lambda = 1e-04. confusionMatrix(predict(model_pda, val_test_data[, -1]), val_test_data$outcome) #&gt; Confusion Matrix and Statistics #&gt; #&gt; Reference #&gt; Prediction Death Recover #&gt; Death 3 2 #&gt; Recover 6 12 #&gt; #&gt; Accuracy : 0.652 #&gt; 95% CI : (0.427, 0.836) #&gt; No Information Rate : 0.609 #&gt; P-Value [Acc &gt; NIR] : 0.422 #&gt; #&gt; Kappa : 0.207 #&gt; #&gt; Mcnemar&#39;s Test P-Value : 0.289 #&gt; #&gt; Sensitivity : 0.333 #&gt; Specificity : 0.857 #&gt; Pos Pred Value : 0.600 #&gt; Neg Pred Value : 0.667 #&gt; Prevalence : 0.391 #&gt; Detection Rate : 0.130 #&gt; Detection Prevalence : 0.217 #&gt; Balanced Accuracy : 0.595 #&gt; #&gt; &#39;Positive&#39; Class : Death #&gt; 6.5.5 Stabilized Linear Discriminant Analysis Stabilized Linear Discriminant Analysis is designed for high-dimensional data and correlated co-variables. This model classified 15 out of 23 cases correctly. set.seed(27) model_slda &lt;- caret::train(outcome ~ ., data = val_train_data, method = &quot;slda&quot;, preProcess = NULL, trControl = train_control) model_slda #&gt; Stabilized Linear Discriminant Analysis #&gt; #&gt; 56 samples #&gt; 11 predictors #&gt; 2 classes: &#39;Death&#39;, &#39;Recover&#39; #&gt; #&gt; No pre-processing #&gt; Resampling: Cross-Validated (10 fold, repeated 10 times) #&gt; Summary of sample sizes: 51, 49, 50, 51, 49, 51, ... #&gt; Resampling results: #&gt; #&gt; Accuracy Kappa #&gt; 0.682 0.358 confusionMatrix(predict(model_slda, val_test_data[, -1]), val_test_data$outcome) #&gt; Confusion Matrix and Statistics #&gt; #&gt; Reference #&gt; Prediction Death Recover #&gt; Death 3 3 #&gt; Recover 6 11 #&gt; #&gt; Accuracy : 0.609 #&gt; 95% CI : (0.385, 0.803) #&gt; No Information Rate : 0.609 #&gt; P-Value [Acc &gt; NIR] : 0.590 #&gt; #&gt; Kappa : 0.127 #&gt; #&gt; Mcnemar&#39;s Test P-Value : 0.505 #&gt; #&gt; Sensitivity : 0.333 #&gt; Specificity : 0.786 #&gt; Pos Pred Value : 0.500 #&gt; Neg Pred Value : 0.647 #&gt; Prevalence : 0.391 #&gt; Detection Rate : 0.130 #&gt; Detection Prevalence : 0.261 #&gt; Balanced Accuracy : 0.560 #&gt; #&gt; &#39;Positive&#39; Class : Death #&gt; 6.5.6 Nearest Shrunken Centroids Nearest Shrunken Centroids computes a standardized centroid for each class and shrinks each centroid toward the overall centroid for all classes. This model classified 15 out of 23 cases correctly. set.seed(27) model_pam &lt;- caret::train(outcome ~ ., data = val_train_data, method = &quot;pam&quot;, preProcess = NULL, trControl = train_control) #&gt; 12345678910111213141516171819202122232425262728293011111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111 model_pam #&gt; Nearest Shrunken Centroids #&gt; #&gt; 56 samples #&gt; 11 predictors #&gt; 2 classes: &#39;Death&#39;, &#39;Recover&#39; #&gt; #&gt; No pre-processing #&gt; Resampling: Cross-Validated (10 fold, repeated 10 times) #&gt; Summary of sample sizes: 51, 49, 50, 51, 49, 51, ... #&gt; Resampling results across tuning parameters: #&gt; #&gt; threshold Accuracy Kappa #&gt; 0.142 0.709 0.416 #&gt; 2.065 0.714 0.382 #&gt; 3.987 0.590 0.000 #&gt; #&gt; Accuracy was used to select the optimal model using the largest value. #&gt; The final value used for the model was threshold = 2.06. confusionMatrix(predict(model_pam, val_test_data[, -1]), val_test_data$outcome) #&gt; Confusion Matrix and Statistics #&gt; #&gt; Reference #&gt; Prediction Death Recover #&gt; Death 1 3 #&gt; Recover 8 11 #&gt; #&gt; Accuracy : 0.522 #&gt; 95% CI : (0.306, 0.732) #&gt; No Information Rate : 0.609 #&gt; P-Value [Acc &gt; NIR] : 0.857 #&gt; #&gt; Kappa : -0.115 #&gt; #&gt; Mcnemar&#39;s Test P-Value : 0.228 #&gt; #&gt; Sensitivity : 0.1111 #&gt; Specificity : 0.7857 #&gt; Pos Pred Value : 0.2500 #&gt; Neg Pred Value : 0.5789 #&gt; Prevalence : 0.3913 #&gt; Detection Rate : 0.0435 #&gt; Detection Prevalence : 0.1739 #&gt; Balanced Accuracy : 0.4484 #&gt; #&gt; &#39;Positive&#39; Class : Death #&gt; 6.5.7 Single C5.0 Tree C5.0 is another tree-based modeling algorithm. This model classified 15 out of 23 cases correctly. set.seed(27) model_C5.0Tree &lt;- caret::train(outcome ~ ., data = val_train_data, method = &quot;C5.0Tree&quot;, preProcess = NULL, trControl = train_control) model_C5.0Tree #&gt; Single C5.0 Tree #&gt; #&gt; 56 samples #&gt; 11 predictors #&gt; 2 classes: &#39;Death&#39;, &#39;Recover&#39; #&gt; #&gt; No pre-processing #&gt; Resampling: Cross-Validated (10 fold, repeated 10 times) #&gt; Summary of sample sizes: 51, 49, 50, 51, 49, 51, ... #&gt; Resampling results: #&gt; #&gt; Accuracy Kappa #&gt; 0.696 0.359 confusionMatrix(predict(model_C5.0Tree, val_test_data[, -1]), val_test_data$outcome) #&gt; Confusion Matrix and Statistics #&gt; #&gt; Reference #&gt; Prediction Death Recover #&gt; Death 4 1 #&gt; Recover 5 13 #&gt; #&gt; Accuracy : 0.739 #&gt; 95% CI : (0.516, 0.898) #&gt; No Information Rate : 0.609 #&gt; P-Value [Acc &gt; NIR] : 0.142 #&gt; #&gt; Kappa : 0.405 #&gt; #&gt; Mcnemar&#39;s Test P-Value : 0.221 #&gt; #&gt; Sensitivity : 0.444 #&gt; Specificity : 0.929 #&gt; Pos Pred Value : 0.800 #&gt; Neg Pred Value : 0.722 #&gt; Prevalence : 0.391 #&gt; Detection Rate : 0.174 #&gt; Detection Prevalence : 0.217 #&gt; Balanced Accuracy : 0.687 #&gt; #&gt; &#39;Positive&#39; Class : Death #&gt; 6.5.8 Partial Least Squares modeling with correlated features. This model classified 15 out of 23 cases correctly. set.seed(27) model_pls &lt;- caret::train(outcome ~ ., data = val_train_data, method = &quot;pls&quot;, preProcess = NULL, trControl = train_control) model_pls #&gt; Partial Least Squares #&gt; #&gt; 56 samples #&gt; 11 predictors #&gt; 2 classes: &#39;Death&#39;, &#39;Recover&#39; #&gt; #&gt; No pre-processing #&gt; Resampling: Cross-Validated (10 fold, repeated 10 times) #&gt; Summary of sample sizes: 51, 49, 50, 51, 49, 51, ... #&gt; Resampling results across tuning parameters: #&gt; #&gt; ncomp Accuracy Kappa #&gt; 1 0.663 0.315 #&gt; 2 0.676 0.341 #&gt; 3 0.691 0.376 #&gt; #&gt; Accuracy was used to select the optimal model using the largest value. #&gt; The final value used for the model was ncomp = 3. confusionMatrix(predict(model_pls, val_test_data[, -1]), val_test_data$outcome) #&gt; Confusion Matrix and Statistics #&gt; #&gt; Reference #&gt; Prediction Death Recover #&gt; Death 2 3 #&gt; Recover 7 11 #&gt; #&gt; Accuracy : 0.565 #&gt; 95% CI : (0.345, 0.768) #&gt; No Information Rate : 0.609 #&gt; P-Value [Acc &gt; NIR] : 0.742 #&gt; #&gt; Kappa : 0.009 #&gt; #&gt; Mcnemar&#39;s Test P-Value : 0.343 #&gt; #&gt; Sensitivity : 0.222 #&gt; Specificity : 0.786 #&gt; Pos Pred Value : 0.400 #&gt; Neg Pred Value : 0.611 #&gt; Prevalence : 0.391 #&gt; Detection Rate : 0.087 #&gt; Detection Prevalence : 0.217 #&gt; Balanced Accuracy : 0.504 #&gt; #&gt; &#39;Positive&#39; Class : Death #&gt; 6.6 Comparing accuracy of models All models were similarly accurate. 6.6.1 Summary Accuracy and Kappa # Create a list of models models &lt;- list(rf = model_rf, glmnet = model_glmnet, kknn = model_kknn, pda = model_pda, slda = model_slda, pam = model_pam, C5.0Tree = model_C5.0Tree, pls = model_pls) # Resample the models resample_results &lt;- resamples(models) # Generate a summary summary(resample_results, metric = c(&quot;Kappa&quot;, &quot;Accuracy&quot;)) #&gt; #&gt; Call: #&gt; summary.resamples(object = resample_results, metric = c(&quot;Kappa&quot;, &quot;Accuracy&quot;)) #&gt; #&gt; Models: rf, glmnet, kknn, pda, slda, pam, C5.0Tree, pls #&gt; Number of resamples: 100 #&gt; #&gt; Kappa #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s #&gt; rf -0.500 0.167 0.545 0.432 0.667 1 0 #&gt; glmnet -0.667 0.167 0.545 0.414 0.667 1 0 #&gt; kknn -0.667 0.125 0.333 0.313 0.615 1 0 #&gt; pda -0.667 0.142 0.333 0.343 0.615 1 0 #&gt; slda -0.667 0.167 0.367 0.358 0.615 1 0 #&gt; pam -0.667 0.167 0.545 0.382 0.571 1 0 #&gt; C5.0Tree -0.667 0.167 0.333 0.359 0.615 1 0 #&gt; pls -0.667 0.167 0.333 0.376 0.667 1 0 #&gt; #&gt; Accuracy #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s #&gt; rf 0.333 0.600 0.800 0.732 0.833 1 0 #&gt; glmnet 0.167 0.600 0.800 0.714 0.833 1 0 #&gt; kknn 0.167 0.600 0.667 0.666 0.800 1 0 #&gt; pda 0.167 0.593 0.667 0.681 0.800 1 0 #&gt; slda 0.167 0.600 0.667 0.682 0.800 1 0 #&gt; pam 0.200 0.600 0.800 0.714 0.800 1 0 #&gt; C5.0Tree 0.167 0.600 0.667 0.696 0.800 1 0 #&gt; pls 0.167 0.600 0.667 0.691 0.833 1 0 bwplot(resample_results , metric = c(&quot;Kappa&quot;,&quot;Accuracy&quot;)) 6.6.2 Combined results of predicting validation test samples To compare the predictions from all models, I summed up the prediction probabilities for Death and Recovery from all models and calculated the log2 of the ratio between the summed probabilities for Recovery by the summed probabilities for Death. All cases with a log2 ratio bigger than 1.5 were defined as Recover, cases with a log2 ratio below -1.5 were defined as Death, and the remaining cases were defined as uncertain. results &lt;- data.frame( randomForest = predict(model_rf, newdata = val_test_data[, -1], type=&quot;prob&quot;), glmnet = predict(model_glmnet, newdata = val_test_data[, -1], type=&quot;prob&quot;), kknn = predict(model_kknn, newdata = val_test_data[, -1], type=&quot;prob&quot;), pda = predict(model_pda, newdata = val_test_data[, -1], type=&quot;prob&quot;), slda = predict(model_slda, newdata = val_test_data[, -1], type=&quot;prob&quot;), pam = predict(model_pam, newdata = val_test_data[, -1], type=&quot;prob&quot;), C5.0Tree = predict(model_C5.0Tree, newdata = val_test_data[, -1], type=&quot;prob&quot;), pls = predict(model_pls, newdata = val_test_data[, -1], type=&quot;prob&quot;)) results$sum_Death &lt;- rowSums(results[, grep(&quot;Death&quot;, colnames(results))]) results$sum_Recover &lt;- rowSums(results[, grep(&quot;Recover&quot;, colnames(results))]) results$log2_ratio &lt;- log2(results$sum_Recover/results$sum_Death) results$true_outcome &lt;- val_test_data$outcome results$pred_outcome &lt;- ifelse(results$log2_ratio &gt; 1.5, &quot;Recover&quot;, ifelse(results$log2_ratio &lt; -1.5, &quot;Death&quot;, &quot;uncertain&quot;)) results$prediction &lt;- ifelse(results$pred_outcome == results$true_outcome, &quot;CORRECT&quot;, ifelse(results$pred_outcome == &quot;uncertain&quot;, &quot;uncertain&quot;, &quot;wrong&quot;)) results[, -c(1:16)] #&gt; sum_Death sum_Recover log2_ratio true_outcome pred_outcome prediction #&gt; case_10 4.237 3.76 -0.1709 Death uncertain uncertain #&gt; case_11 5.181 2.82 -0.8778 Death uncertain uncertain #&gt; case_116 2.412 5.59 1.2123 Recover uncertain uncertain #&gt; case_12 5.219 2.78 -0.9085 Death uncertain uncertain #&gt; case_121 2.356 5.64 1.2606 Death uncertain uncertain #&gt; case_127 0.694 7.31 3.3972 Recover Recover CORRECT #&gt; case_131 0.685 7.31 3.4164 Recover Recover CORRECT #&gt; case_133 0.649 7.35 3.5024 Recover Recover CORRECT #&gt; case_135 2.027 5.97 1.5589 Death Recover wrong #&gt; case_2 2.161 5.84 1.4337 Death uncertain uncertain #&gt; case_20 3.144 4.86 0.6272 Recover uncertain uncertain #&gt; case_30 4.493 3.51 -0.3576 Recover uncertain uncertain #&gt; case_45 2.594 5.41 1.0590 Death uncertain uncertain #&gt; case_5 3.019 4.98 0.7227 Recover uncertain uncertain #&gt; case_55 3.925 4.08 0.0543 Recover uncertain uncertain #&gt; case_59 1.894 6.11 1.6886 Recover Recover CORRECT #&gt; case_72 2.545 5.46 1.1002 Recover uncertain uncertain #&gt; case_74 2.339 5.66 1.2748 Recover uncertain uncertain #&gt; case_77 0.845 7.15 3.0819 Recover Recover CORRECT #&gt; case_8 2.237 5.76 1.3650 Death uncertain uncertain #&gt; case_89 1.712 6.29 1.8772 Recover Recover CORRECT #&gt; case_97 2.959 5.04 0.7687 Recover uncertain uncertain #&gt; case_98 4.798 3.20 -0.5833 Death uncertain uncertain All predictions based on all models were either correct or uncertain. 6.7 Predicting unknown outcomes The above models will now be used to predict the outcome of cases with unknown fate. train_control &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 10, repeats = 10, verboseIter = FALSE) set.seed(27) model_rf &lt;- caret::train(outcome ~ ., data = train_data, method = &quot;rf&quot;, preProcess = NULL, trControl = train_control) model_glmnet &lt;- caret::train(outcome ~ ., data = train_data, method = &quot;glmnet&quot;, preProcess = NULL, trControl = train_control) model_kknn &lt;- caret::train(outcome ~ ., data = train_data, method = &quot;kknn&quot;, preProcess = NULL, trControl = train_control) model_pda &lt;- caret::train(outcome ~ ., data = train_data, method = &quot;pda&quot;, preProcess = NULL, trControl = train_control) #&gt; Warning: predictions failed for Fold01.Rep01: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold02.Rep01: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold03.Rep01: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold04.Rep01: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold05.Rep01: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold06.Rep01: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold07.Rep01: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold08.Rep01: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold09.Rep01: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold10.Rep01: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold01.Rep02: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold02.Rep02: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold03.Rep02: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold04.Rep02: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold05.Rep02: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold06.Rep02: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold07.Rep02: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold08.Rep02: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold09.Rep02: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold10.Rep02: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold01.Rep03: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold02.Rep03: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold03.Rep03: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold04.Rep03: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold05.Rep03: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold06.Rep03: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold07.Rep03: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold08.Rep03: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold09.Rep03: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold10.Rep03: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold01.Rep04: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold02.Rep04: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold03.Rep04: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold04.Rep04: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold05.Rep04: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold06.Rep04: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold07.Rep04: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold08.Rep04: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold09.Rep04: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold10.Rep04: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold01.Rep05: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold02.Rep05: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold03.Rep05: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold04.Rep05: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold05.Rep05: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold06.Rep05: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold07.Rep05: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold08.Rep05: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold09.Rep05: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold10.Rep05: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold01.Rep06: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold02.Rep06: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold03.Rep06: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold04.Rep06: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold05.Rep06: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold06.Rep06: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold07.Rep06: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold08.Rep06: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold09.Rep06: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold10.Rep06: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold01.Rep07: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold02.Rep07: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold03.Rep07: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold04.Rep07: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold05.Rep07: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold06.Rep07: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold07.Rep07: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold08.Rep07: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold09.Rep07: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold10.Rep07: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold01.Rep08: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold02.Rep08: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold03.Rep08: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold04.Rep08: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold05.Rep08: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold06.Rep08: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold07.Rep08: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold08.Rep08: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold09.Rep08: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold10.Rep08: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold01.Rep09: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold02.Rep09: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold03.Rep09: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold04.Rep09: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold05.Rep09: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold06.Rep09: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold07.Rep09: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold08.Rep09: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold09.Rep09: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold10.Rep09: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold01.Rep10: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold02.Rep10: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold03.Rep10: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold04.Rep10: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold05.Rep10: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold06.Rep10: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold07.Rep10: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold08.Rep10: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold09.Rep10: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning: predictions failed for Fold10.Rep10: lambda=0e+00 Error in mindist[l] &lt;- ndist[l] : #&gt; NAs are not allowed in subscripted assignments #&gt; Warning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, : There were missing values in resampled performance measures. #&gt; Warning in train.default(x, y, weights = w, ...): missing values found in aggregated results model_slda &lt;- caret::train(outcome ~ ., data = train_data, method = &quot;slda&quot;, preProcess = NULL, trControl = train_control) model_pam &lt;- caret::train(outcome ~ ., data = train_data, method = &quot;pam&quot;, preProcess = NULL, trControl = train_control) #&gt; 12345678910111213141516171819202122232425262728293011111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111 model_C5.0Tree &lt;- caret::train(outcome ~ ., data = train_data, method = &quot;C5.0Tree&quot;, preProcess = NULL, trControl = train_control) model_pls &lt;- caret::train(outcome ~ ., data = train_data, method = &quot;pls&quot;, preProcess = NULL, trControl = train_control) models &lt;- list(rf = model_rf, glmnet = model_glmnet, kknn = model_kknn, pda = model_pda, slda = model_slda, pam = model_pam, C5.0Tree = model_C5.0Tree, pls = model_pls) # Resample the models resample_results &lt;- resamples(models) bwplot(resample_results , metric = c(&quot;Kappa&quot;,&quot;Accuracy&quot;)) Here again, the accuracy is similar in all models. 6.7.1 Final results The final results are calculated as described above. results &lt;- data.frame( randomForest = predict(model_rf, newdata = test_data, type=&quot;prob&quot;), glmnet = predict(model_glmnet, newdata = test_data, type=&quot;prob&quot;), kknn = predict(model_kknn, newdata = test_data, type=&quot;prob&quot;), pda = predict(model_pda, newdata = test_data, type=&quot;prob&quot;), slda = predict(model_slda, newdata = test_data, type=&quot;prob&quot;), pam = predict(model_pam, newdata = test_data, type=&quot;prob&quot;), C5.0Tree = predict(model_C5.0Tree, newdata = test_data, type=&quot;prob&quot;), pls = predict(model_pls, newdata = test_data, type=&quot;prob&quot;)) results$sum_Death &lt;- rowSums(results[, grep(&quot;Death&quot;, colnames(results))]) results$sum_Recover &lt;- rowSums(results[, grep(&quot;Recover&quot;, colnames(results))]) results$log2_ratio &lt;- log2(results$sum_Recover/results$sum_Death) results$predicted_outcome &lt;- ifelse(results$log2_ratio &gt; 1.5, &quot;Recover&quot;, ifelse(results$log2_ratio &lt; -1.5, &quot;Death&quot;, &quot;uncertain&quot;)) results[, -c(1:16)] #&gt; sum_Death sum_Recover log2_ratio predicted_outcome #&gt; case_100 1.854 6.15 1.7287 Recover #&gt; case_101 5.432 2.57 -1.0807 uncertain #&gt; case_102 2.806 5.19 0.8887 uncertain #&gt; case_103 2.342 5.66 1.2729 uncertain #&gt; case_104 1.744 6.26 1.8432 Recover #&gt; case_105 0.955 7.04 2.8828 Recover #&gt; case_108 4.489 3.51 -0.3543 uncertain #&gt; case_109 4.515 3.48 -0.3736 uncertain #&gt; case_110 2.411 5.59 1.2132 uncertain #&gt; case_112 2.632 5.37 1.0281 uncertain #&gt; case_113 2.198 5.80 1.4004 uncertain #&gt; case_114 3.339 4.66 0.4810 uncertain #&gt; case_115 1.112 6.89 2.6307 Recover #&gt; case_118 2.778 5.22 0.9109 uncertain #&gt; case_120 2.213 5.79 1.3868 uncertain #&gt; case_122 3.235 4.77 0.5590 uncertain #&gt; case_126 3.186 4.81 0.5952 uncertain #&gt; case_130 2.300 5.70 1.3091 uncertain #&gt; case_132 4.473 3.53 -0.3427 uncertain #&gt; case_136 3.281 4.72 0.5243 uncertain #&gt; case_15 2.270 5.73 1.3355 uncertain #&gt; case_16 2.820 5.18 0.8772 uncertain #&gt; case_22 4.779 3.22 -0.5689 uncertain #&gt; case_28 2.862 5.14 0.8445 uncertain #&gt; case_31 2.412 5.59 1.2117 uncertain #&gt; case_32 2.591 5.41 1.0616 uncertain #&gt; case_38 2.060 5.94 1.5280 Recover #&gt; case_39 4.749 3.25 -0.5466 uncertain #&gt; case_4 5.342 2.66 -1.0074 uncertain #&gt; case_40 6.550 1.45 -2.1750 Death #&gt; case_41 4.611 3.39 -0.4441 uncertain #&gt; case_42 5.570 2.43 -1.1966 uncertain #&gt; case_47 2.563 5.44 1.0850 uncertain #&gt; case_48 4.850 3.15 -0.6224 uncertain #&gt; case_52 4.709 3.29 -0.5173 uncertain #&gt; case_54 2.718 5.28 0.9586 uncertain #&gt; case_56 6.394 1.61 -1.9933 Death #&gt; case_62 6.048 1.95 -1.6319 Death #&gt; case_63 2.337 5.66 1.2766 uncertain #&gt; case_66 2.176 5.82 1.4205 uncertain #&gt; case_67 1.893 6.11 1.6895 Recover #&gt; case_68 3.907 4.09 0.0672 uncertain #&gt; case_69 4.465 3.53 -0.3370 uncertain #&gt; case_70 3.885 4.12 0.0833 uncertain #&gt; case_71 2.524 5.48 1.1172 uncertain #&gt; case_80 2.759 5.24 0.9261 uncertain #&gt; case_84 3.661 4.34 0.2448 uncertain #&gt; case_85 4.921 3.08 -0.6762 uncertain #&gt; case_86 3.563 4.44 0.3164 uncertain #&gt; case_88 0.566 7.43 3.7160 Recover #&gt; case_9 5.981 2.02 -1.5665 Death #&gt; case_90 3.570 4.43 0.3116 uncertain #&gt; case_92 4.664 3.34 -0.4835 uncertain #&gt; case_93 2.056 5.94 1.5319 Recover #&gt; case_95 4.495 3.50 -0.3589 uncertain #&gt; case_96 1.313 6.69 2.3482 Recover #&gt; case_99 4.799 3.20 -0.5839 uncertain write.table(results, &quot;results_prediction_unknown_outcome_ML_part1.txt&quot;, col.names = T, sep = &quot;\\t&quot;) results %&gt;% filter(predicted_outcome == &quot;Recover&quot;) %&gt;% select(-c(1:16)) #&gt; sum_Death sum_Recover log2_ratio predicted_outcome #&gt; 1 1.854 6.15 1.73 Recover #&gt; 2 1.744 6.26 1.84 Recover #&gt; 3 0.955 7.04 2.88 Recover #&gt; 4 1.112 6.89 2.63 Recover #&gt; 5 2.060 5.94 1.53 Recover #&gt; 6 1.893 6.11 1.69 Recover #&gt; 7 0.566 7.43 3.72 Recover #&gt; 8 2.056 5.94 1.53 Recover #&gt; 9 1.313 6.69 2.35 Recover 6.7.2 Predicted outcome results %&gt;% group_by(predicted_outcome) %&gt;% summarize(n = n()) #&gt; # A tibble: 3 x 2 #&gt; predicted_outcome n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 Death 4 #&gt; 2 Recover 9 #&gt; 3 uncertain 44 From 57 cases, 14 were defined as Recover, 3 as Death and 40 as uncertain. results_combined &lt;- merge(results[, -c(1:16)], fluH7N9_china_2013[which(fluH7N9_china_2013$case_ID %in% rownames(results)), ], by.x = &quot;row.names&quot;, by.y = &quot;case_ID&quot;) # results_combined &lt;- results_combined[, -c(2, 3, 8, 9)] results_combined &lt;- results_combined[, -c(1, 2, 3, 9, 10)] results_combined #&gt; log2_ratio predicted_outcome case_id date_of_onset date_of_hospitalisation gender age province #&gt; 1 1.7287 Recover 100 2013-04-16 &lt;NA&gt; m 58 Zhejiang #&gt; 2 -1.0807 uncertain 101 2013-04-13 &lt;NA&gt; f 79 Zhejiang #&gt; 3 0.8887 uncertain 102 2013-04-12 &lt;NA&gt; m 81 Zhejiang #&gt; 4 1.2729 uncertain 103 2013-04-13 2013-04-19 m 68 Jiangsu #&gt; 5 1.8432 Recover 104 2013-04-16 &lt;NA&gt; f 54 Zhejiang #&gt; 6 2.8828 Recover 105 2013-04-14 &lt;NA&gt; m 32 Zhejiang #&gt; 7 -0.3543 uncertain 108 2013-04-15 &lt;NA&gt; m 84 Zhejiang #&gt; 8 -0.3736 uncertain 109 2013-04-15 &lt;NA&gt; m 62 Zhejiang #&gt; 9 1.2132 uncertain 110 2013-04-12 2013-04-16 m 53 Taiwan #&gt; 10 1.0281 uncertain 112 2013-04-17 &lt;NA&gt; m 69 Jiangxi #&gt; 11 1.4004 uncertain 113 2013-04-15 &lt;NA&gt; f 60 Zhejiang #&gt; 12 0.4810 uncertain 114 2013-04-18 &lt;NA&gt; f 50 Zhejiang #&gt; 13 2.6307 Recover 115 2013-04-17 &lt;NA&gt; m 38 Zhejiang #&gt; 14 0.9109 uncertain 118 2013-04-17 &lt;NA&gt; m 49 Jiangsu #&gt; 15 1.3868 uncertain 120 2013-03-08 &lt;NA&gt; m 60 Jiangsu #&gt; 16 0.5590 uncertain 122 2013-04-18 &lt;NA&gt; m 38 Zhejiang #&gt; 17 0.5952 uncertain 126 2013-04-17 2013-04-27 m 80 Fujian #&gt; 18 1.3091 uncertain 130 2013-04-29 2013-04-30 m 69 Fujian #&gt; 19 -0.3427 uncertain 132 2013-05-03 2013-05-03 f 79 Jiangxi #&gt; 20 0.5243 uncertain 136 2013-07-27 2013-07-28 f 51 Guangdong #&gt; 21 1.3355 uncertain 15 2013-03-20 &lt;NA&gt; f 61 Jiangsu #&gt; 22 0.8772 uncertain 16 2013-03-21 &lt;NA&gt; m 79 Jiangsu #&gt; 23 -0.5689 uncertain 22 2013-03-28 2013-04-01 m 85 Jiangsu #&gt; 24 0.8445 uncertain 28 2013-03-29 &lt;NA&gt; m 79 Zhejiang #&gt; 25 1.2117 uncertain 31 2013-03-29 &lt;NA&gt; m 70 Jiangsu #&gt; 26 1.0616 uncertain 32 2013-04-02 &lt;NA&gt; m 74 Jiangsu #&gt; 27 1.5280 Recover 38 2013-04-03 &lt;NA&gt; m 56 Jiangsu #&gt; 28 -0.5466 uncertain 39 2013-04-08 2013-04-08 m 66 Zhejiang #&gt; 29 -1.0074 uncertain 4 2013-03-19 2013-03-27 f 45 Jiangsu #&gt; 30 -2.1750 Death 40 2013-04-06 2013-04-11 m 74 Zhejiang #&gt; 31 -0.4441 uncertain 41 2013-04-06 2013-04-12 f 54 Zhejiang #&gt; 32 -1.1966 uncertain 42 2013-04-03 2013-04-10 m 53 Shanghai #&gt; 33 1.0850 uncertain 47 2013-04-01 &lt;NA&gt; m 72 Jiangsu #&gt; 34 -0.6224 uncertain 48 2013-04-03 2013-04-09 m 65 Zhejiang #&gt; 35 -0.5173 uncertain 52 2013-04-06 &lt;NA&gt; f 64 Zhejiang #&gt; 36 0.9586 uncertain 54 2013-04-06 &lt;NA&gt; m 75 Zhejiang #&gt; 37 -1.9933 Death 56 2013-04-05 2013-04-11 m 73 Shanghai #&gt; 38 -1.6319 Death 62 2013-04-03 &lt;NA&gt; f 68 Zhejiang #&gt; 39 1.2766 uncertain 63 2013-04-10 &lt;NA&gt; m 60 Anhui #&gt; 40 1.4205 uncertain 66 &lt;NA&gt; &lt;NA&gt; m 72 Jiangsu #&gt; 41 1.6895 Recover 67 2013-04-12 &lt;NA&gt; m 56 Zhejiang #&gt; 42 0.0672 uncertain 68 2013-04-10 &lt;NA&gt; m 57 Zhejiang #&gt; 43 -0.3370 uncertain 69 2013-04-10 &lt;NA&gt; m 62 Zhejiang #&gt; 44 0.0833 uncertain 70 2013-04-11 &lt;NA&gt; f 58 Zhejiang #&gt; 45 1.1172 uncertain 71 2013-04-10 &lt;NA&gt; f 72 Zhejiang #&gt; 46 0.9261 uncertain 80 2013-04-08 &lt;NA&gt; m 74 Zhejiang #&gt; 47 0.2448 uncertain 84 &lt;NA&gt; &lt;NA&gt; f 26 Jiangsu #&gt; 48 -0.6762 uncertain 85 2013-04-09 2013-04-16 f 80 Shanghai #&gt; 49 0.3164 uncertain 86 2013-04-13 &lt;NA&gt; f 54 Zhejiang #&gt; 50 3.7160 Recover 88 &lt;NA&gt; &lt;NA&gt; m 4 Beijing #&gt; 51 -1.5665 Death 9 2013-03-25 2013-03-25 m 67 Zhejiang #&gt; 52 0.3116 uncertain 90 2013-04-12 2013-04-15 m 43 Zhejiang #&gt; 53 -0.4835 uncertain 92 2013-04-10 2013-04-17 f 66 Zhejiang #&gt; 54 1.5319 Recover 93 2013-04-11 &lt;NA&gt; m 56 Zhejiang #&gt; 55 -0.3589 uncertain 95 2013-03-30 &lt;NA&gt; m 37 Zhejiang #&gt; 56 2.3482 Recover 96 2013-04-07 &lt;NA&gt; m 43 Jiangsu #&gt; 57 -0.5839 uncertain 99 2013-04-12 &lt;NA&gt; f 68 Zhejiang # tidy dataframe for plotting results_combined_gather &lt;- results_combined %&gt;% gather(group_dates, date, date_of_onset:date_of_hospitalisation) results_combined_gather$group_dates &lt;- factor(results_combined_gather$group_dates, levels = c(&quot;date_of_onset&quot;, &quot;date_of_hospitalisation&quot;)) results_combined_gather$group_dates &lt;- mapvalues(results_combined_gather$group_dates, from = c(&quot;date_of_onset&quot;, &quot;date_of_hospitalisation&quot;), to = c(&quot;Date of onset&quot;, &quot;Date of hospitalisation&quot;)) results_combined_gather$gender &lt;- mapvalues(results_combined_gather$gender, from = c(&quot;f&quot;, &quot;m&quot;), to = c(&quot;Female&quot;, &quot;Male&quot;)) levels(results_combined_gather$gender) &lt;- c(levels(results_combined_gather$gender), &quot;unknown&quot;) results_combined_gather$gender[is.na(results_combined_gather$gender)] &lt;- &quot;unknown&quot; ggplot(data = results_combined_gather, aes(x = date, y = log2_ratio, color = predicted_outcome)) + geom_jitter(aes(size = as.numeric(age)), alpha = 0.3) + geom_rug() + facet_grid(gender ~ group_dates) + labs( color = &quot;Predicted outcome&quot;, size = &quot;Age&quot;, x = &quot;Date in 2013&quot;, y = &quot;log2 ratio of prediction Recover vs Death&quot;, title = &quot;2013 Influenza A H7N9 cases in China&quot;, subtitle = &quot;Predicted outcome&quot;, caption = &quot;&quot; ) + my_theme() + scale_shape_manual(values = c(15, 16, 17)) + scale_color_brewer(palette=&quot;Set1&quot;) + scale_fill_brewer(palette=&quot;Set1&quot;) #&gt; Warning: Removed 42 rows containing missing values (geom_point). The comparison of date of onset, data of hospitalisation, gender and age with predicted outcome shows that predicted deaths were associated with older age than predicted recoveries. Date of onset does not show an obvious bias in either direction. 6.8 Conclusions This dataset posed a couple of difficulties to begin with, like unequal distribution of data points across variables and missing data. This makes the modeling inherently prone to flaws. However, real life data isn’t perfect either, so I went ahead and tested the modeling success anyway. By accounting for uncertain classification with low predictions probability, the validation data could be classified accurately. However, for a more accurate model, these few cases don’t give enough information to reliably predict the outcome. More cases, more information (i.e. more features) and fewer missing data would improve the modeling outcome. Also, this example is only applicable for this specific case of flu. In order to be able to draw more general conclusions about flu outcome, other cases and additional information, for example on medical parameters like preexisting medical conditions, disase parameters, demographic information, etc. would be necessary. All in all, this dataset served as a nice example of the possibilities (and pitfalls) of machine learning applications and showcases a basic workflow for building prediction models with R. For a comparison of feature selection methods see here. If you see any mistakes or have tips and tricks for improvement, please don’t hesitate to let me know! Thanks. :-) sessionInfo() #&gt; R version 3.6.0 (2019-04-26) #&gt; Platform: x86_64-pc-linux-gnu (64-bit) #&gt; Running under: Ubuntu 18.04.3 LTS #&gt; #&gt; Matrix products: default #&gt; BLAS/LAPACK: /usr/lib/x86_64-linux-gnu/libopenblasp-r0.2.20.so #&gt; #&gt; locale: #&gt; [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C LC_TIME=en_US.UTF-8 LC_COLLATE=en_US.UTF-8 LC_MONETARY=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8 LC_PAPER=en_US.UTF-8 LC_NAME=C LC_ADDRESS=C LC_TELEPHONE=C LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C #&gt; #&gt; attached base packages: #&gt; [1] grid stats graphics grDevices utils datasets methods base #&gt; #&gt; other attached packages: #&gt; [1] RColorBrewer_1.1-2 rpart.plot_3.0.7 rattle_5.2.0 rpart_4.1-15 caret_6.0-84 mice_3.4.0 lattice_0.20-38 dplyr_0.8.0.1 gridExtra_2.3 ggplot2_3.1.1 plyr_1.8.4 tidyr_0.8.3 outbreaks_1.5.0 logging_0.9-107 #&gt; #&gt; loaded via a namespace (and not attached): #&gt; [1] nlme_3.1-139 lubridate_1.7.4 rprojroot_1.3-2 C50_0.1.2 tools_3.6.0 backports_1.1.4 utf8_1.1.4 R6_2.4.0 lazyeval_0.2.2 colorspace_1.4-1 jomo_2.6-7 nnet_7.3-12 withr_2.1.2 tidyselect_0.2.5 compiler_3.6.0 mda_0.4-10 cli_1.1.0 glmnet_2.0-16 Cubist_0.2.2 labeling_0.3 bookdown_0.10 scales_1.0.0 mvtnorm_1.0-10 randomForest_4.6-14 stringr_1.4.0 digest_0.6.18 minqa_1.2.4 rmarkdown_1.12 pkgconfig_2.0.2 htmltools_0.3.6 lme4_1.1-21 rlang_0.3.4 rstudioapi_0.10 generics_0.0.2 ModelMetrics_1.2.2 magrittr_1.5 Formula_1.2-3 Matrix_1.2-17 Rcpp_1.0.1 munsell_0.5.0 fansi_0.4.0 partykit_1.2-3 stringi_1.4.3 yaml_2.2.0 inum_1.0-1 MASS_7.3-51.4 pamr_1.56.1 recipes_0.1.5 parallel_3.6.0 #&gt; [50] pls_2.7-1 mitml_0.3-7 crayon_1.3.4 splines_3.6.0 zeallot_0.1.0 knitr_1.22 pillar_1.4.0 igraph_1.2.4.1 boot_1.3-22 reshape2_1.4.3 codetools_0.2-16 stats4_3.6.0 pan_1.6 glue_1.3.1 evaluate_0.13 data.table_1.12.2 vctrs_0.1.0 nloptr_1.2.1 foreach_1.4.4 gtable_0.3.0 purrr_0.3.2 assertthat_0.2.1 xfun_0.6 gower_0.2.0 prodlim_2018.04.18 libcoin_1.0-4 broom_0.5.2 e1071_1.7-1 class_7.3-15 survival_2.44-1.1 timeDate_3043.102 tibble_2.1.1 iterators_1.0.10 kknn_1.3.1 cluster_2.0.9 lava_1.6.5 ipred_0.9-9 "],
["a-gentle-introduction-to-support-vector-machines-using-r.html", "Chapter 7 A gentle introduction to support vector machines using R 7.1 Introduction 7.2 The rationale 7.3 The kernel trick 7.4 Support vector machines in R 7.5 SVM on iris dataset 7.6 SVM with Radial Basis Function kernel. Linear 7.7 SVM with Radial Basis Function kernel. Non-linear 7.8 Wrapping up", " Chapter 7 A gentle introduction to support vector machines using R 7.1 Introduction Source: https://eight2late.wordpress.com/2017/02/07/a-gentle-introduction-to-support-vector-machines-using-r/ Most machine learning algorithms involve minimising an error measure of some kind (this measure is often called an objective function or loss function). For example, the error measure in linear regression problems is the famous mean squared error – i.e. the averaged sum of the squared differences between the predicted and actual values. Like the mean squared error, most objective functions depend on all points in the training dataset. In this post, I describe the support vector machine (SVM) approach which focuses instead on finding the optimal separation boundary between datapoints that have different classifications. I’ll elaborate on what this means in the next section. Here’s the plan in brief. I’ll begin with the rationale behind SVMs using a simple case of a binary (two class) dataset with a simple separation boundary (I’ll clarify what “simple” means in a minute). Following that, I’ll describe how this can be generalised to datasets with more complex boundaries. Finally, I’ll work through a couple of examples in R, illustrating the principles behind SVMs. In line with the general philosophy of my “Gentle Introduction to Data Science Using R” series, the focus is on developing an intuitive understanding of the algorithm along with a practical demonstration of its use through a toy example. 7.2 The rationale The basic idea behind SVMs is best illustrated by considering a simple case: a set of data points that belong to one of two classes, red and blue, as illustrated in figure 1 below. To make things simpler still, I have assumed that the boundary separating the two classes is a straight line, represented by the solid green line in the diagram. In the technical literature, such datasets are called linearly separable. In the linearly separable case, there is usually a fair amount of freedom in the way a separating line can be drawn. Figure 2 illustrates this point: the two broken green lines are also valid separation boundaries. Indeed, because there is a non-zero distance between the two closest points between categories, there are an infinite number of possible separation lines. This, quite naturally, raises the question as to whether it is possible to choose a separation boundary that is optimal. The short answer is, yes there is. One way to do this is to select a boundary line that maximises the margin, i.e. the distance between the separation boundary and the points that are closest to it. Such an optimal boundary is illustrated by the black brace in Figure 3. The really cool thing about this criterion is that the location of the separation boundary depends only on the points that are closest to it. This means, unlike other classification methods, the classifier does not depend on any other points in dataset. The directed lines between the boundary and the closest points on either side are called support vectors (these are the solid black lines in figure 3). A direct implication of this is that the fewer the support vectors, the better the generalizability of the boundary. Figure 7.1: Figure 3 Although the above sounds great, it is of limited practical value because real data sets are seldom (if ever) linearly separable. So, what can we do when dealing with real (i.e. non linearly separable) data sets? A simple approach to tackle small deviations from linear separability is to allow a small number of points (those that are close to the boundary) to be misclassified. The number of possible misclassifications is governed by a free parameter C, which is called the cost. The cost is essentially the penalty associated with making an error: the higher the value of C, the less likely it is that the algorithm will misclassify a point. This approach – which is called soft margin classification – is illustrated in Figure 4. Note the points on the wrong side of the separation boundary. We will demonstrate soft margin SVMs in the next section. (Note: At the risk of belabouring the obvious, the purely linearly separable case discussed in the previous para is simply is a special case of the soft margin classifier.) Real life situations are much more complex and cannot be dealt with using soft margin classifiers. For example, as shown in Figure 5, one could have widely separated clusters of points that belong to the same classes. Such situations, which require the use of multiple (and nonlinear) boundaries, can sometimes be dealt with using a clever approach called the kernel trick. 7.3 The kernel trick Recall that in the linearly separable (or soft margin) case, the SVM algorithm works by finding a separation boundary that maximises the margin, which is the distance between the boundary and the points closest to it. The distance here is the usual straight line distance between the boundary and the closest point(s). This is called the Euclidean distance in honour of the great geometer of antiquity. The point to note is that this process results in a separation boundary that is a straight line, which as Figure 5 illustrates, does not always work. In fact in most cases it won’t. So what can we do? To answer this question, we have to take a bit of a detour… What if we were able to generalize the notion of distance in a way that generates nonlinear separation boundaries? It turns out that this is possible. To see how, one has to first understand how the notion of distance can be generalized. The key properties that any measure of distance must satisfy are: Non-negativity – a distance cannot be negative, a point that needs no further explanation I reckon 🙂 Symmetry – that is, the distance between point A and point B is the same as the distance between point B and point A. Identity– the distance between a point and itself is zero. Triangle inequality – that is the sum of distances between point A and B and points B and C must be less than or equal to the distance between A and C (equality holds only if all three points lie along the same line). Any mathematical object that displays the above properties is akin to a distance. Such generalized distances are called metrics and the mathematical space in which they live is called a metric space. Metrics are defined using special mathematical functions designed to satisfy the above conditions. These functions are known as kernels. The essence of the kernel trick lies in mapping the classification problem to a metric space in which the problem is rendered separable via a separation boundary that is simple in the new space, but complex – as it has to be – in the original one. Generally, the transformed space has a higher dimensionality, with each of the dimensions being (possibly complex) combinations of the original problem variables. However, this is not necessarily a problem because in practice one doesn’t actually mess around with transformations, one just tries different kernels (the transformation being implicit in the kernel) and sees which one does the job. The check is simple: we simply test the predictions resulting from using different kernels against a held out subset of the data (as one would for any machine learning algorithm). It turns out that a particular function – called the radial basis function kernel (RBF kernel) – is very effective in many cases. The RBF kernel is essentially a Gaussian (or Normal) function with the Euclidean distance between pairs of points as the variable (see equation 1 below). The basic rationale behind the RBF kernel is that it creates separation boundaries that it tends to classify points close together (in the Euclidean sense) in the original space in the same way. This is reflected in the fact that the kernel decays (i.e. drops off to zero) as the Euclidean distance between points increases. The rate at which a kernel decays is governed by the parameter \\(\\gamma\\) – the higher the value of \\(\\gamma\\), the more rapid the decay. This serves to illustrate that the RBF kernel is extremely flexible….but the flexibility comes at a price – the danger of overfitting for large values of \\(\\gamma\\) . One should choose appropriate values of C and \\(\\gamma\\) so as to ensure that the resulting kernel represents the best possible balance between flexibility and accuracy. We’ll discuss how this is done in practice later in this article. Finally, though it is probably obvious, it is worth mentioning that the separation boundaries for arbitrary kernels are also defined through support vectors as in Figure 3. To reiterate a point made earlier, this means that a solution that has fewer support vectors is likely to be more robust than one with many. Why? Because the data points defining support vectors are ones that are most sensitive to noise- therefore the fewer, the better. There are many other types of kernels, each with their own pros and cons. However, I’ll leave these for adventurous readers to explore by themselves. Finally, for a much more detailed….and dare I say, better… explanation of the kernel trick, I highly recommend this article by Eric Kim. 7.4 Support vector machines in R In this demo we’ll use the svm interface that is implemented in the e1071 R package. This interface provides R programmers access to the comprehensive libsvm library written by Chang and Lin. I’ll use two toy datasets: the famous iris dataset available with the base R package and the sonar dataset from the mlbench package. I won’t describe details of the datasets as they are discussed at length in the documentation that I have linked to. However, it is worth mentioning the reasons why I chose these datasets: As mentioned earlier, no real life dataset is linearly separable, but the iris dataset is almost so. Consequently, it is a good illustration of using linear SVMs. Although one almost never uses these in practice, I have illustrated their use primarily for pedagogical reasons. The sonar dataset is a good illustration of the benefits of using RBF kernels in cases where the dataset is hard to visualise (60 variables in this case!). In general, one would almost always use RBF (or other nonlinear) kernels in practice. With that said, let’s get right to it. I assume you have R and RStudio installed. For instructions on how to do this, have a look at the first article in this series. The processing preliminaries – loading libraries, data and creating training and test datasets are much the same as in my previous articles so I won’t dwell on these here. For completeness, however, I’ll list all the code so you can run it directly in R or R studio (a complete listing of the code can be found here): 7.5 SVM on iris dataset 7.5.1 Training and test datasets #load required library library(e1071) #load built-in iris dataset data(iris) #set seed to ensure reproducible results set.seed(42) #split into training and test sets iris[, &quot;train&quot;] &lt;- ifelse(runif(nrow(iris)) &lt; 0.8, 1, 0) #separate training and test sets trainset &lt;- iris[iris$train == 1,] testset &lt;- iris[iris$train == 0,] #get column index of train flag trainColNum &lt;- grep(&quot;train&quot;, names(trainset)) #remove train flag column from train and test sets trainset &lt;- trainset[,-trainColNum] testset &lt;- testset[,-trainColNum] dim(trainset) #&gt; [1] 115 5 dim(testset) #&gt; [1] 35 5 7.5.2 Build the SVM model #get column index of predicted variable in dataset typeColNum &lt;- grep(&quot;Species&quot;, names(iris)) #build model – linear kernel and C-classification (soft margin) with default cost (C=1) svm_model &lt;- svm(Species~ ., data = trainset, method = &quot;C-classification&quot;, kernel = &quot;linear&quot;) svm_model #&gt; #&gt; Call: #&gt; svm(formula = Species ~ ., data = trainset, method = &quot;C-classification&quot;, #&gt; kernel = &quot;linear&quot;) #&gt; #&gt; #&gt; Parameters: #&gt; SVM-Type: C-classification #&gt; SVM-Kernel: linear #&gt; cost: 1 #&gt; gamma: 0.25 #&gt; #&gt; Number of Support Vectors: 24 The output from the SVM model show that there are 24 support vectors. If desired, these can be examined using the SV variable in the model – i.e via svm_model$SV. 7.5.3 Support Vectors # support vectors svm_model$SV #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width #&gt; 19 -0.2564 1.7668 -1.323 -1.305 #&gt; 42 -1.7006 -1.7045 -1.559 -1.305 #&gt; 45 -0.9785 1.7668 -1.205 -1.171 #&gt; 53 1.1878 0.1469 0.568 0.309 #&gt; 55 0.7064 -0.5474 0.390 0.309 #&gt; 57 0.4657 0.6097 0.450 0.443 #&gt; 58 -1.2192 -1.4730 -0.378 -0.364 #&gt; 69 0.3453 -1.9359 0.331 0.309 #&gt; 71 -0.0157 0.3783 0.509 0.712 #&gt; 73 0.4657 -1.2416 0.568 0.309 #&gt; 78 0.9471 -0.0845 0.627 0.578 #&gt; 84 0.1046 -0.7788 0.686 0.443 #&gt; 85 -0.6174 -0.0845 0.331 0.309 #&gt; 86 0.1046 0.8412 0.331 0.443 #&gt; 99 -0.9785 -1.2416 -0.555 -0.229 #&gt; 107 -1.2192 -1.2416 0.331 0.578 #&gt; 111 0.7064 0.3783 0.686 0.981 #&gt; 117 0.7064 -0.0845 0.922 0.712 #&gt; 124 0.4657 -0.7788 0.568 0.712 #&gt; 130 1.5488 -0.0845 1.099 0.443 #&gt; 138 0.5860 0.1469 0.922 0.712 #&gt; 139 0.1046 -0.0845 0.509 0.712 #&gt; 147 0.4657 -1.2416 0.627 0.847 #&gt; 150 -0.0157 -0.0845 0.686 0.712 The test prediction accuracy indicates that the linear performs quite well on this dataset, confirming that it is indeed near linearly separable. To check performance by class, one can create a confusion matrix as described in my post on random forests. I’ll leave this as an exercise for you. Another point is that we have used a soft-margin classification scheme with a cost C=1. You can experiment with this by explicitly changing the value of C. Again, I’ll leave this for you an exercise. 7.5.4 Predictions on training model # training set predictions pred_train &lt;- predict(svm_model, trainset) mean(pred_train == trainset$Species) #&gt; [1] 0.983 # [1] 0.9826087 7.5.5 Predictions on test model # test set predictions pred_test &lt;-predict(svm_model, testset) mean(pred_test == testset$Species) #&gt; [1] 0.914 # [1] 0.9142857 7.5.6 Confusion matrix and Accuracy # confusion matrix cm &lt;- table(pred_test, testset$Species) cm #&gt; #&gt; pred_test setosa versicolor virginica #&gt; setosa 18 0 0 #&gt; versicolor 0 5 3 #&gt; virginica 0 0 9 # accuracy sum(diag(cm)) / sum(cm) #&gt; [1] 0.914 7.6 SVM with Radial Basis Function kernel. Linear 7.6.1 Training and test sets #load required library (assuming e1071 is already loaded) library(mlbench) #load Sonar dataset data(Sonar) #set seed to ensure reproducible results set.seed(42) #split into training and test sets Sonar[, &quot;train&quot;] &lt;- ifelse(runif(nrow(Sonar))&lt;0.8,1,0) #separate training and test sets trainset &lt;- Sonar[Sonar$train==1,] testset &lt;- Sonar[Sonar$train==0,] #get column index of train flag trainColNum &lt;- grep(&quot;train&quot;,names(trainset)) #remove train flag column from train and test sets trainset &lt;- trainset[,-trainColNum] testset &lt;- testset[,-trainColNum] #get column index of predicted variable in dataset typeColNum &lt;- grep(&quot;Class&quot;,names(Sonar)) 7.6.2 Predictions on Training model #build model – linear kernel and C-classification with default cost (C=1) svm_model &lt;- svm(Class~ ., data=trainset, method=&quot;C-classification&quot;, kernel=&quot;linear&quot;) #training set predictions pred_train &lt;-predict(svm_model,trainset) mean(pred_train==trainset$Class) #&gt; [1] 0.97 7.6.3 Predictions on test model #test set predictions pred_test &lt;-predict(svm_model,testset) mean(pred_test==testset$Class) #&gt; [1] 0.605 I’ll leave you to examine the contents of the model. The important point to note here is that the performance of the model with the test set is quite dismal compared to the previous case. This simply indicates that the linear kernel is not appropriate here. Let’s take a look at what happens if we use the RBF kernel with default values for the parameters: 7.7 SVM with Radial Basis Function kernel. Non-linear 7.7.1 Predictions on training model #build model: radial kernel, default params svm_model &lt;- svm(Class~ ., data=trainset, method=&quot;C-classification&quot;, kernel=&quot;radial&quot;) # print params svm_model$cost #&gt; [1] 1 svm_model$gamma #&gt; [1] 0.0167 #training set predictions pred_train &lt;-predict(svm_model,trainset) mean(pred_train==trainset$Class) #&gt; [1] 0.988 7.7.2 Predictions on test model #test set predictions pred_test &lt;-predict(svm_model,testset) mean(pred_test==testset$Class) #&gt; [1] 0.767 That’s a pretty decent improvement from the linear kernel. Let’s see if we can do better by doing some parameter tuning. To do this we first invoke tune.svm and use the parameters it gives us in the call to svm: 7.7.3 Tuning of parameters # find optimal parameters in a specified range tune_out &lt;- tune.svm(x = trainset[,-typeColNum], y = trainset[, typeColNum], gamma = 10^(-3:3), cost = c(0.01, 0.1, 1, 10, 100, 1000), kernel = &quot;radial&quot;) #print best values of cost and gamma tune_out$best.parameters$cost #&gt; [1] 10 tune_out$best.parameters$gamma #&gt; [1] 0.01 #build model svm_model &lt;- svm(Class~ ., data = trainset, method = &quot;C-classification&quot;, kernel = &quot;radial&quot;, cost = tune_out$best.parameters$cost, gamma = tune_out$best.parameters$gamma) 7.7.4 Prediction on training model with new parameters # training set predictions pred_train &lt;-predict(svm_model,trainset) mean(pred_train==trainset$Class) #&gt; [1] 1 7.7.5 Prediction on test model with new parameters # test set predictions pred_test &lt;-predict(svm_model,testset) mean(pred_test==testset$Class) #&gt; [1] 0.814 Which is fairly decent improvement on the un-optimised case. 7.8 Wrapping up This bring us to the end of this introductory exploration of SVMs in R. To recap, the distinguishing feature of SVMs in contrast to most other techniques is that they attempt to construct optimal separation boundaries between different categories. SVMs are quite versatile and have been applied to a wide variety of domains ranging from chemistry to pattern recognition. They are best used in binary classification scenarios. This brings up a question as to where SVMs are to be preferred to other binary classification techniques such as logistic regression. The honest response is, “it depends” – but here are some points to keep in mind when choosing between the two. A general point to keep in mind is that SVM algorithms tend to be expensive both in terms of memory and computation, issues that can start to hurt as the size of the dataset increases. Given all the above caveats and considerations, the best way to figure out whether an SVM approach will work for your problem may be to do what most machine learning practitioners do: try it out! "],
["classification-with-svm-social-network-dataset.html", "Chapter 8 Classification with SVM. Social Network dataset 8.1 Introduction 8.2 Data Operations", " Chapter 8 Classification with SVM. Social Network dataset 8.1 Introduction Source: https://www.geeksforgeeks.org/classifying-data-using-support-vector-machinessvms-in-r/ 8.2 Data Operations 8.2.1 Load libraries # load packages library(dplyr) library(caTools) library(e1071) library(ElemStatLearn) 8.2.2 Importing dataset # Importing the dataset dataset = read.csv(file.path(data_raw_dir, &#39;Social_Network_Ads.csv&#39;)) dplyr::glimpse(dataset) #&gt; Observations: 400 #&gt; Variables: 5 #&gt; $ User.ID &lt;int&gt; 15624510, 15810944, 15668575, 15603246, 15804002… #&gt; $ Gender &lt;fct&gt; Male, Male, Female, Female, Male, Male, Female, … #&gt; $ Age &lt;int&gt; 19, 35, 26, 27, 19, 27, 27, 32, 25, 35, 26, 26, … #&gt; $ EstimatedSalary &lt;int&gt; 19000, 20000, 43000, 57000, 76000, 58000, 84000,… #&gt; $ Purchased &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, … tibble::as_tibble(dataset) #&gt; # A tibble: 400 x 5 #&gt; User.ID Gender Age EstimatedSalary Purchased #&gt; &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 15624510 Male 19 19000 0 #&gt; 2 15810944 Male 35 20000 0 #&gt; 3 15668575 Female 26 43000 0 #&gt; 4 15603246 Female 27 57000 0 #&gt; 5 15804002 Male 19 76000 0 #&gt; 6 15728773 Male 27 58000 0 #&gt; # … with 394 more rows # Taking columns 3-5 dataset = dataset[3:5] tibble::as_tibble(dataset) #&gt; # A tibble: 400 x 3 #&gt; Age EstimatedSalary Purchased #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 19 19000 0 #&gt; 2 35 20000 0 #&gt; 3 26 43000 0 #&gt; 4 27 57000 0 #&gt; 5 19 76000 0 #&gt; 6 27 58000 0 #&gt; # … with 394 more rows # Encoding the target feature as factor dataset$Purchased = factor(dataset$Purchased, levels = c(0, 1)) str(dataset) #&gt; &#39;data.frame&#39;: 400 obs. of 3 variables: #&gt; $ Age : int 19 35 26 27 19 27 27 32 25 35 ... #&gt; $ EstimatedSalary: int 19000 20000 43000 57000 76000 58000 84000 150000 33000 65000 ... #&gt; $ Purchased : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 1 1 1 1 1 1 2 1 1 ... # Splitting the dataset into the Training set and Test set set.seed(123) split = sample.split(dataset$Purchased, SplitRatio = 0.75) training_set = subset(dataset, split == TRUE) test_set = subset(dataset, split == FALSE) dim(training_set) #&gt; [1] 300 3 dim(test_set) #&gt; [1] 100 3 # Feature Scaling training_set[-3] = scale(training_set[-3]) test_set[-3] = scale(test_set[-3]) # Fitting SVM to the Training set classifier = svm(formula = Purchased ~ ., data = training_set, type = &#39;C-classification&#39;, kernel = &#39;linear&#39;) classifier #&gt; #&gt; Call: #&gt; svm(formula = Purchased ~ ., data = training_set, type = &quot;C-classification&quot;, #&gt; kernel = &quot;linear&quot;) #&gt; #&gt; #&gt; Parameters: #&gt; SVM-Type: C-classification #&gt; SVM-Kernel: linear #&gt; cost: 1 #&gt; gamma: 0.5 #&gt; #&gt; Number of Support Vectors: 116 summary(classifier) #&gt; #&gt; Call: #&gt; svm(formula = Purchased ~ ., data = training_set, type = &quot;C-classification&quot;, #&gt; kernel = &quot;linear&quot;) #&gt; #&gt; #&gt; Parameters: #&gt; SVM-Type: C-classification #&gt; SVM-Kernel: linear #&gt; cost: 1 #&gt; gamma: 0.5 #&gt; #&gt; Number of Support Vectors: 116 #&gt; #&gt; ( 58 58 ) #&gt; #&gt; #&gt; Number of Classes: 2 #&gt; #&gt; Levels: #&gt; 0 1 # Predicting the Test set results y_pred = predict(classifier, newdata = test_set[-3]) y_pred #&gt; 2 4 5 9 12 18 19 20 22 29 32 34 35 38 45 46 48 52 #&gt; 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #&gt; 66 69 74 75 82 84 85 86 87 89 103 104 107 108 109 117 124 126 #&gt; 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 #&gt; 127 131 134 139 148 154 156 159 162 163 170 175 176 193 199 200 208 213 #&gt; 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 #&gt; 224 226 228 229 230 234 236 237 239 241 255 264 265 266 273 274 281 286 #&gt; 1 0 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1 0 #&gt; 292 299 302 305 307 310 316 324 326 332 339 341 343 347 353 363 364 367 #&gt; 1 1 1 0 1 0 0 0 0 1 0 1 0 1 1 0 1 1 #&gt; 368 369 372 373 380 383 389 392 395 400 #&gt; 1 0 1 0 1 1 0 0 0 0 #&gt; Levels: 0 1 # Making the Confusion Matrix cm = table(test_set[, 3], y_pred) cm #&gt; y_pred #&gt; 0 1 #&gt; 0 57 7 #&gt; 1 13 23 xtable::xtable(cm) #&gt; % latex table generated in R 3.6.0 by xtable 1.8-4 package #&gt; % Fri Sep 20 13:29:26 2019 #&gt; \\begin{table}[ht] #&gt; \\centering #&gt; \\begin{tabular}{rrr} #&gt; \\hline #&gt; &amp; 0 &amp; 1 \\\\ #&gt; \\hline #&gt; 0 &amp; 57 &amp; 7 \\\\ #&gt; 1 &amp; 13 &amp; 23 \\\\ #&gt; \\hline #&gt; \\end{tabular} #&gt; \\end{table} # installing library ElemStatLearn # library(ElemStatLearn) # Plotting the training data set results set = training_set X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01) X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01) grid_set = expand.grid(X1, X2) colnames(grid_set) = c(&#39;Age&#39;, &#39;EstimatedSalary&#39;) y_grid = predict(classifier, newdata = grid_set) plot(set[, -3], main = &#39;SVM (Training set)&#39;, xlab = &#39;Age&#39;, ylab = &#39;Estimated Salary&#39;, xlim = range(X1), ylim = range(X2)) contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE) points(grid_set, pch = &#39;.&#39;, col = ifelse(y_grid == 1, &#39;coral1&#39;, &#39;aquamarine&#39;)) points(set, pch = 21, bg = ifelse(set[, 3] == 1, &#39;green4&#39;, &#39;red3&#39;)) set = test_set X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01) X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01) grid_set = expand.grid(X1, X2) colnames(grid_set) = c(&#39;Age&#39;, &#39;EstimatedSalary&#39;) y_grid = predict(classifier, newdata = grid_set) plot(set[, -3], main = &#39;SVM (Test set)&#39;, xlab = &#39;Age&#39;, ylab = &#39;Estimated Salary&#39;, xlim = range(X1), ylim = range(X2)) contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE) points(grid_set, pch = &#39;.&#39;, col = ifelse(y_grid == 1, &#39;coral1&#39;, &#39;aquamarine&#39;)) points(set, pch = 21, bg = ifelse(set[, 3] == 1, &#39;green4&#39;, &#39;red3&#39;)) "],
["broad-view-of-svm.html", "Chapter 9 Broad view of SVM 9.1 Introduction 9.2 Maximal Margin Classifier 9.3 Support Vector Classifiers 9.4 Support Vector Machines 9.5 SVMs for Multiple Classes 9.6 Application", " Chapter 9 Broad view of SVM 9.1 Introduction Source: http://uc-r.github.io/svm # set pseudorandom number generator set.seed(10) # Attach Packages library(tidyverse) # data manipulation and visualization #&gt; Registered S3 methods overwritten by &#39;ggplot2&#39;: #&gt; method from #&gt; [.quosures rlang #&gt; c.quosures rlang #&gt; print.quosures rlang #&gt; Registered S3 method overwritten by &#39;rvest&#39;: #&gt; method from #&gt; read_xml.response xml2 #&gt; ── Attaching packages ───────────────────────────────── tidyverse 1.2.1 ── #&gt; ✔ ggplot2 3.1.1 ✔ purrr 0.3.2 #&gt; ✔ tibble 2.1.1 ✔ dplyr 0.8.0.1 #&gt; ✔ tidyr 0.8.3 ✔ stringr 1.4.0 #&gt; ✔ readr 1.3.1 ✔ forcats 0.4.0 #&gt; ── Conflicts ──────────────────────────────────── tidyverse_conflicts() ── #&gt; ✖ dplyr::filter() masks stats::filter() #&gt; ✖ dplyr::lag() masks stats::lag() library(kernlab) # SVM methodology #&gt; #&gt; Attaching package: &#39;kernlab&#39; #&gt; The following object is masked from &#39;package:purrr&#39;: #&gt; #&gt; cross #&gt; The following object is masked from &#39;package:ggplot2&#39;: #&gt; #&gt; alpha library(e1071) # SVM methodology library(ISLR) # contains example data set &quot;Khan&quot; library(RColorBrewer) # customized coloring of plots The data sets used in the tutorial (with the exception of Khan) will be generated using built-in R commands. The Support Vector Machine methodology is sound for any number of dimensions, but becomes difficult to visualize for more than 2. As previously mentioned, SVMs are robust for any number of classes, but we will stick to no more than 3 for the duration of this tutorial. 9.2 Maximal Margin Classifier If the classes are separable by a linear boundary, we can use a Maximal Margin Classifier to find the classification boundary. To visualize an example of separated data, we generate 40 random observations and assign them to two classes. Upon visual inspection, we can see that infinitely many lines exist that split the two classes. # Construct sample data set - completely separated x &lt;- matrix(rnorm(20*2), ncol = 2) y &lt;- c(rep(-1,10), rep(1,10)) x[y==1,] &lt;- x[y==1,] + 3/2 dat &lt;- data.frame(x=x, y=as.factor(y)) # Plot data ggplot(data = dat, aes(x = x.2, y = x.1, color = y, shape = y)) + geom_point(size = 2) + scale_color_manual(values=c(&quot;#000000&quot;, &quot;#FF0000&quot;)) + theme(legend.position = &quot;none&quot;) The goal of the maximal margin classifier is to identify the linear boundary that maximizes the total distance between the line and the closest point in each class. We can use the svm() function in the e1071 package to find this boundary. # Fit Support Vector Machine model to data set svmfit &lt;- svm(y~., data = dat, kernel = &quot;linear&quot;, scale = FALSE) # Plot Results plot(svmfit, dat) In the plot, points that are represented by an “X” are the support vectors, or the points that directly affect the classification line. The points marked with an “o” are the other points, which don’t affect the calculation of the line. This principle will lay the foundation for support vector machines. The same plot can be generated using the kernlab package, with the following results: # fit model and produce plot kernfit &lt;- ksvm(x, y, type = &quot;C-svc&quot;, kernel = &#39;vanilladot&#39;) #&gt; Setting default kernel parameters plot(kernfit, data = x) kernlab shows a little more detail than e1071, showing a color gradient that indicates how confidently a new point would be classified based on its features. Just as in the first plot, the support vectors are marked, in this case as filled-in points, while the classes are denoted by different shapes. 9.3 Support Vector Classifiers As convenient as the maximal marginal classifier is to understand, most real data sets will not be fully separable by a linear boundary. To handle such data, we must use modified methodology. We simulate a new data set where the classes are more mixed. # Construct sample data set - not completely separated x &lt;- matrix(rnorm(20*2), ncol = 2) y &lt;- c(rep(-1,10), rep(1,10)) x[y==1,] &lt;- x[y==1,] + 1 dat &lt;- data.frame(x=x, y=as.factor(y)) # Plot data set ggplot(data = dat, aes(x = x.2, y = x.1, color = y, shape = y)) + geom_point(size = 2) + scale_color_manual(values=c(&quot;#000000&quot;, &quot;#FF0000&quot;)) + theme(legend.position = &quot;none&quot;) Whether the data is separable or not, the svm() command syntax is the same. In the case of data that is not linearly separable, however, the cost = argument takes on real importance. This quantifies the penalty associated with having an observation on the wrong side of the classification boundary. We can plot the fit in the same way as the completely separable case. We first use e1071: # Fit Support Vector Machine model to data set svmfit &lt;- svm(y~., data = dat, kernel = &quot;linear&quot;, cost = 10) # Plot Results plot(svmfit, dat) By upping the cost of misclassification from 10 to 100, you can see the difference in the classification line. We repeat the process of plotting the SVM using the kernlab package: # Fit Support Vector Machine model to data set kernfit &lt;- ksvm(x,y, type = &quot;C-svc&quot;, kernel = &#39;vanilladot&#39;, C = 100) #&gt; Setting default kernel parameters # Plot results plot(kernfit, data = x) But how do we decide how costly these misclassifications actually are? Instead of specifying a cost up front, we can use the tune() function from e1071 to test various costs and identify which value produces the best fitting model. # find optimal cost of misclassification tune.out &lt;- tune(svm, y~., data = dat, kernel = &quot;linear&quot;, ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100))) # extract the best model (bestmod &lt;- tune.out$best.model) #&gt; #&gt; Call: #&gt; best.tune(method = svm, train.x = y ~ ., data = dat, ranges = list(cost = c(0.001, #&gt; 0.01, 0.1, 1, 5, 10, 100)), kernel = &quot;linear&quot;) #&gt; #&gt; #&gt; Parameters: #&gt; SVM-Type: C-classification #&gt; SVM-Kernel: linear #&gt; cost: 0.1 #&gt; gamma: 0.5 #&gt; #&gt; Number of Support Vectors: 16 For our data set, the optimal cost (from amongst the choices we provided) is calculated to be 0.1, which doesn’t penalize the model much for misclassified observations. Once this model has been identified, we can construct a table of predicted classes against true classes using the predict() command as follows: # Create a table of misclassified observations ypred &lt;- predict(bestmod, dat) (misclass &lt;- table(predict = ypred, truth = dat$y)) #&gt; truth #&gt; predict -1 1 #&gt; -1 9 3 #&gt; 1 1 7 Using this support vector classifier, 80% of the observations were correctly classified, which matches what we see in the plot. If we wanted to test our classifier more rigorously, we could split our data into training and testing sets and then see how our SVC performed with the observations not used to construct the model. We will use this training-testing method later in this tutorial to validate our SVMs. 9.4 Support Vector Machines Support Vector Classifiers are a subset of the group of classification structures known as Support Vector Machines. Support Vector Machines can construct classification boundaries that are nonlinear in shape. The options for classification structures using the svm() command from the e1071 package are linear, polynomial, radial, and sigmoid. To demonstrate a nonlinear classification boundary, we will construct a new data set. # construct larger random data set x &lt;- matrix(rnorm(200*2), ncol = 2) x[1:100,] &lt;- x[1:100,] + 2.5 x[101:150,] &lt;- x[101:150,] - 2.5 y &lt;- c(rep(1,150), rep(2,50)) dat &lt;- data.frame(x=x,y=as.factor(y)) # Plot data ggplot(data = dat, aes(x = x.2, y = x.1, color = y, shape = y)) + geom_point(size = 2) + scale_color_manual(values=c(&quot;#000000&quot;, &quot;#FF0000&quot;)) + theme(legend.position = &quot;none&quot;) Notice that the data is not linearly separable, and furthermore, isn’t all clustered together in a single group. There are two sections of class 1 observations with a cluster of class 2 observations in between. To demonstrate the power of SVMs, we’ll take 100 random observations from the set and use them to construct our boundary. We set kernel = “radial” based on the shape of our data and plot the results. # set pseudorandom number generator set.seed(123) # sample training data and fit model train &lt;- base::sample(200,100, replace = FALSE) svmfit &lt;- svm(y~., data = dat[train,], kernel = &quot;radial&quot;, gamma = 1, cost = 1) # plot classifier plot(svmfit, dat) The same procedure can be run using the kernlab package, which has far more kernel options than the corresponding function in e1071. In addition to the four choices in e1071, this package allows use of a hyperbolic tangent, Laplacian, Bessel, Spline, String, or ANOVA RBF kernel. To fit this data, we set the cost to be the same as it was before, 1. # Fit radial-based SVM in kernlab kernfit &lt;- ksvm(x[train,],y[train], type = &quot;C-svc&quot;, kernel = &#39;rbfdot&#39;, C = 1, scaled = c()) # Plot training data plot(kernfit, data = x[train,]) We see that, at least visually, the SVM does a reasonable job of separating the two classes. To fit the model, we used cost = 1, but as mentioned previously, it isn’t usually obvious which cost will produce the optimal classification boundary. We can use the tune() command to try several different values of cost as well as several different values of \\(\\gamma\\), a scaling parameter used to fit nonlinear boundaries. # tune model to find optimal cost, gamma values tune.out &lt;- tune(svm, y~., data = dat[train,], kernel = &quot;radial&quot;, ranges = list(cost = c(0.1,1,10,100,1000), gamma = c(0.5,1,2,3,4))) # show best model tune.out$best.model #&gt; #&gt; Call: #&gt; best.tune(method = svm, train.x = y ~ ., data = dat[train, ], #&gt; ranges = list(cost = c(0.1, 1, 10, 100, 1000), gamma = c(0.5, #&gt; 1, 2, 3, 4)), kernel = &quot;radial&quot;) #&gt; #&gt; #&gt; Parameters: #&gt; SVM-Type: C-classification #&gt; SVM-Kernel: radial #&gt; cost: 1 #&gt; gamma: 0.5 #&gt; #&gt; Number of Support Vectors: 30 The model that reduces the error the most in the training data uses a cost of 1 and \\(\\gamma\\) value of 0.5. We can now see how well the SVM performs by predicting the class of the 100 testing observations: # validate model performance (valid &lt;- table(true = dat[-train,&quot;y&quot;], pred = predict(tune.out$best.model, newx = dat[-train,]))) #&gt; pred #&gt; true 1 2 #&gt; 1 55 28 #&gt; 2 12 5 ## pred ## true 1 2 ## 1 58 19 ## 2 16 7 Our best-fitting model produces 65% accuracy in identifying classes. For such a complicated shape of observations, this performed reasonably well. We can challenge this method further by adding additional classes of observations. 9.5 SVMs for Multiple Classes The procedure does not change for data sets that involve more than two classes of observations. We construct our data set the same way as we have previously, only now specifying three classes instead of two: # construct data set x &lt;- rbind(x, matrix(rnorm(50*2), ncol = 2)) y &lt;- c(y, rep(0,50)) x[y==0,2] &lt;- x[y==0,2] + 2.5 dat &lt;- data.frame(x=x, y=as.factor(y)) # plot data set ggplot(data = dat, aes(x = x.2, y = x.1, color = y, shape = y)) + geom_point(size = 2) + scale_color_manual(values=c(&quot;#000000&quot;,&quot;#FF0000&quot;,&quot;#00BA00&quot;)) + theme(legend.position = &quot;none&quot;) The commands don’t change for the e1071 package. We specify a cost and tuning parameter \\(\\gamma\\) and fit a support vector machine. The results and interpretation are similar to two-class classification. # fit model svmfit &lt;- svm(y~., data = dat, kernel = &quot;radial&quot;, cost = 10, gamma = 1) # plot results plot(svmfit, dat) We can check to see how well our model fit the data by using the predict() command, as follows: #construct table ypred &lt;- predict(svmfit, dat) (misclass &lt;- table(predict = ypred, truth = dat$y)) #&gt; truth #&gt; predict 0 1 2 #&gt; 0 38 2 5 #&gt; 1 7 145 2 #&gt; 2 5 3 43 ## truth ## predict 0 1 2 ## 0 38 2 4 ## 1 8 143 4 ## 2 4 5 42 As shown in the resulting table, 89% of our training observations were correctly classified. However, since we didn’t break our data into training and testing sets, we didn’t truly validate our results. The kernlab package, on the other hand, can fit more than 2 classes, but cannot plot the results. To visualize the results of the ksvm function, we take the steps listed below to create a grid of points, predict the value of each point, and plot the results: # fit and plot kernfit &lt;- ksvm(as.matrix(dat[,2:1]),dat$y, type = &quot;C-svc&quot;, kernel = &#39;rbfdot&#39;, C = 100, scaled = c()) # Create a fine grid of the feature space x.1 &lt;- seq(from = min(dat$x.1), to = max(dat$x.1), length = 100) x.2 &lt;- seq(from = min(dat$x.2), to = max(dat$x.2), length = 100) x.grid &lt;- expand.grid(x.2, x.1) # Get class predictions over grid pred &lt;- predict(kernfit, newdata = x.grid) # Plot the results cols &lt;- brewer.pal(3, &quot;Set1&quot;) plot(x.grid, pch = 19, col = adjustcolor(cols[pred], alpha.f = 0.05)) classes &lt;- matrix(pred, nrow = 100, ncol = 100) contour(x = x.2, y = x.1, z = classes, levels = 1:3, labels = &quot;&quot;, add = TRUE) points(dat[, 2:1], pch = 19, col = cols[predict(kernfit)]) 9.6 Application The Khan data set contains data on 83 tissue samples with 2308 gene expression measurements on each sample. These were split into 63 training observations and 20 testing observations, and there are four distinct classes in the set. It would be impossible to visualize such data, so we choose the simplest classifier (linear) to construct our model. We will use the svm command from e1071 to conduct our analysis. # fit model dat &lt;- data.frame(x = Khan$xtrain, y=as.factor(Khan$ytrain)) (out &lt;- svm(y~., data = dat, kernel = &quot;linear&quot;, cost=10)) #&gt; #&gt; Call: #&gt; svm(formula = y ~ ., data = dat, kernel = &quot;linear&quot;, cost = 10) #&gt; #&gt; #&gt; Parameters: #&gt; SVM-Type: C-classification #&gt; SVM-Kernel: linear #&gt; cost: 10 #&gt; gamma: 0.000433 #&gt; #&gt; Number of Support Vectors: 58 First of all, we can check how well our model did at classifying the training observations. This is usually high, but again, doesn’t validate the model. If the model doesn’t do a very good job of classifying the training set, it could be a red flag. In our case, all 63 training observations were correctly classified. # check model performance on training set table(out$fitted, dat$y) #&gt; #&gt; 1 2 3 4 #&gt; 1 8 0 0 0 #&gt; 2 0 23 0 0 #&gt; 3 0 0 12 0 #&gt; 4 0 0 0 20 To perform validation, we can check how the model performs on the testing set: # validate model performance dat.te &lt;- data.frame(x=Khan$xtest, y=as.factor(Khan$ytest)) pred.te &lt;- predict(out, newdata=dat.te) table(pred.te, dat.te$y) #&gt; #&gt; pred.te 1 2 3 4 #&gt; 1 3 0 0 0 #&gt; 2 0 6 2 0 #&gt; 3 0 0 4 0 #&gt; 4 0 0 0 5 The model correctly identifies 18 of the 20 testing observations. SVMs and the boundaries they impose are more difficult to interpret at higher dimensions, but these results seem to suggest that our model is a good classifier for the gene data. "],
["sonar-standalone-model-with-random-forest.html", "Chapter 10 Sonar Standalone Model with Random Forest 10.1 Introduction 10.2 Load libraries 10.3 Explore data 10.4 Apply tuning parameters for final model 10.5 Save model 10.6 Use the saved model 10.7 Make prediction with new data", " Chapter 10 Sonar Standalone Model with Random Forest Classification problem 10.1 Introduction mtry: Number of variables randomly sampled as candidates at each split. ntree: Number of trees to grow. 10.2 Load libraries # load packages library(caret) library(mlbench) library(randomForest) library(tictoc) # load dataset data(Sonar) set.seed(7) 10.3 Explore data dplyr::glimpse(Sonar) #&gt; Observations: 208 #&gt; Variables: 61 #&gt; $ V1 &lt;dbl&gt; 0.0200, 0.0453, 0.0262, 0.0100, 0.0762, 0.0286, 0.0317, 0.… #&gt; $ V2 &lt;dbl&gt; 0.0371, 0.0523, 0.0582, 0.0171, 0.0666, 0.0453, 0.0956, 0.… #&gt; $ V3 &lt;dbl&gt; 0.0428, 0.0843, 0.1099, 0.0623, 0.0481, 0.0277, 0.1321, 0.… #&gt; $ V4 &lt;dbl&gt; 0.0207, 0.0689, 0.1083, 0.0205, 0.0394, 0.0174, 0.1408, 0.… #&gt; $ V5 &lt;dbl&gt; 0.0954, 0.1183, 0.0974, 0.0205, 0.0590, 0.0384, 0.1674, 0.… #&gt; $ V6 &lt;dbl&gt; 0.0986, 0.2583, 0.2280, 0.0368, 0.0649, 0.0990, 0.1710, 0.… #&gt; $ V7 &lt;dbl&gt; 0.1539, 0.2156, 0.2431, 0.1098, 0.1209, 0.1201, 0.0731, 0.… #&gt; $ V8 &lt;dbl&gt; 0.1601, 0.3481, 0.3771, 0.1276, 0.2467, 0.1833, 0.1401, 0.… #&gt; $ V9 &lt;dbl&gt; 0.3109, 0.3337, 0.5598, 0.0598, 0.3564, 0.2105, 0.2083, 0.… #&gt; $ V10 &lt;dbl&gt; 0.2111, 0.2872, 0.6194, 0.1264, 0.4459, 0.3039, 0.3513, 0.… #&gt; $ V11 &lt;dbl&gt; 0.1609, 0.4918, 0.6333, 0.0881, 0.4152, 0.2988, 0.1786, 0.… #&gt; $ V12 &lt;dbl&gt; 0.1582, 0.6552, 0.7060, 0.1992, 0.3952, 0.4250, 0.0658, 0.… #&gt; $ V13 &lt;dbl&gt; 0.2238, 0.6919, 0.5544, 0.0184, 0.4256, 0.6343, 0.0513, 0.… #&gt; $ V14 &lt;dbl&gt; 0.0645, 0.7797, 0.5320, 0.2261, 0.4135, 0.8198, 0.3752, 0.… #&gt; $ V15 &lt;dbl&gt; 0.0660, 0.7464, 0.6479, 0.1729, 0.4528, 1.0000, 0.5419, 0.… #&gt; $ V16 &lt;dbl&gt; 0.2273, 0.9444, 0.6931, 0.2131, 0.5326, 0.9988, 0.5440, 0.… #&gt; $ V17 &lt;dbl&gt; 0.3100, 1.0000, 0.6759, 0.0693, 0.7306, 0.9508, 0.5150, 0.… #&gt; $ V18 &lt;dbl&gt; 0.300, 0.887, 0.755, 0.228, 0.619, 0.902, 0.426, 0.120, 0.… #&gt; $ V19 &lt;dbl&gt; 0.508, 0.802, 0.893, 0.406, 0.203, 0.723, 0.202, 0.668, 0.… #&gt; $ V20 &lt;dbl&gt; 0.4797, 0.7818, 0.8619, 0.3973, 0.4636, 0.5122, 0.4233, 0.… #&gt; $ V21 &lt;dbl&gt; 0.578, 0.521, 0.797, 0.274, 0.415, 0.207, 0.772, 0.783, 0.… #&gt; $ V22 &lt;dbl&gt; 0.507, 0.405, 0.674, 0.369, 0.429, 0.399, 0.974, 0.535, 0.… #&gt; $ V23 &lt;dbl&gt; 0.433, 0.396, 0.429, 0.556, 0.573, 0.589, 0.939, 0.681, 0.… #&gt; $ V24 &lt;dbl&gt; 0.555, 0.391, 0.365, 0.485, 0.540, 0.287, 0.556, 0.917, 0.… #&gt; $ V25 &lt;dbl&gt; 0.671, 0.325, 0.533, 0.314, 0.316, 0.204, 0.527, 0.761, 0.… #&gt; $ V26 &lt;dbl&gt; 0.641, 0.320, 0.241, 0.533, 0.229, 0.578, 0.683, 0.822, 0.… #&gt; $ V27 &lt;dbl&gt; 0.7104, 0.3271, 0.5070, 0.5256, 0.6995, 0.5389, 0.5713, 0.… #&gt; $ V28 &lt;dbl&gt; 0.8080, 0.2767, 0.8533, 0.2520, 1.0000, 0.3750, 0.5429, 0.… #&gt; $ V29 &lt;dbl&gt; 0.6791, 0.4423, 0.6036, 0.2090, 0.7262, 0.3411, 0.2177, 0.… #&gt; $ V30 &lt;dbl&gt; 0.3857, 0.2028, 0.8514, 0.3559, 0.4724, 0.5067, 0.2149, 0.… #&gt; $ V31 &lt;dbl&gt; 0.131, 0.379, 0.851, 0.626, 0.510, 0.558, 0.581, 0.132, 0.… #&gt; $ V32 &lt;dbl&gt; 0.2604, 0.2947, 0.5045, 0.7340, 0.5459, 0.4778, 0.6323, 0.… #&gt; $ V33 &lt;dbl&gt; 0.512, 0.198, 0.186, 0.612, 0.288, 0.330, 0.296, 0.099, 0.… #&gt; $ V34 &lt;dbl&gt; 0.7547, 0.2341, 0.2709, 0.3497, 0.0981, 0.2198, 0.1873, 0.… #&gt; $ V35 &lt;dbl&gt; 0.8537, 0.1306, 0.4232, 0.3953, 0.1951, 0.1407, 0.2969, 0.… #&gt; $ V36 &lt;dbl&gt; 0.851, 0.418, 0.304, 0.301, 0.418, 0.286, 0.516, 0.105, 0.… #&gt; $ V37 &lt;dbl&gt; 0.669, 0.384, 0.612, 0.541, 0.460, 0.381, 0.615, 0.192, 0.… #&gt; $ V38 &lt;dbl&gt; 0.6097, 0.1057, 0.6756, 0.8814, 0.3217, 0.4158, 0.4283, 0.… #&gt; $ V39 &lt;dbl&gt; 0.4943, 0.1840, 0.5375, 0.9857, 0.2828, 0.4054, 0.5479, 0.… #&gt; $ V40 &lt;dbl&gt; 0.2744, 0.1970, 0.4719, 0.9167, 0.2430, 0.3296, 0.6133, 0.… #&gt; $ V41 &lt;dbl&gt; 0.0510, 0.1674, 0.4647, 0.6121, 0.1979, 0.2707, 0.5017, 0.… #&gt; $ V42 &lt;dbl&gt; 0.2834, 0.0583, 0.2587, 0.5006, 0.2444, 0.2650, 0.2377, 0.… #&gt; $ V43 &lt;dbl&gt; 0.2825, 0.1401, 0.2129, 0.3210, 0.1847, 0.0723, 0.1957, 0.… #&gt; $ V44 &lt;dbl&gt; 0.4256, 0.1628, 0.2222, 0.3202, 0.0841, 0.1238, 0.1749, 0.… #&gt; $ V45 &lt;dbl&gt; 0.2641, 0.0621, 0.2111, 0.4295, 0.0692, 0.1192, 0.1304, 0.… #&gt; $ V46 &lt;dbl&gt; 0.1386, 0.0203, 0.0176, 0.3654, 0.0528, 0.1089, 0.0597, 0.… #&gt; $ V47 &lt;dbl&gt; 0.1051, 0.0530, 0.1348, 0.2655, 0.0357, 0.0623, 0.1124, 0.… #&gt; $ V48 &lt;dbl&gt; 0.1343, 0.0742, 0.0744, 0.1576, 0.0085, 0.0494, 0.1047, 0.… #&gt; $ V49 &lt;dbl&gt; 0.0383, 0.0409, 0.0130, 0.0681, 0.0230, 0.0264, 0.0507, 0.… #&gt; $ V50 &lt;dbl&gt; 0.0324, 0.0061, 0.0106, 0.0294, 0.0046, 0.0081, 0.0159, 0.… #&gt; $ V51 &lt;dbl&gt; 0.0232, 0.0125, 0.0033, 0.0241, 0.0156, 0.0104, 0.0195, 0.… #&gt; $ V52 &lt;dbl&gt; 0.0027, 0.0084, 0.0232, 0.0121, 0.0031, 0.0045, 0.0201, 0.… #&gt; $ V53 &lt;dbl&gt; 0.0065, 0.0089, 0.0166, 0.0036, 0.0054, 0.0014, 0.0248, 0.… #&gt; $ V54 &lt;dbl&gt; 0.0159, 0.0048, 0.0095, 0.0150, 0.0105, 0.0038, 0.0131, 0.… #&gt; $ V55 &lt;dbl&gt; 0.0072, 0.0094, 0.0180, 0.0085, 0.0110, 0.0013, 0.0070, 0.… #&gt; $ V56 &lt;dbl&gt; 0.0167, 0.0191, 0.0244, 0.0073, 0.0015, 0.0089, 0.0138, 0.… #&gt; $ V57 &lt;dbl&gt; 0.0180, 0.0140, 0.0316, 0.0050, 0.0072, 0.0057, 0.0092, 0.… #&gt; $ V58 &lt;dbl&gt; 0.0084, 0.0049, 0.0164, 0.0044, 0.0048, 0.0027, 0.0143, 0.… #&gt; $ V59 &lt;dbl&gt; 0.0090, 0.0052, 0.0095, 0.0040, 0.0107, 0.0051, 0.0036, 0.… #&gt; $ V60 &lt;dbl&gt; 0.0032, 0.0044, 0.0078, 0.0117, 0.0094, 0.0062, 0.0103, 0.… #&gt; $ Class &lt;fct&gt; R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R… tibble::as_tibble(Sonar) #&gt; # A tibble: 208 x 61 #&gt; V1 V2 V3 V4 V5 V6 V7 V8 V9 V10 V11 #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.02 0.0371 0.0428 0.0207 0.0954 0.0986 0.154 0.160 0.311 0.211 0.161 #&gt; 2 0.0453 0.0523 0.0843 0.0689 0.118 0.258 0.216 0.348 0.334 0.287 0.492 #&gt; 3 0.0262 0.0582 0.110 0.108 0.0974 0.228 0.243 0.377 0.560 0.619 0.633 #&gt; 4 0.01 0.0171 0.0623 0.0205 0.0205 0.0368 0.110 0.128 0.0598 0.126 0.0881 #&gt; 5 0.0762 0.0666 0.0481 0.0394 0.059 0.0649 0.121 0.247 0.356 0.446 0.415 #&gt; 6 0.0286 0.0453 0.0277 0.0174 0.0384 0.099 0.120 0.183 0.210 0.304 0.299 #&gt; # … with 202 more rows, and 50 more variables: V12 &lt;dbl&gt;, V13 &lt;dbl&gt;, #&gt; # V14 &lt;dbl&gt;, V15 &lt;dbl&gt;, V16 &lt;dbl&gt;, V17 &lt;dbl&gt;, V18 &lt;dbl&gt;, V19 &lt;dbl&gt;, #&gt; # V20 &lt;dbl&gt;, V21 &lt;dbl&gt;, V22 &lt;dbl&gt;, V23 &lt;dbl&gt;, V24 &lt;dbl&gt;, V25 &lt;dbl&gt;, #&gt; # V26 &lt;dbl&gt;, V27 &lt;dbl&gt;, V28 &lt;dbl&gt;, V29 &lt;dbl&gt;, V30 &lt;dbl&gt;, V31 &lt;dbl&gt;, #&gt; # V32 &lt;dbl&gt;, V33 &lt;dbl&gt;, V34 &lt;dbl&gt;, V35 &lt;dbl&gt;, V36 &lt;dbl&gt;, V37 &lt;dbl&gt;, #&gt; # V38 &lt;dbl&gt;, V39 &lt;dbl&gt;, V40 &lt;dbl&gt;, V41 &lt;dbl&gt;, V42 &lt;dbl&gt;, V43 &lt;dbl&gt;, #&gt; # V44 &lt;dbl&gt;, V45 &lt;dbl&gt;, V46 &lt;dbl&gt;, V47 &lt;dbl&gt;, V48 &lt;dbl&gt;, V49 &lt;dbl&gt;, #&gt; # V50 &lt;dbl&gt;, V51 &lt;dbl&gt;, V52 &lt;dbl&gt;, V53 &lt;dbl&gt;, V54 &lt;dbl&gt;, V55 &lt;dbl&gt;, #&gt; # V56 &lt;dbl&gt;, V57 &lt;dbl&gt;, V58 &lt;dbl&gt;, V59 &lt;dbl&gt;, V60 &lt;dbl&gt;, Class &lt;fct&gt; # create 80%/20% for training and validation datasets validationIndex &lt;- createDataPartition(Sonar$Class, p=0.80, list=FALSE) validation &lt;- Sonar[-validationIndex,] training &lt;- Sonar[validationIndex,] tic() # train a model and summarize model set.seed(7) trainControl &lt;- trainControl(method=&quot;repeatedcv&quot;, number=10, repeats=3) fit.rf &lt;- train(Class~., data=training, method = &quot;rf&quot;, metric = &quot;Accuracy&quot;, trControl = trainControl, ntree = 2000) toc() #&gt; 61.803 sec elapsed print(fit.rf) #&gt; Random Forest #&gt; #&gt; 167 samples #&gt; 60 predictor #&gt; 2 classes: &#39;M&#39;, &#39;R&#39; #&gt; #&gt; No pre-processing #&gt; Resampling: Cross-Validated (10 fold, repeated 3 times) #&gt; Summary of sample sizes: 150, 150, 150, 151, 151, 150, ... #&gt; Resampling results across tuning parameters: #&gt; #&gt; mtry Accuracy Kappa #&gt; 2 0.845 0.682 #&gt; 31 0.828 0.651 #&gt; 60 0.808 0.611 #&gt; #&gt; Accuracy was used to select the optimal model using the largest value. #&gt; The final value used for the model was mtry = 2. print(fit.rf$finalModel) #&gt; #&gt; Call: #&gt; randomForest(x = x, y = y, ntree = 2000, mtry = param$mtry) #&gt; Type of random forest: classification #&gt; Number of trees: 2000 #&gt; No. of variables tried at each split: 2 #&gt; #&gt; OOB estimate of error rate: 14.4% #&gt; Confusion matrix: #&gt; M R class.error #&gt; M 84 5 0.0562 #&gt; R 19 59 0.2436 Accuracy: 85.26% at mtry=2 10.4 Apply tuning parameters for final model # create standalone model using all training data set.seed(7) finalModel &lt;- randomForest(Class~., training, mtry=2, ntree=2000) # make a predictions on &quot;new data&quot; using the final model finalPredictions &lt;- predict(finalModel, validation[,1:60]) confusionMatrix(finalPredictions, validation$Class) #&gt; Confusion Matrix and Statistics #&gt; #&gt; Reference #&gt; Prediction M R #&gt; M 20 4 #&gt; R 2 15 #&gt; #&gt; Accuracy : 0.854 #&gt; 95% CI : (0.708, 0.944) #&gt; No Information Rate : 0.537 #&gt; P-Value [Acc &gt; NIR] : 1.88e-05 #&gt; #&gt; Kappa : 0.704 #&gt; #&gt; Mcnemar&#39;s Test P-Value : 0.683 #&gt; #&gt; Sensitivity : 0.909 #&gt; Specificity : 0.789 #&gt; Pos Pred Value : 0.833 #&gt; Neg Pred Value : 0.882 #&gt; Prevalence : 0.537 #&gt; Detection Rate : 0.488 #&gt; Detection Prevalence : 0.585 #&gt; Balanced Accuracy : 0.849 #&gt; #&gt; &#39;Positive&#39; Class : M #&gt; Accuracy: 82.93% 10.5 Save model # save the model to disk saveRDS(finalModel, file.path(model_out_dir, &quot;sonar-finalModel.rds&quot;)) 10.6 Use the saved model # load the model superModel &lt;- readRDS(file.path(model_out_dir, &quot;sonar-finalModel.rds&quot;)) print(superModel) #&gt; #&gt; Call: #&gt; randomForest(formula = Class ~ ., data = training, mtry = 2, ntree = 2000) #&gt; Type of random forest: classification #&gt; Number of trees: 2000 #&gt; No. of variables tried at each split: 2 #&gt; #&gt; OOB estimate of error rate: 16.2% #&gt; Confusion matrix: #&gt; M R class.error #&gt; M 81 8 0.0899 #&gt; R 19 59 0.2436 10.7 Make prediction with new data # make a predictions on &quot;new data&quot; using the final model finalPredictions &lt;- predict(superModel, validation[,1:60]) confusionMatrix(finalPredictions, validation$Class) #&gt; Confusion Matrix and Statistics #&gt; #&gt; Reference #&gt; Prediction M R #&gt; M 20 4 #&gt; R 2 15 #&gt; #&gt; Accuracy : 0.854 #&gt; 95% CI : (0.708, 0.944) #&gt; No Information Rate : 0.537 #&gt; P-Value [Acc &gt; NIR] : 1.88e-05 #&gt; #&gt; Kappa : 0.704 #&gt; #&gt; Mcnemar&#39;s Test P-Value : 0.683 #&gt; #&gt; Sensitivity : 0.909 #&gt; Specificity : 0.789 #&gt; Pos Pred Value : 0.833 #&gt; Neg Pred Value : 0.882 #&gt; Prevalence : 0.537 #&gt; Detection Rate : 0.488 #&gt; Detection Prevalence : 0.585 #&gt; Balanced Accuracy : 0.849 #&gt; #&gt; &#39;Positive&#39; Class : M #&gt; "],
["glass-classification.html", "Chapter 11 Glass classification", " Chapter 11 Glass classification https://cran.r-project.org/web/packages/e1071/vignettes/svmdoc.pdf In this example, we use the glass data from the UCI Repository of Machine Learning Databases for classification. The task is to predict the type of a glass on basis of its chemical analysis. We start by splitting the data into a train and test set: library(caret) #&gt; Loading required package: lattice #&gt; Loading required package: ggplot2 #&gt; Registered S3 methods overwritten by &#39;ggplot2&#39;: #&gt; method from #&gt; [.quosures rlang #&gt; c.quosures rlang #&gt; print.quosures rlang library(e1071) library(rpart) data(Glass, package=&quot;mlbench&quot;) str(Glass) #&gt; &#39;data.frame&#39;: 214 obs. of 10 variables: #&gt; $ RI : num 1.52 1.52 1.52 1.52 1.52 ... #&gt; $ Na : num 13.6 13.9 13.5 13.2 13.3 ... #&gt; $ Mg : num 4.49 3.6 3.55 3.69 3.62 3.61 3.6 3.61 3.58 3.6 ... #&gt; $ Al : num 1.1 1.36 1.54 1.29 1.24 1.62 1.14 1.05 1.37 1.36 ... #&gt; $ Si : num 71.8 72.7 73 72.6 73.1 ... #&gt; $ K : num 0.06 0.48 0.39 0.57 0.55 0.64 0.58 0.57 0.56 0.57 ... #&gt; $ Ca : num 8.75 7.83 7.78 8.22 8.07 8.07 8.17 8.24 8.3 8.4 ... #&gt; $ Ba : num 0 0 0 0 0 0 0 0 0 0 ... #&gt; $ Fe : num 0 0 0 0 0 0.26 0 0 0 0.11 ... #&gt; $ Type: Factor w/ 6 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;5&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## split data into a train and test set index &lt;- 1:nrow(Glass) testindex &lt;- sample(index, trunc(length(index)/3)) testset &lt;- Glass[testindex,] trainset &lt;- Glass[-testindex,] Both for the SVM and the partitioning tree (via rpart()), we fit the model and try to predict the test set values: ## svm svm.model &lt;- svm(Type ~ ., data = trainset, cost = 100, gamma = 1) svm.pred &lt;- predict(svm.model, testset[,-10]) (The dependent variable, Type, has column number 10. cost is a general penalizing parameter for C-classification and gamma is the radial basis function-specific kernel parameter.) ## rpart rpart.model &lt;- rpart(Type ~ ., data = trainset) rpart.pred &lt;- predict(rpart.model, testset[,-10], type = &quot;class&quot;) A cross-tabulation of the true versus the predicted values yields: ## compute svm confusion matrix table(pred = svm.pred, true = testset[,10]) #&gt; true #&gt; pred 1 2 3 5 6 7 #&gt; 1 20 3 3 0 0 0 #&gt; 2 6 13 5 4 2 4 #&gt; 3 2 1 0 0 0 0 #&gt; 5 0 0 0 1 0 0 #&gt; 6 0 0 0 0 0 0 #&gt; 7 0 0 0 0 0 7 ## compute rpart confusion matrix table(pred = rpart.pred, true = testset[,10]) #&gt; true #&gt; pred 1 2 3 5 6 7 #&gt; 1 22 0 3 0 0 0 #&gt; 2 5 12 4 0 0 0 #&gt; 3 0 2 1 0 0 0 #&gt; 5 0 2 0 5 2 1 #&gt; 6 0 0 0 0 0 0 #&gt; 7 1 1 0 0 0 10 11.0.1 Comparison test sets confusionMatrix(svm.pred, testset$Type) #&gt; Confusion Matrix and Statistics #&gt; #&gt; Reference #&gt; Prediction 1 2 3 5 6 7 #&gt; 1 20 3 3 0 0 0 #&gt; 2 6 13 5 4 2 4 #&gt; 3 2 1 0 0 0 0 #&gt; 5 0 0 0 1 0 0 #&gt; 6 0 0 0 0 0 0 #&gt; 7 0 0 0 0 0 7 #&gt; #&gt; Overall Statistics #&gt; #&gt; Accuracy : 0.577 #&gt; 95% CI : (0.454, 0.694) #&gt; No Information Rate : 0.394 #&gt; P-Value [Acc &gt; NIR] : 0.00137 #&gt; #&gt; Kappa : 0.413 #&gt; #&gt; Mcnemar&#39;s Test P-Value : NA #&gt; #&gt; Statistics by Class: #&gt; #&gt; Class: 1 Class: 2 Class: 3 Class: 5 Class: 6 Class: 7 #&gt; Sensitivity 0.714 0.765 0.0000 0.2000 0.0000 0.6364 #&gt; Specificity 0.860 0.611 0.9524 1.0000 1.0000 1.0000 #&gt; Pos Pred Value 0.769 0.382 0.0000 1.0000 NaN 1.0000 #&gt; Neg Pred Value 0.822 0.892 0.8824 0.9429 0.9718 0.9375 #&gt; Prevalence 0.394 0.239 0.1127 0.0704 0.0282 0.1549 #&gt; Detection Rate 0.282 0.183 0.0000 0.0141 0.0000 0.0986 #&gt; Detection Prevalence 0.366 0.479 0.0423 0.0141 0.0000 0.0986 #&gt; Balanced Accuracy 0.787 0.688 0.4762 0.6000 0.5000 0.8182 confusionMatrix(rpart.pred, testset$Type) #&gt; Confusion Matrix and Statistics #&gt; #&gt; Reference #&gt; Prediction 1 2 3 5 6 7 #&gt; 1 22 0 3 0 0 0 #&gt; 2 5 12 4 0 0 0 #&gt; 3 0 2 1 0 0 0 #&gt; 5 0 2 0 5 2 1 #&gt; 6 0 0 0 0 0 0 #&gt; 7 1 1 0 0 0 10 #&gt; #&gt; Overall Statistics #&gt; #&gt; Accuracy : 0.704 #&gt; 95% CI : (0.584, 0.807) #&gt; No Information Rate : 0.394 #&gt; P-Value [Acc &gt; NIR] : 1.23e-07 #&gt; #&gt; Kappa : 0.605 #&gt; #&gt; Mcnemar&#39;s Test P-Value : NA #&gt; #&gt; Statistics by Class: #&gt; #&gt; Class: 1 Class: 2 Class: 3 Class: 5 Class: 6 Class: 7 #&gt; Sensitivity 0.786 0.706 0.1250 1.0000 0.0000 0.909 #&gt; Specificity 0.930 0.833 0.9683 0.9242 1.0000 0.967 #&gt; Pos Pred Value 0.880 0.571 0.3333 0.5000 NaN 0.833 #&gt; Neg Pred Value 0.870 0.900 0.8971 1.0000 0.9718 0.983 #&gt; Prevalence 0.394 0.239 0.1127 0.0704 0.0282 0.155 #&gt; Detection Rate 0.310 0.169 0.0141 0.0704 0.0000 0.141 #&gt; Detection Prevalence 0.352 0.296 0.0423 0.1408 0.0000 0.169 #&gt; Balanced Accuracy 0.858 0.770 0.5466 0.9621 0.5000 0.938 11.0.2 Comparison with resamples Finally, we compare the performance of the two methods by computing the respective accuracy rates and the kappa indices (as computed by classAgreement() also contained in package e1071). In Table 1, we summarize the results of 10 replications—Support Vector Machines show better results. set.seed(1234567) # SVM fit.svm &lt;- train(Type ~., data = trainset, method = &quot;svmRadial&quot;) # Random Forest fit.rpart &lt;- train(Type ~., data = trainset, method=&quot;rpart&quot;) # collect resamples results &lt;- resamples(list(svm = fit.svm, rpart = fit.rpart)) summary(results) #&gt; #&gt; Call: #&gt; summary.resamples(object = results) #&gt; #&gt; Models: svm, rpart #&gt; Number of resamples: 25 #&gt; #&gt; Accuracy #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s #&gt; svm 0.510 0.565 0.600 0.599 0.625 0.704 0 #&gt; rpart 0.462 0.519 0.554 0.558 0.600 0.660 0 #&gt; #&gt; Kappa #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s #&gt; svm 0.267 0.376 0.406 0.410 0.446 0.559 0 #&gt; rpart 0.135 0.299 0.358 0.363 0.443 0.545 0 "],
["ozone-svm.html", "Chapter 12 Ozone SVM", " Chapter 12 Ozone SVM https://cran.r-project.org/web/packages/e1071/vignettes/svmdoc.pdf library(e1071) library(rpart) data(Ozone, package=&quot;mlbench&quot;) ## split data into a train and test set index &lt;- 1:nrow(Ozone) testindex &lt;- sample(index, trunc(length(index)/3)) testset &lt;- na.omit(Ozone[testindex,-3]) trainset &lt;- na.omit(Ozone[-testindex,-3]) ## svm svm.model &lt;- svm(V4 ~ ., data = trainset, cost = 1000, gamma = 0.0001) svm.pred &lt;- predict(svm.model, testset[,-3]) crossprod(svm.pred - testset[,3]) / length(testindex) #&gt; [,1] #&gt; [1,] 10.7 ## rpart rpart.model &lt;- rpart(V4 ~ ., data = trainset) rpart.pred &lt;- predict(rpart.model, testset[,-3]) crossprod(rpart.pred - testset[,3]) / length(testindex) #&gt; [,1] #&gt; [1,] 11.9 "],
["a-gentle-introduction-to-support-vector-machines-using-r-1.html", "Chapter 13 A gentle introduction to support vector machines using R 13.1 Support vector machines in R 13.2 SVM on iris dataset 13.3 SVM with Radial Basis Function kernel. Linear 13.4 SVM with Radial Basis Function kernel. Non-linear 13.5 Wrapping up", " Chapter 13 A gentle introduction to support vector machines using R https://eight2late.wordpress.com/2017/02/07/a-gentle-introduction-to-support-vector-machines-using-r/ 13.1 Support vector machines in R In this demo we’ll use the svm interface that is implemented in the e1071 R package. This interface provides R programmers access to the comprehensive libsvm library written by Chang and Lin. I’ll use two toy datasets: the famous iris dataset available with the base R package and the sonar dataset from the mlbench package. I won’t describe details of the datasets as they are discussed at length in the documentation that I have linked to. However, it is worth mentioning the reasons why I chose these datasets: As mentioned earlier, no real life dataset is linearly separable, but the iris dataset is almost so. Consequently, it is a good illustration of using linear SVMs. Although one almost never uses these in practice, I have illustrated their use primarily for pedagogical reasons. The sonar dataset is a good illustration of the benefits of using RBF kernels in cases where the dataset is hard to visualise (60 variables in this case!). In general, one would almost always use RBF (or other nonlinear) kernels in practice. With that said, let’s get right to it. I assume you have R and RStudio installed. For instructions on how to do this, have a look at the first article in this series. The processing preliminaries – loading libraries, data and creating training and test datasets are much the same as in my previous articles so I won’t dwell on these here. For completeness, however, I’ll list all the code so you can run it directly in R or R studio (a complete listing of the code can be found here): 13.2 SVM on iris dataset 13.2.1 Training and test datasets #load required library library(e1071) #load built-in iris dataset data(iris) #set seed to ensure reproducible results set.seed(42) #split into training and test sets iris[, &quot;train&quot;] &lt;- ifelse(runif(nrow(iris)) &lt; 0.8, 1, 0) #separate training and test sets trainset &lt;- iris[iris$train == 1,] testset &lt;- iris[iris$train == 0,] #get column index of train flag trainColNum &lt;- grep(&quot;train&quot;, names(trainset)) #remove train flag column from train and test sets trainset &lt;- trainset[,-trainColNum] testset &lt;- testset[,-trainColNum] dim(trainset) #&gt; [1] 115 5 dim(testset) #&gt; [1] 35 5 13.2.2 Build the SVM model #get column index of predicted variable in dataset typeColNum &lt;- grep(&quot;Species&quot;, names(iris)) #build model – linear kernel and C-classification (soft margin) with default cost (C=1) svm_model &lt;- svm(Species~ ., data = trainset, method = &quot;C-classification&quot;, kernel = &quot;linear&quot;) svm_model #&gt; #&gt; Call: #&gt; svm(formula = Species ~ ., data = trainset, method = &quot;C-classification&quot;, #&gt; kernel = &quot;linear&quot;) #&gt; #&gt; #&gt; Parameters: #&gt; SVM-Type: C-classification #&gt; SVM-Kernel: linear #&gt; cost: 1 #&gt; gamma: 0.25 #&gt; #&gt; Number of Support Vectors: 24 The output from the SVM model show that there are 24 support vectors. If desired, these can be examined using the SV variable in the model – i.e via svm_model$SV. 13.2.3 Support Vectors # support vectors svm_model$SV #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width #&gt; 19 -0.2564 1.7668 -1.323 -1.305 #&gt; 42 -1.7006 -1.7045 -1.559 -1.305 #&gt; 45 -0.9785 1.7668 -1.205 -1.171 #&gt; 53 1.1878 0.1469 0.568 0.309 #&gt; 55 0.7064 -0.5474 0.390 0.309 #&gt; 57 0.4657 0.6097 0.450 0.443 #&gt; 58 -1.2192 -1.4730 -0.378 -0.364 #&gt; 69 0.3453 -1.9359 0.331 0.309 #&gt; 71 -0.0157 0.3783 0.509 0.712 #&gt; 73 0.4657 -1.2416 0.568 0.309 #&gt; 78 0.9471 -0.0845 0.627 0.578 #&gt; 84 0.1046 -0.7788 0.686 0.443 #&gt; 85 -0.6174 -0.0845 0.331 0.309 #&gt; 86 0.1046 0.8412 0.331 0.443 #&gt; 99 -0.9785 -1.2416 -0.555 -0.229 #&gt; 107 -1.2192 -1.2416 0.331 0.578 #&gt; 111 0.7064 0.3783 0.686 0.981 #&gt; 117 0.7064 -0.0845 0.922 0.712 #&gt; 124 0.4657 -0.7788 0.568 0.712 #&gt; 130 1.5488 -0.0845 1.099 0.443 #&gt; 138 0.5860 0.1469 0.922 0.712 #&gt; 139 0.1046 -0.0845 0.509 0.712 #&gt; 147 0.4657 -1.2416 0.627 0.847 #&gt; 150 -0.0157 -0.0845 0.686 0.712 The test prediction accuracy indicates that the linear performs quite well on this dataset, confirming that it is indeed near linearly separable. To check performance by class, one can create a confusion matrix as described in my post on random forests. I’ll leave this as an exercise for you. Another point is that we have used a soft-margin classification scheme with a cost C=1. You can experiment with this by explicitly changing the value of C. Again, I’ll leave this for you an exercise. 13.2.4 Predictions on training model # training set predictions pred_train &lt;- predict(svm_model, trainset) mean(pred_train == trainset$Species) #&gt; [1] 0.983 # [1] 0.9826087 13.2.5 Predictions on test model # test set predictions pred_test &lt;-predict(svm_model, testset) mean(pred_test == testset$Species) #&gt; [1] 0.914 # [1] 0.9142857 13.2.6 Confusion matrix and Accuracy # confusion matrix cm &lt;- table(pred_test, testset$Species) cm #&gt; #&gt; pred_test setosa versicolor virginica #&gt; setosa 18 0 0 #&gt; versicolor 0 5 3 #&gt; virginica 0 0 9 # accuracy sum(diag(cm)) / sum(cm) #&gt; [1] 0.914 13.3 SVM with Radial Basis Function kernel. Linear 13.3.1 Training and test sets #load required library (assuming e1071 is already loaded) library(mlbench) #load Sonar dataset data(Sonar) #set seed to ensure reproducible results set.seed(42) #split into training and test sets Sonar[, &quot;train&quot;] &lt;- ifelse(runif(nrow(Sonar))&lt;0.8,1,0) #separate training and test sets trainset &lt;- Sonar[Sonar$train==1,] testset &lt;- Sonar[Sonar$train==0,] #get column index of train flag trainColNum &lt;- grep(&quot;train&quot;,names(trainset)) #remove train flag column from train and test sets trainset &lt;- trainset[,-trainColNum] testset &lt;- testset[,-trainColNum] #get column index of predicted variable in dataset typeColNum &lt;- grep(&quot;Class&quot;,names(Sonar)) 13.3.2 Predictions on Training model #build model – linear kernel and C-classification with default cost (C=1) svm_model &lt;- svm(Class~ ., data=trainset, method=&quot;C-classification&quot;, kernel=&quot;linear&quot;) #training set predictions pred_train &lt;-predict(svm_model,trainset) mean(pred_train==trainset$Class) #&gt; [1] 0.97 13.3.3 Predictions on test model #test set predictions pred_test &lt;-predict(svm_model,testset) mean(pred_test==testset$Class) #&gt; [1] 0.605 I’ll leave you to examine the contents of the model. The important point to note here is that the performance of the model with the test set is quite dismal compared to the previous case. This simply indicates that the linear kernel is not appropriate here. Let’s take a look at what happens if we use the RBF kernel with default values for the parameters: 13.4 SVM with Radial Basis Function kernel. Non-linear 13.4.1 Predictions on training model #build model: radial kernel, default params svm_model &lt;- svm(Class~ ., data=trainset, method=&quot;C-classification&quot;, kernel=&quot;radial&quot;) # print params svm_model$cost #&gt; [1] 1 svm_model$gamma #&gt; [1] 0.0167 #training set predictions pred_train &lt;-predict(svm_model,trainset) mean(pred_train==trainset$Class) #&gt; [1] 0.988 13.4.2 Predictions on test model #test set predictions pred_test &lt;-predict(svm_model,testset) mean(pred_test==testset$Class) #&gt; [1] 0.767 That’s a pretty decent improvement from the linear kernel. Let’s see if we can do better by doing some parameter tuning. To do this we first invoke tune.svm and use the parameters it gives us in the call to svm: 13.4.3 Tuning of parameters # find optimal parameters in a specified range tune_out &lt;- tune.svm(x = trainset[,-typeColNum], y = trainset[, typeColNum], gamma = 10^(-3:3), cost = c(0.01, 0.1, 1, 10, 100, 1000), kernel = &quot;radial&quot;) #print best values of cost and gamma tune_out$best.parameters$cost #&gt; [1] 10 tune_out$best.parameters$gamma #&gt; [1] 0.01 #build model svm_model &lt;- svm(Class~ ., data = trainset, method = &quot;C-classification&quot;, kernel = &quot;radial&quot;, cost = tune_out$best.parameters$cost, gamma = tune_out$best.parameters$gamma) 13.4.4 Prediction on training model with new parameters # training set predictions pred_train &lt;-predict(svm_model,trainset) mean(pred_train==trainset$Class) #&gt; [1] 1 13.4.5 Prediction on test model with new parameters # test set predictions pred_test &lt;-predict(svm_model,testset) mean(pred_test==testset$Class) #&gt; [1] 0.814 Which is fairly decent improvement on the un-optimised case. 13.5 Wrapping up This bring us to the end of this introductory exploration of SVMs in R. To recap, the distinguishing feature of SVMs in contrast to most other techniques is that they attempt to construct optimal separation boundaries between different categories. SVMs are quite versatile and have been applied to a wide variety of domains ranging from chemistry to pattern recognition. They are best used in binary classification scenarios. This brings up a question as to where SVMs are to be preferred to other binary classification techniques such as logistic regression. The honest response is, “it depends” – but here are some points to keep in mind when choosing between the two. A general point to keep in mind is that SVM algorithms tend to be expensive both in terms of memory and computation, issues that can start to hurt as the size of the dataset increases. Given all the above caveats and considerations, the best way to figure out whether an SVM approach will work for your problem may be to do what most machine learning practitioners do: try it out! "],
["sms-spam-naive-bayes-classification.html", "Chapter 14 SMS spam. Naive Bayes. Classification 14.1 Some conversion 14.2 Convert to Document Term Matrix (dtm 14.3 split in training and test datasets 14.4 plot wordcloud 14.5 Limit Frequent words 14.6 Improve model performance", " Chapter 14 SMS spam. Naive Bayes. Classification Dataset: https://github.com/stedy/Machine-Learning-with-R-datasets/blob/master/sms_spam.csv Instructions: Machine Learning with R. Page 104. library(tictoc) sms_raw &lt;- read.csv(file.path(data_raw_dir, &quot;sms_spam.csv&quot;), stringsAsFactors = FALSE) str(sms_raw) #&gt; &#39;data.frame&#39;: 5574 obs. of 2 variables: #&gt; $ type: chr &quot;ham&quot; &quot;ham&quot; &quot;spam&quot; &quot;ham&quot; ... #&gt; $ text: chr &quot;Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...&quot; &quot;Ok lar... Joking wif u oni...&quot; &quot;Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(&quot;| __truncated__ &quot;U dun say so early hor... U c already then say...&quot; ... 14.0.1 convert type to a factor sms_raw$type &lt;- factor(sms_raw$type) str(sms_raw$type) #&gt; Factor w/ 2 levels &quot;ham&quot;,&quot;spam&quot;: 1 1 2 1 1 2 1 1 2 2 ... How many email of type ham or spam: table(sms_raw$type) #&gt; #&gt; ham spam #&gt; 4827 747 Create the corpus: library(tm) #&gt; Loading required package: NLP sms_corpus &lt;- VCorpus(VectorSource(sms_raw$text)) print(sms_corpus) #&gt; &lt;&lt;VCorpus&gt;&gt; #&gt; Metadata: corpus specific: 0, document level (indexed): 0 #&gt; Content: documents: 5574 Let’s see a couple of documents: inspect(sms_corpus[1:2]) #&gt; &lt;&lt;VCorpus&gt;&gt; #&gt; Metadata: corpus specific: 0, document level (indexed): 0 #&gt; Content: documents: 2 #&gt; #&gt; [[1]] #&gt; &lt;&lt;PlainTextDocument&gt;&gt; #&gt; Metadata: 7 #&gt; Content: chars: 111 #&gt; #&gt; [[2]] #&gt; &lt;&lt;PlainTextDocument&gt;&gt; #&gt; Metadata: 7 #&gt; Content: chars: 29 # show some text as.character(sms_corpus[[1]]) #&gt; [1] &quot;Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...&quot; # show three documents lapply(sms_corpus[1:3], as.character) #&gt; $`1` #&gt; [1] &quot;Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...&quot; #&gt; #&gt; $`2` #&gt; [1] &quot;Ok lar... Joking wif u oni...&quot; #&gt; #&gt; $`3` #&gt; [1] &quot;Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&amp;C&#39;s apply 08452810075over18&#39;s&quot; 14.1 Some conversion # convert to lowercase sms_corpus_clean &lt;- tm_map(sms_corpus, content_transformer(tolower)) as.character(sms_corpus[[1]]) #&gt; [1] &quot;Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...&quot; # converted to lowercase as.character(sms_corpus_clean[[1]]) #&gt; [1] &quot;go until jurong point, crazy.. available only in bugis n great world la e buffet... cine there got amore wat...&quot; # remove numbers sms_corpus_clean &lt;- tm_map(sms_corpus_clean, removeNumbers) What transformations are available # what transformations are available getTransformations() #&gt; [1] &quot;removeNumbers&quot; &quot;removePunctuation&quot; &quot;removeWords&quot; #&gt; [4] &quot;stemDocument&quot; &quot;stripWhitespace&quot; # remove stop words sms_corpus_clean &lt;- tm_map(sms_corpus_clean, removeWords, stopwords()) # remove punctuation sms_corpus_clean &lt;- tm_map(sms_corpus_clean, removePunctuation) Stemming: library(SnowballC) wordStem(c(&quot;learn&quot;, &quot;learned&quot;, &quot;learning&quot;, &quot;learns&quot;)) #&gt; [1] &quot;learn&quot; &quot;learn&quot; &quot;learn&quot; &quot;learn&quot; # stemming corpus sms_corpus_clean &lt;- tm_map(sms_corpus_clean, stemDocument) # remove white spaces sms_corpus_clean &lt;- tm_map(sms_corpus_clean, stripWhitespace) Show what we’ve got so far # show what we&#39;ve got so far lapply(sms_corpus[1:3], as.character) #&gt; $`1` #&gt; [1] &quot;Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...&quot; #&gt; #&gt; $`2` #&gt; [1] &quot;Ok lar... Joking wif u oni...&quot; #&gt; #&gt; $`3` #&gt; [1] &quot;Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&amp;C&#39;s apply 08452810075over18&#39;s&quot; lapply(sms_corpus_clean[1:3], as.character) #&gt; $`1` #&gt; [1] &quot;go jurong point crazi avail bugi n great world la e buffet cine got amor wat&quot; #&gt; #&gt; $`2` #&gt; [1] &quot;ok lar joke wif u oni&quot; #&gt; #&gt; $`3` #&gt; [1] &quot;free entri wkli comp win fa cup final tkts st may text fa receiv entri questionstd txt ratetc appli s&quot; 14.2 Convert to Document Term Matrix (dtm ) sms_dtm &lt;- DocumentTermMatrix(sms_corpus_clean) sms_dtm #&gt; &lt;&lt;DocumentTermMatrix (documents: 5574, terms: 6592)&gt;&gt; #&gt; Non-/sparse entries: 42608/36701200 #&gt; Sparsity : 100% #&gt; Maximal term length: 40 #&gt; Weighting : term frequency (tf) 14.3 split in training and test datasets sms_dtm_train &lt;- sms_dtm[1:4169, ] sms_dtm_test &lt;- sms_dtm[4170:5559, ] 14.3.1 separate the labels sms_train_labels &lt;- sms_raw[1:4169, ]$type sms_test_labels &lt;- sms_raw[4170:5559, ]$type prop.table(table(sms_train_labels)) #&gt; sms_train_labels #&gt; ham spam #&gt; 0.865 0.135 prop.table(table(sms_test_labels)) #&gt; sms_test_labels #&gt; ham spam #&gt; 0.87 0.13 # convert dtm to matrix sms_mat_train &lt;- as.matrix(t(sms_dtm_train)) dtm.rs &lt;- sort(rowSums(sms_mat_train), decreasing=TRUE) # dataframe with word-frequency dtm.df &lt;- data.frame(word = names(dtm.rs), freq = as.integer(dtm.rs), stringsAsFactors = FALSE) 14.4 plot wordcloud library(wordcloud) #&gt; Loading required package: RColorBrewer wordcloud(sms_corpus_clean, min.freq = 50, random.order = FALSE) #&gt; Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order = #&gt; FALSE): nokia could not be fit on page. It will not be plotted. #&gt; Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order = #&gt; FALSE): buy could not be fit on page. It will not be plotted. #&gt; Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order = #&gt; FALSE): tone could not be fit on page. It will not be plotted. #&gt; Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order = #&gt; FALSE): also could not be fit on page. It will not be plotted. #&gt; Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order = #&gt; FALSE): start could not be fit on page. It will not be plotted. #&gt; Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order = #&gt; FALSE): smile could not be fit on page. It will not be plotted. #&gt; Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order = #&gt; FALSE): urgent could not be fit on page. It will not be plotted. #&gt; Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order = #&gt; FALSE): use could not be fit on page. It will not be plotted. #&gt; Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order = #&gt; FALSE): wish could not be fit on page. It will not be plotted. #&gt; Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order = #&gt; FALSE): end could not be fit on page. It will not be plotted. #&gt; Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order = #&gt; FALSE): someth could not be fit on page. It will not be plotted. #&gt; Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order = #&gt; FALSE): watch could not be fit on page. It will not be plotted. #&gt; Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order = #&gt; FALSE): sent could not be fit on page. It will not be plotted. #&gt; Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order = #&gt; FALSE): award could not be fit on page. It will not be plotted. #&gt; Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order = #&gt; FALSE): finish could not be fit on page. It will not be plotted. #&gt; Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order = #&gt; FALSE): place could not be fit on page. It will not be plotted. #&gt; Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order = #&gt; FALSE): cant could not be fit on page. It will not be plotted. #&gt; Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order = #&gt; FALSE): gud could not be fit on page. It will not be plotted. #&gt; Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order = #&gt; FALSE): guy could not be fit on page. It will not be plotted. #&gt; Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order = #&gt; FALSE): custom could not be fit on page. It will not be plotted. #&gt; Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order = #&gt; FALSE): next could not be fit on page. It will not be plotted. #&gt; Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order = #&gt; FALSE): person could not be fit on page. It will not be plotted. #&gt; Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order = #&gt; FALSE): someon could not be fit on page. It will not be plotted. #&gt; Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order = #&gt; FALSE): tonight could not be fit on page. It will not be plotted. #&gt; Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order = #&gt; FALSE): went could not be fit on page. It will not be plotted. #&gt; Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order = #&gt; FALSE): late could not be fit on page. It will not be plotted. #&gt; Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order = #&gt; FALSE): chat could not be fit on page. It will not be plotted. #&gt; Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order = #&gt; FALSE): money could not be fit on page. It will not be plotted. #&gt; Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order = #&gt; FALSE): collect could not be fit on page. It will not be plotted. #&gt; Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order = #&gt; FALSE): mani could not be fit on page. It will not be plotted. #&gt; Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order = #&gt; FALSE): soon could not be fit on page. It will not be plotted. #&gt; Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order = #&gt; FALSE): gonna could not be fit on page. It will not be plotted. #&gt; Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order = #&gt; FALSE): plan could not be fit on page. It will not be plotted. #&gt; Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order = #&gt; FALSE): alway could not be fit on page. It will not be plotted. #&gt; Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order = #&gt; FALSE): live could not be fit on page. It will not be plotted. #&gt; Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order = #&gt; FALSE): nice could not be fit on page. It will not be plotted. #&gt; Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order = #&gt; FALSE): wan could not be fit on page. It will not be plotted. #&gt; Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order = #&gt; FALSE): minut could not be fit on page. It will not be plotted. #&gt; Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order = #&gt; FALSE): check could not be fit on page. It will not be plotted. #&gt; Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order = #&gt; FALSE): dun could not be fit on page. It will not be plotted. #&gt; Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order = #&gt; FALSE): special could not be fit on page. It will not be plotted. #&gt; Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order = #&gt; FALSE): shop could not be fit on page. It will not be plotted. #&gt; Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order = #&gt; FALSE): hour could not be fit on page. It will not be plotted. #&gt; Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order = #&gt; FALSE): mean could not be fit on page. It will not be plotted. #&gt; Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order = #&gt; FALSE): month could not be fit on page. It will not be plotted. #&gt; Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order = #&gt; FALSE): guarante could not be fit on page. It will not be plotted. #&gt; Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order = #&gt; FALSE): peopl could not be fit on page. It will not be plotted. #&gt; Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order = #&gt; FALSE): reach could not be fit on page. It will not be plotted. #&gt; Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order = #&gt; FALSE): happen could not be fit on page. It will not be plotted. #&gt; Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order = #&gt; FALSE): offer could not be fit on page. It will not be plotted. spam &lt;- subset(sms_raw, type == &quot;spam&quot;) ham &lt;- subset(sms_raw, type == &quot;ham&quot;) Words related to spam wordcloud(spam$text, max.words = 40, scale = c(3, 0.5)) #&gt; Warning in tm_map.SimpleCorpus(corpus, tm::removePunctuation): #&gt; transformation drops documents #&gt; Warning in tm_map.SimpleCorpus(corpus, function(x) tm::removeWords(x, #&gt; tm::stopwords())): transformation drops documents Words related to ham wordcloud(ham$text, max.words = 40, scale = c(3, 0.5)) #&gt; Warning in tm_map.SimpleCorpus(corpus, tm::removePunctuation): #&gt; transformation drops documents #&gt; Warning in tm_map.SimpleCorpus(corpus, function(x) tm::removeWords(x, #&gt; tm::stopwords())): transformation drops documents 14.5 Limit Frequent words # words that appear at least in 5 messages sms_freq_words &lt;- findFreqTerms(sms_dtm_train, 6) str(sms_freq_words) #&gt; chr [1:997] &quot;abiola&quot; &quot;abl&quot; &quot;abt&quot; &quot;accept&quot; &quot;access&quot; &quot;account&quot; &quot;across&quot; ... 14.5.1 get only frequent words sms_dtm_freq_train&lt;- sms_dtm_train[ , sms_freq_words] sms_dtm_freq_test &lt;- sms_dtm_test[ , sms_freq_words] 14.5.2 function to change value to Yes/No convert_counts &lt;- function(x) { x &lt;- ifelse(x &gt; 0, &quot;Yes&quot;, &quot;No&quot;) } # change from number to Yes/No # also the result returns a matrix sms_train &lt;- apply(sms_dtm_freq_train, MARGIN = 2, convert_counts) sms_test &lt;- apply(sms_dtm_freq_test, MARGIN = 2, convert_counts) # matrix of # 4169 documents as rows # 1159 terms as columns dim(sms_train) #&gt; [1] 4169 997 length(sms_train_labels) #&gt; [1] 4169 # this is how the matrix looks sms_train[1:10, 10:15] #&gt; Terms #&gt; Docs add address admir advanc aft afternoon #&gt; 1 &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; #&gt; 2 &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; #&gt; 3 &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; #&gt; 4 &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; #&gt; 5 &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; #&gt; 6 &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; #&gt; 7 &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; #&gt; 8 &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; #&gt; 9 &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; #&gt; 10 &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; library(e1071) sms_classifier &lt;- naiveBayes(sms_train, sms_train_labels) tic() sms_test_pred &lt;- predict(sms_classifier, sms_test) toc() #&gt; 20.87 sec elapsed library(gmodels) CrossTable(sms_test_pred, sms_test_labels, prop.chisq = FALSE, prop.t = FALSE, dnn = c(&#39;predicted&#39;, &#39;actual&#39;)) #&gt; #&gt; #&gt; Cell Contents #&gt; |-------------------------| #&gt; | N | #&gt; | N / Row Total | #&gt; | N / Col Total | #&gt; |-------------------------| #&gt; #&gt; #&gt; Total Observations in Table: 1390 #&gt; #&gt; #&gt; | actual #&gt; predicted | ham | spam | Row Total | #&gt; -------------|-----------|-----------|-----------| #&gt; ham | 1202 | 21 | 1223 | #&gt; | 0.983 | 0.017 | 0.880 | #&gt; | 0.994 | 0.116 | | #&gt; -------------|-----------|-----------|-----------| #&gt; spam | 7 | 160 | 167 | #&gt; | 0.042 | 0.958 | 0.120 | #&gt; | 0.006 | 0.884 | | #&gt; -------------|-----------|-----------|-----------| #&gt; Column Total | 1209 | 181 | 1390 | #&gt; | 0.870 | 0.130 | | #&gt; -------------|-----------|-----------|-----------| #&gt; #&gt; Misclassified: 20+9 (frequency = 5) 25+7 (freq=4) 23+7 (freq=3) 25+8 (freq=2) 21+7 (freq=6) Decreasing the minimum word frequency doesn’t make the model better. 14.6 Improve model performance sms_classifier2 &lt;- naiveBayes(sms_train, sms_train_labels, laplace = 1) tic() sms_test_pred2 &lt;- predict(sms_classifier2, sms_test) toc() #&gt; 20.483 sec elapsed CrossTable(sms_test_pred2, sms_test_labels, prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE, dnn = c(&#39;predicted&#39;, &#39;actual&#39;)) #&gt; #&gt; #&gt; Cell Contents #&gt; |-------------------------| #&gt; | N | #&gt; | N / Col Total | #&gt; |-------------------------| #&gt; #&gt; #&gt; Total Observations in Table: 1390 #&gt; #&gt; #&gt; | actual #&gt; predicted | ham | spam | Row Total | #&gt; -------------|-----------|-----------|-----------| #&gt; ham | 1203 | 28 | 1231 | #&gt; | 0.995 | 0.155 | | #&gt; -------------|-----------|-----------|-----------| #&gt; spam | 6 | 153 | 159 | #&gt; | 0.005 | 0.845 | | #&gt; -------------|-----------|-----------|-----------| #&gt; Column Total | 1209 | 181 | 1390 | #&gt; | 0.870 | 0.130 | | #&gt; -------------|-----------|-----------|-----------| #&gt; #&gt; Misclassified: 28+7 "],
["classification-tree-vehicle-example.html", "Chapter 15 Classification Tree: Vehicle example 15.1 Load packages 15.2 Prepare data 15.3 Estimate the decision tree 15.4 Assess model 15.5 Make predictions", " Chapter 15 Classification Tree: Vehicle example Dataset: Vehicle (mlbench) Instructions: book “Applied Predictive Modeling Techniques”, Lewis, N.D. 15.1 Load packages library(tree) library(mlbench) data(Vehicle) str(Vehicle) #&gt; &#39;data.frame&#39;: 846 obs. of 19 variables: #&gt; $ Comp : num 95 91 104 93 85 107 97 90 86 93 ... #&gt; $ Circ : num 48 41 50 41 44 57 43 43 34 44 ... #&gt; $ D.Circ : num 83 84 106 82 70 106 73 66 62 98 ... #&gt; $ Rad.Ra : num 178 141 209 159 205 172 173 157 140 197 ... #&gt; $ Pr.Axis.Ra : num 72 57 66 63 103 50 65 65 61 62 ... #&gt; $ Max.L.Ra : num 10 9 10 9 52 6 6 9 7 11 ... #&gt; $ Scat.Ra : num 162 149 207 144 149 255 153 137 122 183 ... #&gt; $ Elong : num 42 45 32 46 45 26 42 48 54 36 ... #&gt; $ Pr.Axis.Rect: num 20 19 23 19 19 28 19 18 17 22 ... #&gt; $ Max.L.Rect : num 159 143 158 143 144 169 143 146 127 146 ... #&gt; $ Sc.Var.Maxis: num 176 170 223 160 241 280 176 162 141 202 ... #&gt; $ Sc.Var.maxis: num 379 330 635 309 325 957 361 281 223 505 ... #&gt; $ Ra.Gyr : num 184 158 220 127 188 264 172 164 112 152 ... #&gt; $ Skew.Maxis : num 70 72 73 63 127 85 66 67 64 64 ... #&gt; $ Skew.maxis : num 6 9 14 6 9 5 13 3 2 4 ... #&gt; $ Kurt.maxis : num 16 14 9 10 11 9 1 3 14 14 ... #&gt; $ Kurt.Maxis : num 187 189 188 199 180 181 200 193 200 195 ... #&gt; $ Holl.Ra : num 197 199 196 207 183 183 204 202 208 204 ... #&gt; $ Class : Factor w/ 4 levels &quot;bus&quot;,&quot;opel&quot;,&quot;saab&quot;,..: 4 4 3 4 1 1 1 4 4 3 ... summary(Vehicle[1]) #&gt; Comp #&gt; Min. : 73.0 #&gt; 1st Qu.: 87.0 #&gt; Median : 93.0 #&gt; Mean : 93.7 #&gt; 3rd Qu.:100.0 #&gt; Max. :119.0 summary(Vehicle[2]) #&gt; Circ #&gt; Min. :33.0 #&gt; 1st Qu.:40.0 #&gt; Median :44.0 #&gt; Mean :44.9 #&gt; 3rd Qu.:49.0 #&gt; Max. :59.0 attributes(Vehicle$Class) #&gt; $levels #&gt; [1] &quot;bus&quot; &quot;opel&quot; &quot;saab&quot; &quot;van&quot; #&gt; #&gt; $class #&gt; [1] &quot;factor&quot; 15.2 Prepare data set.seed(107) N = nrow(Vehicle) train &lt;- sample(1:N, 500, FALSE) # training and test sets trainset &lt;- Vehicle[train,] testset &lt;- Vehicle[-train,] 15.3 Estimate the decision tree fit &lt;- tree(Class ~., data = trainset, split = &quot;deviance&quot;) fit #&gt; node), split, n, deviance, yval, (yprob) #&gt; * denotes terminal node #&gt; #&gt; 1) root 500 1000 opel ( 0 0 0 0 ) #&gt; 2) Elong &lt; 41.5 215 500 saab ( 0 0 0 0 ) #&gt; 4) Max.L.Ra &lt; 7.5 51 50 bus ( 1 0 0 0 ) #&gt; 8) Comp &lt; 93.5 12 20 bus ( 0 0 0 0 ) #&gt; 16) Pr.Axis.Ra &lt; 67.5 7 8 saab ( 0 0 1 0 ) * #&gt; 17) Pr.Axis.Ra &gt; 67.5 5 0 bus ( 1 0 0 0 ) * #&gt; 9) Comp &gt; 93.5 39 9 bus ( 1 0 0 0 ) * #&gt; 5) Max.L.Ra &gt; 7.5 164 200 opel ( 0 1 0 0 ) #&gt; 10) Sc.Var.maxis &lt; 723 149 200 saab ( 0 0 1 0 ) #&gt; 20) Comp &lt; 109.5 137 200 opel ( 0 1 0 0 ) * #&gt; 21) Comp &gt; 109.5 12 0 saab ( 0 0 1 0 ) * #&gt; 11) Sc.Var.maxis &gt; 723 15 7 opel ( 0 1 0 0 ) * #&gt; 3) Elong &gt; 41.5 285 700 van ( 0 0 0 0 ) #&gt; 6) Sc.Var.maxis &lt; 305.5 116 200 van ( 0 0 0 1 ) #&gt; 12) Max.L.Rect &lt; 128.5 40 90 saab ( 0 0 0 0 ) #&gt; 24) Scat.Ra &lt; 120.5 15 30 van ( 0 0 0 1 ) * #&gt; 25) Scat.Ra &gt; 120.5 25 30 saab ( 0 0 1 0 ) * #&gt; 13) Max.L.Rect &gt; 128.5 76 90 van ( 0 0 0 1 ) #&gt; 26) Max.L.Rect &lt; 138.5 38 60 van ( 0 0 0 1 ) #&gt; 52) Circ &lt; 37.5 17 10 van ( 0 0 0 1 ) * #&gt; 53) Circ &gt; 37.5 21 40 opel ( 0 0 0 0 ) * #&gt; 27) Max.L.Rect &gt; 138.5 38 20 van ( 0 0 0 1 ) * #&gt; 7) Sc.Var.maxis &gt; 305.5 169 400 bus ( 0 0 0 0 ) #&gt; 14) Max.L.Ra &lt; 8.5 116 200 bus ( 1 0 0 0 ) #&gt; 28) D.Circ &lt; 76.5 97 100 bus ( 1 0 0 0 ) #&gt; 56) Skew.maxis &lt; 10.5 87 70 bus ( 1 0 0 0 ) #&gt; 112) Max.L.Rect &lt; 134.5 12 20 bus ( 0 0 0 0 ) * #&gt; 113) Max.L.Rect &gt; 134.5 75 20 bus ( 1 0 0 0 ) * #&gt; 57) Skew.maxis &gt; 10.5 10 20 opel ( 0 0 0 0 ) * #&gt; 29) D.Circ &gt; 76.5 19 30 opel ( 0 1 0 0 ) * #&gt; 15) Max.L.Ra &gt; 8.5 53 20 van ( 0 0 0 1 ) * # fit &lt;- tree(Class ~., data = Vehicle[train,], split =&quot;deviance&quot;) # fit We use deviance as the splitting criteria, a common alternative is to use split=“gini”. At each branch of the tree (after root) we see in order: 1. The branch number (e.g. in this case 1,2,14 and 15); 2. the split (e.g. Elong &lt; 41.5); 3. the number of samples going along that split (e.g. 229); 4. the deviance associated with that split (e.g. 489.1); 5. the predicted class (e.g. opel); 6. the associated probabilities (e.g. ( 0.222707 0.410480 0.366812 0.000000 ); 7. and for a terminal node (or leaf), the symbol &quot;*&quot;. summary(fit) #&gt; #&gt; Classification tree: #&gt; tree(formula = Class ~ ., data = trainset, split = &quot;deviance&quot;) #&gt; Variables actually used in tree construction: #&gt; [1] &quot;Elong&quot; &quot;Max.L.Ra&quot; &quot;Comp&quot; &quot;Pr.Axis.Ra&quot; #&gt; [5] &quot;Sc.Var.maxis&quot; &quot;Max.L.Rect&quot; &quot;Scat.Ra&quot; &quot;Circ&quot; #&gt; [9] &quot;D.Circ&quot; &quot;Skew.maxis&quot; #&gt; Number of terminal nodes: 16 #&gt; Residual mean deviance: 0.943 = 456 / 484 #&gt; Misclassification error rate: 0.252 = 126 / 500 Notice that summary(fit) shows: 1. The type of tree, in this case a Classification tree; 2. the formula used to fit the tree; 3. the variables used to fit the tree; 4. the number of terminal nodes in this case 15; 5. the residual mean deviance - 0.9381; 6. the misclassification error rate 0.232 or 23.2%. plot(fit); text(fit) 15.4 Assess model Unfortunately, classification trees have a tendency to overfit the data. One approach to reduce this risk is to use cross-validation. For each hold out sample we fit the model and note at what level the tree gives the best results (using deviance or the misclassification rate). Then we hold out a different sample and repeat. This can be carried out using the cv.tree() function. We use a leave-one-out cross-validation using the misclassification rate and deviance (FUN=prune.misclass, followed by FUN=prune.tree). fitM.cv &lt;- cv.tree(fit, K=346, FUN = prune.misclass) fitP.cv &lt;- cv.tree(fit, K=346, FUN = prune.tree) The results are plotted out side by side in Figure 1.2. The jagged lines shows where the minimum deviance / misclassification occurred with the cross-validated tree. Since the cross validated misclassification and deviance both reach their minimum close to the number of branches in the original fitted tree there is little to be gained from pruning this tree par(mfrow = c(1, 2)) plot(fitM.cv) plot(fitP.cv) 15.5 Make predictions We use the validation data set and the fitted decision tree to predict vehicle classes; then we display the confusion matrix and calculate the error rate of the fitted tree. Overall, the model has an error rate of 32%. testLabels &lt;- Vehicle$Class[-train] testLabels #&gt; [1] van bus bus van van bus bus saab opel bus van saab van saab #&gt; [15] saab van saab opel van saab saab saab bus bus saab opel bus opel #&gt; [29] bus opel van opel opel saab saab bus bus bus van van saab opel #&gt; [43] bus opel van opel saab bus van bus opel van saab bus opel bus #&gt; [57] opel opel van bus van saab opel bus van saab opel opel saab saab #&gt; [71] saab opel bus van bus opel bus saab bus bus bus opel opel van #&gt; [85] saab bus bus bus van saab opel van van bus bus opel bus opel #&gt; [99] saab opel bus opel bus saab van van saab saab bus van opel van #&gt; [113] saab opel saab saab van van van van bus bus opel bus bus van #&gt; [127] saab bus opel bus bus bus bus opel van saab saab bus opel van #&gt; [141] bus saab bus van bus opel van saab opel saab opel van saab van #&gt; [155] saab opel bus van bus saab saab opel opel bus bus opel van van #&gt; [169] bus van van saab bus saab opel saab opel bus bus bus saab bus #&gt; [183] opel opel saab saab saab van van opel opel van van opel bus saab #&gt; [197] bus van opel opel bus bus bus opel saab opel van bus opel opel #&gt; [211] saab opel bus opel opel opel van opel van saab saab van saab saab #&gt; [225] saab saab van van van saab bus van van bus saab opel saab saab #&gt; [239] opel saab saab saab saab van saab opel bus saab bus opel opel opel #&gt; [253] saab bus van opel saab opel bus bus saab van opel bus saab van #&gt; [267] opel saab saab saab saab van opel bus bus bus opel saab saab saab #&gt; [281] van saab bus opel saab van opel bus saab saab opel opel van saab #&gt; [295] bus opel bus van van opel bus bus saab bus van saab bus van #&gt; [309] saab van opel bus bus opel saab opel bus bus saab van saab saab #&gt; [323] bus opel opel opel bus saab bus van bus van saab opel saab van #&gt; [337] opel opel van bus saab saab van saab opel saab #&gt; Levels: bus opel saab van # Confusion Matrix pred &lt;- predict(fit, newdata = testset) # find column whih has the maximum of all rows pred.class &lt;- colnames(pred)[max.col(pred, ties.method = c(&quot;random&quot;))] cm &lt;- table(testLabels, pred.class, dnn = c(&quot;Observed Class&quot;, &quot;Predicted Class&quot;)) cm #&gt; Predicted Class #&gt; Observed Class bus opel saab van #&gt; bus 85 1 1 5 #&gt; opel 3 70 10 2 #&gt; saab 7 67 14 7 #&gt; van 1 4 5 64 # Sensitivity sum(diag(cm)) / sum(cm) #&gt; [1] 0.673 # pred &lt;- predict(fit, newdata = Vehicle[-train,]) # pred.class &lt;- colnames(pred)[max.col(pred, ties.method = c(&quot;random&quot;))] # table(Vehicle$Class[-train], pred.class, # dnn = c(&quot;Observed Class&quot;, &quot;Predicted Class&quot;)) error_rate = (1 - sum(pred.class == testset) / nrow(testset)) round(error_rate, 3) #&gt; [1] 0.327 # error_rate = (1 - sum(pred.class == Vehicle$Class[-train])/346) # round(error_rate,3) "],
["bike-sharing-demand.html", "Chapter 16 Bike sharing demand 16.1 Hypothesis Generation 16.2 Understanding the Data Set 16.3 Importing the dataset and Data Exploration 16.4 Hypothesis Testing (using multivariate analysis) 16.5 Feature Engineering 16.6 Model Building 16.7 End Notes", " Chapter 16 Bike sharing demand #loading the required libraries library(rpart) library(rattle) #&gt; Rattle: A free graphical interface for data science with R. #&gt; Version 5.2.0 Copyright (c) 2006-2018 Togaware Pty Ltd. #&gt; Type &#39;rattle()&#39; to shake, rattle, and roll your data. library(rpart.plot) library(RColorBrewer) library(randomForest) #&gt; randomForest 4.6-14 #&gt; Type rfNews() to see new features/changes/bug fixes. #&gt; #&gt; Attaching package: &#39;randomForest&#39; #&gt; The following object is masked from &#39;package:rattle&#39;: #&gt; #&gt; importance library(corrplot) #&gt; corrplot 0.84 loaded library(dplyr) #&gt; #&gt; Attaching package: &#39;dplyr&#39; #&gt; The following object is masked from &#39;package:randomForest&#39;: #&gt; #&gt; combine #&gt; The following objects are masked from &#39;package:stats&#39;: #&gt; #&gt; filter, lag #&gt; The following objects are masked from &#39;package:base&#39;: #&gt; #&gt; intersect, setdiff, setequal, union library(tictoc) Source: https://www.analyticsvidhya.com/blog/2015/06/solution-kaggle-competition-bike-sharing-demand/ 16.1 Hypothesis Generation Before exploring the data to understand the relationship between variables, I’d recommend you to focus on hypothesis generation first. Now, this might sound counter-intuitive for solving a data science problem, but if there is one thing I have learnt over years, it is this. Before exploring data, you should spend some time thinking about the business problem, gaining the domain knowledge and may be gaining first hand experience of the problem (only if I could travel to North America!) How does it help? This practice usually helps you form better features later on, which are not biased by the data available in the dataset. At this stage, you are expected to posses structured thinking i.e. a thinking process which takes into consideration all the possible aspects of a particular problem. Here are some of the hypothesis which I thought could influence the demand of bikes: Hourly trend: There must be high demand during office timings. Early morning and late evening can have different trend (cyclist) and low demand during 10:00 pm to 4:00 am. Daily Trend: Registered users demand more bike on weekdays as compared to weekend or holiday. Rain: The demand of bikes will be lower on a rainy day as compared to a sunny day. Similarly, higher humidity will cause to lower the demand and vice versa. Temperature: Would high or low temperature encourage or disencourage bike riding? Pollution: If the pollution level in a city starts soaring, people may start using Bike (it may be influenced by government / company policies or increased awareness). Time: Total demand should have higher contribution of registered user as compared to casual because registered user base would increase over time. Traffic: It can be positively correlated with Bike demand. Higher traffic may force people to use bike as compared to other road transport medium like car, taxi etc 16.2 Understanding the Data Set The dataset shows hourly rental data for two years (2011 and 2012). The training data set is for the first 19 days of each month. The test dataset is from 20th day to month’s end. We are required to predict the total count of bikes rented during each hour covered by the test set. In the training data set, they have separately given bike demand by registered, casual users and sum of both is given as count. Training data set has 12 variables (see below) and Test has 9 (excluding registered, casual and count). 16.2.1 Independent variables datetime: date and hour in &quot;mm/dd/yyyy hh:mm&quot; format season: Four categories-&gt; 1 = spring, 2 = summer, 3 = fall, 4 = winter holiday: whether the day is a holiday or not (1/0) workingday: whether the day is neither a weekend nor holiday (1/0) weather: Four Categories of weather 1-&gt; Clear, Few clouds, Partly cloudy, Partly cloudy 2-&gt; Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist 3-&gt; Light Snow and Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds 4-&gt; Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog temp: hourly temperature in Celsius atemp: &quot;feels like&quot; temperature in Celsius humidity: relative humidity windspeed: wind speed 16.2.2 Dependent variables registered: number of registered user casual: number of non-registered user count: number of total rentals (registered + casual) 16.3 Importing the dataset and Data Exploration For this solution, I have used R (R Studio 0.99.442) in Windows Environment. Below are the steps to import and perform data exploration. If you are new to this concept, you can refer this guide on Data Exploration in R Import Train and Test Data Set # https://www.kaggle.com/c/bike-sharing-demand/data train = read.csv(file.path(data_raw_dir, &quot;bike_train.csv&quot;)) test = read.csv(file.path(data_raw_dir, &quot;bike_test.csv&quot;)) glimpse(train) #&gt; Observations: 10,886 #&gt; Variables: 12 #&gt; $ datetime &lt;fct&gt; 2011-01-01 00:00:00, 2011-01-01 01:00:00, 2011-01-01 … #&gt; $ season &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… #&gt; $ holiday &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… #&gt; $ workingday &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… #&gt; $ weather &lt;int&gt; 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2,… #&gt; $ temp &lt;dbl&gt; 9.84, 9.02, 9.02, 9.84, 9.84, 9.84, 9.02, 8.20, 9.84,… #&gt; $ atemp &lt;dbl&gt; 14.4, 13.6, 13.6, 14.4, 14.4, 12.9, 13.6, 12.9, 14.4,… #&gt; $ humidity &lt;int&gt; 81, 80, 80, 75, 75, 75, 80, 86, 75, 76, 76, 81, 77, 7… #&gt; $ windspeed &lt;dbl&gt; 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 17, 19, 19, 20, 19, 20,… #&gt; $ casual &lt;int&gt; 3, 8, 5, 3, 0, 0, 2, 1, 1, 8, 12, 26, 29, 47, 35, 40,… #&gt; $ registered &lt;int&gt; 13, 32, 27, 10, 1, 1, 0, 2, 7, 6, 24, 30, 55, 47, 71,… #&gt; $ count &lt;int&gt; 16, 40, 32, 13, 1, 1, 2, 3, 8, 14, 36, 56, 84, 94, 10… glimpse(test) #&gt; Observations: 6,493 #&gt; Variables: 9 #&gt; $ datetime &lt;fct&gt; 2011-01-20 00:00:00, 2011-01-20 01:00:00, 2011-01-20 … #&gt; $ season &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… #&gt; $ holiday &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… #&gt; $ workingday &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… #&gt; $ weather &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2,… #&gt; $ temp &lt;dbl&gt; 10.66, 10.66, 10.66, 10.66, 10.66, 9.84, 9.02, 9.02, … #&gt; $ atemp &lt;dbl&gt; 11.4, 13.6, 13.6, 12.9, 12.9, 11.4, 10.6, 10.6, 10.6,… #&gt; $ humidity &lt;int&gt; 56, 56, 56, 56, 56, 60, 60, 55, 55, 52, 48, 45, 42, 4… #&gt; $ windspeed &lt;dbl&gt; 26, 0, 0, 11, 11, 15, 15, 15, 19, 15, 20, 11, 0, 7, 9… Combine both Train and Test Data set (to understand the distribution of independent variable together). # add variables to test dataset before merging test$registered=0 test$casual=0 test$count=0 data = rbind(train,test) Variable Type Identification str(data) #&gt; &#39;data.frame&#39;: 17379 obs. of 12 variables: #&gt; $ datetime : Factor w/ 17379 levels &quot;2011-01-01 00:00:00&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... #&gt; $ season : int 1 1 1 1 1 1 1 1 1 1 ... #&gt; $ holiday : int 0 0 0 0 0 0 0 0 0 0 ... #&gt; $ workingday: int 0 0 0 0 0 0 0 0 0 0 ... #&gt; $ weather : int 1 1 1 1 1 2 1 1 1 1 ... #&gt; $ temp : num 9.84 9.02 9.02 9.84 9.84 ... #&gt; $ atemp : num 14.4 13.6 13.6 14.4 14.4 ... #&gt; $ humidity : int 81 80 80 75 75 75 80 86 75 76 ... #&gt; $ windspeed : num 0 0 0 0 0 ... #&gt; $ casual : num 3 8 5 3 0 0 2 1 1 8 ... #&gt; $ registered: num 13 32 27 10 1 1 0 2 7 6 ... #&gt; $ count : num 16 40 32 13 1 1 2 3 8 14 ... Find missing values in the dataset if any table(is.na(data)) #&gt; #&gt; FALSE #&gt; 208548 No NAs in the dataset. Understand the distribution of numerical variables and generate a frequency table for numeric variables. Analyze the distribution. 16.3.1 histograms # histograms each attribute par(mfrow=c(2,4)) for(i in 2:9) { hist(data[,i], main = names(data)[i]) } 16.3.2 density plots # density plot for each attribute par(mfrow=c(2,4)) for(i in 2:9) { plot(density(data[,i]), main=names(data)[i]) } 16.3.3 boxplots # boxplots for each attribute par(mfrow=c(2,4)) for(i in 2:9) { boxplot(data[,i], main=names(data)[i]) } 16.3.4 Unique values of discrete variables # the discrete variables in this case are integers ints &lt;- unlist(lapply(data, is.integer)) names(data)[ints] #&gt; [1] &quot;season&quot; &quot;holiday&quot; &quot;workingday&quot; &quot;weather&quot; &quot;humidity&quot; Humidity should not be an integer or discrete variable; it is a continuous or numeric variable. # convert humidity to numeric data$humidity &lt;- as.numeric(data$humidity) # list unique values of integer variables ints &lt;- unlist(lapply(data, is.integer)) int_vars &lt;- names(data)[ints] sapply(int_vars, function(x) unique(data[x])) #&gt; $season.season #&gt; [1] 1 2 3 4 #&gt; #&gt; $holiday.holiday #&gt; [1] 0 1 #&gt; #&gt; $workingday.workingday #&gt; [1] 0 1 #&gt; #&gt; $weather.weather #&gt; [1] 1 2 3 4 16.3.5 Inferences The variables season, holiday, workingday and weather are discrete (integer). Activity is even through all seasons. Most of the activitity happens during non-holidays. Activity doubles during the working days. Activity happens mostly during clear (1) weather. temp, atemp and humidity are continuous variables (numeric). 16.4 Hypothesis Testing (using multivariate analysis) Till now, we have got a fair understanding of the data set. Now, let’s test the hypothesis which we had generated earlier. Here I have added some additional hypothesis from the dataset. Let’s test them one by one: 16.4.1 Hourly trend There must be high demand during office timings. Early morning and late evening can have different trend (cyclist) and low demand during 10:00 pm to 4:00 am. We don’t have the variable ‘hour’ with us. But we can extract it using the datetime column. head(data$datetime) #&gt; [1] 2011-01-01 00:00:00 2011-01-01 01:00:00 2011-01-01 02:00:00 #&gt; [4] 2011-01-01 03:00:00 2011-01-01 04:00:00 2011-01-01 05:00:00 #&gt; 17379 Levels: 2011-01-01 00:00:00 2011-01-01 01:00:00 ... 2012-12-31 23:00:00 class(data$datetime) #&gt; [1] &quot;factor&quot; # show hour and day from the variable datetime head(substr(data$datetime, 12, 13)) # hour #&gt; [1] &quot;00&quot; &quot;01&quot; &quot;02&quot; &quot;03&quot; &quot;04&quot; &quot;05&quot; head(substr(data$datetime, 9, 10)) # day #&gt; [1] &quot;01&quot; &quot;01&quot; &quot;01&quot; &quot;01&quot; &quot;01&quot; &quot;01&quot; # extracting hour data$hour = substr(data$datetime,12,13) data$hour = as.factor(data$hour) head(data$hour) #&gt; [1] 00 01 02 03 04 05 #&gt; 24 Levels: 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 ... 23 ### dividing again in train and test # the train dataset is for the first 19 days train = data[as.integer(substr(data$datetime, 9, 10)) &lt; 20,] # the test dataset is from day 20 to the end of the month test = data[as.integer(substr(data$datetime, 9, 10)) &gt; 19,] 16.4.2 boxplot count vs hour in training set boxplot(train$count ~ train$hour, xlab=&quot;hour&quot;, ylab=&quot;count of users&quot;) Rides increase from 6 am to 6pm, during office hours. # casual users casual &lt;- data[data$casual &gt; 0, ] registered &lt;- data[data$registered &gt; 0, ] dim(casual) #&gt; [1] 9900 13 dim(registered) #&gt; [1] 10871 13 16.4.3 Boxplot hourly: casual vs registered users in the training set # by hour: casual vs registered users par(mfrow=c(2,1)) boxplot(train$casual ~ train$hour, xlab=&quot;hour&quot;, ylab=&quot;casual users&quot;) boxplot(train$registered ~ train$hour, xlab=&quot;hour&quot;, ylab=&quot;registered users&quot;) Casual and Registered users have different distributions. Casual users tend to rent more during office hours. 16.4.4 outliers in the training set par(mfrow=c(2,1)) boxplot(train$count ~ train$hour, xlab=&quot;hour&quot;, ylab=&quot;count of users&quot;) boxplot(log(train$count) ~ train$hour,xlab=&quot;hour&quot;,ylab=&quot;log(count)&quot;) 16.4.5 Daily trend Registered users demand more bike on weekdays as compared to weekend or holiday. # extracting days of week date &lt;- substr(data$datetime, 1, 10) days &lt;- weekdays(as.Date(date)) data$day &lt;- days # split the dataset again at day 20 of the month, before and after train = data[as.integer(substr(data$datetime,9,10)) &lt; 20,] test = data[as.integer(substr(data$datetime,9,10)) &gt; 19,] 16.4.6 Boxplot daily trend: casual vs registered users, training set # creating boxplots for rentals with different variables to see the variation par(mfrow=c(2,1)) boxplot(train$casual ~ train$day, xlab=&quot;day&quot;, ylab=&quot;casual users&quot;) boxplot(train$registered ~ train$day, xlab=&quot;day&quot;, ylab=&quot;registered users&quot;) Demand of casual users increases during the weekend, contrary of registered users. 16.4.7 Rain The demand of bikes will be lower on a rainy day as compared to a sunny day. Similarly, higher humidity will cause to lower the demand and vice versa. We use the variable weather (1 to 4) to analyze riding under rain conditions. 16.4.7.1 Boxplot of rain effect on bike riding, training set par(mfrow=c(2,1)) boxplot(train$casual ~ train$weather, xlab=&quot;day&quot;, ylab=&quot;casual users&quot;) boxplot(train$registered ~ train$weather, xlab=&quot;day&quot;, ylab=&quot;registered users&quot;) Registered used tend to ride even with rain. 16.4.8 Temperature Would high or low temperature encourage or disencourage bike riding? 16.4.8.1 boxplot of temperature effect, training set par(mfrow=c(2,1)) boxplot(train$casual ~ train$temp, xlab=&quot;temp&quot;, ylab=&quot;casual users&quot;) boxplot(train$registered ~ train$temp, xlab=&quot;temp&quot;, ylab=&quot;registered users&quot;) Casual users tend to ride with milder temperatures while registered users ride even at low temperatures. 16.4.9 Correlation sub = data.frame(train$registered, train$casual, train$count, train$temp, train$humidity, train$atemp, train$windspeed) cor(sub) #&gt; train.registered train.casual train.count train.temp #&gt; train.registered 1.0000 0.4972 0.971 0.3186 #&gt; train.casual 0.4972 1.0000 0.690 0.4671 #&gt; train.count 0.9709 0.6904 1.000 0.3945 #&gt; train.temp 0.3186 0.4671 0.394 1.0000 #&gt; train.humidity -0.2655 -0.3482 -0.317 -0.0649 #&gt; train.atemp 0.3146 0.4621 0.390 0.9849 #&gt; train.windspeed 0.0911 0.0923 0.101 -0.0179 #&gt; train.humidity train.atemp train.windspeed #&gt; train.registered -0.2655 0.3146 0.0911 #&gt; train.casual -0.3482 0.4621 0.0923 #&gt; train.count -0.3174 0.3898 0.1014 #&gt; train.temp -0.0649 0.9849 -0.0179 #&gt; train.humidity 1.0000 -0.0435 -0.3186 #&gt; train.atemp -0.0435 1.0000 -0.0575 #&gt; train.windspeed -0.3186 -0.0575 1.0000 # do not show the diagonal corrplot(cor(sub), diag = FALSE) correlation between casual and atemp, temp. Strong correlation between temp and atemp. 16.4.10 Activity by year 16.4.10.1 Year extraction # extracting year data$year = substr(data$datetime, 1, 4) data$year = as.factor(data$year) # ignore the division of data again and again, this could have been done together also train = data[as.integer(substr(data$datetime,9,10)) &lt; 20,] test = data[as.integer(substr(data$datetime,9,10)) &gt; 19,] 16.4.10.2 Trend by year, training set par(mfrow=c(2,1)) # again some boxplots with different variables # these boxplots give important information about the dependent variable with respect to the independent variables boxplot(train$casual ~ train$year, xlab=&quot;year&quot;, ylab=&quot;casual users&quot;) boxplot(train$registered ~ train$year, xlab=&quot;year&quot;, ylab=&quot;registered users&quot;) Activity increased in 2012. 16.4.10.3 trend by windspeed, training set par(mfrow=c(2,1)) boxplot(train$casual ~ train$windspeed, xlab=&quot;windspeed&quot;, ylab=&quot;casual users&quot;) boxplot(train$registered ~ train$windspeed, xlab=&quot;windspeed&quot;, ylab=&quot;registered users&quot;) Casual users ride even with stron winds. 16.4.10.4 trend by humidity, training set par(mfrow=c(2,1)) boxplot(train$casual ~ train$humidity, xlab=&quot;humidity&quot;, ylab=&quot;casual users&quot;) boxplot(train$registered ~ train$humidity, xlab=&quot;humidity&quot;, ylab=&quot;registered users&quot;) Casual users prefer not to ride with humid weather. 16.5 Feature Engineering 16.5.1 Prepare data # factoring some variables from integer data$season &lt;- as.factor(data$season) data$weather &lt;- as.factor(data$weather) data$holiday &lt;- as.factor(data$holiday) data$workingday &lt;- as.factor(data$workingday) # new column data$hour &lt;- as.integer(data$hour) # created this variable to divide a day into parts, but did not finally use it data$day_part &lt;- 0 # split in training and test sets again train &lt;- data[as.integer(substr(data$datetime, 9, 10)) &lt; 20,] test &lt;- data[as.integer(substr(data$datetime, 9, 10)) &gt; 19,] # combine the sets data &lt;- rbind(train, test) 16.5.2 Build hour bins # for registered users d = rpart(registered ~ hour, data = train) fancyRpartPlot(d) # for casual users d = rpart(casual ~ hour, data = train) fancyRpartPlot(d) # Assign the timings according to tree # fill the hour bins data = rbind(train,test) # create hour buckets for registered users # 0,1,2,3,4,5,6,7 &lt; 7.5 # 22,23,24 &gt;=22 # 10,11,12,13,14,15,16,17: h&gt;=9.5 &amp; h&lt;18 # h&lt;9.5 &amp; h&lt;8.5 : 8 # h&lt;9.5 &amp; h&gt;=8.5 : 9 # h&gt;=20: 20,21 # h &lt; 20: 18,19 data$dp_reg = 0 data$dp_reg[data$hour &lt; 8] = 1 data$dp_reg[data$hour &gt;= 22] = 2 data$dp_reg[data$hour &gt; 9 &amp; data$hour &lt; 18] = 3 data$dp_reg[data$hour == 8] = 4 data$dp_reg[data$hour == 9] = 5 data$dp_reg[data$hour == 20 | data$hour == 21] = 6 data$dp_reg[data$hour == 19 | data$hour == 18] = 7 # casual users # h&lt;11, h&lt;8.5: 0,1,2,3,4,5,6,7,8 # h&gt;=8.5 &amp; h&lt;11: 9, 10 # h &gt;=11 &amp; h&gt;=21: 21,22,23,24 # h &gt;=11 &amp; h&lt;21: 11,12,13,14,15,16,17,18,19,20 data$dp_cas = 0 data$dp_cas[data$hour &lt; 11 &amp; data$hour &gt;= 8] = 1 data$dp_cas[data$hour == 9 | data$hour == 10] = 2 data$dp_cas[data$hour &gt;= 11 &amp; data$hour &lt; 21] = 3 data$dp_cas[data$hour &gt;= 21] = 4 16.5.3 Temperature bins # partition the data by temperature, registered users f = rpart(registered ~ temp, data=train) fancyRpartPlot(f) # partition the data by temperature,, casual users f=rpart(casual ~ temp, data=train) fancyRpartPlot(f) 16.5.3.1 Assign temperature ranges accoding to trees data$temp_reg = 0 data$temp_reg[data$temp &lt; 13] = 1 data$temp_reg[data$temp &gt;= 13 &amp; data$temp &lt; 23] = 2 data$temp_reg[data$temp &gt;= 23 &amp; data$temp &lt; 30] = 3 data$temp_reg[data$temp &gt;= 30] = 4 data$temp_cas = 0 data$temp_cas[data$temp &lt; 15] = 1 data$temp_cas[data$temp &gt;= 15 &amp; data$temp &lt; 23] = 2 data$temp_cas[data$temp &gt;= 23 &amp; data$temp &lt; 30] = 3 data$temp_cas[data$temp &gt;= 30] = 4 16.5.4 Year bins by quarter # add new variable with the month number data$month &lt;- substr(data$datetime, 6, 7) data$month &lt;- as.integer(data$month) # bin by quarter manually data$year_part[data$year==&#39;2011&#39;] = 1 data$year_part[data$year==&#39;2011&#39; &amp; data$month&gt;3] = 2 data$year_part[data$year==&#39;2011&#39; &amp; data$month&gt;6] = 3 data$year_part[data$year==&#39;2011&#39; &amp; data$month&gt;9] = 4 data$year_part[data$year==&#39;2012&#39;] = 5 data$year_part[data$year==&#39;2012&#39; &amp; data$month&gt;3] = 6 data$year_part[data$year==&#39;2012&#39; &amp; data$month&gt;6] = 7 data$year_part[data$year==&#39;2012&#39; &amp; data$month&gt;9] = 8 table(data$year_part) #&gt; #&gt; 1 2 3 4 5 6 7 8 #&gt; 2067 2183 2192 2203 2176 2182 2208 2168 16.5.5 Day Type Created a variable having categories like “weekday”, “weekend” and “holiday”. # creating another variable day_type which may affect our accuracy as weekends and weekdays are important in deciding rentals data$day_type = 0 data$day_type[data$holiday==0 &amp; data$workingday==0] = &quot;weekend&quot; data$day_type[data$holiday==1] = &quot;holiday&quot; data$day_type[data$holiday==0 &amp; data$workingday==1] = &quot;working day&quot; # split dataset again train = data[as.integer(substr(data$datetime,9,10)) &lt; 20,] test = data[as.integer(substr(data$datetime,9,10)) &gt; 19,] par(mfrow=c(2,1)) boxplot(train$casual ~ train$dp_cas, xlab = &quot;day partition&quot;, ylab=&quot;casual users&quot;) boxplot(train$registered ~ train$dp_reg, xlab = &quot;day partition&quot;, ylab=&quot;registered users&quot;) par(mfrow=c(2,1)) boxplot(train$casual ~ train$day_type, xlab = &quot;day type&quot;, ylab=&quot;casual users&quot;, ylim = c(0,900)) boxplot(train$registered ~ train$day_type, xlab = &quot;day type&quot;, ylab=&quot;registered users&quot;, ylim = c(0,900)) par(mfrow=c(2,1)) boxplot(train$casual ~ train$year_part, xlab = &quot;year partition, quarter&quot;, ylab=&quot;casual users&quot;, ylim = c(0,900)) boxplot(train$registered ~ train$year_part, xlab = &quot;year partition, quarter&quot;, ylab=&quot;registered users&quot;, ylim = c(0,900)) 16.5.6 Temperatures par(mfrow=c(2,1)) boxplot(train$casual ~ train$temp, xlab = &quot;temperature&quot;, ylab=&quot;casual users&quot;, ylim = c(0,900)) boxplot(train$registered ~ train$temp, xlab = &quot;temperature&quot;, ylab=&quot;registered users&quot;, ylim = c(0,900)) plot(train$temp, train$count) data &lt;- rbind(train, test) # data$month &lt;- substr(data$datetime, 6, 7) # data$month &lt;- as.integer(data$month) 16.5.7 Imputting missing data to wind speed # dividing total data depending on windspeed to impute/predict the missing values table(data$windspeed == 0) #&gt; #&gt; FALSE TRUE #&gt; 15199 2180 # FALSE TRUE # 15199 2180 k = data$windspeed == 0 wind_0 = subset(data, k) # windspeed is zero wind_1 = subset(data, !k) # windspeed not zero tic() # predicting missing values in windspeed using a random forest model # this is a different approach to impute missing values rather than # just using the mean or median or some other statistic for imputation set.seed(415) fit &lt;- randomForest(windspeed ~ season + weather + humidity + month + temp + year + atemp, data = wind_1, importance = TRUE, ntree = 250) pred = predict(fit, wind_0) wind_0$windspeed = pred # fill with wind speed predictions toc() #&gt; 56.047 sec elapsed # recompose the whole dataset data = rbind(wind_0, wind_1) # how many zero values now? sum(data$windspeed == 0) #&gt; [1] 0 16.5.8 Weekend variable Created a separate variable for weekend (0/1) data$weekend = 0 data$weekend[data$day==&quot;Sunday&quot; | data$day==&quot;Saturday&quot; ] = 1 16.6 Model Building As this was our first attempt, we applied decision tree, conditional inference tree and random forest algorithms and found that random forest is performing the best. You can also go with regression, boosted regression, neural network and find which one is working well for you. Before executing the random forest model code, I have followed following steps: Convert discrete variables into factor (weather, season, hour, holiday, working day, month, day) str(data) #&gt; &#39;data.frame&#39;: 17379 obs. of 24 variables: #&gt; $ datetime : Factor w/ 17379 levels &quot;2011-01-01 00:00:00&quot;,..: 1 2 3 4 5 7 8 9 10 65 ... #&gt; $ season : Factor w/ 4 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;: 1 1 1 1 1 1 1 1 1 1 ... #&gt; $ holiday : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 1 1 1 1 1 1 1 1 1 ... #&gt; $ workingday: Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 1 1 1 1 1 1 1 1 2 ... #&gt; $ weather : Factor w/ 4 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;: 1 1 1 1 1 1 1 1 1 1 ... #&gt; $ temp : num 9.84 9.02 9.02 9.84 9.84 ... #&gt; $ atemp : num 14.4 13.6 13.6 14.4 14.4 ... #&gt; $ humidity : num 81 80 80 75 75 80 86 75 76 47 ... #&gt; $ windspeed : num 9.03 9.05 9.05 9.15 9.15 ... #&gt; $ casual : num 3 8 5 3 0 2 1 1 8 8 ... #&gt; $ registered: num 13 32 27 10 1 0 2 7 6 102 ... #&gt; $ count : num 16 40 32 13 1 2 3 8 14 110 ... #&gt; $ hour : int 1 2 3 4 5 7 8 9 10 20 ... #&gt; $ day : chr &quot;Saturday&quot; &quot;Saturday&quot; &quot;Saturday&quot; &quot;Saturday&quot; ... #&gt; $ year : Factor w/ 2 levels &quot;2011&quot;,&quot;2012&quot;: 1 1 1 1 1 1 1 1 1 1 ... #&gt; $ day_part : num 0 0 0 0 0 0 0 0 0 0 ... #&gt; $ dp_reg : num 1 1 1 1 1 1 4 5 3 6 ... #&gt; $ dp_cas : num 0 0 0 0 0 0 1 2 2 3 ... #&gt; $ temp_reg : num 1 1 1 1 1 1 1 1 2 1 ... #&gt; $ temp_cas : num 1 1 1 1 1 1 1 1 1 1 ... #&gt; $ month : int 1 1 1 1 1 1 1 1 1 1 ... #&gt; $ year_part : num 1 1 1 1 1 1 1 1 1 1 ... #&gt; $ day_type : chr &quot;weekend&quot; &quot;weekend&quot; &quot;weekend&quot; &quot;weekend&quot; ... #&gt; $ weekend : num 1 1 1 1 1 1 1 1 1 0 ... 16.6.1 Convert variables to factors # converting all relevant categorical variables into factors to feed to our random forest model data$season = as.factor(data$season) data$holiday = as.factor(data$holiday) data$workingday = as.factor(data$workingday) data$weather = as.factor(data$weather) data$hour = as.factor(data$hour) data$month = as.factor(data$month) data$day_part = as.factor(data$dp_cas) data$day_type = as.factor(data$dp_reg) data$day = as.factor(data$day) data$temp_cas = as.factor(data$temp_cas) data$temp_reg = as.factor(data$temp_reg) str(data) #&gt; &#39;data.frame&#39;: 17379 obs. of 24 variables: #&gt; $ datetime : Factor w/ 17379 levels &quot;2011-01-01 00:00:00&quot;,..: 1 2 3 4 5 7 8 9 10 65 ... #&gt; $ season : Factor w/ 4 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;: 1 1 1 1 1 1 1 1 1 1 ... #&gt; $ holiday : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 1 1 1 1 1 1 1 1 1 ... #&gt; $ workingday: Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 1 1 1 1 1 1 1 1 2 ... #&gt; $ weather : Factor w/ 4 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;: 1 1 1 1 1 1 1 1 1 1 ... #&gt; $ temp : num 9.84 9.02 9.02 9.84 9.84 ... #&gt; $ atemp : num 14.4 13.6 13.6 14.4 14.4 ... #&gt; $ humidity : num 81 80 80 75 75 80 86 75 76 47 ... #&gt; $ windspeed : num 9.03 9.05 9.05 9.15 9.15 ... #&gt; $ casual : num 3 8 5 3 0 2 1 1 8 8 ... #&gt; $ registered: num 13 32 27 10 1 0 2 7 6 102 ... #&gt; $ count : num 16 40 32 13 1 2 3 8 14 110 ... #&gt; $ hour : Factor w/ 24 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 1 2 3 4 5 7 8 9 10 20 ... #&gt; $ day : Factor w/ 7 levels &quot;Friday&quot;,&quot;Monday&quot;,..: 3 3 3 3 3 3 3 3 3 2 ... #&gt; $ year : Factor w/ 2 levels &quot;2011&quot;,&quot;2012&quot;: 1 1 1 1 1 1 1 1 1 1 ... #&gt; $ day_part : Factor w/ 5 levels &quot;0&quot;,&quot;1&quot;,&quot;2&quot;,&quot;3&quot;,..: 1 1 1 1 1 1 2 3 3 4 ... #&gt; $ dp_reg : num 1 1 1 1 1 1 4 5 3 6 ... #&gt; $ dp_cas : num 0 0 0 0 0 0 1 2 2 3 ... #&gt; $ temp_reg : Factor w/ 4 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;: 1 1 1 1 1 1 1 1 2 1 ... #&gt; $ temp_cas : Factor w/ 4 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;: 1 1 1 1 1 1 1 1 1 1 ... #&gt; $ month : Factor w/ 12 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... #&gt; $ year_part : num 1 1 1 1 1 1 1 1 1 1 ... #&gt; $ day_type : Factor w/ 7 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 1 1 1 1 1 1 4 5 3 6 ... #&gt; $ weekend : num 1 1 1 1 1 1 1 1 1 0 ... As we know that dependent variables have natural outliers so we will predict log of dependent variables. Predict bike demand registered and casual users separately. \\(y1 = \\log(casual+1)\\) and \\(y2 = \\log(registered+1)\\), Here we have added 1 to deal with zero values in the casual and registered columns. # separate again as train and test set train = data[as.integer(substr(data$datetime, 9, 10)) &lt; 20,] test = data[as.integer(substr(data$datetime, 9, 10)) &gt; 19,] 16.6.2 Log transform # log transformation for some skewed variables, # which can be seen from their distribution train$reg1 = train$registered + 1 train$cas1 = train$casual + 1 train$logcas = log(train$cas1) train$logreg = log(train$reg1) test$logreg = 0 test$logcas = 0 16.6.2.1 Plot by weather, by season # cartesian plot par(mfrow=c(2,1)) boxplot(train$registered ~ train$weather, xlab=&quot;weather&quot;, ylab=&quot;registered users&quot;) boxplot(train$registered ~ train$season, xlab=&quot;season&quot;, ylab=&quot;registered users&quot;) # semilog plot par(mfrow=c(2,1)) boxplot(train$logreg ~ train$weather, xlab = &quot;weather&quot;) boxplot(train$logreg ~ train$season, xlab = &quot;season&quot;) 16.6.3 Predicting for registered and casual users, test dataset tic() # final model building using random forest # note that we build different models for predicting for # registered and casual users # this was seen as giving best result after a lot of experimentation set.seed(415) fit1 &lt;- randomForest(logreg ~ hour + workingday + day + holiday + day_type + temp_reg + humidity + atemp + windspeed + season + weather + dp_reg + weekend + year + year_part, data = train, importance = TRUE, ntree = 250) pred1 = predict(fit1, test) test$logreg = pred1 toc() #&gt; 125.798 sec elapsed # casual users set.seed(415) fit2 &lt;- randomForest(logcas ~ hour + day_type + day + humidity + atemp + temp_cas + windspeed + season + weather + holiday + workingday + dp_cas + weekend + year + year_part, data = train, importance = TRUE, ntree = 250) pred2 = predict(fit2, test) test$logcas = pred2 16.6.4 Preparing and exporting results # creating the final submission file # reverse log conversion test$registered &lt;- exp(test$logreg) - 1 test$casual &lt;- exp(test$logcas) - 1 test$count &lt;- test$casual + test$registered r &lt;- data.frame(datetime = test$datetime, casual = test$casual, registered = test$registered) print(sum(r$casual)) #&gt; [1] 205804 print(sum(r$registered)) #&gt; [1] 962834 s &lt;- data.frame(datetime = test$datetime, count = test$count) write.csv(s, file =file.path(data_out_dir, &quot;bike-submit.csv&quot;), row.names = FALSE) # sum(cas+reg) = 1168638 # month number now is correct After following the steps mentioned above, you can score 0.38675 on Kaggle leaderboard i.e. top 5 percentile of total participants. As you might have seen, we have not applied any extraordinary science in getting to this level. But, the real competition starts here. I would like to see, if I can improve this further by use of more features and some more advanced modeling techniques. 16.7 End Notes In this article, we have looked at structured approach of problem solving and how this method can help you to improve performance. I would recommend you to generate hypothesis before you deep dive in the data set as this technique will not limit your thought process. You can improve your performance by applying advanced techniques (or ensemble methods) and understand your data trend better. You can find the complete solution here : GitHub Link # this is the older submission. months were incomplete old &lt;- read.csv(file = file.path(data_raw_dir, &quot;bike-submit-old.csv&quot;)) sum(old$count) #&gt; [1] 1164829 "],
["breast-cancer-wisconsin.html", "Chapter 17 Breast Cancer Wisconsin 17.1 Read and process the data 17.2 Principal Component Analysis (PCA) 17.3 Feature importance 17.4 Feature Selection 17.5 Model comparison 17.6 Create comparison tables 17.7 Notes", " Chapter 17 Breast Cancer Wisconsin Source: https://shiring.github.io/machine_learning/2017/01/15/rfe_ga_post 17.1 Read and process the data bc_data &lt;- read.table(file.path(data_raw_dir, &quot;breast-cancer-wisconsin.data&quot;), header = FALSE, sep = &quot;,&quot;) # assign the column names colnames(bc_data) &lt;- c(&quot;sample_code_number&quot;, &quot;clump_thickness&quot;, &quot;uniformity_of_cell_size&quot;, &quot;uniformity_of_cell_shape&quot;, &quot;marginal_adhesion&quot;, &quot;single_epithelial_cell_size&quot;, &quot;bare_nuclei&quot;, &quot;bland_chromatin&quot;, &quot;normal_nucleoli&quot;, &quot;mitosis&quot;, &quot;classes&quot;) # change classes from numeric to character bc_data$classes &lt;- ifelse(bc_data$classes == &quot;2&quot;, &quot;benign&quot;, ifelse(bc_data$classes == &quot;4&quot;, &quot;malignant&quot;, NA)) # if query sign make NA bc_data[bc_data == &quot;?&quot;] &lt;- NA # how many NAs are in the data length(which(is.na(bc_data))) #&gt; [1] 16 names(bc_data) #&gt; [1] &quot;sample_code_number&quot; &quot;clump_thickness&quot; #&gt; [3] &quot;uniformity_of_cell_size&quot; &quot;uniformity_of_cell_shape&quot; #&gt; [5] &quot;marginal_adhesion&quot; &quot;single_epithelial_cell_size&quot; #&gt; [7] &quot;bare_nuclei&quot; &quot;bland_chromatin&quot; #&gt; [9] &quot;normal_nucleoli&quot; &quot;mitosis&quot; #&gt; [11] &quot;classes&quot; 17.1.1 Missing data # impute missing data library(mice) #&gt; Loading required package: lattice #&gt; #&gt; Attaching package: &#39;mice&#39; #&gt; The following objects are masked from &#39;package:base&#39;: #&gt; #&gt; cbind, rbind # skip these columns: sample_code_number and classes # convert to numeric bc_data[,2:10] &lt;- apply(bc_data[, 2:10], 2, function(x) as.numeric(as.character(x))) # impute but mute dataset_impute &lt;- mice(bc_data[, 2:10], print = FALSE) # bind &quot;classes&quot; with the rest. skip &quot;sample_code_number&quot; bc_data &lt;- cbind(bc_data[, 11, drop = FALSE], mice::complete(dataset_impute, action =1)) bc_data$classes &lt;- as.factor(bc_data$classes) # how many benign and malignant cases are there? summary(bc_data$classes) #&gt; benign malignant #&gt; 458 241 # confirm NAs have been removed length(which(is.na(bc_data))) #&gt; [1] 0 str(bc_data) #&gt; &#39;data.frame&#39;: 699 obs. of 10 variables: #&gt; $ classes : Factor w/ 2 levels &quot;benign&quot;,&quot;malignant&quot;: 1 1 1 1 1 2 1 1 1 1 ... #&gt; $ clump_thickness : num 5 5 3 6 4 8 1 2 2 4 ... #&gt; $ uniformity_of_cell_size : num 1 4 1 8 1 10 1 1 1 2 ... #&gt; $ uniformity_of_cell_shape : num 1 4 1 8 1 10 1 2 1 1 ... #&gt; $ marginal_adhesion : num 1 5 1 1 3 8 1 1 1 1 ... #&gt; $ single_epithelial_cell_size: num 2 7 2 3 2 7 2 2 2 2 ... #&gt; $ bare_nuclei : num 1 10 2 4 1 10 10 1 1 1 ... #&gt; $ bland_chromatin : num 3 3 3 3 3 9 3 3 1 2 ... #&gt; $ normal_nucleoli : num 1 2 1 7 1 7 1 1 1 1 ... #&gt; $ mitosis : num 1 1 1 1 1 1 1 1 5 1 ... 17.2 Principal Component Analysis (PCA) To get an idea about the dimensionality and variance of the datasets, I am first looking at PCA plots for samples and features. The first two principal components (PCs) show the two components that explain the majority of variation in the data. After defining my custom ggplot2 theme, I am creating a function that performs the PCA (using the pcaGoPromoter package), calculates ellipses of the data points (with the ellipse package) and produces the plot with ggplot2. Some of the features in datasets 2 and 3 are not very distinct and overlap in the PCA plots, therefore I am also plotting hierarchical clustering dendrograms. 17.2.0.1 theme # plotting theme library(ggplot2) #&gt; Registered S3 methods overwritten by &#39;ggplot2&#39;: #&gt; method from #&gt; [.quosures rlang #&gt; c.quosures rlang #&gt; print.quosures rlang my_theme &lt;- function(base_size = 12, base_family = &quot;sans&quot;){ theme_minimal(base_size = base_size, base_family = base_family) + theme( axis.text = element_text(size = 12), axis.text.x = element_text(angle = 0, vjust = 0.5, hjust = 0.5), axis.title = element_text(size = 14), panel.grid.major = element_line(color = &quot;grey&quot;), panel.grid.minor = element_blank(), panel.background = element_rect(fill = &quot;aliceblue&quot;), strip.background = element_rect(fill = &quot;navy&quot;, color = &quot;navy&quot;, size = 1), strip.text = element_text(face = &quot;bold&quot;, size = 12, color = &quot;white&quot;), legend.position = &quot;right&quot;, legend.justification = &quot;top&quot;, legend.background = element_blank(), panel.border = element_rect(color = &quot;grey&quot;, fill = NA, size = 0.5) ) } theme_set(my_theme()) 17.2.0.2 PCA function # function for PCA plotting library(pcaGoPromoter) # install from BioConductor #&gt; Loading required package: ellipse #&gt; #&gt; Attaching package: &#39;ellipse&#39; #&gt; The following object is masked from &#39;package:graphics&#39;: #&gt; #&gt; pairs #&gt; Loading required package: Biostrings #&gt; Loading required package: BiocGenerics #&gt; Loading required package: parallel #&gt; #&gt; Attaching package: &#39;BiocGenerics&#39; #&gt; The following objects are masked from &#39;package:parallel&#39;: #&gt; #&gt; clusterApply, clusterApplyLB, clusterCall, clusterEvalQ, #&gt; clusterExport, clusterMap, parApply, parCapply, parLapply, #&gt; parLapplyLB, parRapply, parSapply, parSapplyLB #&gt; The following objects are masked from &#39;package:mice&#39;: #&gt; #&gt; cbind, rbind #&gt; The following objects are masked from &#39;package:stats&#39;: #&gt; #&gt; IQR, mad, sd, var, xtabs #&gt; The following objects are masked from &#39;package:base&#39;: #&gt; #&gt; anyDuplicated, append, as.data.frame, basename, cbind, #&gt; colnames, dirname, do.call, duplicated, eval, evalq, Filter, #&gt; Find, get, grep, grepl, intersect, is.unsorted, lapply, Map, #&gt; mapply, match, mget, order, paste, pmax, pmax.int, pmin, #&gt; pmin.int, Position, rank, rbind, Reduce, rownames, sapply, #&gt; setdiff, sort, table, tapply, union, unique, unsplit, which, #&gt; which.max, which.min #&gt; Loading required package: S4Vectors #&gt; Loading required package: stats4 #&gt; #&gt; Attaching package: &#39;S4Vectors&#39; #&gt; The following object is masked from &#39;package:base&#39;: #&gt; #&gt; expand.grid #&gt; Loading required package: IRanges #&gt; Loading required package: XVector #&gt; #&gt; Attaching package: &#39;Biostrings&#39; #&gt; The following object is masked from &#39;package:base&#39;: #&gt; #&gt; strsplit library(ellipse) pca_func &lt;- function(data, groups, title, print_ellipse = TRUE) { # perform pca and extract scores for all principal components: PC1:PC9 pcaOutput &lt;- pca(data, printDropped = FALSE, scale = TRUE, center = TRUE) pcaOutput2 &lt;- as.data.frame(pcaOutput$scores) # define groups for plotting. will group the classes pcaOutput2$groups &lt;- groups # when plotting samples calculate ellipses for plotting # (when plotting features, there are no replicates) if (print_ellipse) { # group and summarize by classes: benign, malignant # centroids w/3 columns: groups, PC1, PC2 centroids &lt;- aggregate(cbind(PC1, PC2) ~ groups, pcaOutput2, mean) # bind for the two groups (classes) # conf.rgn w/3 columns: groups, PC1, PC2 conf.rgn &lt;- do.call(rbind, lapply(unique(pcaOutput2$groups), function(t) data.frame(groups = as.character(t), # ellipse data for PC1 and PC2 ellipse(cov(pcaOutput2[pcaOutput2$groups == t, 1:2]), centre = as.matrix(centroids[centroids$groups == t, 2:3]), level = 0.95), stringsAsFactors = FALSE))) plot &lt;- ggplot(data = pcaOutput2, aes(x = PC1, y = PC2, group = groups, color = groups)) + geom_polygon(data = conf.rgn, aes(fill = groups), alpha = 0.2) + # ellipses geom_point(size = 2, alpha = 0.6) + scale_color_brewer(palette = &quot;Set1&quot;) + labs(title = title, color = &quot;&quot;, fill = &quot;&quot;, x = paste0(&quot;PC1: &quot;, round(pcaOutput$pov[1], digits = 2) * 100, &quot;% variance&quot;), y = paste0(&quot;PC2: &quot;, round(pcaOutput$pov[2], digits = 2) * 100, &quot;% variance&quot;)) } else { # if &lt; 10 groups (e.g. the predictor classes) have colors from RColorBrewer if (length(unique(pcaOutput2$groups)) &lt;= 10) { plot &lt;- ggplot(data = pcaOutput2, aes(x = PC1, y = PC2, group = groups, color = groups)) + geom_point(size = 2, alpha = 0.6) + scale_color_brewer(palette = &quot;Set1&quot;) + labs(title = title, color = &quot;&quot;, fill = &quot;&quot;, x = paste0(&quot;PC1: &quot;, round(pcaOutput$pov[1], digits = 2) * 100, &quot;% variance&quot;), y = paste0(&quot;PC2: &quot;, round(pcaOutput$pov[2], digits = 2) * 100, &quot;% variance&quot;)) } else { # otherwise use the default rainbow colors plot &lt;- ggplot(data = pcaOutput2, aes(x = PC1, y = PC2, group = groups, color = groups)) + geom_point(size = 2, alpha = 0.6) + labs(title = title, color = &quot;&quot;, fill = &quot;&quot;, x = paste0(&quot;PC1: &quot;, round(pcaOutput$pov[1], digits = 2) * 100, &quot;% variance&quot;), y = paste0(&quot;PC2: &quot;, round(pcaOutput$pov[2], digits = 2) * 100, &quot;% variance&quot;)) } } return(plot) } library(gridExtra) #&gt; #&gt; Attaching package: &#39;gridExtra&#39; #&gt; The following object is masked from &#39;package:BiocGenerics&#39;: #&gt; #&gt; combine library(grid) # plot all data. one row is a feature p1 &lt;- pca_func(data = t(bc_data[, 2:10]), groups = as.character(bc_data$classes), title = &quot;Breast cancer dataset 1: Samples&quot;) # plot features only. features as columns p2 &lt;- pca_func(data = bc_data[, 2:10], groups = as.character(colnames(bc_data[, 2:10])), title = &quot;Breast cancer dataset 1: Features&quot;, print_ellipse = FALSE) grid.arrange(p1, p2, ncol = 2) h_1 &lt;- hclust(dist(t(bc_data[, 2:10]), method = &quot;euclidean&quot;), method = &quot;complete&quot;) plot(h_1) 17.2.1 density plots vs class # density plot showing the feature vs classes library(tidyr) #&gt; #&gt; Attaching package: &#39;tidyr&#39; #&gt; The following object is masked from &#39;package:S4Vectors&#39;: #&gt; #&gt; expand #&gt; The following object is masked from &#39;package:mice&#39;: #&gt; #&gt; complete # gather data. from column clump_thickness to mitosis bc_data_gather &lt;- bc_data %&gt;% gather(measure, value, clump_thickness:mitosis) ggplot(data = bc_data_gather, aes(x = value, fill = classes, color = classes)) + geom_density(alpha = 0.3, size = 1) + geom_rug() + scale_fill_brewer(palette = &quot;Set1&quot;) + scale_color_brewer(palette = &quot;Set1&quot;) + facet_wrap( ~ measure, scales = &quot;free_y&quot;, ncol = 3) 17.3 Feature importance To get an idea about the feature’s respective importances, I’m running Random Forest models with 10 x 10 cross validation using the caret package. If I wanted to use feature importance to select features for modeling, I would need to perform it on the training data instead of on the complete dataset. But here, I only want to use it to get acquainted with my data. I am again defining a function that estimates the feature importance and produces a plot. library(caret) # library(doParallel) # parallel processing # registerDoParallel() # prepare training scheme control &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 10, repeats = 10) feature_imp &lt;- function(model, title) { # estimate variable importance importance &lt;- varImp(model, scale = TRUE) # prepare dataframes for plotting importance_df_1 &lt;- importance$importance importance_df_1$group &lt;- rownames(importance_df_1) importance_df_2 &lt;- importance_df_1 importance_df_2$Overall &lt;- 0 importance_df &lt;- rbind(importance_df_1, importance_df_2) plot &lt;- ggplot() + geom_point(data = importance_df_1, aes(x = Overall, y = group, color = group), size = 2) + geom_path(data = importance_df, aes(x = Overall, y = group, color = group, group = group), size = 1) + theme(legend.position = &quot;none&quot;) + labs( x = &quot;Importance&quot;, y = &quot;&quot;, title = title, subtitle = &quot;Scaled feature importance&quot;, caption = &quot;\\nDetermined with Random Forest and repeated cross validation (10 repeats, 10 times)&quot; ) return(plot) } # train the model set.seed(27) imp_1 &lt;- train(classes ~ ., data = bc_data, method = &quot;rf&quot;, preProcess = c(&quot;scale&quot;, &quot;center&quot;), trControl = control) p1 &lt;- feature_imp(imp_1, title = &quot;Breast cancer dataset 1&quot;) p1 17.4 Feature Selection By correlation By Recursive Feature Elimination By Genetic Algorithm set.seed(27) bc_data_index &lt;- createDataPartition(bc_data$classes, p = 0.7, list = FALSE) bc_data_train &lt;- bc_data[bc_data_index, ] bc_data_test &lt;- bc_data[-bc_data_index, ] 17.4.1 Correlation library(corrplot) #&gt; corrplot 0.84 loaded # calculate correlation matrix corMatMy &lt;- cor(bc_data_train[, -1]) corrplot(corMatMy, order = &quot;hclust&quot;) # Apply correlation filter at 0.70, highlyCor &lt;- colnames(bc_data_train[, -1])[findCorrelation(corMatMy, cutoff = 0.7, verbose = TRUE)] #&gt; Compare row 2 and column 3 with corr 0.9 #&gt; Means: 0.709 vs 0.595 so flagging column 2 #&gt; Compare row 3 and column 7 with corr 0.737 #&gt; Means: 0.674 vs 0.572 so flagging column 3 #&gt; All correlations &lt;= 0.7 # which variables are flagged for removal? highlyCor #&gt; [1] &quot;uniformity_of_cell_size&quot; &quot;uniformity_of_cell_shape&quot; # then we remove these variables bc_data_cor &lt;- bc_data_train[, which(!colnames(bc_data_train) %in% highlyCor)] names(bc_data_cor) #&gt; [1] &quot;classes&quot; &quot;clump_thickness&quot; #&gt; [3] &quot;marginal_adhesion&quot; &quot;single_epithelial_cell_size&quot; #&gt; [5] &quot;bare_nuclei&quot; &quot;bland_chromatin&quot; #&gt; [7] &quot;normal_nucleoli&quot; &quot;mitosis&quot; # confirm features were removed outersect &lt;- function(x, y) { sort(c(setdiff(x, y), setdiff(y, x))) } outersect(names(bc_data_cor), names(bc_data_train)) #&gt; [1] &quot;uniformity_of_cell_shape&quot; &quot;uniformity_of_cell_size&quot; Four features removed 17.4.2 Recursive Feature Elimination (RFE) # ensure the results are repeatable set.seed(7) # define the control using a random forest selection function with cross validation control &lt;- rfeControl(functions = rfFuncs, method = &quot;cv&quot;, number = 10) # run the RFE algorithm results_1 &lt;- rfe(x = bc_data_train[, -1], y = bc_data_train$classes, sizes = c(1:9), rfeControl = control) # chosen features predictors(results_1) #&gt; [1] &quot;bare_nuclei&quot; &quot;clump_thickness&quot; #&gt; [3] &quot;normal_nucleoli&quot; &quot;uniformity_of_cell_size&quot; #&gt; [5] &quot;uniformity_of_cell_shape&quot; &quot;single_epithelial_cell_size&quot; #&gt; [7] &quot;bland_chromatin&quot; &quot;marginal_adhesion&quot; # subset the chosen features sel_cols &lt;- which(colnames(bc_data_train) %in% predictors(results_1)) bc_data_rfe &lt;- bc_data_train[, c(1, sel_cols)] names(bc_data_rfe) #&gt; [1] &quot;classes&quot; &quot;clump_thickness&quot; #&gt; [3] &quot;uniformity_of_cell_size&quot; &quot;uniformity_of_cell_shape&quot; #&gt; [5] &quot;marginal_adhesion&quot; &quot;single_epithelial_cell_size&quot; #&gt; [7] &quot;bare_nuclei&quot; &quot;bland_chromatin&quot; #&gt; [9] &quot;normal_nucleoli&quot; # confirm features removed by RFE outersect(names(bc_data_rfe), names(bc_data_train)) #&gt; [1] &quot;mitosis&quot; No features removed with RFE 17.4.3 Genetic Algorithm (GA) library(dplyr) #&gt; #&gt; Attaching package: &#39;dplyr&#39; #&gt; The following object is masked from &#39;package:gridExtra&#39;: #&gt; #&gt; combine #&gt; The following objects are masked from &#39;package:Biostrings&#39;: #&gt; #&gt; collapse, intersect, setdiff, setequal, union #&gt; The following object is masked from &#39;package:XVector&#39;: #&gt; #&gt; slice #&gt; The following objects are masked from &#39;package:IRanges&#39;: #&gt; #&gt; collapse, desc, intersect, setdiff, slice, union #&gt; The following objects are masked from &#39;package:S4Vectors&#39;: #&gt; #&gt; first, intersect, rename, setdiff, setequal, union #&gt; The following objects are masked from &#39;package:BiocGenerics&#39;: #&gt; #&gt; combine, intersect, setdiff, union #&gt; The following objects are masked from &#39;package:stats&#39;: #&gt; #&gt; filter, lag #&gt; The following objects are masked from &#39;package:base&#39;: #&gt; #&gt; intersect, setdiff, setequal, union ga_ctrl &lt;- gafsControl(functions = rfGA, # Assess fitness with RF method = &quot;cv&quot;, # 10 fold cross validation genParallel = TRUE, # Use parallel programming allowParallel = TRUE) lev &lt;- c(&quot;malignant&quot;, &quot;benign&quot;) # Set the levels set.seed(27) model_1 &lt;- gafs(x = bc_data_train[, -1], y = bc_data_train$classes, iters = 10, # generations of algorithm popSize = 5, # population size for each generation levels = lev, gafsControl = ga_ctrl) #&gt; #&gt; Attaching package: &#39;recipes&#39; #&gt; The following object is masked from &#39;package:stats&#39;: #&gt; #&gt; step plot(model_1) # Plot mean fitness (AUC) by generation # features model_1$ga$final #&gt; [1] &quot;clump_thickness&quot; &quot;uniformity_of_cell_size&quot; #&gt; [3] &quot;uniformity_of_cell_shape&quot; &quot;marginal_adhesion&quot; #&gt; [5] &quot;bare_nuclei&quot; &quot;bland_chromatin&quot; #&gt; [7] &quot;normal_nucleoli&quot; # select features sel_cols_ga &lt;- which(colnames(bc_data_train) %in% model_1$ga$final) bc_data_ga &lt;- bc_data_train[, c(1, sel_cols_ga)] names(bc_data_ga) #&gt; [1] &quot;classes&quot; &quot;clump_thickness&quot; #&gt; [3] &quot;uniformity_of_cell_size&quot; &quot;uniformity_of_cell_shape&quot; #&gt; [5] &quot;marginal_adhesion&quot; &quot;bare_nuclei&quot; #&gt; [7] &quot;bland_chromatin&quot; &quot;normal_nucleoli&quot; # features removed GA outersect(names(bc_data_ga), names(bc_data_train)) #&gt; [1] &quot;mitosis&quot; &quot;single_epithelial_cell_size&quot; Two features removed with GA. 17.5 Model comparison 17.5.1 Using all features set.seed(27) model_bc_data_all &lt;- train(classes ~ ., data = bc_data_train, method = &quot;rf&quot;, preProcess = c(&quot;scale&quot;, &quot;center&quot;), trControl = trainControl(method = &quot;repeatedcv&quot;, number = 5, repeats = 10, verboseIter = FALSE)) # confusion matrix cm_all_1 &lt;- confusionMatrix(predict(model_bc_data_all, bc_data_test[, -1]), bc_data_test$classes) cm_all_1 #&gt; Confusion Matrix and Statistics #&gt; #&gt; Reference #&gt; Prediction benign malignant #&gt; benign 131 2 #&gt; malignant 6 70 #&gt; #&gt; Accuracy : 0.962 #&gt; 95% CI : (0.926, 0.983) #&gt; No Information Rate : 0.656 #&gt; P-Value [Acc &gt; NIR] : &lt;2e-16 #&gt; #&gt; Kappa : 0.916 #&gt; #&gt; Mcnemar&#39;s Test P-Value : 0.289 #&gt; #&gt; Sensitivity : 0.956 #&gt; Specificity : 0.972 #&gt; Pos Pred Value : 0.985 #&gt; Neg Pred Value : 0.921 #&gt; Prevalence : 0.656 #&gt; Detection Rate : 0.627 #&gt; Detection Prevalence : 0.636 #&gt; Balanced Accuracy : 0.964 #&gt; #&gt; &#39;Positive&#39; Class : benign #&gt; 17.5.2 Compare selection methods # compare features selected by the three methods library(gplots) #&gt; #&gt; Attaching package: &#39;gplots&#39; #&gt; The following object is masked from &#39;package:IRanges&#39;: #&gt; #&gt; space #&gt; The following object is masked from &#39;package:S4Vectors&#39;: #&gt; #&gt; space #&gt; The following object is masked from &#39;package:stats&#39;: #&gt; #&gt; lowess venn_list &lt;- list(cor = colnames(bc_data_cor)[-1], rfe = colnames(bc_data_rfe)[-1], ga = colnames(bc_data_ga)[-1]) venn &lt;- venn(venn_list) venn #&gt; num cor rfe ga #&gt; 000 0 0 0 0 #&gt; 001 0 0 0 1 #&gt; 010 0 0 1 0 #&gt; 011 2 0 1 1 #&gt; 100 1 1 0 0 #&gt; 101 0 1 0 1 #&gt; 110 1 1 1 0 #&gt; 111 5 1 1 1 #&gt; attr(,&quot;intersections&quot;) #&gt; attr(,&quot;intersections&quot;)$`cor:rfe:ga` #&gt; [1] &quot;clump_thickness&quot; &quot;marginal_adhesion&quot; &quot;bare_nuclei&quot; #&gt; [4] &quot;bland_chromatin&quot; &quot;normal_nucleoli&quot; #&gt; #&gt; attr(,&quot;intersections&quot;)$cor #&gt; [1] &quot;mitosis&quot; #&gt; #&gt; attr(,&quot;intersections&quot;)$`rfe:ga` #&gt; [1] &quot;uniformity_of_cell_size&quot; &quot;uniformity_of_cell_shape&quot; #&gt; #&gt; attr(,&quot;intersections&quot;)$`cor:rfe` #&gt; [1] &quot;single_epithelial_cell_size&quot; #&gt; #&gt; attr(,&quot;class&quot;) #&gt; [1] &quot;venn&quot; 4 out of 10 features were chosen by all three methods; the biggest overlap is seen between GA and RFE with 7 features. RFE and GA both retained 8 features for modeling, compared to only 5 based on the correlation method. 17.5.3 Correlation # correlation set.seed(127) model_bc_data_cor &lt;- train(classes ~ ., data = bc_data_cor, method = &quot;rf&quot;, preProcess = c(&quot;scale&quot;, &quot;center&quot;), trControl = trainControl(method = &quot;repeatedcv&quot;, number = 5, repeats = 10, verboseIter = FALSE)) cm_cor_1 &lt;- confusionMatrix(predict(model_bc_data_cor, bc_data_test[, -1]), bc_data_test$classes) cm_cor_1 #&gt; Confusion Matrix and Statistics #&gt; #&gt; Reference #&gt; Prediction benign malignant #&gt; benign 130 4 #&gt; malignant 7 68 #&gt; #&gt; Accuracy : 0.947 #&gt; 95% CI : (0.908, 0.973) #&gt; No Information Rate : 0.656 #&gt; P-Value [Acc &gt; NIR] : &lt;2e-16 #&gt; #&gt; Kappa : 0.885 #&gt; #&gt; Mcnemar&#39;s Test P-Value : 0.546 #&gt; #&gt; Sensitivity : 0.949 #&gt; Specificity : 0.944 #&gt; Pos Pred Value : 0.970 #&gt; Neg Pred Value : 0.907 #&gt; Prevalence : 0.656 #&gt; Detection Rate : 0.622 #&gt; Detection Prevalence : 0.641 #&gt; Balanced Accuracy : 0.947 #&gt; #&gt; &#39;Positive&#39; Class : benign #&gt; 17.5.4 Recursive Feature Elimination set.seed(127) model_bc_data_rfe &lt;- train(classes ~ ., data = bc_data_rfe, method = &quot;rf&quot;, preProcess = c(&quot;scale&quot;, &quot;center&quot;), trControl = trainControl(method = &quot;repeatedcv&quot;, number = 5, repeats = 10, verboseIter = FALSE)) cm_rfe_1 &lt;- confusionMatrix(predict(model_bc_data_rfe, bc_data_test[, -1]), bc_data_test$classes) cm_rfe_1 #&gt; Confusion Matrix and Statistics #&gt; #&gt; Reference #&gt; Prediction benign malignant #&gt; benign 130 3 #&gt; malignant 7 69 #&gt; #&gt; Accuracy : 0.952 #&gt; 95% CI : (0.914, 0.977) #&gt; No Information Rate : 0.656 #&gt; P-Value [Acc &gt; NIR] : &lt;2e-16 #&gt; #&gt; Kappa : 0.895 #&gt; #&gt; Mcnemar&#39;s Test P-Value : 0.343 #&gt; #&gt; Sensitivity : 0.949 #&gt; Specificity : 0.958 #&gt; Pos Pred Value : 0.977 #&gt; Neg Pred Value : 0.908 #&gt; Prevalence : 0.656 #&gt; Detection Rate : 0.622 #&gt; Detection Prevalence : 0.636 #&gt; Balanced Accuracy : 0.954 #&gt; #&gt; &#39;Positive&#39; Class : benign #&gt; 17.5.5 GA set.seed(127) model_bc_data_ga &lt;- train(classes ~ ., data = bc_data_ga, method = &quot;rf&quot;, preProcess = c(&quot;scale&quot;, &quot;center&quot;), trControl = trainControl(method = &quot;repeatedcv&quot;, number = 5, repeats = 10, verboseIter = FALSE)) cm_ga_1 &lt;- confusionMatrix(predict(model_bc_data_ga, bc_data_test[, -1]), bc_data_test$classes) cm_ga_1 #&gt; Confusion Matrix and Statistics #&gt; #&gt; Reference #&gt; Prediction benign malignant #&gt; benign 131 2 #&gt; malignant 6 70 #&gt; #&gt; Accuracy : 0.962 #&gt; 95% CI : (0.926, 0.983) #&gt; No Information Rate : 0.656 #&gt; P-Value [Acc &gt; NIR] : &lt;2e-16 #&gt; #&gt; Kappa : 0.916 #&gt; #&gt; Mcnemar&#39;s Test P-Value : 0.289 #&gt; #&gt; Sensitivity : 0.956 #&gt; Specificity : 0.972 #&gt; Pos Pred Value : 0.985 #&gt; Neg Pred Value : 0.921 #&gt; Prevalence : 0.656 #&gt; Detection Rate : 0.627 #&gt; Detection Prevalence : 0.636 #&gt; Balanced Accuracy : 0.964 #&gt; #&gt; &#39;Positive&#39; Class : benign #&gt; 17.6 Create comparison tables # take &quot;overall&quot; variable only from Confusion Matrix overall &lt;- data.frame(dataset = 1, model = rep(c(&quot;all&quot;, &quot;cor&quot;, &quot;rfe&quot;, &quot;ga&quot;), 1), rbind(cm_all_1$overall, cm_cor_1$overall, cm_rfe_1$overall, cm_ga_1$overall) ) # convert to tidy data library(tidyr) overall_gather &lt;- overall[, 1:4] %&gt;% # take the first columns: gather(measure, value, Accuracy:Kappa) # dataset, model, Accuracy and Kappa # take &quot;byClass&quot; variable only from Confusion Matrix byClass &lt;- data.frame(dataset = 1, model = rep(c(&quot;all&quot;, &quot;cor&quot;, &quot;rfe&quot;, &quot;ga&quot;), 1), rbind(cm_all_1$byClass, cm_cor_1$byClass, cm_rfe_1$byClass, cm_ga_1$byClass) ) # convert to tidy data byClass_gather &lt;- byClass[, c(1:4, 7)] %&gt;% # select columns: dataset, model gather(measure, value, Sensitivity:Precision) # Sensitiv, Specific, Precis # join the two tables overall_byClass_gather &lt;- rbind(overall_gather, byClass_gather) overall_byClass_gather &lt;- within( overall_byClass_gather, model &lt;- factor(model, levels = c(&quot;all&quot;, &quot;cor&quot;, &quot;rfe&quot;, &quot;ga&quot;))) # convert to factor ggplot(overall_byClass_gather, aes(x = model, y = value, color = measure, shape = measure, group = measure)) + geom_point(size = 4, alpha = 0.8) + geom_path(alpha = 0.7) + scale_colour_brewer(palette = &quot;Set1&quot;) + facet_grid(dataset ~ ., scales = &quot;free_y&quot;) + labs( x = &quot;Feature Selection method&quot;, y = &quot;Value&quot;, color = &quot;&quot;, shape = &quot;&quot;, title = &quot;Comparison of feature selection methods&quot;, subtitle = &quot;in three breast cancer datasets&quot;, caption = &quot;\\nBreast Cancer Wisconsin (Diagnostic) Data Sets: 1, 2 &amp; 3 Street et al., 1993; all: no feature selection cor: features with correlation &gt; 0.7 removed rfe: Recursive Feature Elimination ga: Genetic Algorithm&quot; ) Less accurate: selection of features by correlation More accurate: genetic algorithm Including all features is more accurate to removing features by correlation. 17.7 Notes pcaGoPromoter is a BioConductor package. Its dependencies are BioGenerics, AnnotationDbi and BioStrings, which at their turn require DBI and RSQLite packages from CRAN. Install first those from CRAN, and then move to install pcaGoPromoter. "],
["titanic-with-naive-bayes-classifier.html", "Chapter 18 Titanic with Naive-Bayes Classifier", " Chapter 18 Titanic with Naive-Bayes Classifier The Titanic dataset in R is a table for about 2200 passengers summarised according to four factors – economic status ranging from 1st class, 2nd class, 3rd class and crew; gender which is either male or female; Age category which is either Child or Adult and whether the type of passenger survived. For each combination of Age, Gender, Class and Survived status, the table gives the number of passengers who fall into the combination. We will use the Naive Bayes Technique to classify such passengers and check how well it performs. #Getting started with Naive Bayes #Install the package #install.packages(“e1071”) #Loading the library library(e1071) #Next load the Titanic dataset data(&quot;Titanic&quot;) #Save into a data frame and view it Titanic_df = as.data.frame(Titanic) We see that there are 32 observations which represent all possible combinations of Class, Sex, Age and Survived with their frequency. Since it is summarised, this table is not suitable for modelling purposes. We need to expand the table into individual rows. Let’s create a repeating sequence of rows based on the frequencies in the table #Creating data from table repeating_sequence=rep.int(seq_len(nrow(Titanic_df)), Titanic_df$Freq) #This will repeat each combination equal to the frequency of each combination # Create the dataset by row repetition created Titanic_dataset=Titanic_df[repeating_sequence,] # We no longer need the frequency, drop the feature Titanic_dataset$Freq=NULL The data is now ready for Naive Bayes to process. Let’s fit the model # Fitting the Naive Bayes model Naive_Bayes_Model=naiveBayes(Survived ~., data=Titanic_dataset) # What does the model say? Print the model summary Naive_Bayes_Model #&gt; #&gt; Naive Bayes Classifier for Discrete Predictors #&gt; #&gt; Call: #&gt; naiveBayes.default(x = X, y = Y, laplace = laplace) #&gt; #&gt; A-priori probabilities: #&gt; Y #&gt; No Yes #&gt; 0.677 0.323 #&gt; #&gt; Conditional probabilities: #&gt; Class #&gt; Y 1st 2nd 3rd Crew #&gt; No 0.0819 0.1121 0.3544 0.4517 #&gt; Yes 0.2855 0.1660 0.2504 0.2982 #&gt; #&gt; Sex #&gt; Y Male Female #&gt; No 0.9154 0.0846 #&gt; Yes 0.5162 0.4838 #&gt; #&gt; Age #&gt; Y Child Adult #&gt; No 0.0349 0.9651 #&gt; Yes 0.0802 0.9198 The model creates the conditional probability for each feature separately. We also have the a-priori probabilities which indicates the distribution of our data. Let’s calculate how we perform on the data. # Prediction on the dataset NB_Predictions=predict(Naive_Bayes_Model,Titanic_dataset) # Confusion matrix to check accuracy table(NB_Predictions,Titanic_dataset$Survived) #&gt; #&gt; NB_Predictions No Yes #&gt; No 1364 362 #&gt; Yes 126 349 We have the results! We are able to classify 1364 out of 1490 “No” cases correctly and 349 out of 711 “Yes” cases correctly. This means the ability of Naive Bayes algorithm to predict “No” cases is about 91.5% but it falls down to only 49% of the “Yes” cases resulting in an overall accuracy of 77.8% "],
["can-we-do-any-better.html", "Chapter 19 Can we Do any Better?", " Chapter 19 Can we Do any Better? Naive Bayes is a parametric algorithm which implies that you cannot perform differently in different runs as long as the data remains the same. We will, however, learn another implementation of Naive Bayes algorithm using the ‘mlr’ package. Assuming the same session is going on for the readers, I will install and load the package and start fitting a model # Getting started with Naive Bayes in mlr # install.packages(“mlr”) # Loading the library library(mlr) #&gt; Loading required package: ParamHelpers #&gt; Registered S3 methods overwritten by &#39;ggplot2&#39;: #&gt; method from #&gt; [.quosures rlang #&gt; c.quosures rlang #&gt; print.quosures rlang #&gt; #&gt; Attaching package: &#39;mlr&#39; #&gt; The following object is masked from &#39;package:e1071&#39;: #&gt; #&gt; impute The mlr package consists of a lot of models and works by creating tasks and learners which are then trained. Let’s create a classification task using the titanic dataset and fit a model with the naive bayes algorithm. # Create a classification task for learning on Titanic Dataset and specify the target feature task = makeClassifTask(data = Titanic_dataset, target = &quot;Survived&quot;) # Initialize the Naive Bayes classifier selected_model = makeLearner(&quot;classif.naiveBayes&quot;) # Train the model NB_mlr = train(selected_model, task) The summary of the model which was printed in e3071 package is stored in learner model. Let’s print it and compare # Read the model learned NB_mlr$learner.model #&gt; #&gt; Naive Bayes Classifier for Discrete Predictors #&gt; #&gt; Call: #&gt; naiveBayes.default(x = X, y = Y, laplace = laplace) #&gt; #&gt; A-priori probabilities: #&gt; Y #&gt; No Yes #&gt; 0.677 0.323 #&gt; #&gt; Conditional probabilities: #&gt; Class #&gt; Y 1st 2nd 3rd Crew #&gt; No 0.0819 0.1121 0.3544 0.4517 #&gt; Yes 0.2855 0.1660 0.2504 0.2982 #&gt; #&gt; Sex #&gt; Y Male Female #&gt; No 0.9154 0.0846 #&gt; Yes 0.5162 0.4838 #&gt; #&gt; Age #&gt; Y Child Adult #&gt; No 0.0349 0.9651 #&gt; Yes 0.0802 0.9198 The a-priori probabilities and the conditional probabilities for the model are similar to the one calculated by e3071 package as was expected. This means that our predictions will also be the same. # Predict on the dataset without passing the target feature predictions_mlr = as.data.frame(predict(NB_mlr, newdata = Titanic_dataset[,1:3])) ## Confusion matrix to check accuracy table(predictions_mlr[,1],Titanic_dataset$Survived) #&gt; #&gt; No Yes #&gt; No 1364 362 #&gt; Yes 126 349 As we see, the predictions are exactly same. The only way to improve is to have more features or more data. Perhaps, if we have more features such as the exact age, size of family, number of parents in the ship and siblings then we may arrive at a better model using Naive Bayes. In essence, Naive Bayes has an advantage of a strong foundation build and is very robust. I know of the ‘caret’ package which also consists of Naive Bayes function but it will also give us the same predictions and probability. "],
["building-a-naive-bayes-classifier-in-r.html", "Chapter 20 Building a Naive Bayes Classifier in R 20.1 8. Building a Naive Bayes Classifier in R", " Chapter 20 Building a Naive Bayes Classifier in R https://www.machinelearningplus.com/predictive-modeling/how-naive-bayes-algorithm-works-with-example-and-full-code/ 20.1 8. Building a Naive Bayes Classifier in R Understanding Naive Bayes was the (slightly) tricky part. Implementing it is fairly straightforward. In R, Naive Bayes classifier is implemented in packages such as e1071, klaR and bnlearn. In Python, it is implemented in scikit-learn. For sake of demonstration, let’s use the standard iris dataset to predict the Species of flower using 4 different features: Sepal.Length, Sepal.Width, Petal.Length, Petal.Width # Import Data training &lt;- read.csv(&#39;https://raw.githubusercontent.com/selva86/datasets/master/iris_train.csv&#39;) test &lt;- read.csv(&#39;https://raw.githubusercontent.com/selva86/datasets/master/iris_test.csv&#39;) The training data is now contained in training and test data in test dataframe. Lets load the klaR package and build the naive bayes model. # Using klaR for Naive Bayes library(klaR) #&gt; Loading required package: MASS nb_mod &lt;- NaiveBayes(Species ~ ., data=training) pred &lt;- predict(nb_mod, test) Lets see the confusion matrix. # Confusion Matrix tab &lt;- table(pred$class, test$Species) caret::confusionMatrix(tab) #&gt; Registered S3 methods overwritten by &#39;ggplot2&#39;: #&gt; method from #&gt; [.quosures rlang #&gt; c.quosures rlang #&gt; print.quosures rlang #&gt; Confusion Matrix and Statistics #&gt; #&gt; #&gt; setosa versicolor virginica #&gt; setosa 15 0 0 #&gt; versicolor 0 11 0 #&gt; virginica 0 4 15 #&gt; #&gt; Overall Statistics #&gt; #&gt; Accuracy : 0.911 #&gt; 95% CI : (0.788, 0.975) #&gt; No Information Rate : 0.333 #&gt; P-Value [Acc &gt; NIR] : 8.47e-16 #&gt; #&gt; Kappa : 0.867 #&gt; #&gt; Mcnemar&#39;s Test P-Value : NA #&gt; #&gt; Statistics by Class: #&gt; #&gt; Class: setosa Class: versicolor Class: virginica #&gt; Sensitivity 1.000 0.733 1.000 #&gt; Specificity 1.000 1.000 0.867 #&gt; Pos Pred Value 1.000 1.000 0.789 #&gt; Neg Pred Value 1.000 0.882 1.000 #&gt; Prevalence 0.333 0.333 0.333 #&gt; Detection Rate 0.333 0.244 0.333 #&gt; Detection Prevalence 0.333 0.244 0.422 #&gt; Balanced Accuracy 1.000 0.867 0.933 # Plot density of each feature using nb_mod opar = par(mfrow=c(2, 2), mar=c(4,0,0,0)) plot(nb_mod, main=&quot;&quot;) par(opar) # Plot the Confusion Matrix library(ggplot2) test$pred &lt;- pred$class ggplot(test, aes(Species, pred, color = Species)) + geom_jitter(width = 0.2, height = 0.1, size=2) + labs(title=&quot;Confusion Matrix&quot;, subtitle=&quot;Predicted vs. Observed from Iris dataset&quot;, y=&quot;Predicted&quot;, x=&quot;Truth&quot;, caption=&quot;machinelearningplus.com&quot;) "],
["employee-attrition-employee-attrition-dataset-lime-package.html", "Chapter 21 Employee attrition. Employee-Attrition dataset. LIME package 21.1 Introduction 21.2 Modeling Employee attrition 21.3 Model 21.4 Predict 21.5 Performance 21.6 The lime package 21.7 Feature Importance Visualization 21.8 Conclusions", " Chapter 21 Employee attrition. Employee-Attrition dataset. LIME package Article: https://www.business-science.io/business/2017/09/18/hr_employee_attrition.html Data: https://www.ibm.com/communities/analytics/watson-analytics-blog/hr-employee-attrition/ 21.1 Introduction 21.1.1 Employee attrition: a major problem Bill Gates was once quoted as saying, “You take away our top 20 employees and we [Microsoft] become a mediocre company”. His statement cuts to the core of a major problem: employee attrition. An organization is only as good as its employees, and these people are the true source of its competitive advantage. Organizations face huge costs resulting from employee turnover. Some costs are tangible such as training expenses and the time it takes from when an employee starts to when they become a productive member. However, the most important costs are intangible. Consider what’s lost when a productive employee quits: new product ideas, great project management, or customer relationships. With advances in machine learning and data science, its possible to not only predict employee attrition but to understand the key variables that influence turnover. We’ll take a look at two cutting edge techniques: Machine Learning with h2o.automl() from the h2o package: This function takes automated machine learning to the next level by testing a number of advanced algorithms such as random forests, ensemble methods, and deep learning along with more traditional algorithms such as logistic regression. The main takeaway is that we can now easily achieve predictive performance that is in the same ball park (and in some cases even better than) commercial algorithms and ML/AI software. Feature Importance with the lime package: The problem with advanced machine learning algorithms such as deep learning is that it’s near impossible to understand the algorithm because of its complexity. This has all changed with the lime package. The major advancement with lime is that, by recursively analyzing the models locally, it can extract feature importance that repeats globally. What this means to us is that lime has opened the door to understanding the ML models regardless of complexity. Now the best (and typically very complex) models can also be investigated and potentially understood as to what variables or features make the model tick. 21.1.2 Employee attrition: machine learning analysis With these new automated ML tools combined with tools to uncover critical variables, we now have capabilities for both extreme predictive accuracy and understandability, which was previously impossible! We’ll investigate an HR Analytic example of employee attrition that was evaluated by IBM Watson. 21.1.3 Where we got the data The example comes from IBM Watson Analytics website. You can download the data and read the analysis here: Get data used in this post here. Read IBM Watson Analytics article here. To summarize, the article makes a usage case for IBM Watson as an automated ML platform. The article shows that using Watson, the analyst was able to detect features that led to increased probability of attrition. 21.1.4 Automated machine learning (what we did with the data) In this example we’ll show how we can use the combination of H2O for developing a complex model with high predictive accuracy on unseen data and then how we can use LIME to understand important features related to employee attrition. 21.1.5 Load packages Load the following packages. # Load the following packages library(tidyquant) # Loads tidyverse and several other pkgs library(readxl) # Super simple excel reader library(h2o) # Professional grade ML pkg library(lime) # Explain complex black-box ML models 21.1.6 Load data Download the data here. You can load the data using read_excel(), pointing the path to your local file. # Read excel data hr_data_raw &lt;- read_excel(path = file.path(data_raw_dir, &quot;WA_Fn-UseC_-HR-Employee-Attrition.xlsx&quot;)) Let’s check out the raw data. It’s 1470 rows (observations) by 35 columns (features). The “Attrition” column is our target. We’ll use all other columns as features to our model. # View first 10 rows hr_data_raw[1:10,] %&gt;% knitr::kable(caption = &quot;First 10 rows&quot;) Table 21.1: First 10 rows Age Attrition BusinessTravel DailyRate Department DistanceFromHome Education EducationField EmployeeCount EmployeeNumber EnvironmentSatisfaction Gender HourlyRate JobInvolvement JobLevel JobRole JobSatisfaction MaritalStatus MonthlyIncome MonthlyRate NumCompaniesWorked Over18 OverTime PercentSalaryHike PerformanceRating RelationshipSatisfaction StandardHours StockOptionLevel TotalWorkingYears TrainingTimesLastYear WorkLifeBalance YearsAtCompany YearsInCurrentRole YearsSinceLastPromotion YearsWithCurrManager 41 Yes Travel_Rarely 1102 Sales 1 2 Life Sciences 1 1 2 Female 94 3 2 Sales Executive 4 Single 5993 19479 8 Y Yes 11 3 1 80 0 8 0 1 6 4 0 5 49 No Travel_Frequently 279 Research &amp; Development 8 1 Life Sciences 1 2 3 Male 61 2 2 Research Scientist 2 Married 5130 24907 1 Y No 23 4 4 80 1 10 3 3 10 7 1 7 37 Yes Travel_Rarely 1373 Research &amp; Development 2 2 Other 1 4 4 Male 92 2 1 Laboratory Technician 3 Single 2090 2396 6 Y Yes 15 3 2 80 0 7 3 3 0 0 0 0 33 No Travel_Frequently 1392 Research &amp; Development 3 4 Life Sciences 1 5 4 Female 56 3 1 Research Scientist 3 Married 2909 23159 1 Y Yes 11 3 3 80 0 8 3 3 8 7 3 0 27 No Travel_Rarely 591 Research &amp; Development 2 1 Medical 1 7 1 Male 40 3 1 Laboratory Technician 2 Married 3468 16632 9 Y No 12 3 4 80 1 6 3 3 2 2 2 2 32 No Travel_Frequently 1005 Research &amp; Development 2 2 Life Sciences 1 8 4 Male 79 3 1 Laboratory Technician 4 Single 3068 11864 0 Y No 13 3 3 80 0 8 2 2 7 7 3 6 59 No Travel_Rarely 1324 Research &amp; Development 3 3 Medical 1 10 3 Female 81 4 1 Laboratory Technician 1 Married 2670 9964 4 Y Yes 20 4 1 80 3 12 3 2 1 0 0 0 30 No Travel_Rarely 1358 Research &amp; Development 24 1 Life Sciences 1 11 4 Male 67 3 1 Laboratory Technician 3 Divorced 2693 13335 1 Y No 22 4 2 80 1 1 2 3 1 0 0 0 38 No Travel_Frequently 216 Research &amp; Development 23 3 Life Sciences 1 12 4 Male 44 2 3 Manufacturing Director 3 Single 9526 8787 0 Y No 21 4 2 80 0 10 2 3 9 7 1 8 36 No Travel_Rarely 1299 Research &amp; Development 27 3 Medical 1 13 3 Male 94 3 2 Healthcare Representative 3 Married 5237 16577 6 Y No 13 3 2 80 2 17 3 2 7 7 7 7 The only pre-processing we’ll do in this example is change all character data types to factors. This is needed for H2O. We could make a number of other numeric data that is actually categorical factors, but this tends to increase modeling time and can have little improvement on model performance. hr_data &lt;- hr_data_raw %&gt;% mutate_if(is.character, as.factor) %&gt;% select(Attrition, everything()) Let’s take a glimpse at the processed dataset. We can see all of the columns. Note our target (“Attrition”) is the first column. glimpse(hr_data) #&gt; Observations: 1,470 #&gt; Variables: 35 #&gt; $ Attrition &lt;fct&gt; Yes, No, Yes, No, No, No, No, No, No, N… #&gt; $ Age &lt;dbl&gt; 41, 49, 37, 33, 27, 32, 59, 30, 38, 36,… #&gt; $ BusinessTravel &lt;fct&gt; Travel_Rarely, Travel_Frequently, Trave… #&gt; $ DailyRate &lt;dbl&gt; 1102, 279, 1373, 1392, 591, 1005, 1324,… #&gt; $ Department &lt;fct&gt; Sales, Research &amp; Development, Research… #&gt; $ DistanceFromHome &lt;dbl&gt; 1, 8, 2, 3, 2, 2, 3, 24, 23, 27, 16, 15… #&gt; $ Education &lt;dbl&gt; 2, 1, 2, 4, 1, 2, 3, 1, 3, 3, 3, 2, 1, … #&gt; $ EducationField &lt;fct&gt; Life Sciences, Life Sciences, Other, Li… #&gt; $ EmployeeCount &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … #&gt; $ EmployeeNumber &lt;dbl&gt; 1, 2, 4, 5, 7, 8, 10, 11, 12, 13, 14, 1… #&gt; $ EnvironmentSatisfaction &lt;dbl&gt; 2, 3, 4, 4, 1, 4, 3, 4, 4, 3, 1, 4, 1, … #&gt; $ Gender &lt;fct&gt; Female, Male, Male, Female, Male, Male,… #&gt; $ HourlyRate &lt;dbl&gt; 94, 61, 92, 56, 40, 79, 81, 67, 44, 94,… #&gt; $ JobInvolvement &lt;dbl&gt; 3, 2, 2, 3, 3, 3, 4, 3, 2, 3, 4, 2, 3, … #&gt; $ JobLevel &lt;dbl&gt; 2, 2, 1, 1, 1, 1, 1, 1, 3, 2, 1, 2, 1, … #&gt; $ JobRole &lt;fct&gt; Sales Executive, Research Scientist, La… #&gt; $ JobSatisfaction &lt;dbl&gt; 4, 2, 3, 3, 2, 4, 1, 3, 3, 3, 2, 3, 3, … #&gt; $ MaritalStatus &lt;fct&gt; Single, Married, Single, Married, Marri… #&gt; $ MonthlyIncome &lt;dbl&gt; 5993, 5130, 2090, 2909, 3468, 3068, 267… #&gt; $ MonthlyRate &lt;dbl&gt; 19479, 24907, 2396, 23159, 16632, 11864… #&gt; $ NumCompaniesWorked &lt;dbl&gt; 8, 1, 6, 1, 9, 0, 4, 1, 0, 6, 0, 0, 1, … #&gt; $ Over18 &lt;fct&gt; Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, … #&gt; $ OverTime &lt;fct&gt; Yes, No, Yes, Yes, No, No, Yes, No, No,… #&gt; $ PercentSalaryHike &lt;dbl&gt; 11, 23, 15, 11, 12, 13, 20, 22, 21, 13,… #&gt; $ PerformanceRating &lt;dbl&gt; 3, 4, 3, 3, 3, 3, 4, 4, 4, 3, 3, 3, 3, … #&gt; $ RelationshipSatisfaction &lt;dbl&gt; 1, 4, 2, 3, 4, 3, 1, 2, 2, 2, 3, 4, 4, … #&gt; $ StandardHours &lt;dbl&gt; 80, 80, 80, 80, 80, 80, 80, 80, 80, 80,… #&gt; $ StockOptionLevel &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 3, 1, 0, 2, 1, 0, 1, … #&gt; $ TotalWorkingYears &lt;dbl&gt; 8, 10, 7, 8, 6, 8, 12, 1, 10, 17, 6, 10… #&gt; $ TrainingTimesLastYear &lt;dbl&gt; 0, 3, 3, 3, 3, 2, 3, 2, 2, 3, 5, 3, 1, … #&gt; $ WorkLifeBalance &lt;dbl&gt; 1, 3, 3, 3, 3, 2, 2, 3, 3, 2, 3, 3, 2, … #&gt; $ YearsAtCompany &lt;dbl&gt; 6, 10, 0, 8, 2, 7, 1, 1, 9, 7, 5, 9, 5,… #&gt; $ YearsInCurrentRole &lt;dbl&gt; 4, 7, 0, 7, 2, 7, 0, 0, 7, 7, 4, 5, 2, … #&gt; $ YearsSinceLastPromotion &lt;dbl&gt; 0, 1, 0, 3, 2, 3, 0, 0, 1, 7, 0, 0, 4, … #&gt; $ YearsWithCurrManager &lt;dbl&gt; 5, 7, 0, 0, 2, 6, 0, 0, 8, 7, 3, 8, 3, … 21.2 Modeling Employee attrition We are going to use the h2o.automl() function from the H2O platform to model employee attrition. 21.2.1 Machine Learning with h2o First, we need to initialize the Java Virtual Machine (JVM) that H2O uses locally. # Initialize H2O JVM h2o.init() #&gt; #&gt; H2O is not running yet, starting it now... #&gt; #&gt; Note: In case of errors look at the following log files: #&gt; /tmp/RtmpUt6yYm/h2o_datascience_started_from_r.out #&gt; /tmp/RtmpUt6yYm/h2o_datascience_started_from_r.err #&gt; #&gt; #&gt; Starting H2O JVM and connecting: . Connection successful! #&gt; #&gt; R is connected to the H2O cluster: #&gt; H2O cluster uptime: 1 seconds 196 milliseconds #&gt; H2O cluster timezone: America/Chicago #&gt; H2O data parsing timezone: UTC #&gt; H2O cluster version: 3.22.1.1 #&gt; H2O cluster version age: 8 months and 22 days !!! #&gt; H2O cluster name: H2O_started_from_R_datascience_mwl453 #&gt; H2O cluster total nodes: 1 #&gt; H2O cluster total memory: 6.96 GB #&gt; H2O cluster total cores: 8 #&gt; H2O cluster allowed cores: 8 #&gt; H2O cluster healthy: TRUE #&gt; H2O Connection ip: localhost #&gt; H2O Connection port: 54321 #&gt; H2O Connection proxy: NA #&gt; H2O Internal Security: FALSE #&gt; H2O API Extensions: XGBoost, Algos, AutoML, Core V3, Core V4 #&gt; R Version: R version 3.6.0 (2019-04-26) h2o.no_progress() # Turn off output of progress bars Next, we change our data to an h2o object that the package can interpret. We also split the data into training, validation, and test sets. Our preference is to use 70%, 15%, 15%, respectively. # Split data into Train/Validation/Test Sets hr_data_h2o &lt;- as.h2o(hr_data) split_h2o &lt;- h2o.splitFrame(hr_data_h2o, c(0.7, 0.15), seed = 1234 ) train_h2o &lt;- h2o.assign(split_h2o[[1]], &quot;train&quot; ) # 70% valid_h2o &lt;- h2o.assign(split_h2o[[2]], &quot;valid&quot; ) # 15% test_h2o &lt;- h2o.assign(split_h2o[[3]], &quot;test&quot; ) # 15% 21.3 Model Now we are ready to model. We’ll set the target and feature names. The target is what we aim to predict (in our case “Attrition”). The features (every other column) are what we will use to model the prediction. # Set names for h2o y &lt;- &quot;Attrition&quot; x &lt;- setdiff(names(train_h2o), y) Now the fun begins. We run the h2o.automl() setting the arguments it needs to run models against. For more information, see the h2o.automl documentation. x = x: The names of our feature columns. y = y: The name of our target column. training_frame = train_h2o: Our training set consisting of 70% of the data. leaderboard_frame = valid_h2o: Our validation set consisting of 15% of the data. H2O uses this to ensure the model does not overfit the data. max_runtime_secs = 30: We supply this to speed up H2O’s modeling. The algorithm has a large number of complex models so we want to keep things moving at the expense of some accuracy. # Run the automated machine learning automl_models_h2o &lt;- h2o.automl( x = x, y = y, training_frame = train_h2o, leaderboard_frame = valid_h2o, max_runtime_secs = 30 ) All of the models are stored the automl_models_h2o object. However, we are only concerned with the leader, which is the best model in terms of accuracy on the validation set. We’ll extract it from the models object. # Extract leader model automl_leader &lt;- automl_models_h2o@leader 21.4 Predict Now we are ready to predict on our test set, which is unseen from during our modeling process. This is the true test of performance. We use the h2o.predict() function to make predictions. # Predict on hold-out set, test_h2o pred_h2o &lt;- h2o.predict(object = automl_leader, newdata = test_h2o) 21.5 Performance Now we can evaluate our leader model. We’ll reformat the test set an add the predictions as column so we have the actual and prediction columns side-by-side. # Prep for performance assessment test_performance &lt;- test_h2o %&gt;% tibble::as_tibble() %&gt;% select(Attrition) %&gt;% add_column(pred = as.vector(pred_h2o$predict)) %&gt;% mutate_if(is.character, as.factor) test_performance #&gt; # A tibble: 211 x 2 #&gt; Attrition pred #&gt; &lt;fct&gt; &lt;fct&gt; #&gt; 1 No No #&gt; 2 No No #&gt; 3 Yes Yes #&gt; 4 No No #&gt; 5 No No #&gt; 6 No No #&gt; # … with 205 more rows We can use the table() function to quickly get a confusion table of the results. We see that the leader model wasn’t perfect, but it did a decent job identifying employees that are likely to quit. For perspective, a logistic regression would not perform nearly this well. # Confusion table counts confusion_matrix &lt;- test_performance %&gt;% table() confusion_matrix #&gt; pred #&gt; Attrition No Yes #&gt; No 169 13 #&gt; Yes 13 16 We’ll run through a binary classification analysis to understand the model performance. # Performance analysis tn &lt;- confusion_matrix[1] tp &lt;- confusion_matrix[4] fp &lt;- confusion_matrix[3] fn &lt;- confusion_matrix[2] accuracy &lt;- (tp + tn) / (tp + tn + fp + fn) misclassification_rate &lt;- 1 - accuracy recall &lt;- tp / (tp + fn) precision &lt;- tp / (tp + fp) null_error_rate &lt;- tn / (tp + tn + fp + fn) tibble( accuracy, misclassification_rate, recall, precision, null_error_rate ) %&gt;% transpose() #&gt; [[1]] #&gt; [[1]]$accuracy #&gt; [1] 0.877 #&gt; #&gt; [[1]]$misclassification_rate #&gt; [1] 0.123 #&gt; #&gt; [[1]]$recall #&gt; [1] 0.552 #&gt; #&gt; [[1]]$precision #&gt; [1] 0.552 #&gt; #&gt; [[1]]$null_error_rate #&gt; [1] 0.801 It is important to understand is that the accuracy can be misleading: 88% sounds pretty good especially for modeling HR data, but if we just pick Attrition = NO we would get an accuracy of about 79%. Doesn’t sound so great now. Before we make our final judgement, let’s dive a little deeper into precision and recall. Precision is when the model predicts yes, how often is it actually yes. Recall (also true positive rate or specificity) is when the actual value is yes how often is the model correct. Confused yet? Let’s explain in terms of what’s important to HR. Most HR groups would probably prefer to incorrectly classify folks not looking to quit as high potential of quiting rather than classify those that are likely to quit as not at risk. Because it’s important to not miss at risk employees, HR will really care about recall or when the actual value is Attrition = YES how often the model predicts YES. Recall for our model is 62%. In an HR context, this is 62% more employees that could potentially be targeted prior to quiting. From that standpoint, an organization that loses 100 people per year could possibly target 62 implementing measures to retain. 21.6 The lime package We have a very good model that is capable of making very accurate predictions on unseen data, but what can it tell us about what causes attrition? Let’s find out using LIME. 21.6.1 Set up The lime package implements LIME in R. One thing to note is that it’s not setup out-of-the-box to work with h2o. The good news is with a few functions we can get everything working properly. We’ll need to make two custom functions: model_type: Used to tell lime what type of model we are dealing with. It could be classification, regression, survival, etc. predict_model: Used to allow lime to perform predictions that its algorithm can interpret. The first thing we need to do is identify the class of our model leader object. We do this with the class() function. class(automl_leader) #&gt; [1] &quot;H2OBinomialModel&quot; #&gt; attr(,&quot;package&quot;) #&gt; [1] &quot;h2o&quot; Next we create our model_type function. It’s only input is x the h2o model. The function simply returns “classification”, which tells LIME we are classifying. # Setup lime::model_type() function for h2o model_type.H2OBinomialModel &lt;- function(x, ...) { # Function tells lime() what model type we are dealing with # &#39;classification&#39;, &#39;regression&#39;, &#39;survival&#39;, &#39;clustering&#39;, &#39;multilabel&#39;, etc # # x is our h2o model return(&quot;classification&quot;) } Now we can create our predict_model function. The trick here is to realize that it’s inputs must be x a model, newdata a dataframe object (this is important), and type which is not used but can be use to switch the output type. The output is also a little tricky because it must be in the format of probabilities by classification (this is important; shown next). Internally we just call the h2o.predict() function. # Setup lime::predict_model() function for h2o predict_model.H2OBinomialModel &lt;- function(x, newdata, type, ...) { # Function performs prediction and returns dataframe with Response # # x is h2o model # newdata is data frame # type is only setup for data frame pred &lt;- h2o.predict(x, as.h2o(newdata)) # return probs return(as.data.frame(pred[,-1])) } Run this next script to show you what the output looks like and to test our predict_model function. See how it’s the probabilities by classification. It must be in this form for model_type = “classification”. # Test our predict_model() function predict_model(x = automl_leader, newdata = as.data.frame(test_h2o[,-1]), type = &#39;raw&#39;) %&gt;% tibble::as_tibble() #&gt; # A tibble: 211 x 2 #&gt; No Yes #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.846 0.154 #&gt; 2 0.953 0.0474 #&gt; 3 0.0360 0.964 #&gt; 4 0.964 0.0356 #&gt; 5 0.883 0.117 #&gt; 6 0.964 0.0358 #&gt; # … with 205 more rows Now the fun part, we create an explainer using the lime() function. Just pass the training data set without the “Attribution column”. The form must be a data frame, which is OK since our predict_model function will switch it to an h2o object. Setmodel = automl_leader our leader model, and bin_continuous = FALSE. We could tell the algorithm to bin continuous variables, but this may not make sense for categorical numeric data that we didn’t change to factors. # Run lime() on training set explainer &lt;- lime::lime( as.data.frame(train_h2o[,-1]), model = automl_leader, bin_continuous = FALSE) #&gt; Warning: Data contains numeric columns with zero variance Now we run the explain() function, which returns our explanation. This can take a minute to run so we limit it to just the first ten rows of the test data set. We set n_labels = 1 because we care about explaining a single class. Setting n_features = 4 returns the top four features that are critical to each case. Finally, setting kernel_width = 0.5 allows us to increase the “model_r2” value by shrinking the localized evaluation. # Run explain() on explainer explanation &lt;- lime::explain( as.data.frame(test_h2o[1:10,-1]), explainer = explainer, n_labels = 1, n_features = 4, kernel_width = 0.5) 21.7 Feature Importance Visualization The payoff for the work we put in using LIME is this feature importance plot. This allows us to visualize each of the ten cases (observations) from the test data. The top four features for each case are shown. Note that they are not the same for each case. The green bars mean that the feature supports the model conclusion, and the red bars contradict. We’ll focus in on Cases with Label = Yes, which are predicted to have attrition. We can see a common theme with Case 3 and Case 7: Training Time, Job Role, and Over Time are among the top factors influencing attrition. These are only two cases, but they can be used to potentially generalize to the larger population as we will see next. plot_features(explanation) + labs(title = &quot;HR Predictive Analytics: LIME Feature Importance Visualization&quot;, subtitle = &quot;Hold Out (Test) Set, First 10 Cases Shown&quot;) 21.7.1 What features are linked to employee attrition Now we turn to our three critical features from the LIME Feature Importance Plot: Training Time Job Role Over Time We’ll subset this data and visualize to detect trends. # Focus on critical features of attrition attrition_critical_features &lt;- hr_data %&gt;% tibble::as_tibble() %&gt;% select(Attrition, TrainingTimesLastYear, JobRole, OverTime) %&gt;% rowid_to_column(var = &quot;Case&quot;) attrition_critical_features #&gt; # A tibble: 1,470 x 5 #&gt; Case Attrition TrainingTimesLastYear JobRole OverTime #&gt; &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; #&gt; 1 1 Yes 0 Sales Executive Yes #&gt; 2 2 No 3 Research Scientist No #&gt; 3 3 Yes 3 Laboratory Technician Yes #&gt; 4 4 No 3 Research Scientist Yes #&gt; 5 5 No 3 Laboratory Technician No #&gt; 6 6 No 2 Laboratory Technician No #&gt; # … with 1,464 more rows 21.7.2 Training From the violin plot, the employees that stay tend to have a large peaks at two and three trainings per year whereas the employees that leave tend to have a large peak at two trainings per year. This suggests that employees with more trainings may be less likely to leave. ggplot(attrition_critical_features, aes(x = Attrition, y = TrainingTimesLastYear)) + geom_violin() + geom_jitter(alpha = 0.25) attrition_critical_features %&gt;% ggplot(aes(Attrition, TrainingTimesLastYear)) + geom_jitter(alpha = 0.5, fill = palette_light()[[1]]) + geom_violin(alpha = 0.7, fill = palette_light()[[1]]) + theme_tq() + labs( title = &quot;Prevalance of Training is Lower in Attrition = Yes&quot;, subtitle = &quot;Suggests that increased training is related to lower attrition&quot; ) 21.7.3 Overtime The plot below shows a very interesting relationship: a very high proportion of employees that turnover are working over time. The opposite is true for employees that stay. attrition_critical_features %&gt;% mutate(OverTime = case_when( OverTime == &quot;Yes&quot; ~ 1, OverTime == &quot;No&quot; ~ 0 )) %&gt;% ggplot(aes(Attrition, OverTime)) + geom_jitter(alpha = 0.5, fill = palette_light()[[1]]) + geom_violin(alpha = 0.7, fill = palette_light()[[1]]) + theme_tq() + labs( title = &quot;Prevalance of Over Time is Higher in Attrition = Yes&quot;, subtitle = &quot;Suggests that increased overtime is related to higher attrition&quot;) ggplot(attrition_critical_features, aes(x = Attrition, y = OverTime, )) + # geom_violin(aes(y = ..prop.., group = 1)) + geom_jitter(alpha = 0.5) 21.7.4 Job Role Several job roles are experiencing more turnover. Sales reps have the highest turnover at about 40% followed by Lab Technician, Human Resources, Sales Executive, and Research Scientist. It may be worthwhile to investigate what localized issues could be creating the high turnover among these groups within the organization. p &lt;- ggplot(data = subset(attrition_critical_features, Attrition == &quot;Yes&quot;), mapping = aes(x = JobRole)) p + geom_bar(mapping = aes(y = ..prop.., group = 1)) + coord_flip() # geom_bar(mapping = aes(y = ..prop.., group = 1)) p &lt;- ggplot(data = attrition_critical_features, mapping = aes(x = JobRole)) p + geom_bar(mapping = aes(y = ..prop.., group = 1)) + coord_flip() + facet_wrap(Attrition ~ .) attrition_critical_features %&gt;% group_by(JobRole, Attrition) %&gt;% summarize(total = n()) #&gt; # A tibble: 18 x 3 #&gt; # Groups: JobRole [9] #&gt; JobRole Attrition total #&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 Healthcare Representative No 122 #&gt; 2 Healthcare Representative Yes 9 #&gt; 3 Human Resources No 40 #&gt; 4 Human Resources Yes 12 #&gt; 5 Laboratory Technician No 197 #&gt; 6 Laboratory Technician Yes 62 #&gt; # … with 12 more rows attrition_critical_features %&gt;% group_by(JobRole, Attrition) %&gt;% summarize(total = n()) %&gt;% spread(key = Attrition, value = total) %&gt;% mutate(pct_attrition = Yes / (Yes + No)) #&gt; # A tibble: 9 x 4 #&gt; # Groups: JobRole [9] #&gt; JobRole No Yes pct_attrition #&gt; &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 Healthcare Representative 122 9 0.0687 #&gt; 2 Human Resources 40 12 0.231 #&gt; 3 Laboratory Technician 197 62 0.239 #&gt; 4 Manager 97 5 0.0490 #&gt; 5 Manufacturing Director 135 10 0.0690 #&gt; 6 Research Director 78 2 0.025 #&gt; # … with 3 more rows attrition_critical_features %&gt;% group_by(JobRole, Attrition) %&gt;% summarize(total = n()) %&gt;% spread(key = Attrition, value = total) %&gt;% mutate(pct_attrition = Yes / (Yes + No)) %&gt;% ggplot(aes(x = forcats::fct_reorder(JobRole, pct_attrition), y = pct_attrition)) + geom_bar(stat = &quot;identity&quot;, alpha = 1, fill = palette_light()[[1]]) + expand_limits(y = c(0, 1)) + coord_flip() + theme_tq() + labs( title = &quot;Attrition Varies By Job Role&quot;, subtitle = &quot;Sales Rep, Lab Tech, HR, Sales Exec, and Research Scientist have much higher turnover&quot;, y = &quot;Attrition Percentage (Yes / Total)&quot;, x = &quot;JobRole&quot; ) 21.8 Conclusions There’s a lot to take away from this article. We showed how you can use predictive analytics to develop sophisticated models that very accurately detect employees that are at risk of turnover. The autoML algorithm from H2O.ai worked well for classifying attrition with an accuracy around 87% on unseen / unmodeled data. We then used LIME to breakdown the complex ensemble model returned from H2O into critical features that are related to attrition. Overall, this is a really useful example where we can see how machine learning and data science can be used in business applications. "],
["dealing-with-unbalanced-data.html", "Chapter 22 Dealing with unbalanced data 22.1 Breast cancer dataset 22.2 Introduction 22.3 Read and process the data 22.4 Under-sampling 22.5 Oversampling 22.6 Predictions 22.7 Final notes", " Chapter 22 Dealing with unbalanced data 22.1 Breast cancer dataset 22.2 Introduction Source: https://shiring.github.io/machine_learning/2017/04/02/unbalanced library(caret) #&gt; Loading required package: lattice #&gt; Loading required package: ggplot2 #&gt; Registered S3 methods overwritten by &#39;ggplot2&#39;: #&gt; method from #&gt; [.quosures rlang #&gt; c.quosures rlang #&gt; print.quosures rlang library(mice) #&gt; #&gt; Attaching package: &#39;mice&#39; #&gt; The following objects are masked from &#39;package:base&#39;: #&gt; #&gt; cbind, rbind library(ggplot2) In my last post, where I shared the code that I used to produce an example analysis to go along with my webinar on building meaningful models for disease prediction, I mentioned that it is advised to consider over- or under-sampling when you have unbalanced data sets. Because my focus in this webinar was on evaluating model performance, I did not want to add an additional layer of complexity and therefore did not further discuss how to specifically deal with unbalanced data. But because I had gotten a few questions regarding this, I thought it would be worthwhile to explain over- and under-sampling techniques in more detail and show how you can very easily implement them with caret. 22.3 Read and process the data bc_data &lt;- read.table(file.path(data_raw_dir, &quot;breast-cancer-wisconsin.data&quot;), header = FALSE, sep = &quot;,&quot;) colnames(bc_data) &lt;- c(&quot;sample_code_number&quot;, &quot;clump_thickness&quot;, &quot;uniformity_of_cell_size&quot;, &quot;uniformity_of_cell_shape&quot;, &quot;marginal_adhesion&quot;, &quot;single_epithelial_cell_size&quot;, &quot;bare_nuclei&quot;, &quot;bland_chromatin&quot;, &quot;normal_nucleoli&quot;, &quot;mitosis&quot;, &quot;classes&quot;) bc_data$classes &lt;- ifelse(bc_data$classes == &quot;2&quot;, &quot;benign&quot;, ifelse(bc_data$classes == &quot;4&quot;, &quot;malignant&quot;, NA)) bc_data[bc_data == &quot;?&quot;] &lt;- NA # how many NAs are in the data length(which(is.na(bc_data))) #&gt; [1] 16 # impute missing data # skip columns: sample_code_number and classes bc_data[,2:10] &lt;- apply(bc_data[, 2:10], 2, function(x) as.numeric(as.character(x))) # impute but stay mute dataset_impute &lt;- mice(bc_data[, 2:10], print = FALSE) # bind &quot;classes&quot; with the rest. skip &quot;sample_code_number&quot; bc_data &lt;- cbind(bc_data[, 11, drop = FALSE], mice::complete(dataset_impute, action = 1)) bc_data$classes &lt;- as.factor(bc_data$classes) 22.3.1 Unbalanced data In this context, unbalanced data refers to classification problems where we have unequal instances for different classes. Having unbalanced data is actually very common in general, but it is especially prevalent when working with disease data where we usually have more healthy control samples than disease cases. Even more extreme unbalance is seen with fraud detection, where e.g. most credit card uses are okay and only very few will be fraudulent. In the example I used for my webinar, a breast cancer dataset, we had about twice as many benign than malignant samples. # how many benign and malignant cases are there? summary(bc_data$classes) #&gt; benign malignant #&gt; 458 241 22.3.1.1 Why is unbalanced data a problem in machine learning? Most machine learning classification algorithms are sensitive to unbalance in the predictor classes. Let’s consider an even more extreme example than our breast cancer dataset: assume we had 10 malignant vs 90 benign samples. A machine learning model that has been trained and tested on such a dataset could now predict “benign” for all samples and still gain a very high accuracy. An unbalanced dataset will bias the prediction model towards the more common class! 22.3.1.2 How to balance data for modeling The basic theoretical concepts behind over- and under-sampling are very simple: With under-sampling, we randomly select a subset of samples from the class with more instances to match the number of samples coming from each class. In our example, we would randomly pick 241 out of the 458 benign cases. The main disadvantage of under-sampling is that we lose potentially relevant information from the left-out samples. With oversampling, we randomly duplicate samples from the class with fewer instances or we generate additional instances based on the data that we have, so as to match the number of samples in each class. While we avoid losing information with this approach, we also run the risk of overfitting our model as we are more likely to get the same samples in the training and in the test data, i.e. the test data is no longer independent from training data. This would lead to an overestimation of our model’s performance and generalizability. In reality though, we should not simply perform over- or under-sampling on our training data and then run the model. We need to account for cross-validation and perform over- or under-sampling on each fold independently to get an honest estimate of model performance! 22.3.1.3 Modeling the original unbalanced data Here is the same model I used in my webinar example: I randomly divide the data into training and test sets (stratified by class) and perform Random Forest modeling with 10 x 10 repeated cross-validation. Final model performance is then measured on the test set. set.seed(42) index &lt;- createDataPartition(bc_data$classes, p = 0.7, list = FALSE) train_data &lt;- bc_data[index, ] test_data &lt;- bc_data[-index, ] set.seed(42) model_rf &lt;- caret::train(classes ~ ., data = train_data, method = &quot;rf&quot;, preProcess = c(&quot;scale&quot;, &quot;center&quot;), trControl = trainControl(method = &quot;repeatedcv&quot;, number = 10, repeats = 10, verboseIter = FALSE)) final &lt;- data.frame(actual = test_data$classes, predict(model_rf, newdata = test_data, type = &quot;prob&quot;)) final$predict &lt;- ifelse(final$benign &gt; 0.5, &quot;benign&quot;, &quot;malignant&quot;) final_predict &lt;- as.factor(final$predict) test_data_classes &lt;- as.factor(test_data$classes) cm_original &lt;- confusionMatrix(final_predict, test_data_classes) cm_original$byClass[&#39;Sensitivity&#39;] #&gt; Sensitivity #&gt; 0.978 22.4 Under-sampling Luckily, caret makes it very easy to incorporate over- and under-sampling techniques with cross-validation resampling. We can simply add the sampling option to our trainControl and choose down for under- (also called down-) sampling. The rest stays the same as with our original model. set.seed(42) ctrl &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 10, repeats = 10, verboseIter = FALSE, sampling = &quot;down&quot;) model_rf_under &lt;- caret::train(classes ~ ., data = train_data, method = &quot;rf&quot;, preProcess = c(&quot;scale&quot;, &quot;center&quot;), trControl = ctrl) final_under &lt;- data.frame(actual = test_data$classes, predict(model_rf_under, newdata = test_data, type = &quot;prob&quot;)) final_under$predict &lt;- ifelse(final_under$benign &gt; 0.5, &quot;benign&quot;, &quot;malignant&quot;) final_under_predict &lt;- as.factor(final_under$predict) test_data_classes &lt;- test_data$classes cm_under &lt;- confusionMatrix(final_under_predict, test_data_classes) cm_under$byClass[&#39;Sensitivity&#39;] #&gt; Sensitivity #&gt; 0.978 22.5 Oversampling For over- (also called up-) sampling we simply specify sampling = “up”. set.seed(42) ctrl &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 10, repeats = 10, verboseIter = FALSE, sampling = &quot;up&quot;) model_rf_over &lt;- caret::train(classes ~ ., data = train_data, method = &quot;rf&quot;, preProcess = c(&quot;scale&quot;, &quot;center&quot;), trControl = ctrl) final_over &lt;- data.frame(actual = test_data$classes, predict(model_rf_over, newdata = test_data, type = &quot;prob&quot;)) final_over$predict &lt;- ifelse(final_over$benign &gt; 0.5, &quot;benign&quot;, &quot;malignant&quot;) final_over_predict &lt;- as.factor(final_over$predict) test_data_classes &lt;- test_data$classes cm_over &lt;- confusionMatrix(final_over_predict, test_data_classes) cm_over$byClass[&#39;Sensitivity&#39;] #&gt; Sensitivity #&gt; 0.978 22.5.1 ROSE Besides over- and under-sampling, there are hybrid methods that combine under-sampling with the generation of additional data. Two of the most popular are ROSE and SMOTE. From Nicola Lunardon, Giovanna Menardi and Nicola Torelli’s “ROSE: A Package for Binary Imbalanced Learning” (R Journal, 2014, Vol. 6 Issue 1, p. 79): “The ROSE package provides functions to deal with binary classification problems in the presence of imbalanced classes. Artificial balanced samples are generated according to a smoothed bootstrap approach and allow for aiding both the phases of estimation and accuracy evaluation of a binary classifier in the presence of a rare class. Functions that implement more traditional remedies for the class imbalance and different metrics to evaluate accuracy are also provided. These are estimated by holdout, bootstrap, or cross-validation methods.” You implement them the same way as before, this time choosing sampling = “rose”… set.seed(42) ctrl &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 10, repeats = 10, verboseIter = FALSE, sampling = &quot;rose&quot;) model_rf_rose &lt;- caret::train(classes ~ ., data = train_data, method = &quot;rf&quot;, preProcess = c(&quot;scale&quot;, &quot;center&quot;), trControl = ctrl) #&gt; Loaded ROSE 0.0-3 final_rose &lt;- data.frame(actual = test_data$classes, predict(model_rf_rose, newdata = test_data, type = &quot;prob&quot;)) final_rose$predict &lt;- ifelse(final_rose$benign &gt; 0.5, &quot;benign&quot;, &quot;malignant&quot;) cm_rose &lt;- confusionMatrix(as.factor(final_rose$predict), as.factor(test_data$classes)) cm_rose$byClass[&#39;Sensitivity&#39;] #&gt; Sensitivity #&gt; 0.985 22.5.2 SMOTE … or by choosing sampling = “smote” in the trainControl settings. From Nitesh V. Chawla, Kevin W. Bowyer, Lawrence O. Hall and W. Philip Kegelmeyer’s “SMOTE: Synthetic Minority Over-sampling Technique” (Journal of Artificial Intelligence Research, 2002, Vol. 16, pp. 321–357): “This paper shows that a combination of our method of over-sampling the minority (abnormal) class and under-sampling the majority (normal) class can achieve better classifier performance (in ROC space) than only under-sampling the majority class. This paper also shows that a combination of our method of over-sampling the minority class and under-sampling the majority class can achieve better classifier performance (in ROC space) than varying the loss ratios in Ripper or class priors in Naive Bayes. Our method of over-sampling the minority class involves creating synthetic minority class examples.” set.seed(42) ctrl &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 10, repeats = 10, verboseIter = FALSE, sampling = &quot;smote&quot;) model_rf_smote &lt;- caret::train(classes ~ ., data = train_data, method = &quot;rf&quot;, preProcess = c(&quot;scale&quot;, &quot;center&quot;), trControl = ctrl) #&gt; Loading required package: grid #&gt; Registered S3 method overwritten by &#39;xts&#39;: #&gt; method from #&gt; as.zoo.xts zoo #&gt; Registered S3 method overwritten by &#39;quantmod&#39;: #&gt; method from #&gt; as.zoo.data.frame zoo final_smote &lt;- data.frame(actual = test_data$classes, predict(model_rf_smote, newdata = test_data, type = &quot;prob&quot;)) final_smote$predict &lt;- ifelse(final_smote$benign &gt; 0.5, &quot;benign&quot;, &quot;malignant&quot;) cm_smote &lt;- confusionMatrix(as.factor(final_smote$predict), as.factor(test_data$classes)) cm_smote$byClass[&#39;Sensitivity&#39;] #&gt; Sensitivity #&gt; 0.978 22.6 Predictions Now let’s compare the predictions of all these models: models &lt;- list( original = model_rf, under = model_rf_under, over = model_rf_over, smote = model_rf_smote, rose = model_rf_rose) resampling &lt;- resamples(models) bwplot(resampling) library(dplyr) #&gt; #&gt; Attaching package: &#39;dplyr&#39; #&gt; The following objects are masked from &#39;package:stats&#39;: #&gt; #&gt; filter, lag #&gt; The following objects are masked from &#39;package:base&#39;: #&gt; #&gt; intersect, setdiff, setequal, union comparison &lt;- data.frame(model = names(models), Sensitivity = rep(NA, length(models)), Specificity = rep(NA, length(models)), Precision = rep(NA, length(models)), Recall = rep(NA, length(models)), F1 = rep(NA, length(models))) for (name in names(models)) { cm_model &lt;- get(paste0(&quot;cm_&quot;, name)) comparison[comparison$model==name, ] &lt;- filter(comparison, model==name) %&gt;% mutate(Sensitivity = cm_model$byClass[&quot;Sensitivity&quot;], Specificity = cm_model$byClass[&quot;Specificity&quot;], Precision = cm_model$byClass[&quot;Precision&quot;], Recall = cm_model$byClass[&quot;Recall&quot;], F1 = cm_model$byClass[&quot;F1&quot;] ) } print(comparison) #&gt; model Sensitivity Specificity Precision Recall F1 #&gt; 1 original 0.978 0.986 0.993 0.978 0.985 #&gt; 2 under 0.978 1.000 1.000 0.978 0.989 #&gt; 3 over 0.978 0.986 0.993 0.978 0.985 #&gt; 4 smote 0.978 0.986 0.993 0.978 0.985 #&gt; 5 rose 0.985 0.986 0.993 0.985 0.989 library(tidyr) #&gt; #&gt; Attaching package: &#39;tidyr&#39; #&gt; The following object is masked from &#39;package:mice&#39;: #&gt; #&gt; complete comparison %&gt;% gather(x, y, Sensitivity:F1) %&gt;% ggplot(aes(x = x, y = y, color = model)) + geom_jitter(width = 0.2, alpha = 0.5, size = 3) With this small dataset, we can already see how the different techniques can influence model performance. Sensitivity (or recall) describes the proportion of benign cases that have been predicted correctly, while specificity describes the proportion of malignant cases that have been predicted correctly. Precision describes the true positives, i.e. the proportion of benign predictions that were actual from benign samples. F1 is the weighted average of precision and sensitivity/ recall. 22.7 Final notes Here, all four methods improved specificity and precision compared to the original model. Under-sampling, over-sampling and ROSE additionally improved precision and the F1 score. This post shows a simple example of how to correct for unbalance in datasets for machine learning. For more advanced instructions and potential caveats with these techniques, check out the excellent caret documentation. If you are interested in more machine learning posts, check out the category listing for machine_learning on my blog. "],
["ten-different-methods-to-assess-variable-importance.html", "Chapter 23 Ten different methods to assess Variable Importance 23.1 Glaucoma dataset 23.2 Introduction 23.3 1. Boruta 23.4 Variable Importance from Machine Learning Algorithms 23.5 Lasso Regression 23.6 Step wise Forward and Backward Selection 23.7 Relative Importance from Linear Regression 23.8 Recursive Feature Elimination (RFE) 23.9 Genetic Algorithm 23.10 Simulated Annealing 23.11 Information Value and Weights of Evidence 23.12 DALEX Package 23.13 Conclusion", " Chapter 23 Ten different methods to assess Variable Importance 23.1 Glaucoma dataset Source: https://www.machinelearningplus.com/machine-learning/feature-selection/ 23.2 Introduction In real-world datasets, it is fairly common to have columns that are nothing but noise. You are better off getting rid of such variables because of the memory space they occupy, the time and the computational esources it is going to cost, especially in large datasets. Sometimes, you have a variable that makes business sense, but you are not sure if it actually helps in predicting the Y. You also need to consider the fact that, a feature that could be useful in one ML algorithm (say a decision tree) may go underrepresented or unused by another (like a regression model). Having said that, it is still possible that a variable that shows poor signs of helping to explain the response variable (Y), can turn out to be significantly useful in the presence of (or combination with) other predictors. What I mean by that is, a variable might have a low correlation value of (~0.2) with Y. But in the presence of other variables, it can help to explain certain patterns/phenomenon that other variables can’t explain. In such cases, it can be hard to make a call whether to include or exclude such variables. The strategies we are about to discuss can help fix such problems. Not only that, it will also help understand if a particular variable is important or not and how much it is contributing to the model An important caveat. It is always best to have variables that have sound business logic backing the inclusion of a variable and rely solely on variable importance metrics. Alright. Let’s load up the ‘Glaucoma’ dataset where the goal is to predict if a patient has Glaucoma or not based on 63 different physiological measurements. You can directly run the codes or download the dataset here. A lot of interesting examples ahead. Let’s get started. # Load Packages and prepare dataset library(TH.data) #&gt; Loading required package: survival #&gt; Loading required package: MASS #&gt; #&gt; Attaching package: &#39;TH.data&#39; #&gt; The following object is masked from &#39;package:MASS&#39;: #&gt; #&gt; geyser library(caret) #&gt; Loading required package: lattice #&gt; Loading required package: ggplot2 #&gt; Registered S3 methods overwritten by &#39;ggplot2&#39;: #&gt; method from #&gt; [.quosures rlang #&gt; c.quosures rlang #&gt; print.quosures rlang #&gt; #&gt; Attaching package: &#39;caret&#39; #&gt; The following object is masked from &#39;package:survival&#39;: #&gt; #&gt; cluster library(tictoc) data(&quot;GlaucomaM&quot;, package = &quot;TH.data&quot;) trainData &lt;- GlaucomaM head(trainData) #&gt; ag at as an ai eag eat eas ean eai abrg abrt #&gt; 2 2.22 0.354 0.580 0.686 0.601 1.267 0.336 0.346 0.255 0.331 0.479 0.260 #&gt; 43 2.68 0.475 0.672 0.868 0.667 2.053 0.440 0.520 0.639 0.454 1.090 0.377 #&gt; 25 1.98 0.343 0.508 0.624 0.504 1.200 0.299 0.396 0.259 0.246 0.465 0.209 #&gt; 65 1.75 0.269 0.476 0.525 0.476 0.612 0.147 0.017 0.044 0.405 0.170 0.062 #&gt; 70 2.99 0.599 0.686 1.039 0.667 2.513 0.543 0.607 0.871 0.492 1.800 0.431 #&gt; 16 2.92 0.483 0.763 0.901 0.770 2.200 0.462 0.637 0.504 0.597 1.311 0.394 #&gt; abrs abrn abri hic mhcg mhct mhcs mhcn mhci phcg #&gt; 2 0.107 0.014 0.098 0.214 0.111 0.412 0.036 0.105 -0.022 -0.139 #&gt; 43 0.257 0.212 0.245 0.382 0.140 0.338 0.104 0.080 0.109 -0.015 #&gt; 25 0.112 0.041 0.103 0.195 0.062 0.356 0.045 -0.009 -0.048 -0.149 #&gt; 65 0.000 0.000 0.108 -0.030 -0.015 0.074 -0.084 -0.050 0.035 -0.182 #&gt; 70 0.494 0.601 0.274 0.383 0.089 0.233 0.145 0.023 0.007 -0.131 #&gt; 16 0.365 0.251 0.301 0.442 0.128 0.375 0.049 0.111 0.052 -0.088 #&gt; phct phcs phcn phci hvc vbsg vbst vbss vbsn vbsi vasg #&gt; 2 0.242 -0.053 0.010 -0.139 0.613 0.303 0.103 0.088 0.022 0.090 0.062 #&gt; 43 0.296 -0.015 -0.015 0.036 0.382 0.676 0.181 0.186 0.141 0.169 0.029 #&gt; 25 0.206 -0.092 -0.081 -0.149 0.557 0.300 0.084 0.088 0.046 0.082 0.036 #&gt; 65 -0.097 -0.125 -0.138 -0.182 0.373 0.048 0.011 0.000 0.000 0.036 0.070 #&gt; 70 0.163 0.055 -0.131 -0.115 0.405 0.889 0.151 0.253 0.330 0.155 0.020 #&gt; 16 0.281 -0.067 -0.062 -0.088 0.507 0.972 0.213 0.316 0.197 0.246 0.043 #&gt; vast vass vasn vasi vbrg vbrt vbrs vbrn vbri varg vart vars #&gt; 2 0.000 0.011 0.032 0.018 0.075 0.039 0.021 0.002 0.014 0.756 0.009 0.209 #&gt; 43 0.001 0.007 0.011 0.010 0.370 0.127 0.099 0.050 0.093 0.410 0.006 0.105 #&gt; 25 0.002 0.004 0.016 0.013 0.081 0.034 0.019 0.007 0.021 0.565 0.014 0.132 #&gt; 65 0.005 0.030 0.033 0.002 0.005 0.001 0.000 0.000 0.004 0.380 0.032 0.147 #&gt; 70 0.001 0.004 0.008 0.007 0.532 0.103 0.173 0.181 0.075 0.228 0.011 0.026 #&gt; 16 0.001 0.005 0.028 0.009 0.467 0.136 0.148 0.078 0.104 0.540 0.008 0.133 #&gt; varn vari mdg mdt mds mdn mdi tmg tmt tms tmn #&gt; 2 0.298 0.240 0.705 0.637 0.738 0.596 0.691 -0.236 -0.018 -0.230 -0.510 #&gt; 43 0.181 0.117 0.898 0.850 0.907 0.771 0.940 -0.211 -0.014 -0.165 -0.317 #&gt; 25 0.243 0.177 0.687 0.643 0.689 0.684 0.700 -0.185 -0.097 -0.235 -0.337 #&gt; 65 0.151 0.050 0.207 0.171 0.022 0.046 0.221 -0.148 -0.035 -0.449 -0.217 #&gt; 70 0.105 0.087 0.721 0.638 0.730 0.730 0.640 -0.052 -0.105 0.084 -0.012 #&gt; 16 0.232 0.167 0.927 0.842 0.953 0.906 0.898 -0.040 0.087 0.018 -0.094 #&gt; tmi mr rnf mdic emd mv Class #&gt; 2 -0.158 0.841 0.410 0.137 0.239 0.035 normal #&gt; 43 -0.192 0.924 0.256 0.252 0.329 0.022 normal #&gt; 25 -0.020 0.795 0.378 0.152 0.250 0.029 normal #&gt; 65 -0.091 0.746 0.200 0.027 0.078 0.023 normal #&gt; 70 -0.054 0.977 0.193 0.297 0.354 0.034 normal #&gt; 16 -0.051 0.965 0.339 0.333 0.442 0.028 normal 23.3 1. Boruta Boruta is a feature ranking and selection algorithm based on random forests algorithm. The advantage with Boruta is that it clearly decides if a variable is important or not and helps to select variables that are statistically significant. Besides, you can adjust the strictness of the algorithm by adjusting the \\(p\\) values that defaults to 0.01 and the maxRuns. maxRuns is the number of times the algorithm is run. The higher the maxRuns the more selective you get in picking the variables. The default value is 100. In the process of deciding if a feature is important or not, some features may be marked by Boruta as ‘Tentative’. Sometimes increasing the maxRuns can help resolve the ‘Tentativeness’ of the feature. Lets see an example based on the Glaucoma dataset from TH.data package that I created earlier. # install.packages(&#39;Boruta&#39;) library(Boruta) #&gt; Loading required package: ranger The boruta function uses a formula interface just like most predictive modeling functions. So the first argument to boruta() is the formula with the response variable on the left and all the predictors on the right. By placing a dot, all the variables in trainData other than Class will be included in the model. The doTrace argument controls the amount of output printed to the console. Higher the value, more the log details you get. So save space I have set it to 0, but try setting it to 1 and 2 if you are running the code. Finally the output is stored in boruta_output. # Perform Boruta search boruta_output &lt;- Boruta(Class ~ ., data=na.omit(trainData), doTrace=0) Let’s see what the boruta_output contains. names(boruta_output) #&gt; [1] &quot;finalDecision&quot; &quot;ImpHistory&quot; &quot;pValue&quot; &quot;maxRuns&quot; #&gt; [5] &quot;light&quot; &quot;mcAdj&quot; &quot;timeTaken&quot; &quot;roughfixed&quot; #&gt; [9] &quot;call&quot; &quot;impSource&quot; # Get significant variables including tentatives boruta_signif &lt;- getSelectedAttributes(boruta_output, withTentative = TRUE) print(boruta_signif) #&gt; [1] &quot;as&quot; &quot;ai&quot; &quot;eas&quot; &quot;ean&quot; &quot;abrg&quot; &quot;abrs&quot; &quot;abrn&quot; &quot;abri&quot; &quot;hic&quot; &quot;mhcg&quot; #&gt; [11] &quot;mhcs&quot; &quot;mhcn&quot; &quot;mhci&quot; &quot;phcg&quot; &quot;phcn&quot; &quot;phci&quot; &quot;hvc&quot; &quot;vbsg&quot; &quot;vbss&quot; &quot;vbsn&quot; #&gt; [21] &quot;vbsi&quot; &quot;vasg&quot; &quot;vass&quot; &quot;vasi&quot; &quot;vbrg&quot; &quot;vbrs&quot; &quot;vbrn&quot; &quot;vbri&quot; &quot;varg&quot; &quot;vart&quot; #&gt; [31] &quot;vars&quot; &quot;varn&quot; &quot;vari&quot; &quot;mdn&quot; &quot;tmg&quot; &quot;tmt&quot; &quot;tms&quot; &quot;tmi&quot; &quot;mr&quot; &quot;rnf&quot; #&gt; [41] &quot;mdic&quot; &quot;emd&quot; If you are not sure about the tentative variables being selected for granted, you can choose a TentativeRoughFix on boruta_output. # Do a tentative rough fix roughFixMod &lt;- TentativeRoughFix(boruta_output) boruta_signif &lt;- getSelectedAttributes(roughFixMod) print(boruta_signif) #&gt; [1] &quot;as&quot; &quot;ai&quot; &quot;ean&quot; &quot;abrg&quot; &quot;abrs&quot; &quot;abrn&quot; &quot;abri&quot; &quot;hic&quot; &quot;mhcg&quot; &quot;mhcn&quot; #&gt; [11] &quot;mhci&quot; &quot;phcg&quot; &quot;phcn&quot; &quot;phci&quot; &quot;hvc&quot; &quot;vbsn&quot; &quot;vbsi&quot; &quot;vasg&quot; &quot;vass&quot; &quot;vasi&quot; #&gt; [21] &quot;vbrg&quot; &quot;vbrs&quot; &quot;vbrn&quot; &quot;vbri&quot; &quot;varg&quot; &quot;vart&quot; &quot;vars&quot; &quot;varn&quot; &quot;vari&quot; &quot;mdn&quot; #&gt; [31] &quot;tmg&quot; &quot;tms&quot; &quot;tmi&quot; &quot;mr&quot; &quot;rnf&quot; &quot;mdic&quot; There you go. Boruta has decided on the ‘Tentative’ variables on our behalf. Let’s find out the importance scores of these variables. # Variable Importance Scores imps &lt;- attStats(roughFixMod) imps2 = imps[imps$decision != &#39;Rejected&#39;, c(&#39;meanImp&#39;, &#39;decision&#39;)] head(imps2[order(-imps2$meanImp), ]) # descending sort #&gt; meanImp decision #&gt; vari 12.37 Confirmed #&gt; varg 11.74 Confirmed #&gt; vars 10.74 Confirmed #&gt; phci 8.34 Confirmed #&gt; hic 8.21 Confirmed #&gt; varn 7.88 Confirmed Let’s plot it to see the importances of these variables. # Plot variable importance plot(boruta_output, cex.axis=.7, las=2, xlab=&quot;&quot;, main=&quot;Variable Importance&quot;) This plot reveals the importance of each of the features. The columns in green are ‘confirmed’ and the ones in red are not. There are couple of blue bars representing ShadowMax and ShadowMin. They are not actual features, but are used by the boruta algorithm to decide if a variable is important or not. 23.4 Variable Importance from Machine Learning Algorithms Another way to look at feature selection is to consider variables most used by various ML algorithms the most to be important. Depending on how the machine learning algorithm learns the relationship between X’s and Y, different machine learning algorithms may possibly end up using different variables (but mostly common vars) to various degrees. What I mean by that is, the variables that proved useful in a tree-based algorithm like rpart, can turn out to be less useful in a regression-based model. So all variables need not be equally useful to all algorithms. So how do we find the variable importance for a given ML algo? train() the desired model using the caret package. Then, use varImp() to determine the feature importances. You may want to try out multiple algorithms, to get a feel of the usefulness of the features across algos. 23.4.1 rpart # Train an rpart model and compute variable importance. library(caret) set.seed(100) rPartMod &lt;- train(Class ~ ., data=trainData, method=&quot;rpart&quot;) rpartImp &lt;- varImp(rPartMod) print(rpartImp) #&gt; rpart variable importance #&gt; #&gt; only 20 most important variables shown (out of 62) #&gt; #&gt; Overall #&gt; varg 100.0 #&gt; vari 93.2 #&gt; vars 85.2 #&gt; varn 76.9 #&gt; tmi 72.3 #&gt; mhcn 0.0 #&gt; as 0.0 #&gt; phcs 0.0 #&gt; vbst 0.0 #&gt; abrt 0.0 #&gt; vbsg 0.0 #&gt; eai 0.0 #&gt; vbrs 0.0 #&gt; vbsi 0.0 #&gt; eag 0.0 #&gt; tmt 0.0 #&gt; phcn 0.0 #&gt; vart 0.0 #&gt; mds 0.0 #&gt; an 0.0 Only 5 of the 63 features was used by rpart and if you look closely, the 5 variables used here are in the top 6 that boruta selected. Let’s do one more: the variable importances from Regularized Random Forest (RRF) algorithm. 23.4.2 Regularized Random Forest (RRF) tic() # Train an RRF model and compute variable importance. set.seed(100) rrfMod &lt;- train(Class ~ ., data = trainData, method = &quot;RRF&quot;) #&gt; Registered S3 method overwritten by &#39;RRF&#39;: #&gt; method from #&gt; plot.margin randomForest rrfImp &lt;- varImp(rrfMod, scale=F) toc() #&gt; 343.262 sec elapsed rrfImp #&gt; RRF variable importance #&gt; #&gt; only 20 most important variables shown (out of 62) #&gt; #&gt; Overall #&gt; varg 25.07 #&gt; vari 18.78 #&gt; vars 5.29 #&gt; tmi 4.09 #&gt; mhcg 3.25 #&gt; mhci 2.81 #&gt; hic 2.69 #&gt; hvc 2.50 #&gt; mv 2.00 #&gt; vasg 1.99 #&gt; phci 1.77 #&gt; phcn 1.53 #&gt; phct 1.43 #&gt; vass 1.37 #&gt; phcg 1.37 #&gt; tms 1.32 #&gt; tmg 1.16 #&gt; abrs 1.16 #&gt; tmt 1.13 #&gt; mdic 1.13 plot(rrfImp, top = 20, main=&#39;Variable Importance&#39;) The topmost important variables are pretty much from the top tier of Boruta’s selections. Some of the other algorithms available in train() that you can use to compute varImp are the following: ada, AdaBag, AdaBoost.M1, adaboost, bagEarth, bagEarthGCV, bagFDA, bagFDAGCV, bartMachine, blasso, BstLm, bstSm, C5.0, C5.0Cost, C5.0Rules, C5.0Tree, cforest, chaid, ctree, ctree2, cubist, deepboost, earth, enet, evtree, extraTrees, fda, gamboost, gbm_h2o, gbm, gcvEarth, glmnet_h2o, glmnet, glmStepAIC, J48, JRip, lars, lars2, lasso, LMT, LogitBoost, M5, M5Rules, msaenet, nodeHarvest, OneR, ordinalNet, ORFlog, ORFpls, ORFridge, ORFsvm, pam, parRF, PART, penalized, PenalizedLDA, qrf, ranger, Rborist, relaxo, rf, rFerns, rfRules, rotationForest, rotationForestCp, rpart, rpart1SE, rpart2, rpartCost, rpartScore, rqlasso, rqnc, RRF, RRFglobal, sdwd, smda, sparseLDA, spikeslab, wsrf, xgbLinear, xgbTree. 23.5 Lasso Regression Least Absolute Shrinkage and Selection Operator (LASSO) regression is a type of regularization method that penalizes with L1-norm. It basically imposes a cost to having large weights (value of coefficients). And its called L1 regularization, because the cost added, is proportional to the absolute value of weight coefficients. As a result, in the process of shrinking the coefficients, it eventually reduces the coefficients of certain unwanted features all the to zero. That is, it removes the unneeded variables altogether. So effectively, LASSO regression can be considered as a variable selection technique as well. library(glmnet) #&gt; Loading required package: Matrix #&gt; Loading required package: foreach #&gt; Loaded glmnet 2.0-16 # online data # trainData &lt;- read.csv(&#39;https://raw.githubusercontent.com/selva86/datasets/master/GlaucomaM.csv&#39;) trainData &lt;- read.csv(file.path(data_raw_dir, &quot;glaucoma.csv&quot;)) x &lt;- as.matrix(trainData[,-63]) # all X vars y &lt;- as.double(as.matrix(ifelse(trainData[, 63]==&#39;normal&#39;, 0, 1))) # Only Class # Fit the LASSO model (Lasso: Alpha = 1) set.seed(100) cv.lasso &lt;- cv.glmnet(x, y, family=&#39;binomial&#39;, alpha=1, parallel=TRUE, standardize=TRUE, type.measure=&#39;auc&#39;) #&gt; Warning: executing %dopar% sequentially: no parallel backend registered # Results plot(cv.lasso) Let’s see how to interpret this plot. The X axis of the plot is the log of lambda. That means when it is 2 here, the lambda value is actually 100. The numbers at the top of the plot show how many predictors were included in the model. The position of red dots along the Y-axis tells what AUC we got when you include as many variables shown on the top x-axis. You can also see two dashed vertical lines. The first one on the left points to the lambda with the lowest mean squared error. The one on the right point to the number of variables with the highest deviance within 1 standard deviation. The best lambda value is stored inside ‘cv.lasso$lambda.min’. # plot(cv.lasso$glmnet.fit, xvar=&quot;lambda&quot;, label=TRUE) cat(&#39;Min Lambda: &#39;, cv.lasso$lambda.min, &#39;\\n 1Sd Lambda: &#39;, cv.lasso$lambda.1se) #&gt; Min Lambda: 0.0224 #&gt; 1Sd Lambda: 0.144 df_coef &lt;- round(as.matrix(coef(cv.lasso, s=cv.lasso$lambda.min)), 2) # See all contributing variables df_coef[df_coef[, 1] != 0, ] #&gt; (Intercept) as mhci phci hvc vast #&gt; 2.68 -1.59 3.85 5.60 -2.41 -13.90 #&gt; vars vari mdn mdi tmg tms #&gt; -20.18 -1.58 0.50 0.99 0.06 2.56 #&gt; tmi #&gt; 2.23 The above output shows what variables LASSO considered important. A high positive or low negative implies more important is that variable. 23.6 Step wise Forward and Backward Selection Stepwise regression can be used to select features if the Y variable is a numeric variable. It is particularly used in selecting best linear regression models. It searches for the best possible regression model by iteratively selecting and dropping variables to arrive at a model with the lowest possible AIC. It can be implemented using the step() function and you need to provide it with a lower model, which is the base model from which it won’t remove any features and an upper model, which is a full model that has all possible features you want to have. Our case is not so complicated (&lt; 20 vars), so lets just do a simple stepwise in ‘both’ directions. I will use the ozone dataset for this where the objective is to predict the ozone_reading based on other weather related observations. # Load data # online # trainData &lt;- read.csv(&quot;http://rstatistics.net/wp-content/uploads/2015/09/ozone1.csv&quot;, # stringsAsFactors=F) trainData &lt;- read.csv(file.path(data_raw_dir, &quot;ozone1.csv&quot;)) print(head(trainData)) #&gt; Month Day_of_month Day_of_week ozone_reading pressure_height Wind_speed #&gt; 1 1 1 4 3 5480 8 #&gt; 2 1 2 5 3 5660 6 #&gt; 3 1 3 6 3 5710 4 #&gt; 4 1 4 7 5 5700 3 #&gt; 5 1 5 1 5 5760 3 #&gt; 6 1 6 2 6 5720 4 #&gt; Humidity Temperature_Sandburg Temperature_ElMonte Inversion_base_height #&gt; 1 20 40.5 39.8 5000 #&gt; 2 41 38.0 46.7 4109 #&gt; 3 28 40.0 49.5 2693 #&gt; 4 37 45.0 52.3 590 #&gt; 5 51 54.0 45.3 1450 #&gt; 6 69 35.0 49.6 1568 #&gt; Pressure_gradient Inversion_temperature Visibility #&gt; 1 -15 30.6 200 #&gt; 2 -14 48.0 300 #&gt; 3 -25 47.7 250 #&gt; 4 -24 55.0 100 #&gt; 5 25 57.0 60 #&gt; 6 15 53.8 60 The data is ready. Let’s perform the stepwise. # Step 1: Define base intercept only model base.mod &lt;- lm(ozone_reading ~ 1 , data=trainData) # Step 2: Full model with all predictors all.mod &lt;- lm(ozone_reading ~ . , data= trainData) # Step 3: Perform step-wise algorithm. direction=&#39;both&#39; implies both forward and backward stepwise stepMod &lt;- step(base.mod, scope = list(lower = base.mod, upper = all.mod), direction = &quot;both&quot;, trace = 0, steps = 1000) # Step 4: Get the shortlisted variable. shortlistedVars &lt;- names(unlist(stepMod[[1]])) shortlistedVars &lt;- shortlistedVars[!shortlistedVars %in% &quot;(Intercept)&quot;] # remove intercept # Show print(shortlistedVars) #&gt; [1] &quot;Temperature_Sandburg&quot; &quot;Humidity&quot; &quot;Temperature_ElMonte&quot; #&gt; [4] &quot;Month&quot; &quot;pressure_height&quot; &quot;Inversion_base_height&quot; The selected model has the above 6 features in it. But if you have too many features (&gt; 100) in training data, then it might be a good idea to split the dataset into chunks of 10 variables each with Y as mandatory in each dataset. Loop through all the chunks and collect the best features. We are doing it this way because some variables that came as important in a training data with fewer features may not show up in a linear reg model built on lots of features. Finally, from a pool of shortlisted features (from small chunk models), run a full stepwise model to get the final set of selected features. You can take this as a learning assignment to be solved within 20 minutes. 23.7 Relative Importance from Linear Regression This technique is specific to linear regression models. Relative importance can be used to assess which variables contributed how much in explaining the linear model’s R-squared value. So, if you sum up the produced importances, it will add up to the model’s R-sq value. In essence, it is not directly a feature selection method, because you have already provided the features that go in the model. But after building the model, the relaimpo can provide a sense of how important each feature is in contributing to the R-sq, or in other words, in ‘explaining the Y variable’. So, how to calculate relative importance? It is implemented in the relaimpo package. Basically, you build a linear regression model and pass that as the main argument to calc.relimp(). relaimpo has multiple options to compute the relative importance, but the recommended method is to use type='lmg', as I have done below. # install.packages(&#39;relaimpo&#39;) library(relaimpo) #&gt; Loading required package: boot #&gt; #&gt; Attaching package: &#39;boot&#39; #&gt; The following object is masked from &#39;package:lattice&#39;: #&gt; #&gt; melanoma #&gt; The following object is masked from &#39;package:survival&#39;: #&gt; #&gt; aml #&gt; Loading required package: survey #&gt; Loading required package: grid #&gt; #&gt; Attaching package: &#39;survey&#39; #&gt; The following object is masked from &#39;package:graphics&#39;: #&gt; #&gt; dotchart #&gt; Loading required package: mitools #&gt; This is the global version of package relaimpo. #&gt; If you are a non-US user, a version with the interesting additional metric pmvd is available #&gt; from Ulrike Groempings web site at prof.beuth-hochschule.de/groemping. # Build linear regression model model_formula = ozone_reading ~ Temperature_Sandburg + Humidity + Temperature_ElMonte + Month + pressure_height + Inversion_base_height lmMod &lt;- lm(model_formula, data=trainData) # calculate relative importance relImportance &lt;- calc.relimp(lmMod, type = &quot;lmg&quot;, rela = F) # Sort cat(&#39;Relative Importances: \\n&#39;) #&gt; Relative Importances: sort(round(relImportance$lmg, 3), decreasing=TRUE) #&gt; Temperature_ElMonte Temperature_Sandburg pressure_height #&gt; 0.214 0.203 0.104 #&gt; Inversion_base_height Humidity Month #&gt; 0.096 0.086 0.012 Additionally, you can use bootstrapping (using boot.relimp) to compute the confidence intervals of the produced relative importances. bootsub &lt;- boot.relimp(ozone_reading ~ Temperature_Sandburg + Humidity + Temperature_ElMonte + Month + pressure_height + Inversion_base_height, data=trainData, b = 1000, type = &#39;lmg&#39;, rank = TRUE, diff = TRUE) plot(booteval.relimp(bootsub, level=.95)) 23.8 Recursive Feature Elimination (RFE) Recursive feature elimnation (rfe) offers a rigorous way to determine the important variables before you even feed them into a ML algo. It can be implemented using the rfe() from caret package. The rfe() also takes two important parameters. sizes rfeControl So, what does sizes and rfeControl represent? The sizes determines the number of most important features the rfe should iterate. Below, I have set the size as 1 to 5, 10, 15 and 18. Secondly, the rfeControl parameter receives the output of the rfeControl(). You can set what type of variable evaluation algorithm must be used. Here, I have used random forests based rfFuncs. The method='repeatedCV' means it will do a repeated k-Fold cross validation with repeats=5. Once complete, you get the accuracy and kappa for each model size you provided. The final selected model subset size is marked with a * in the rightmost selected column. str(trainData) #&gt; &#39;data.frame&#39;: 366 obs. of 13 variables: #&gt; $ Month : int 1 1 1 1 1 1 1 1 1 1 ... #&gt; $ Day_of_month : int 1 2 3 4 5 6 7 8 9 10 ... #&gt; $ Day_of_week : int 4 5 6 7 1 2 3 4 5 6 ... #&gt; $ ozone_reading : num 3 3 3 5 5 6 4 4 6 7 ... #&gt; $ pressure_height : num 5480 5660 5710 5700 5760 5720 5790 5790 5700 5700 ... #&gt; $ Wind_speed : int 8 6 4 3 3 4 6 3 3 3 ... #&gt; $ Humidity : num 20 41 28 37 51 ... #&gt; $ Temperature_Sandburg : num 40.5 38 40 45 54 ... #&gt; $ Temperature_ElMonte : num 39.8 46.7 49.5 52.3 45.3 ... #&gt; $ Inversion_base_height: num 5000 4109 2693 590 1450 ... #&gt; $ Pressure_gradient : num -15 -14 -25 -24 25 15 -33 -28 23 -2 ... #&gt; $ Inversion_temperature: num 30.6 48 47.7 55 57 ... #&gt; $ Visibility : int 200 300 250 100 60 60 100 250 120 120 ... tic() set.seed(100) options(warn=-1) subsets &lt;- c(1:5, 10, 15, 18) ctrl &lt;- rfeControl(functions = rfFuncs, method = &quot;repeatedcv&quot;, repeats = 5, verbose = FALSE) lmProfile &lt;- rfe(x=trainData[, c(1:3, 5:13)], y=trainData$ozone_reading, sizes = subsets, rfeControl = ctrl) toc() #&gt; 93.301 sec elapsed lmProfile #&gt; #&gt; Recursive feature selection #&gt; #&gt; Outer resampling method: Cross-Validated (10 fold, repeated 5 times) #&gt; #&gt; Resampling performance over subset size: #&gt; #&gt; Variables RMSE Rsquared MAE RMSESD RsquaredSD MAESD Selected #&gt; 1 5.13 0.595 3.92 0.826 0.1275 0.586 #&gt; 2 4.03 0.746 3.11 0.542 0.0743 0.416 #&gt; 3 3.95 0.756 3.06 0.472 0.0670 0.380 #&gt; 4 3.93 0.759 3.01 0.468 0.0683 0.361 #&gt; 5 3.90 0.763 2.98 0.467 0.0659 0.350 #&gt; 10 3.77 0.782 2.85 0.496 0.0734 0.393 * #&gt; 12 3.77 0.781 2.86 0.508 0.0756 0.401 #&gt; #&gt; The top 5 variables (out of 10): #&gt; Temperature_ElMonte, Pressure_gradient, Temperature_Sandburg, Inversion_temperature, Humidity So, it says, Temperature_ElMonte, Pressure_gradient, Temperature_Sandburg, Inversion_temperature, Humidity are the top 5 variables in that order. And the best model size out of the provided models sizes (in subsets) is 10. You can see all of the top 10 variables from ‘lmProfile$optVariables’ that was created using rfe function above. 23.9 Genetic Algorithm You can perform a supervised feature selection with genetic algorithms using the gafs(). This is quite resource expensive so consider that before choosing the number of iterations (iters) and the number of repeats in gafsControl(). tic() # Define control function ga_ctrl &lt;- gafsControl(functions = rfGA, # another option is `caretGA`. method = &quot;cv&quot;, repeats = 3) # Genetic Algorithm feature selection set.seed(100) ga_obj &lt;- gafs(x=trainData[, c(1:3, 5:13)], y=trainData[, 4], iters = 3, # normally much higher (100+) gafsControl = ga_ctrl) toc() #&gt; 641.345 sec elapsed ga_obj #&gt; #&gt; Genetic Algorithm Feature Selection #&gt; #&gt; 366 samples #&gt; 12 predictors #&gt; #&gt; Maximum generations: 3 #&gt; Population per generation: 50 #&gt; Crossover probability: 0.8 #&gt; Mutation probability: 0.1 #&gt; Elitism: 0 #&gt; #&gt; Internal performance values: RMSE, Rsquared #&gt; Subset selection driven to minimize internal RMSE #&gt; #&gt; External performance values: RMSE, Rsquared, MAE #&gt; Best iteration chose by minimizing external RMSE #&gt; External resampling method: Cross-Validated (10 fold) #&gt; #&gt; During resampling: #&gt; * the top 5 selected variables (out of a possible 12): #&gt; Month (100%), Pressure_gradient (100%), Temperature_ElMonte (100%), Humidity (80%), Visibility (80%) #&gt; * on average, 6.8 variables were selected (min = 5, max = 9) #&gt; #&gt; In the final search using the entire training set: #&gt; * 9 features selected at iteration 2 including: #&gt; Month, Day_of_month, pressure_height, Wind_speed, Humidity ... #&gt; * external performance at this iteration is #&gt; #&gt; RMSE Rsquared MAE #&gt; 3.721 0.788 2.800 # Optimal variables ga_obj$optVariables #&gt; [1] &quot;Month&quot; &quot;Day_of_month&quot; &quot;pressure_height&quot; #&gt; [4] &quot;Wind_speed&quot; &quot;Humidity&quot; &quot;Temperature_ElMonte&quot; #&gt; [7] &quot;Inversion_base_height&quot; &quot;Pressure_gradient&quot; &quot;Inversion_temperature&quot; ‘Month’ ‘Day_of_month’ ‘Wind_speed’ ‘Temperature_ElMonte’ ‘Pressure_gradient’ ‘Visibility’ So the optimal variables according to the genetic algorithms are listed above. But, I wouldn’t use it just yet because, the above variant was tuned for only 3 iterations, which is quite low. I had to set it so low to save computing time. 23.10 Simulated Annealing Simulated annealing is a global search algorithm that allows a suboptimal solution to be accepted in hope that a better solution will show up eventually. It works by making small random changes to an initial solution and sees if the performance improved. The change is accepted if it improves, else it can still be accepted if the difference of performances meet an acceptance criteria. In caret it has been implemented in the safs() which accepts a control parameter that can be set using the safsControl() function. safsControl is similar to other control functions in caret (like you saw in rfe and ga), and additionally it accepts an improve parameter which is the number of iterations it should wait without improvement until the values are reset to previous iteration. tic() # Define control function sa_ctrl &lt;- safsControl(functions = rfSA, method = &quot;repeatedcv&quot;, repeats = 3, improve = 5) # n iterations without improvement before a reset # Genetic Algorithm feature selection set.seed(100) sa_obj &lt;- safs(x=trainData[, c(1:3, 5:13)], y=trainData[, 4], safsControl = sa_ctrl) toc() #&gt; 113.111 sec elapsed sa_obj #&gt; #&gt; Simulated Annealing Feature Selection #&gt; #&gt; 366 samples #&gt; 12 predictors #&gt; #&gt; Maximum search iterations: 10 #&gt; Restart after 5 iterations without improvement (0.3 restarts on average) #&gt; #&gt; Internal performance values: RMSE, Rsquared #&gt; Subset selection driven to minimize internal RMSE #&gt; #&gt; External performance values: RMSE, Rsquared, MAE #&gt; Best iteration chose by minimizing external RMSE #&gt; External resampling method: Cross-Validated (10 fold, repeated 3 times) #&gt; #&gt; During resampling: #&gt; * the top 5 selected variables (out of a possible 12): #&gt; Temperature_Sandburg (80%), Month (66.7%), Pressure_gradient (66.7%), Temperature_ElMonte (63.3%), Visibility (60%) #&gt; * on average, 6.5 variables were selected (min = 3, max = 11) #&gt; #&gt; In the final search using the entire training set: #&gt; * 6 features selected at iteration 9 including: #&gt; Day_of_week, pressure_height, Wind_speed, Humidity, Inversion_base_height ... #&gt; * external performance at this iteration is #&gt; #&gt; RMSE Rsquared MAE #&gt; 4.108 0.743 3.111 # Optimal variables print(sa_obj$optVariables) #&gt; [1] &quot;Day_of_week&quot; &quot;pressure_height&quot; &quot;Wind_speed&quot; #&gt; [4] &quot;Humidity&quot; &quot;Inversion_base_height&quot; &quot;Pressure_gradient&quot; 23.11 Information Value and Weights of Evidence The Information Value can be used to judge how important a given categorical variable is in explaining the binary Y variable. It goes well with logistic regression and other classification models that can model binary variables. Let’s try to find out how important the categorical variables are in predicting if an individual will earn &gt; 50k from the adult.csv dataset. Just run the code below to import the dataset. library(InformationValue) #&gt; #&gt; Attaching package: &#39;InformationValue&#39; #&gt; The following objects are masked from &#39;package:caret&#39;: #&gt; #&gt; confusionMatrix, precision, sensitivity, specificity # online data # inputData &lt;- read.csv(&quot;http://rstatistics.net/wp-content/uploads/2015/09/adult.csv&quot;) inputData &lt;- read.csv(file.path(data_raw_dir, &quot;adult.csv&quot;)) print(head(inputData)) #&gt; AGE WORKCLASS FNLWGT EDUCATION EDUCATIONNUM MARITALSTATUS #&gt; 1 39 State-gov 77516 Bachelors 13 Never-married #&gt; 2 50 Self-emp-not-inc 83311 Bachelors 13 Married-civ-spouse #&gt; 3 38 Private 215646 HS-grad 9 Divorced #&gt; 4 53 Private 234721 11th 7 Married-civ-spouse #&gt; 5 28 Private 338409 Bachelors 13 Married-civ-spouse #&gt; 6 37 Private 284582 Masters 14 Married-civ-spouse #&gt; OCCUPATION RELATIONSHIP RACE SEX CAPITALGAIN CAPITALLOSS #&gt; 1 Adm-clerical Not-in-family White Male 2174 0 #&gt; 2 Exec-managerial Husband White Male 0 0 #&gt; 3 Handlers-cleaners Not-in-family White Male 0 0 #&gt; 4 Handlers-cleaners Husband Black Male 0 0 #&gt; 5 Prof-specialty Wife Black Female 0 0 #&gt; 6 Exec-managerial Wife White Female 0 0 #&gt; HOURSPERWEEK NATIVECOUNTRY ABOVE50K #&gt; 1 40 United-States 0 #&gt; 2 13 United-States 0 #&gt; 3 40 United-States 0 #&gt; 4 40 United-States 0 #&gt; 5 40 Cuba 0 #&gt; 6 40 United-States 0 # Choose Categorical Variables to compute Info Value. cat_vars &lt;- c (&quot;WORKCLASS&quot;, &quot;EDUCATION&quot;, &quot;MARITALSTATUS&quot;, &quot;OCCUPATION&quot;, &quot;RELATIONSHIP&quot;, &quot;RACE&quot;, &quot;SEX&quot;, &quot;NATIVECOUNTRY&quot;) # get all categorical variables factor_vars &lt;- cat_vars # Init Output df_iv &lt;- data.frame(VARS=cat_vars, IV=numeric(length(cat_vars)), STRENGTH=character(length(cat_vars)), stringsAsFactors = F) # init output dataframe # Get Information Value for each variable for (factor_var in factor_vars){ df_iv[df_iv$VARS == factor_var, &quot;IV&quot;] &lt;- InformationValue::IV(X=inputData[, factor_var], Y=inputData$ABOVE50K) df_iv[df_iv$VARS == factor_var, &quot;STRENGTH&quot;] &lt;- attr(InformationValue::IV(X=inputData[, factor_var], Y=inputData$ABOVE50K), &quot;howgood&quot;) } # Sort df_iv &lt;- df_iv[order(-df_iv$IV), ] df_iv #&gt; VARS IV STRENGTH #&gt; 5 RELATIONSHIP 1.5356 Highly Predictive #&gt; 3 MARITALSTATUS 1.3388 Highly Predictive #&gt; 4 OCCUPATION 0.7762 Highly Predictive #&gt; 2 EDUCATION 0.7411 Highly Predictive #&gt; 7 SEX 0.3033 Highly Predictive #&gt; 1 WORKCLASS 0.1634 Highly Predictive #&gt; 8 NATIVECOUNTRY 0.0794 Somewhat Predictive #&gt; 6 RACE 0.0693 Somewhat Predictive Here is what the quantum of Information Value means: Less than 0.02, then the predictor is not useful for modeling (separating the Goods from the Bads) 0.02 to 0.1, then the predictor has only a weak relationship. 0.1 to 0.3, then the predictor has a medium strength relationship. 0.3 or higher, then the predictor has a strong relationship. That was about IV. Then what is Weight of Evidence? Weights of evidence can be useful to find out how important a given categorical variable is in explaining the ‘events’ (called ‘Goods’ in below table.) The ‘Information Value’ of the categorical variable can then be derived from the respective WOE values. IV=(perc good of all goods−perc bad of all bads) *WOE The ‘WOETable’ below given the computation in more detail. WOETable(X=inputData[, &#39;WORKCLASS&#39;], Y=inputData$ABOVE50K) #&gt; CAT GOODS BADS TOTAL PCT_G PCT_B WOE IV #&gt; 1 ? 191 1645 1836 0.02429 0.066545 -1.008 0.042574 #&gt; 2 Federal-gov 371 589 960 0.04719 0.023827 0.683 0.015964 #&gt; 3 Local-gov 617 1476 2093 0.07848 0.059709 0.273 0.005131 #&gt; 4 Never-worked 7 7 7 0.00089 0.000283 1.146 0.000696 #&gt; 5 Private 4963 17733 22696 0.63126 0.717354 -0.128 0.011006 #&gt; 6 Self-emp-inc 622 494 1116 0.07911 0.019984 1.376 0.081363 #&gt; 7 Self-emp-not-inc 724 1817 2541 0.09209 0.073503 0.225 0.004190 #&gt; 8 State-gov 353 945 1298 0.04490 0.038228 0.161 0.001073 #&gt; 9 Without-pay 14 14 14 0.00178 0.000566 1.146 0.001391 The total IV of a variable is the sum of IV’s of its categories. 23.12 DALEX Package The DALEX is a powerful package that explains various things about the variables used in an ML model. For example, using the variable_dropout() function you can find out how important a variable is based on a dropout loss, that is how much loss is incurred by removing a variable from the model. Apart from this, it also has the single_variable() function that gives you an idea of how the model’s output will change by changing the values of one of the X’s in the model. It also has the single_prediction() that can decompose a single model prediction so as to understand which variable caused what effect in predicting the value of Y. library(randomForest) #&gt; randomForest 4.6-14 #&gt; Type rfNews() to see new features/changes/bug fixes. #&gt; #&gt; Attaching package: &#39;randomForest&#39; #&gt; The following object is masked from &#39;package:dplyr&#39;: #&gt; #&gt; combine #&gt; The following object is masked from &#39;package:ranger&#39;: #&gt; #&gt; importance #&gt; The following object is masked from &#39;package:ggplot2&#39;: #&gt; #&gt; margin library(DALEX) #&gt; Welcome to DALEX (version: 0.3.0). #&gt; This is a plain DALEX. Use &#39;install_dependencies()&#39; to get all required packages. #&gt; #&gt; Attaching package: &#39;DALEX&#39; #&gt; The following object is masked from &#39;package:dplyr&#39;: #&gt; #&gt; explain # Load data # inputData &lt;- read.csv(&quot;http://rstatistics.net/wp-content/uploads/2015/09/adult.csv&quot;) inputData &lt;- read.csv(file.path(data_raw_dir, &quot;adult.csv&quot;)) # Train random forest model rf_mod &lt;- randomForest(factor(ABOVE50K) ~ ., data=inputData, ntree=100) rf_mod #&gt; #&gt; Call: #&gt; randomForest(formula = factor(ABOVE50K) ~ ., data = inputData, ntree = 100) #&gt; Type of random forest: classification #&gt; Number of trees: 100 #&gt; No. of variables tried at each split: 3 #&gt; #&gt; OOB estimate of error rate: 13.6% #&gt; Confusion matrix: #&gt; 0 1 class.error #&gt; 0 23051 1669 0.0675 #&gt; 1 2754 5087 0.3512 # Variable importance with DALEX explained_rf &lt;- explain(rf_mod, data=inputData, y=inputData$ABOVE50K) # Get the variable importances varimps = variable_dropout(explained_rf, type=&#39;raw&#39;) print(varimps) #&gt; variable dropout_loss label #&gt; 1 _full_model_ 31.6 randomForest #&gt; 2 ABOVE50K 31.6 randomForest #&gt; 3 RACE 36.6 randomForest #&gt; 4 SEX 39.4 randomForest #&gt; 5 CAPITALLOSS 39.9 randomForest #&gt; 6 NATIVECOUNTRY 40.3 randomForest #&gt; 7 WORKCLASS 51.0 randomForest #&gt; 8 CAPITALGAIN 53.8 randomForest #&gt; 9 FNLWGT 56.2 randomForest #&gt; 10 HOURSPERWEEK 56.7 randomForest #&gt; 11 EDUCATION 58.0 randomForest #&gt; 12 RELATIONSHIP 58.5 randomForest #&gt; 13 EDUCATIONNUM 59.2 randomForest #&gt; 14 MARITALSTATUS 71.0 randomForest #&gt; 15 OCCUPATION 83.1 randomForest #&gt; 16 AGE 86.8 randomForest #&gt; 17 _baseline_ 304.4 randomForest plot(varimps) 23.13 Conclusion Hope you find these methods useful. As it turns out different methods showed different variables as important, or at least the degree of importance changed. This need not be a conflict, because each method gives a different perspective of how the variable can be useful depending on how the algorithms learn Y ~ x. So its cool. If you find any code breaks or bugs, report the issue here or just write it below. "],
["imputting-missing-values-with-random-forest.html", "Chapter 24 Imputting missing values with Random Forest 24.1 Flu Prediction. fluH7N9_china_2013 dataset 24.2 The data 24.3 Features 24.4 Imputing missing values 24.5 Test, train and validation data sets 24.6 Machine Learning algorithms 24.7 Comparing accuracy of models", " Chapter 24 Imputting missing values with Random Forest 24.1 Flu Prediction. fluH7N9_china_2013 dataset Source: https://shirinsplayground.netlify.com/2018/04/flu_prediction/ library(outbreaks) library(tidyverse) library(plyr) library(mice) library(caret) library(purrr) library(&quot;tibble&quot;) library(&quot;dplyr&quot;) library(&quot;tidyr&quot;) Since I migrated my blog from Github Pages to blogdown and Netlify, I wanted to start migrating (most of) my old posts too - and use that opportunity to update them and make sure the code still works. Here I am updating my very first machine learning post from 27 Nov 2016: Can we predict flu deaths with Machine Learning and R?. Changes are marked as bold comments. The main changes I made are: using the tidyverse more consistently throughout the analysis focusing on comparing multiple imputations from the mice package, rather than comparing different algorithms using purrr, map(), nest() and unnest() to model and predict the machine learning algorithm over the different imputed datasets Among the many nice R packages containing data collections is the outbreaks package. It contains a dataset on epidemics and among them is data from the 2013 outbreak of influenza A H7N9 in China as analysed by Kucharski et al. (2014): A. Kucharski, H. Mills, A. Pinsent, C. Fraser, M. Van Kerkhove, C. A. Donnelly, and S. Riley. 2014. Distinguishing between reservoir exposure and human-to-human transmission for emerging pathogens using case onset data. PLOS Currents Outbreaks. Mar 7, edition 1. doi: 10.1371/currents.outbreaks.e1473d9bfc99d080ca242139a06c455f. A. Kucharski, H. Mills, A. Pinsent, C. Fraser, M. Van Kerkhove, C. A. Donnelly, and S. Riley. 2014. Data from: Distinguishing between reservoir exposure and human-to-human transmission for emerging pathogens using case onset data. Dryad Digital Repository. http://dx.doi.org/10.5061/dryad.2g43n. I will be using their data as an example to show how to use Machine Learning algorithms for predicting disease outcome. 24.2 The data The dataset contains case ID, date of onset, date of hospitalization, date of outcome, gender, age, province and of course outcome: Death or Recovery. 24.2.1 Pre-processing Change: variable names (i.e. column names) have been renamed, dots have been replaced with underscores, letters are all lower case now. Change: I am using the tidyverse notation more consistently. First, I’m doing some preprocessing, including: renaming missing data as NA adding an ID column setting column types gathering date columns changing factor names of dates (to make them look nicer in plots) and of province (to combine provinces with few cases) from1 &lt;- c(&quot;date_of_onset&quot;, &quot;date_of_hospitalisation&quot;, &quot;date_of_outcome&quot;) to1 &lt;- c(&quot;date of onset&quot;, &quot;date of hospitalisation&quot;, &quot;date of outcome&quot;) from2 &lt;- c(&quot;Anhui&quot;, &quot;Beijing&quot;, &quot;Fujian&quot;, &quot;Guangdong&quot;, &quot;Hebei&quot;, &quot;Henan&quot;, &quot;Hunan&quot;, &quot;Jiangxi&quot;, &quot;Shandong&quot;, &quot;Taiwan&quot;) to2 &lt;- rep(&quot;Other&quot;, 10) fluH7N9_china_2013$age[which(fluH7N9_china_2013$age == &quot;?&quot;)] &lt;- NA fluH7N9_china_2013_gather &lt;- fluH7N9_china_2013 %&gt;% mutate(case_id = paste(&quot;case&quot;, case_id, sep = &quot;_&quot;), age = as.numeric(age)) %&gt;% gather(Group, Date, date_of_onset:date_of_outcome) %&gt;% mutate(Group = as.factor(mapvalues(Group, from = from1, to = to1)), province = mapvalues(province, from = from2, to = to2)) fluH7N9_china_2013 &lt;- as.tibble(fluH7N9_china_2013) #&gt; Warning: `as.tibble()` is deprecated, use `as_tibble()` (but mind the new semantics). #&gt; This warning is displayed once per session. fluH7N9_china_2013_gather &lt;- as.tibble(fluH7N9_china_2013_gather) print(fluH7N9_china_2013) #&gt; # A tibble: 136 x 8 #&gt; case_id date_of_onset date_of_hospita… date_of_outcome outcome gender #&gt; &lt;fct&gt; &lt;date&gt; &lt;date&gt; &lt;date&gt; &lt;fct&gt; &lt;fct&gt; #&gt; 1 1 2013-02-19 NA 2013-03-04 Death m #&gt; 2 2 2013-02-27 2013-03-03 2013-03-10 Death m #&gt; 3 3 2013-03-09 2013-03-19 2013-04-09 Death f #&gt; 4 4 2013-03-19 2013-03-27 NA &lt;NA&gt; f #&gt; 5 5 2013-03-19 2013-03-30 2013-05-15 Recover f #&gt; 6 6 2013-03-21 2013-03-28 2013-04-26 Death f #&gt; # … with 130 more rows, and 2 more variables: age &lt;fct&gt;, province &lt;fct&gt; I’m also adding a third gender level for unknown gender levels(fluH7N9_china_2013_gather$gender) &lt;- c(levels(fluH7N9_china_2013_gather$gender), &quot;unknown&quot;) fluH7N9_china_2013_gather$gender[is.na(fluH7N9_china_2013_gather$gender)] &lt;- &quot;unknown&quot; print(fluH7N9_china_2013_gather) #&gt; # A tibble: 408 x 7 #&gt; case_id outcome gender age province Group Date #&gt; &lt;chr&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;date&gt; #&gt; 1 case_1 Death m 58 Shanghai date of onset 2013-02-19 #&gt; 2 case_2 Death m 7 Shanghai date of onset 2013-02-27 #&gt; 3 case_3 Death f 11 Other date of onset 2013-03-09 #&gt; 4 case_4 &lt;NA&gt; f 18 Jiangsu date of onset 2013-03-19 #&gt; 5 case_5 Recover f 20 Jiangsu date of onset 2013-03-19 #&gt; 6 case_6 Death f 9 Jiangsu date of onset 2013-03-21 #&gt; # … with 402 more rows For plotting, I am defining a custom ggplot2 theme: my_theme &lt;- function(base_size = 12, base_family = &quot;sans&quot;){ theme_minimal(base_size = base_size, base_family = base_family) + theme( axis.text = element_text(size = 12), axis.text.x = element_text(angle = 45, vjust = 0.5, hjust = 0.5), axis.title = element_text(size = 14), panel.grid.major = element_line(color = &quot;grey&quot;), panel.grid.minor = element_blank(), panel.background = element_rect(fill = &quot;aliceblue&quot;), strip.background = element_rect(fill = &quot;lightgrey&quot;, color = &quot;grey&quot;, size = 1), strip.text = element_text(face = &quot;bold&quot;, size = 12, color = &quot;black&quot;), legend.position = &quot;bottom&quot;, legend.justification = &quot;top&quot;, legend.box = &quot;horizontal&quot;, legend.box.background = element_rect(colour = &quot;grey50&quot;), legend.background = element_blank(), panel.border = element_rect(color = &quot;grey&quot;, fill = NA, size = 0.5) ) } And use that theme to visualize the data: ggplot(data = fluH7N9_china_2013_gather, aes(x = Date, y = age, fill = outcome)) + stat_density2d(aes(alpha = ..level..), geom = &quot;polygon&quot;) + geom_jitter(aes(color = outcome, shape = gender), size = 1.5) + geom_rug(aes(color = outcome)) + scale_y_continuous(limits = c(0, 90)) + labs( fill = &quot;Outcome&quot;, color = &quot;Outcome&quot;, alpha = &quot;Level&quot;, shape = &quot;Gender&quot;, x = &quot;Date in 2013&quot;, y = &quot;Age&quot;, title = &quot;2013 Influenza A H7N9 cases in China&quot;, subtitle = &quot;Dataset from &#39;outbreaks&#39; package (Kucharski et al. 2014)&quot;, caption = &quot;&quot; ) + facet_grid(Group ~ province) + my_theme() + scale_shape_manual(values = c(15, 16, 17)) + scale_color_brewer(palette=&quot;Set1&quot;, na.value = &quot;grey50&quot;) + scale_fill_brewer(palette=&quot;Set1&quot;) #&gt; Warning: Removed 149 rows containing non-finite values (stat_density2d). #&gt; Warning: Removed 149 rows containing missing values (geom_point). ggplot(data = fluH7N9_china_2013_gather, aes(x = Date, y = age, color = outcome)) + geom_point(aes(color = outcome, shape = gender), size = 1.5, alpha = 0.6) + geom_path(aes(group = case_id)) + facet_wrap( ~ province, ncol = 2) + my_theme() + scale_shape_manual(values = c(15, 16, 17)) + scale_color_brewer(palette=&quot;Set1&quot;, na.value = &quot;grey50&quot;) + scale_fill_brewer(palette=&quot;Set1&quot;) + labs( color = &quot;Outcome&quot;, shape = &quot;Gender&quot;, x = &quot;Date in 2013&quot;, y = &quot;Age&quot;, title = &quot;2013 Influenza A H7N9 cases in China&quot;, subtitle = &quot;Dataset from &#39;outbreaks&#39; package (Kucharski et al. 2014)&quot;, caption = &quot;\\nTime from onset of flu to outcome.&quot; ) #&gt; Warning: Removed 149 rows containing missing values (geom_point). #&gt; Warning: Removed 122 rows containing missing values (geom_path). 24.3 Features In machine learning-speak features are what we call the variables used for model training. Using the right features dramatically influences the accuracy and success of your model. For this example, I am keeping age, but I am also generating new features from the date information and converting gender and province into numerical values. delta_dates &lt;- function(onset, ref) { d2 = as.Date(as.character(onset), format = &quot;%Y-%m-%d&quot;) d1 = as.Date(as.character(ref), format = &quot;%Y-%m-%d&quot;) as.numeric(as.character(gsub(&quot; days&quot;, &quot;&quot;, d1 - d2))) } dataset &lt;- fluH7N9_china_2013 %&gt;% mutate( hospital = as.factor(ifelse(is.na(date_of_hospitalisation), 0, 1)), gender_f = as.factor(ifelse(gender == &quot;f&quot;, 1, 0)), province_Jiangsu = as.factor(ifelse(province == &quot;Jiangsu&quot;, 1, 0)), province_Shanghai = as.factor(ifelse(province == &quot;Shanghai&quot;, 1, 0)), province_Zhejiang = as.factor(ifelse(province == &quot;Zhejiang&quot;, 1, 0)), province_other = as.factor(ifelse(province == &quot;Zhejiang&quot; | province == &quot;Jiangsu&quot; | province == &quot;Shanghai&quot;, 0, 1)), days_onset_to_outcome = delta_dates(date_of_onset, date_of_outcome), days_onset_to_hospital = delta_dates(date_of_onset, date_of_hospitalisation), age = age, early_onset = as.factor(ifelse(date_of_onset &lt; summary(date_of_onset)[[3]], 1, 0)), early_outcome = as.factor(ifelse(date_of_outcome &lt; summary(date_of_outcome)[[3]], 1, 0)) ) %&gt;% subset(select = -c(2:4, 6, 8)) rownames(dataset) &lt;- dataset$case_id #&gt; Warning: Setting row names on a tibble is deprecated. dataset[, -2] &lt;- as.numeric(as.matrix(dataset[, -2])) print(dataset) #&gt; # A tibble: 136 x 13 #&gt; case_id outcome age hospital gender_f province_Jiangsu province_Shangh… #&gt; * &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 Death 87 0 0 0 1 #&gt; 2 2 Death 27 1 0 0 1 #&gt; 3 3 Death 35 1 1 0 0 #&gt; 4 4 &lt;NA&gt; 45 1 1 1 0 #&gt; 5 5 Recover 48 1 1 1 0 #&gt; 6 6 Death 32 1 1 1 0 #&gt; # … with 130 more rows, and 6 more variables: province_Zhejiang &lt;dbl&gt;, #&gt; # province_other &lt;dbl&gt;, days_onset_to_outcome &lt;dbl&gt;, #&gt; # days_onset_to_hospital &lt;dbl&gt;, early_onset &lt;dbl&gt;, early_outcome &lt;dbl&gt; summary(dataset$outcome) #&gt; Death Recover NA&#39;s #&gt; 32 47 57 24.4 Imputing missing values I am using the mice package for imputing missing values Note: Since publishing this blogpost I learned that the idea behind using mice is to compare different imputations to see how stable they are, instead of picking one imputed set as fixed for the remainder of the analysis. Therefore, I changed the focus of this post a little bit: in the old post I compared many different algorithms and their outcome; in this updated version I am only showing the Random Forest algorithm and focus on comparing the different imputed datasets. I am ignoring feature importance and feature plots because nothing changed compared to the old post. # plot the missing data in a matrix by variables md_pattern &lt;- md.pattern(dataset, rotate.names = TRUE) dataset_impute &lt;- mice(data = dataset[, -2], print = FALSE) #&gt; Warning: Number of logged events: 150 24.4.1 Generate a dataframe of five imputting strategies by default, mice() calculates five (m = 5) imputed data sets we can combine them all in one output with the complete(“long”) function I did not want to impute missing values in the outcome column, so I have to merge it back in with the imputed data # c(1,2): case_id, outcome datasets_complete &lt;- right_join(dataset[, c(1, 2)], complete(dataset_impute, &quot;long&quot;), by = &quot;case_id&quot;) %&gt;% mutate(.imp = as.factor(.imp)) %&gt;% select(-.id) %&gt;% print() #&gt; # A tibble: 680 x 14 #&gt; case_id outcome .imp age hospital gender_f province_Jiangsu #&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 Death 1 87 0 0 0 #&gt; 2 2 Death 1 27 1 0 0 #&gt; 3 3 Death 1 35 1 1 0 #&gt; 4 4 &lt;NA&gt; 1 45 1 1 1 #&gt; 5 5 Recover 1 48 1 1 1 #&gt; 6 6 Death 1 32 1 1 1 #&gt; # … with 674 more rows, and 7 more variables: province_Shanghai &lt;dbl&gt;, #&gt; # province_Zhejiang &lt;dbl&gt;, province_other &lt;dbl&gt;, #&gt; # days_onset_to_outcome &lt;dbl&gt;, days_onset_to_hospital &lt;dbl&gt;, #&gt; # early_onset &lt;dbl&gt;, early_outcome &lt;dbl&gt; Let’s compare the distributions of the five different imputed datasets: 24.4.2 plot effect of imputting on features datasets_complete %&gt;% gather(x, y, age:early_outcome) %&gt;% ggplot(aes(x = y, fill = .imp, color = .imp)) + geom_density(alpha = 0.20) + facet_wrap(~ x, ncol = 3, scales = &quot;free&quot;) + scale_fill_brewer(palette=&quot;Set1&quot;, na.value = &quot;grey50&quot;) + scale_color_brewer(palette=&quot;Set1&quot;, na.value = &quot;grey50&quot;) + my_theme() 24.5 Test, train and validation data sets Now, we can go ahead with machine learning! The dataset contains a few missing values in the outcome column; those will be the test set used for final predictions (see the old blog post for this). length(which(is.na(datasets_complete$outcome))) length(which(!is.na(datasets_complete$outcome))) #&gt; [1] 285 #&gt; [1] 395 train_index &lt;- which(is.na(datasets_complete$outcome)) train_data &lt;- datasets_complete[-train_index, ] test_data &lt;- datasets_complete[train_index, -2] # remove variable outcome print(train_data) #&gt; # A tibble: 395 x 14 #&gt; case_id outcome .imp age hospital gender_f province_Jiangsu #&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 Death 1 87 0 0 0 #&gt; 2 2 Death 1 27 1 0 0 #&gt; 3 3 Death 1 35 1 1 0 #&gt; 4 5 Recover 1 48 1 1 1 #&gt; 5 6 Death 1 32 1 1 1 #&gt; 6 7 Death 1 83 1 0 1 #&gt; # … with 389 more rows, and 7 more variables: province_Shanghai &lt;dbl&gt;, #&gt; # province_Zhejiang &lt;dbl&gt;, province_other &lt;dbl&gt;, #&gt; # days_onset_to_outcome &lt;dbl&gt;, days_onset_to_hospital &lt;dbl&gt;, #&gt; # early_onset &lt;dbl&gt;, early_outcome &lt;dbl&gt; # outcome variable removed print(test_data) #&gt; # A tibble: 285 x 13 #&gt; case_id .imp age hospital gender_f province_Jiangsu province_Shangh… #&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 4 1 45 1 1 1 0 #&gt; 2 9 1 67 1 0 0 0 #&gt; 3 15 1 61 0 1 1 0 #&gt; 4 16 1 79 0 0 1 0 #&gt; 5 22 1 85 1 0 1 0 #&gt; 6 28 1 79 0 0 0 0 #&gt; # … with 279 more rows, and 6 more variables: province_Zhejiang &lt;dbl&gt;, #&gt; # province_other &lt;dbl&gt;, days_onset_to_outcome &lt;dbl&gt;, #&gt; # days_onset_to_hospital &lt;dbl&gt;, early_onset &lt;dbl&gt;, early_outcome &lt;dbl&gt; The remainder of the data will be used for modeling. Here, I am splitting the data into 70% training and 30% test data. Because I want to model each imputed dataset separately, I am using the nest() and map() functions. train_data_nest &lt;- train_data %&gt;% group_by(.imp) %&gt;% nest() %&gt;% print() #&gt; # A tibble: 5 x 2 #&gt; .imp data #&gt; &lt;fct&gt; &lt;list&gt; #&gt; 1 1 &lt;tibble [79 × 13]&gt; #&gt; 2 2 &lt;tibble [79 × 13]&gt; #&gt; 3 3 &lt;tibble [79 × 13]&gt; #&gt; 4 4 &lt;tibble [79 × 13]&gt; #&gt; 5 5 &lt;tibble [79 × 13]&gt; # split the training data in validation training and validation test set.seed(42) val_data &lt;- train_data_nest %&gt;% mutate(val_index = map(data, ~ createDataPartition(.$outcome, p = 0.7, list = FALSE)), val_train_data = map2(data, val_index, ~ .x[.y, ]), val_test_data = map2(data, val_index, ~ .x[-.y, ])) %&gt;% print() #&gt; # A tibble: 5 x 5 #&gt; .imp data val_index val_train_data val_test_data #&gt; &lt;fct&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; #&gt; 1 1 &lt;tibble [79 × … &lt;int[,1] [56 × … &lt;tibble [56 × 13… &lt;tibble [23 × 1… #&gt; 2 2 &lt;tibble [79 × … &lt;int[,1] [56 × … &lt;tibble [56 × 13… &lt;tibble [23 × 1… #&gt; 3 3 &lt;tibble [79 × … &lt;int[,1] [56 × … &lt;tibble [56 × 13… &lt;tibble [23 × 1… #&gt; 4 4 &lt;tibble [79 × … &lt;int[,1] [56 × … &lt;tibble [56 × 13… &lt;tibble [23 × 1… #&gt; 5 5 &lt;tibble [79 × … &lt;int[,1] [56 × … &lt;tibble [56 × 13… &lt;tibble [23 × 1… 24.6 Machine Learning algorithms 24.6.1 Random Forest To make the code tidier, I am first defining the modeling function with the parameters I want. model_function &lt;- function(df) { caret::train(outcome ~ ., data = df, method = &quot;rf&quot;, preProcess = c(&quot;scale&quot;, &quot;center&quot;), trControl = trainControl(method = &quot;repeatedcv&quot;, number = 5, repeats = 3, verboseIter = FALSE)) } 24.6.2 Add model and prediction to nested dataframe and calculate Next, I am using the nested tibble from before to map() the model function, predict the outcome and calculate confusion matrices. 24.6.2.1 add model list-column val_data_model &lt;- val_data %&gt;% mutate(model = map(val_train_data, ~ model_function(.x))) %&gt;% select(-val_index) %&gt;% print() #&gt; # A tibble: 5 x 5 #&gt; .imp data val_train_data val_test_data model #&gt; &lt;fct&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; #&gt; 1 1 &lt;tibble [79 × 13]&gt; &lt;tibble [56 × 13]&gt; &lt;tibble [23 × 13]&gt; &lt;train&gt; #&gt; 2 2 &lt;tibble [79 × 13]&gt; &lt;tibble [56 × 13]&gt; &lt;tibble [23 × 13]&gt; &lt;train&gt; #&gt; 3 3 &lt;tibble [79 × 13]&gt; &lt;tibble [56 × 13]&gt; &lt;tibble [23 × 13]&gt; &lt;train&gt; #&gt; 4 4 &lt;tibble [79 × 13]&gt; &lt;tibble [56 × 13]&gt; &lt;tibble [23 × 13]&gt; &lt;train&gt; #&gt; 5 5 &lt;tibble [79 × 13]&gt; &lt;tibble [56 × 13]&gt; &lt;tibble [23 × 13]&gt; &lt;train&gt; 24.6.2.2 add prediction and confusion matrix list-columns set.seed(42) val_data_model &lt;- val_data_model %&gt;% mutate( predict = map2(model, val_test_data, ~ data.frame(prediction = predict(.x, .y[, -2]))), predict_prob = map2(model, val_test_data, ~ data.frame(outcome = .y[, 2], prediction = predict(.x, .y[, -2], type = &quot;prob&quot;))), confusion_matrix = map2(val_test_data, predict, ~ confusionMatrix(.x$outcome, .y$prediction)), confusion_matrix_tbl = map(confusion_matrix, ~ as.tibble(.x$table))) %&gt;% print() #&gt; # A tibble: 5 x 9 #&gt; .imp data val_train_data val_test_data model predict predict_prob #&gt; &lt;fct&gt; &lt;lis&gt; &lt;list&gt; &lt;list&gt; &lt;lis&gt; &lt;list&gt; &lt;list&gt; #&gt; 1 1 &lt;tib… &lt;tibble [56 ×… &lt;tibble [23 … &lt;tra… &lt;df[,1… &lt;df[,3] [23… #&gt; 2 2 &lt;tib… &lt;tibble [56 ×… &lt;tibble [23 … &lt;tra… &lt;df[,1… &lt;df[,3] [23… #&gt; 3 3 &lt;tib… &lt;tibble [56 ×… &lt;tibble [23 … &lt;tra… &lt;df[,1… &lt;df[,3] [23… #&gt; 4 4 &lt;tib… &lt;tibble [56 ×… &lt;tibble [23 … &lt;tra… &lt;df[,1… &lt;df[,3] [23… #&gt; 5 5 &lt;tib… &lt;tibble [56 ×… &lt;tibble [23 … &lt;tra… &lt;df[,1… &lt;df[,3] [23… #&gt; # … with 2 more variables: confusion_matrix &lt;list&gt;, #&gt; # confusion_matrix_tbl &lt;list&gt; Finally, we have a nested dataframe of 5 rows or cases, one per imputting strategy with its corresponding models and prediction results. 24.7 Comparing accuracy of models To compare how the different imputations did, I am plotting the confusion matrices: val_data_model_unnest &lt;- val_data_model %&gt;% unnest(confusion_matrix_tbl) %&gt;% print() #&gt; # A tibble: 20 x 4 #&gt; .imp Prediction Reference n #&gt; &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 1 Death Death 5 #&gt; 2 1 Recover Death 3 #&gt; 3 1 Death Recover 4 #&gt; 4 1 Recover Recover 11 #&gt; 5 2 Death Death 3 #&gt; 6 2 Recover Death 3 #&gt; # … with 14 more rows val_data_model_unnest %&gt;% ggplot(aes(x = Prediction, y = Reference, fill = n)) + facet_wrap(~ .imp, ncol = 5, scales = &quot;free&quot;) + geom_tile() + my_theme() and the prediction probabilities for correct and wrong predictions: val_data_model_gather &lt;- val_data_model %&gt;% unnest(predict_prob) %&gt;% gather(x, y, prediction.Death:prediction.Recover) %&gt;% print() #&gt; # A tibble: 230 x 4 #&gt; .imp outcome x y #&gt; &lt;fct&gt; &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 1 Death prediction.Death 0.758 #&gt; 2 1 Recover prediction.Death 0.864 #&gt; 3 1 Death prediction.Death 0.828 #&gt; 4 1 Death prediction.Death 0.828 #&gt; 5 1 Recover prediction.Death 0.342 #&gt; 6 1 Recover prediction.Death 0.552 #&gt; # … with 224 more rows val_data_model_gather %&gt;% ggplot(aes(x = x, y = y, fill = outcome)) + facet_wrap(~ .imp, ncol = 5) + geom_boxplot() + scale_fill_brewer(palette=&quot;Set1&quot;, na.value = &quot;grey50&quot;) + my_theme() Hope, you found that example interesting and helpful! sessionInfo() #&gt; R version 3.6.0 (2019-04-26) #&gt; Platform: x86_64-pc-linux-gnu (64-bit) #&gt; Running under: Ubuntu 18.04.3 LTS #&gt; #&gt; Matrix products: default #&gt; BLAS/LAPACK: /usr/lib/x86_64-linux-gnu/libopenblasp-r0.2.20.so #&gt; #&gt; locale: #&gt; [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C #&gt; [3] LC_TIME=en_US.UTF-8 LC_COLLATE=en_US.UTF-8 #&gt; [5] LC_MONETARY=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8 #&gt; [7] LC_PAPER=en_US.UTF-8 LC_NAME=C #&gt; [9] LC_ADDRESS=C LC_TELEPHONE=C #&gt; [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C #&gt; #&gt; attached base packages: #&gt; [1] stats graphics grDevices utils datasets methods base #&gt; #&gt; other attached packages: #&gt; [1] caret_6.0-84 mice_3.4.0 lattice_0.20-38 plyr_1.8.4 #&gt; [5] forcats_0.4.0 stringr_1.4.0 dplyr_0.8.0.1 purrr_0.3.2 #&gt; [9] readr_1.3.1 tidyr_0.8.3 tibble_2.1.1 ggplot2_3.1.1 #&gt; [13] tidyverse_1.2.1 outbreaks_1.5.0 logging_0.9-107 #&gt; #&gt; loaded via a namespace (and not attached): #&gt; [1] nlme_3.1-139 lubridate_1.7.4 RColorBrewer_1.1-2 #&gt; [4] httr_1.4.0 rprojroot_1.3-2 tools_3.6.0 #&gt; [7] backports_1.1.4 utf8_1.1.4 R6_2.4.0 #&gt; [10] rpart_4.1-15 lazyeval_0.2.2 colorspace_1.4-1 #&gt; [13] jomo_2.6-7 nnet_7.3-12 withr_2.1.2 #&gt; [16] tidyselect_0.2.5 compiler_3.6.0 cli_1.1.0 #&gt; [19] rvest_0.3.3 xml2_1.2.0 labeling_0.3 #&gt; [22] bookdown_0.10 scales_1.0.0 randomForest_4.6-14 #&gt; [25] digest_0.6.18 minqa_1.2.4 rmarkdown_1.12 #&gt; [28] pkgconfig_2.0.2 htmltools_0.3.6 lme4_1.1-21 #&gt; [31] rlang_0.3.4 readxl_1.3.1 rstudioapi_0.10 #&gt; [34] generics_0.0.2 jsonlite_1.6 ModelMetrics_1.2.2 #&gt; [37] magrittr_1.5 Matrix_1.2-17 Rcpp_1.0.1 #&gt; [40] munsell_0.5.0 fansi_0.4.0 stringi_1.4.3 #&gt; [43] yaml_2.2.0 MASS_7.3-51.4 recipes_0.1.5 #&gt; [46] grid_3.6.0 parallel_3.6.0 mitml_0.3-7 #&gt; [49] crayon_1.3.4 haven_2.1.0 splines_3.6.0 #&gt; [52] hms_0.4.2 zeallot_0.1.0 knitr_1.22 #&gt; [55] pillar_1.4.0 boot_1.3-22 reshape2_1.4.3 #&gt; [58] codetools_0.2-16 stats4_3.6.0 pan_1.6 #&gt; [61] glue_1.3.1 evaluate_0.13 data.table_1.12.2 #&gt; [64] modelr_0.1.4 vctrs_0.1.0 nloptr_1.2.1 #&gt; [67] foreach_1.4.4 cellranger_1.1.0 gtable_0.3.0 #&gt; [70] assertthat_0.2.1 xfun_0.6 gower_0.2.0 #&gt; [73] prodlim_2018.04.18 broom_0.5.2 e1071_1.7-1 #&gt; [76] class_7.3-15 survival_2.44-1.1 timeDate_3043.102 #&gt; [79] iterators_1.0.10 lava_1.6.5 ipred_0.9-9 "],
["pca-prcomp-vs-princomp.html", "Chapter 25 PCA: prcomp vs princomp 25.1 General methods for principal component analysis 25.2 prcomp() and princomp() functions 25.3 factoextra 25.4 demo dataset 25.5 Compute PCA in R using prcomp() 25.6 Plots: quality and contribution 25.7 Access to the PCA results 25.8 Predict using PCA 25.9 Supplementary variables 25.10 Theory behind PCA results", " Chapter 25 PCA: prcomp vs princomp http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/118-principal-component-analysis-in-r-prcomp-vs-princomp/ 25.1 General methods for principal component analysis There are two general methods to perform PCA in R : Spectral decomposition which examines the covariances / correlations between variables Singular value decomposition which examines the covariances / correlations between individuals The function princomp() uses the spectral decomposition approach. The functions prcomp() and PCA()[FactoMineR] use the singular value decomposition (SVD). 25.2 prcomp() and princomp() functions The simplified format of these 2 functions are : prcomp(x, scale = FALSE) princomp(x, cor = FALSE, scores = TRUE) Arguments for prcomp(): x: a numeric matrix or data frame scale: a logical value indicating whether the variables should be scaled to have unit variance before the analysis takes place Arguments for princomp(): x: a numeric matrix or data frame cor: a logical value. If TRUE, the data will be centered and scaled before the analysis scores: a logical value. If TRUE, the coordinates on each principal component are calculated 25.3 factoextra # install.packages(&quot;factoextra&quot;) library(factoextra) #&gt; Loading required package: ggplot2 #&gt; Registered S3 methods overwritten by &#39;ggplot2&#39;: #&gt; method from #&gt; [.quosures rlang #&gt; c.quosures rlang #&gt; print.quosures rlang #&gt; Welcome! Related Books: `Practical Guide To Cluster Analysis in R` at https://goo.gl/13EFCZ 25.4 demo dataset We’ll use the data sets decathlon2 [in factoextra], which has been already described at: PCA - Data format. Briefly, it contains: Active individuals (rows 1 to 23) and active variables (columns 1 to 10), which are used to perform the principal component analysis Supplementary individuals (rows 24 to 27) and supplementary variables (columns 11 to 13), which coordinates will be predicted using the PCA information and parameters obtained with active individuals/variables. library(&quot;factoextra&quot;) data(decathlon2) decathlon2.active &lt;- decathlon2[1:23, 1:10] head(decathlon2.active[, 1:6]) #&gt; X100m Long.jump Shot.put High.jump X400m X110m.hurdle #&gt; SEBRLE 11.0 7.58 14.8 2.07 49.8 14.7 #&gt; CLAY 10.8 7.40 14.3 1.86 49.4 14.1 #&gt; BERNARD 11.0 7.23 14.2 1.92 48.9 15.0 #&gt; YURKOV 11.3 7.09 15.2 2.10 50.4 15.3 #&gt; ZSIVOCZKY 11.1 7.30 13.5 2.01 48.6 14.2 #&gt; McMULLEN 10.8 7.31 13.8 2.13 49.9 14.4 decathlon2.supplementary &lt;- decathlon2[24:27, 1:10] head(decathlon2.supplementary[, 1:6]) #&gt; X100m Long.jump Shot.put High.jump X400m X110m.hurdle #&gt; KARPOV 11.0 7.30 14.8 2.04 48.4 14.1 #&gt; WARNERS 11.1 7.60 14.3 1.98 48.7 14.2 #&gt; Nool 10.8 7.53 14.3 1.88 48.8 14.8 #&gt; Drews 10.9 7.38 13.1 1.88 48.5 14.0 25.5 Compute PCA in R using prcomp() In this section we’ll provide an easy-to-use R code to compute and visualize PCA in R using the prcomp() function and the factoextra package. `. Load factoextra for visualization library(factoextra) compute PCA # compute PCA res.pca &lt;- prcomp(decathlon2.active, scale = TRUE) Visualize eigenvalues (scree plot). Show the percentage of variances explained by each principal component. # Visualize eigenvalues (scree plot). fviz_eig(res.pca, addlabels = TRUE, ylim = c(0, 50)) From the plot above, we might want to stop at the fifth principal component. 87% of the information (variances) contained in the data are retained by the first five principal components. 25.6 Plots: quality and contribution Graph of individuals. Individuals with a similar profile are grouped together. # Graph of individuals. fviz_pca_ind(res.pca, col.ind = &quot;cos2&quot;, # Color by the quality of representation gradient.cols = c(&quot;#00AFBB&quot;, &quot;#E7B800&quot;, &quot;#FC4E07&quot;), repel = TRUE # Avoid text overlapping ) Graph of variables. Positive correlated variables point to the same side of the plot. Negative correlated variables point to opposite sides of the graph. # Graph of variables. fviz_pca_var(res.pca, col.var = &quot;contrib&quot;, # Color by contributions to the PC gradient.cols = c(&quot;#00AFBB&quot;, &quot;#E7B800&quot;, &quot;#FC4E07&quot;), repel = TRUE # Avoid text overlapping ) Biplot of individuals and variables # Biplot of individuals and variables fviz_pca_biplot(res.pca, repel = TRUE, col.var = &quot;#2E9FDF&quot;, # Variables color col.ind = &quot;#696969&quot; # Individuals color ) 25.7 Access to the PCA results library(factoextra) # Eigenvalues eig.val &lt;- get_eigenvalue(res.pca) eig.val #&gt; eigenvalue variance.percent cumulative.variance.percent #&gt; Dim.1 4.124 41.24 41.2 #&gt; Dim.2 1.839 18.39 59.6 #&gt; Dim.3 1.239 12.39 72.0 #&gt; Dim.4 0.819 8.19 80.2 #&gt; Dim.5 0.702 7.02 87.2 #&gt; Dim.6 0.423 4.23 91.5 #&gt; Dim.7 0.303 3.03 94.5 #&gt; Dim.8 0.274 2.74 97.2 #&gt; Dim.9 0.155 1.55 98.8 #&gt; Dim.10 0.122 1.22 100.0 # Results for Variables res.var &lt;- get_pca_var(res.pca) res.var$coord # Coordinates #&gt; Dim.1 Dim.2 Dim.3 Dim.4 Dim.5 Dim.6 Dim.7 #&gt; X100m -0.85063 0.1794 -0.3016 0.0336 -0.194 0.03537 -0.09134 #&gt; Long.jump 0.79418 -0.2809 0.1905 -0.1154 0.233 -0.03373 -0.15433 #&gt; Shot.put 0.73391 -0.0854 -0.5176 0.1285 -0.249 -0.23979 -0.00989 #&gt; High.jump 0.61008 0.4652 -0.3301 0.1446 0.403 -0.28464 0.02816 #&gt; X400m -0.70160 -0.2902 -0.2835 0.4308 0.104 -0.04929 0.28611 #&gt; X110m.hurdle -0.76413 0.0247 -0.4489 -0.0169 0.224 0.00263 -0.37007 #&gt; Discus 0.74321 -0.0497 -0.1765 0.3950 -0.408 0.19854 -0.14273 #&gt; Pole.vault -0.21727 -0.8075 -0.0941 -0.3390 -0.222 -0.32746 -0.01039 #&gt; Javeline 0.42823 -0.3861 -0.6041 -0.3317 0.198 0.36210 0.13356 #&gt; X1500m 0.00428 -0.7845 0.2195 0.4480 0.263 0.04205 -0.11137 #&gt; Dim.8 Dim.9 Dim.10 #&gt; X100m -0.10472 -0.3031 0.04442 #&gt; Long.jump -0.39738 -0.0516 0.02972 #&gt; Shot.put 0.02436 0.0478 0.21745 #&gt; High.jump 0.08441 -0.1121 -0.13357 #&gt; X400m -0.23355 0.0822 -0.03417 #&gt; X110m.hurdle -0.00834 0.1618 -0.01563 #&gt; Discus -0.03956 0.0134 -0.17259 #&gt; Pole.vault 0.03291 -0.0258 -0.13721 #&gt; Javeline 0.05284 -0.0405 -0.00385 #&gt; X1500m 0.19447 -0.1022 0.06283 res.var$contrib # Contributions to the PCs #&gt; Dim.1 Dim.2 Dim.3 Dim.4 Dim.5 Dim.6 Dim.7 #&gt; X100m 1.75e+01 1.7505 7.339 0.1376 5.39 0.29592 2.7571 #&gt; Long.jump 1.53e+01 4.2904 2.930 1.6249 7.75 0.26900 7.8716 #&gt; Shot.put 1.31e+01 0.3967 21.620 2.0141 8.82 13.59686 0.0323 #&gt; High.jump 9.02e+00 11.7716 8.793 2.5499 23.12 19.15961 0.2620 #&gt; X400m 1.19e+01 4.5799 6.488 22.6509 1.54 0.57451 27.0527 #&gt; X110m.hurdle 1.42e+01 0.0333 16.261 0.0348 7.17 0.00164 45.2616 #&gt; Discus 1.34e+01 0.1341 2.515 19.0413 23.76 9.32175 6.7323 #&gt; Pole.vault 1.14e+00 35.4619 0.714 14.0231 7.01 25.35762 0.0357 #&gt; Javeline 4.45e+00 8.1087 29.453 13.4296 5.58 31.00496 5.8957 #&gt; X1500m 4.44e-04 33.4729 3.887 24.4939 9.88 0.41813 4.0989 #&gt; Dim.8 Dim.9 Dim.10 #&gt; X100m 3.9952 59.174 1.6176 #&gt; Long.jump 57.5332 1.715 0.7241 #&gt; Shot.put 0.2162 1.471 38.7677 #&gt; High.jump 2.5957 8.102 14.6265 #&gt; X400m 19.8734 4.349 0.9573 #&gt; X110m.hurdle 0.0254 16.858 0.2003 #&gt; Discus 0.5702 0.115 24.4217 #&gt; Pole.vault 0.3947 0.428 15.4356 #&gt; Javeline 1.0173 1.054 0.0122 #&gt; X1500m 13.7787 6.734 3.2370 res.var$cos2 # Quality of representation #&gt; Dim.1 Dim.2 Dim.3 Dim.4 Dim.5 Dim.6 Dim.7 #&gt; X100m 7.24e-01 0.032184 0.09094 0.001127 0.0378 1.25e-03 8.34e-03 #&gt; Long.jump 6.31e-01 0.078881 0.03631 0.013315 0.0544 1.14e-03 2.38e-02 #&gt; Shot.put 5.39e-01 0.007294 0.26791 0.016504 0.0619 5.75e-02 9.77e-05 #&gt; High.jump 3.72e-01 0.216424 0.10896 0.020895 0.1622 8.10e-02 7.93e-04 #&gt; X400m 4.92e-01 0.084203 0.08039 0.185611 0.0108 2.43e-03 8.19e-02 #&gt; X110m.hurdle 5.84e-01 0.000612 0.20150 0.000285 0.0503 6.93e-06 1.37e-01 #&gt; Discus 5.52e-01 0.002466 0.03116 0.156032 0.1667 3.94e-02 2.04e-02 #&gt; Pole.vault 4.72e-02 0.651977 0.00885 0.114911 0.0491 1.07e-01 1.08e-04 #&gt; Javeline 1.83e-01 0.149080 0.36497 0.110048 0.0391 1.31e-01 1.78e-02 #&gt; X1500m 1.83e-05 0.615409 0.04817 0.200713 0.0693 1.77e-03 1.24e-02 #&gt; Dim.8 Dim.9 Dim.10 #&gt; X100m 1.10e-02 0.091848 1.97e-03 #&gt; Long.jump 1.58e-01 0.002661 8.83e-04 #&gt; Shot.put 5.93e-04 0.002284 4.73e-02 #&gt; High.jump 7.12e-03 0.012575 1.78e-02 #&gt; X400m 5.45e-02 0.006750 1.17e-03 #&gt; X110m.hurdle 6.96e-05 0.026166 2.44e-04 #&gt; Discus 1.56e-03 0.000179 2.98e-02 #&gt; Pole.vault 1.08e-03 0.000664 1.88e-02 #&gt; Javeline 2.79e-03 0.001637 1.49e-05 #&gt; X1500m 3.78e-02 0.010453 3.95e-03 # Results for individuals res.ind &lt;- get_pca_ind(res.pca) res.ind$coord # Coordinates #&gt; Dim.1 Dim.2 Dim.3 Dim.4 Dim.5 Dim.6 Dim.7 #&gt; SEBRLE 0.191 -1.554 -0.628 0.0821 1.142614 -0.4639 -0.2080 #&gt; CLAY 0.790 -2.420 1.357 1.2698 -0.806848 1.3042 -0.2129 #&gt; BERNARD -1.329 -1.612 -0.196 -1.9209 0.082343 -0.4006 -0.4064 #&gt; YURKOV -0.869 0.433 -2.474 0.6972 0.398858 0.1029 -0.3249 #&gt; ZSIVOCZKY -0.106 2.023 1.305 -0.0993 -0.197024 0.8955 0.0883 #&gt; McMULLEN 0.119 0.992 0.844 1.3122 1.585871 0.1866 0.4783 #&gt; MARTINEAU -2.392 1.285 -0.898 0.3731 -2.243352 -0.4567 -0.2998 #&gt; HERNU -1.891 -1.178 -0.156 0.8913 -0.126741 0.4362 -0.5661 #&gt; BARRAS -1.774 0.413 0.658 0.2287 -0.233837 0.0903 0.2159 #&gt; NOOL -2.777 1.573 0.607 -1.5555 1.424184 0.4972 -0.5321 #&gt; BOURGUIGNON -4.414 -1.264 -0.010 0.6668 0.419152 -0.0820 -0.5983 #&gt; Sebrle 3.451 -1.217 -1.678 -0.8087 -0.025053 -0.0828 0.0102 #&gt; Clay 3.316 -1.623 -0.618 -0.3168 0.569165 0.7772 0.2575 #&gt; Karpov 4.070 0.798 1.015 0.3134 -0.797426 -0.3296 -1.3637 #&gt; Macey 1.848 2.064 -0.979 0.5847 -0.000216 -0.1973 -0.2693 #&gt; Warners 1.387 -0.282 2.000 -1.0196 -0.040540 -0.5567 -0.2674 #&gt; Zsivoczky 0.472 0.927 -1.728 -0.1848 0.407303 -0.1138 0.0399 #&gt; Hernu 0.276 1.166 0.171 -0.8487 -0.689480 -0.3317 0.4431 #&gt; Bernard 1.367 1.478 0.831 0.7453 0.859802 -0.3281 0.3636 #&gt; Schwarzl -0.710 -0.658 1.041 -0.9272 -0.288757 -0.6889 0.5657 #&gt; Pogorelov -0.214 -0.861 0.298 1.3556 -0.015053 -1.5938 0.7837 #&gt; Schoenbeck -0.495 -1.300 0.103 -0.2493 -0.645226 0.1617 0.8575 #&gt; Barras -0.316 0.819 -0.862 -0.5894 -0.779739 1.1742 0.9451 #&gt; Dim.8 Dim.9 Dim.10 #&gt; SEBRLE 0.04346 -0.65934 0.0327 #&gt; CLAY 0.61724 -0.06013 -0.3172 #&gt; BERNARD 0.70386 0.17008 -0.0991 #&gt; YURKOV 0.11500 -0.10952 -0.1197 #&gt; ZSIVOCZKY -0.20234 -0.52310 -0.3484 #&gt; McMULLEN 0.29309 -0.10562 -0.3932 #&gt; MARTINEAU -0.29163 -0.22342 -0.6164 #&gt; HERNU -1.52940 0.00618 0.5537 #&gt; BARRAS 0.68258 -0.66928 0.5309 #&gt; NOOL -0.43339 -0.11578 -0.0962 #&gt; BOURGUIGNON 0.56362 0.52581 0.0586 #&gt; Sebrle -0.03059 -0.84721 0.2197 #&gt; Clay -0.58064 0.40978 -0.6160 #&gt; Karpov 0.34531 0.19306 0.2172 #&gt; Macey -0.36322 0.36826 0.2125 #&gt; Warners -0.10947 0.18028 0.2421 #&gt; Zsivoczky 0.53804 0.58597 -0.1427 #&gt; Hernu 0.24729 0.06691 -0.2087 #&gt; Bernard 0.00617 0.27949 0.3207 #&gt; Schwarzl -0.68705 -0.00836 -0.3021 #&gt; Pogorelov -0.03762 -0.13053 -0.0370 #&gt; Schoenbeck -0.25585 0.56422 0.2968 #&gt; Barras 0.36555 0.10226 0.6119 res.ind$contrib # Contributions to the PCs #&gt; Dim.1 Dim.2 Dim.3 Dim.4 Dim.5 Dim.6 Dim.7 #&gt; SEBRLE 0.0385 5.712 1.39e+00 0.0357 8.09e+00 2.2126 0.62143 #&gt; CLAY 0.6581 13.854 6.46e+00 8.5557 4.03e+00 17.4880 0.65141 #&gt; BERNARD 1.8627 6.144 1.35e-01 19.5783 4.20e-02 1.6502 2.37365 #&gt; YURKOV 0.7969 0.443 2.15e+01 2.5794 9.86e-01 0.1088 1.51656 #&gt; ZSIVOCZKY 0.0118 9.682 5.97e+00 0.0523 2.41e-01 8.2456 0.11192 #&gt; McMULLEN 0.0148 2.325 2.50e+00 9.1353 1.56e+01 0.3579 3.28702 #&gt; MARTINEAU 6.0337 3.904 2.83e+00 0.7386 3.12e+01 2.1441 1.29111 #&gt; HERNU 3.7700 3.284 8.58e-02 4.2151 9.96e-02 1.9566 4.60485 #&gt; BARRAS 3.3194 0.402 1.52e+00 0.2776 3.39e-01 0.0838 0.67004 #&gt; NOOL 8.1299 5.849 1.29e+00 12.8376 1.26e+01 2.5413 4.06767 #&gt; BOURGUIGNON 20.5373 3.776 3.53e-04 2.3588 1.09e+00 0.0691 5.14425 #&gt; Sebrle 12.5584 3.502 9.88e+00 3.4701 3.89e-03 0.0705 0.00148 #&gt; Clay 11.5936 6.232 1.34e+00 0.5325 2.01e+00 6.2097 0.95282 #&gt; Karpov 17.4661 1.507 3.61e+00 0.5210 3.94e+00 1.1168 26.72016 #&gt; Macey 3.6021 10.073 3.36e+00 1.8139 2.89e-07 0.4001 1.04191 #&gt; Warners 2.0291 0.188 1.40e+01 5.5159 1.02e-02 3.1867 1.02738 #&gt; Zsivoczky 0.2344 2.031 1.05e+01 0.1813 1.03e+00 0.1332 0.02289 #&gt; Hernu 0.0805 3.214 1.02e-01 3.8217 2.95e+00 1.1311 2.82103 #&gt; Bernard 1.9708 5.166 2.43e+00 2.9474 4.58e+00 1.1066 1.89945 #&gt; Schwarzl 0.5318 1.025 3.80e+00 4.5612 5.17e-01 4.8796 4.59812 #&gt; Pogorelov 0.0484 1.753 3.11e-01 9.7503 1.40e-03 26.1167 8.82532 #&gt; Schoenbeck 0.2586 3.997 3.72e-02 0.3297 2.58e+00 0.2689 10.56627 #&gt; Barras 0.1052 1.588 2.61e+00 1.8430 3.77e+00 14.1743 12.83542 #&gt; Dim.8 Dim.9 Dim.10 #&gt; SEBRLE 2.99e-02 12.17748 0.0382 #&gt; CLAY 6.04e+00 0.10126 3.5857 #&gt; BERNARD 7.85e+00 0.81032 0.3499 #&gt; YURKOV 2.09e-01 0.33601 0.5107 #&gt; ZSIVOCZKY 6.49e-01 7.66492 4.3274 #&gt; McMULLEN 1.36e+00 0.31250 5.5105 #&gt; MARTINEAU 1.35e+00 1.39820 13.5440 #&gt; HERNU 3.71e+01 0.00107 10.9278 #&gt; BARRAS 7.38e+00 12.54733 10.0454 #&gt; NOOL 2.98e+00 0.37548 0.3300 #&gt; BOURGUIGNON 5.03e+00 7.74457 0.1222 #&gt; Sebrle 1.48e-02 20.10555 1.7206 #&gt; Clay 5.34e+00 4.70357 13.5271 #&gt; Karpov 1.89e+00 1.04399 1.6819 #&gt; Macey 2.09e+00 3.79877 1.6096 #&gt; Warners 1.90e-01 0.91042 2.0890 #&gt; Zsivoczky 4.59e+00 9.61785 0.7261 #&gt; Hernu 9.69e-01 0.12540 1.5523 #&gt; Bernard 6.02e-04 2.18807 3.6657 #&gt; Schwarzl 7.48e+00 0.00196 3.2536 #&gt; Pogorelov 2.24e-02 0.47727 0.0487 #&gt; Schoenbeck 1.04e+00 8.91730 3.1402 #&gt; Barras 2.12e+00 0.29289 13.3453 res.ind$cos2 # Quality of representation #&gt; Dim.1 Dim.2 Dim.3 Dim.4 Dim.5 Dim.6 Dim.7 #&gt; SEBRLE 0.00753 0.4975 8.13e-02 0.00139 2.69e-01 0.044324 8.91e-03 #&gt; CLAY 0.04870 0.4570 1.44e-01 0.12579 5.08e-02 0.132691 3.54e-03 #&gt; BERNARD 0.19720 0.2900 4.29e-03 0.41182 7.57e-04 0.017913 1.84e-02 #&gt; YURKOV 0.09611 0.0238 7.78e-01 0.06181 2.02e-02 0.001345 1.34e-02 #&gt; ZSIVOCZKY 0.00157 0.5764 2.40e-01 0.00139 5.47e-03 0.112918 1.10e-03 #&gt; McMULLEN 0.00218 0.1522 1.10e-01 0.26649 3.89e-01 0.005388 3.54e-02 #&gt; MARTINEAU 0.40401 0.1165 5.69e-02 0.00983 3.55e-01 0.014721 6.34e-03 #&gt; HERNU 0.39928 0.1551 2.73e-03 0.08870 1.79e-03 0.021248 3.58e-02 #&gt; BARRAS 0.61624 0.0333 8.48e-02 0.01024 1.07e-02 0.001594 9.13e-03 #&gt; NOOL 0.48987 0.1571 2.34e-02 0.15369 1.29e-01 0.015701 1.80e-02 #&gt; BOURGUIGNON 0.85970 0.0705 4.45e-06 0.01962 7.75e-03 0.000297 1.58e-02 #&gt; Sebrle 0.67538 0.0840 1.60e-01 0.03708 3.56e-05 0.000389 5.85e-06 #&gt; Clay 0.68759 0.1648 2.39e-02 0.00627 2.03e-02 0.037763 4.15e-03 #&gt; Karpov 0.78367 0.0301 4.87e-02 0.00464 3.01e-02 0.005138 8.80e-02 #&gt; Macey 0.36344 0.4531 1.02e-01 0.03636 4.95e-09 0.004140 7.71e-03 #&gt; Warners 0.25565 0.0106 5.31e-01 0.13808 2.18e-04 0.041169 9.50e-03 #&gt; Zsivoczky 0.04505 0.1740 6.05e-01 0.00692 3.36e-02 0.002625 3.23e-04 #&gt; Hernu 0.02482 0.4418 9.46e-03 0.23420 1.55e-01 0.035771 6.38e-02 #&gt; Bernard 0.28935 0.3381 1.07e-01 0.08598 1.14e-01 0.016659 2.05e-02 #&gt; Schwarzl 0.11672 0.1003 2.51e-01 0.19889 1.93e-02 0.109806 7.40e-02 #&gt; Pogorelov 0.00780 0.1259 1.50e-02 0.31210 3.85e-05 0.431416 1.04e-01 #&gt; Schoenbeck 0.06707 0.4620 2.90e-03 0.01699 1.14e-01 0.007150 2.01e-01 #&gt; Barras 0.01897 0.1277 1.41e-01 0.06604 1.16e-01 0.262130 1.70e-01 #&gt; Dim.8 Dim.9 Dim.10 #&gt; SEBRLE 3.89e-04 8.95e-02 0.000221 #&gt; CLAY 2.97e-02 2.82e-04 0.007847 #&gt; BERNARD 5.53e-02 3.23e-03 0.001096 #&gt; YURKOV 1.68e-03 1.53e-03 0.001822 #&gt; ZSIVOCZKY 5.76e-03 3.85e-02 0.017092 #&gt; McMULLEN 1.33e-02 1.73e-03 0.023927 #&gt; MARTINEAU 6.00e-03 3.52e-03 0.026821 #&gt; HERNU 2.61e-01 4.27e-06 0.034229 #&gt; BARRAS 9.12e-02 8.77e-02 0.055153 #&gt; NOOL 1.19e-02 8.51e-04 0.000588 #&gt; BOURGUIGNON 1.40e-02 1.22e-02 0.000151 #&gt; Sebrle 5.30e-05 4.07e-02 0.002737 #&gt; Clay 2.11e-02 1.05e-02 0.023726 #&gt; Karpov 5.64e-03 1.76e-03 0.002232 #&gt; Macey 1.40e-02 1.44e-02 0.004803 #&gt; Warners 1.59e-03 4.32e-03 0.007784 #&gt; Zsivoczky 5.87e-02 6.96e-02 0.004127 #&gt; Hernu 1.99e-02 1.46e-03 0.014160 #&gt; Bernard 5.88e-06 1.21e-02 0.015917 #&gt; Schwarzl 1.09e-01 1.62e-05 0.021117 #&gt; Pogorelov 2.40e-04 2.89e-03 0.000232 #&gt; Schoenbeck 1.79e-02 8.70e-02 0.024083 #&gt; Barras 2.54e-02 1.99e-03 0.071184 25.8 Predict using PCA In this section, we’ll show how to predict the coordinates of supplementary individuals and variables using only the information provided by the previously performed PCA. Data: rows 24 to 27 and columns 1 to to 10 [in decathlon2 data sets]. The new data must contain columns (variables) with the same names and in the same order as the active data used to compute PCA. # Data for the supplementary individuals ind.sup &lt;- decathlon2[24:27, 1:10] ind.sup[, 1:6] #&gt; X100m Long.jump Shot.put High.jump X400m X110m.hurdle #&gt; KARPOV 11.0 7.30 14.8 2.04 48.4 14.1 #&gt; WARNERS 11.1 7.60 14.3 1.98 48.7 14.2 #&gt; Nool 10.8 7.53 14.3 1.88 48.8 14.8 #&gt; Drews 10.9 7.38 13.1 1.88 48.5 14.0 Predict the coordinates of new individuals data. Use the R base function predict(): ind.sup.coord &lt;- predict(res.pca, newdata = ind.sup) ind.sup.coord[, 1:4] #&gt; PC1 PC2 PC3 PC4 #&gt; KARPOV 0.777 -0.762 1.597 1.686 #&gt; WARNERS -0.378 0.119 1.701 -0.691 #&gt; Nool -0.547 -1.934 0.472 -2.228 #&gt; Drews -1.085 -0.017 2.982 -1.501 Graph of individuals including the supplementary individuals: # Plot of active individuals p &lt;- fviz_pca_ind(res.pca, repel = TRUE) # Add supplementary individuals fviz_add(p, ind.sup.coord, color =&quot;blue&quot;) The predicted coordinates of individuals can be manually calculated as follow: Center and scale the new individuals data using the center and the scale of the PCA Calculate the predicted coordinates by multiplying the scaled values with the eigenvectors (loadings) of the principal components. The R code below can be used : # Centering and scaling the supplementary individuals ind.scaled &lt;- scale(ind.sup, center = res.pca$center, scale = res.pca$scale) # Coordinates of the individividuals coord_func &lt;- function(ind, loadings){ r &lt;- loadings*ind apply(r, 2, sum) } pca.loadings &lt;- res.pca$rotation ind.sup.coord &lt;- t(apply(ind.scaled, 1, coord_func, pca.loadings )) ind.sup.coord[, 1:4] #&gt; PC1 PC2 PC3 PC4 #&gt; KARPOV 0.777 -0.762 1.597 1.686 #&gt; WARNERS -0.378 0.119 1.701 -0.691 #&gt; Nool -0.547 -1.934 0.472 -2.228 #&gt; Drews -1.085 -0.017 2.982 -1.501 25.9 Supplementary variables 25.9.1 Qualitative / categorical variables The data sets decathlon2 contain a supplementary qualitative variable at columns 13 corresponding to the type of competitions. Qualitative / categorical variables can be used to color individuals by groups. The grouping variable should be of same length as the number of active individuals (here 23). groups &lt;- as.factor(decathlon2$Competition[1:23]) fviz_pca_ind(res.pca, col.ind = groups, # color by groups palette = c(&quot;#00AFBB&quot;, &quot;#FC4E07&quot;), addEllipses = TRUE, # Concentration ellipses ellipse.type = &quot;confidence&quot;, legend.title = &quot;Groups&quot;, repel = TRUE ) Calculate the coordinates for the levels of grouping variables. The coordinates for a given group is calculated as the mean coordinates of the individuals in the group. library(magrittr) # for pipe %&gt;% library(dplyr) # everything else #&gt; #&gt; Attaching package: &#39;dplyr&#39; #&gt; The following objects are masked from &#39;package:stats&#39;: #&gt; #&gt; filter, lag #&gt; The following objects are masked from &#39;package:base&#39;: #&gt; #&gt; intersect, setdiff, setequal, union # 1. Individual coordinates res.ind &lt;- get_pca_ind(res.pca) # 2. Coordinate of groups coord.groups &lt;- res.ind$coord %&gt;% as_data_frame() %&gt;% select(Dim.1, Dim.2) %&gt;% mutate(competition = groups) %&gt;% group_by(competition) %&gt;% summarise( Dim.1 = mean(Dim.1), Dim.2 = mean(Dim.2) ) #&gt; Warning: `as_data_frame()` is deprecated, use `as_tibble()` (but mind the new semantics). #&gt; This warning is displayed once per session. coord.groups #&gt; # A tibble: 2 x 3 #&gt; competition Dim.1 Dim.2 #&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Decastar -1.31 -0.119 #&gt; 2 OlympicG 1.20 0.109 25.9.2 Quantitative variables Data: columns 11:12. Should be of same length as the number of active individuals (here 23) quanti.sup &lt;- decathlon2[1:23, 11:12, drop = FALSE] head(quanti.sup) #&gt; Rank Points #&gt; SEBRLE 1 8217 #&gt; CLAY 2 8122 #&gt; BERNARD 4 8067 #&gt; YURKOV 5 8036 #&gt; ZSIVOCZKY 7 8004 #&gt; McMULLEN 8 7995 The coordinates of a given quantitative variable are calculated as the correlation between the quantitative variables and the principal components. # Predict coordinates and compute cos2 quanti.coord &lt;- cor(quanti.sup, res.pca$x) quanti.cos2 &lt;- quanti.coord^2 # Graph of variables including supplementary variables p &lt;- fviz_pca_var(res.pca) fviz_add(p, quanti.coord, color =&quot;blue&quot;, geom=&quot;arrow&quot;) 25.10 Theory behind PCA results 25.10.1 PCA results for variables Here we’ll show how to calculate the PCA results for variables: coordinates, cos2 and contributions: var.coord = loadings * the component standard deviations var.cos2 = var.coord^2 var.contrib. The contribution of a variable to a given principal component is (in percentage) : (var.cos2 * 100) / (total cos2 of the component) # Helper function #:::::::::::::::::::::::::::::::::::::::: var_coord_func &lt;- function(loadings, comp.sdev){ loadings*comp.sdev } # Compute Coordinates #:::::::::::::::::::::::::::::::::::::::: loadings &lt;- res.pca$rotation sdev &lt;- res.pca$sdev var.coord &lt;- t(apply(loadings, 1, var_coord_func, sdev)) head(var.coord[, 1:4]) #&gt; PC1 PC2 PC3 PC4 #&gt; X100m -0.851 0.1794 -0.302 0.0336 #&gt; Long.jump 0.794 -0.2809 0.191 -0.1154 #&gt; Shot.put 0.734 -0.0854 -0.518 0.1285 #&gt; High.jump 0.610 0.4652 -0.330 0.1446 #&gt; X400m -0.702 -0.2902 -0.284 0.4308 #&gt; X110m.hurdle -0.764 0.0247 -0.449 -0.0169 # Compute Cos2 #:::::::::::::::::::::::::::::::::::::::: var.cos2 &lt;- var.coord^2 head(var.cos2[, 1:4]) #&gt; PC1 PC2 PC3 PC4 #&gt; X100m 0.724 0.032184 0.0909 0.001127 #&gt; Long.jump 0.631 0.078881 0.0363 0.013315 #&gt; Shot.put 0.539 0.007294 0.2679 0.016504 #&gt; High.jump 0.372 0.216424 0.1090 0.020895 #&gt; X400m 0.492 0.084203 0.0804 0.185611 #&gt; X110m.hurdle 0.584 0.000612 0.2015 0.000285 # Compute contributions #:::::::::::::::::::::::::::::::::::::::: comp.cos2 &lt;- apply(var.cos2, 2, sum) contrib &lt;- function(var.cos2, comp.cos2){var.cos2*100/comp.cos2} var.contrib &lt;- t(apply(var.cos2,1, contrib, comp.cos2)) head(var.contrib[, 1:4]) #&gt; PC1 PC2 PC3 PC4 #&gt; X100m 17.54 1.7505 7.34 0.1376 #&gt; Long.jump 15.29 4.2904 2.93 1.6249 #&gt; Shot.put 13.06 0.3967 21.62 2.0141 #&gt; High.jump 9.02 11.7716 8.79 2.5499 #&gt; X400m 11.94 4.5799 6.49 22.6509 #&gt; X110m.hurdle 14.16 0.0333 16.26 0.0348 25.10.2 PCA results for individuals ind.coord = res.pca$x Cos2 of individuals. Two steps: Calculate the square distance between each individual and the PCA center of gravity: d2 = [(var1_ind_i - mean_var1)/sd_var1]^2 + …+ [(var10_ind_i - mean_var10)/sd_var10]^2 + …+.. Calculate the cos2 as ind.coord^2/d2 Contributions of individuals to the principal components: 100 * (1 / number_of_individuals)*(ind.coord^2 / comp_sdev^2). Note that the sum of all the contributions per column is 100 # Coordinates of individuals #:::::::::::::::::::::::::::::::::: ind.coord &lt;- res.pca$x head(ind.coord[, 1:4]) #&gt; PC1 PC2 PC3 PC4 #&gt; SEBRLE 0.191 -1.554 -0.628 0.0821 #&gt; CLAY 0.790 -2.420 1.357 1.2698 #&gt; BERNARD -1.329 -1.612 -0.196 -1.9209 #&gt; YURKOV -0.869 0.433 -2.474 0.6972 #&gt; ZSIVOCZKY -0.106 2.023 1.305 -0.0993 #&gt; McMULLEN 0.119 0.992 0.844 1.3122 # Cos2 of individuals #::::::::::::::::::::::::::::::::: # 1. square of the distance between an individual and the # PCA center of gravity center &lt;- res.pca$center scale&lt;- res.pca$scale getdistance &lt;- function(ind_row, center, scale){ return(sum(((ind_row-center)/scale)^2)) } d2 &lt;- apply(decathlon2.active,1, getdistance, center, scale) # 2. Compute the cos2. The sum of each row is 1 cos2 &lt;- function(ind.coord, d2){return(ind.coord^2/d2)} ind.cos2 &lt;- apply(ind.coord, 2, cos2, d2) head(ind.cos2[, 1:4]) #&gt; PC1 PC2 PC3 PC4 #&gt; SEBRLE 0.00753 0.4975 0.08133 0.00139 #&gt; CLAY 0.04870 0.4570 0.14363 0.12579 #&gt; BERNARD 0.19720 0.2900 0.00429 0.41182 #&gt; YURKOV 0.09611 0.0238 0.77823 0.06181 #&gt; ZSIVOCZKY 0.00157 0.5764 0.23975 0.00139 #&gt; McMULLEN 0.00218 0.1522 0.11014 0.26649 # Contributions of individuals #::::::::::::::::::::::::::::::: contrib &lt;- function(ind.coord, comp.sdev, n.ind){ 100*(1/n.ind)*ind.coord^2/comp.sdev^2 } ind.contrib &lt;- t(apply(ind.coord, 1, contrib, res.pca$sdev, nrow(ind.coord))) head(ind.contrib[, 1:4]) #&gt; PC1 PC2 PC3 PC4 #&gt; SEBRLE 0.0385 5.712 1.385 0.0357 #&gt; CLAY 0.6581 13.854 6.460 8.5557 #&gt; BERNARD 1.8627 6.144 0.135 19.5783 #&gt; YURKOV 0.7969 0.443 21.476 2.5794 #&gt; ZSIVOCZKY 0.0118 9.682 5.975 0.0523 #&gt; McMULLEN 0.0148 2.325 2.497 9.1353 "],
["principal-components-methods.html", "Chapter 26 Principal Components Methods 26.1 Data standardization 26.2 Eigenvalues / Variances 26.3 Graph of variables 26.4 Correlation circle 26.5 Quality of representation 26.6 Contributions of variables to PCs 26.7 Color by a custom continuous variable 26.8 Color by groups 26.9 Dimension description 26.10 Graph of individuals 26.11 Plots: quality and contribution 26.12 Color by a custom continuous variable 26.13 Color by groups 26.14 Graph customization 26.15 Size and shape of plot elements 26.16 Ellipses 26.17 Group mean points 26.18 Axis lines 26.19 Graphical parameters 26.20 Biplot 26.21 Supplementary elements 26.22 Quantitative variables 26.23 Individuals 26.24 Qualitative variables 26.25 Filtering results 26.26 Exporting results 26.27 Export results to txt/csv files 26.28 Summary", " Chapter 26 Principal Components Methods http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/112-pca-principal-component-analysis-essentials/ Principal component analysis (PCA) allows us to summarize and to visualize the information in a data set containing individuals/observations described by multiple inter-correlated quantitative variables. Each variable could be considered as a different dimension. If you have more than 3 variables in your data sets, it could be very difficult to visualize a multi-dimensional hyperspace. Principal component analysis is used to extract the important information from a multivariate data table and to express this information as a set of few new variables called principal components. These new variables correspond to a linear combination of the originals. The number of principal components is less than or equal to the number of original variables. The information in a given data set corresponds to the total variation it contains. The goal of PCA is to identify directions (or principal components) along which the variation in the data is maximal. In other words, PCA reduces the dimensionality of a multivariate data to two or three principal components, that can be visualized graphically, with minimal loss of information. # install.packages(c(&quot;FactoMineR&quot;, &quot;factoextra&quot;)) library(FactoMineR) library(factoextra) #&gt; Loading required package: ggplot2 #&gt; Registered S3 methods overwritten by &#39;ggplot2&#39;: #&gt; method from #&gt; [.quosures rlang #&gt; c.quosures rlang #&gt; print.quosures rlang #&gt; Welcome! Related Books: `Practical Guide To Cluster Analysis in R` at https://goo.gl/13EFCZ data(decathlon2) # head(decathlon2) In PCA terminology, our data contains : Active individuals (in light blue, rows 1:23) : Individuals that are used during the principal component analysis. Supplementary individuals (in dark blue, rows 24:27) : The coordinates of these individuals will be predicted using the PCA information and parameters obtained with active individuals/variables Active variables (in pink, columns 1:10) : Variables that are used for the principal component analysis. Supplementary variables: As supplementary individuals, the coordinates of these variables will be predicted also. These can be: Supplementary continuous variables (red): Columns 11 and 12 corresponding respectively to the rank and the points of athletes. Supplementary qualitative variables (green): Column 13 corresponding to the two athlete-tic meetings (2004 Olympic Game or 2004 Decastar). This is a categorical (or factor) variable factor. It can be used to color individuals by groups. We start by subsetting active individuals and active variables for the principal component analysis: decathlon2.active &lt;- decathlon2[1:23, 1:10] head(decathlon2.active[, 1:6], 4) #&gt; X100m Long.jump Shot.put High.jump X400m X110m.hurdle #&gt; SEBRLE 11.0 7.58 14.8 2.07 49.8 14.7 #&gt; CLAY 10.8 7.40 14.3 1.86 49.4 14.1 #&gt; BERNARD 11.0 7.23 14.2 1.92 48.9 15.0 #&gt; YURKOV 11.3 7.09 15.2 2.10 50.4 15.3 26.1 Data standardization In principal component analysis, variables are often scaled (i.e. standardized). This is particularly recommended when variables are measured in different scales (e.g: kilograms, kilometers, centimeters, …); otherwise, the PCA outputs obtained will be severely affected. The goal is to make the variables comparable. Generally variables are scaled to have i) standard deviation one and ii) mean zero. The function PCA() [FactoMineR package] can be used. A simplified format is: library(FactoMineR) res.pca &lt;- PCA(decathlon2.active, graph = FALSE) print(res.pca) #&gt; **Results for the Principal Component Analysis (PCA)** #&gt; The analysis was performed on 23 individuals, described by 10 variables #&gt; *The results are available in the following objects: #&gt; #&gt; name description #&gt; 1 &quot;$eig&quot; &quot;eigenvalues&quot; #&gt; 2 &quot;$var&quot; &quot;results for the variables&quot; #&gt; 3 &quot;$var$coord&quot; &quot;coord. for the variables&quot; #&gt; 4 &quot;$var$cor&quot; &quot;correlations variables - dimensions&quot; #&gt; 5 &quot;$var$cos2&quot; &quot;cos2 for the variables&quot; #&gt; 6 &quot;$var$contrib&quot; &quot;contributions of the variables&quot; #&gt; 7 &quot;$ind&quot; &quot;results for the individuals&quot; #&gt; 8 &quot;$ind$coord&quot; &quot;coord. for the individuals&quot; #&gt; 9 &quot;$ind$cos2&quot; &quot;cos2 for the individuals&quot; #&gt; 10 &quot;$ind$contrib&quot; &quot;contributions of the individuals&quot; #&gt; 11 &quot;$call&quot; &quot;summary statistics&quot; #&gt; 12 &quot;$call$centre&quot; &quot;mean of the variables&quot; #&gt; 13 &quot;$call$ecart.type&quot; &quot;standard error of the variables&quot; #&gt; 14 &quot;$call$row.w&quot; &quot;weights for the individuals&quot; #&gt; 15 &quot;$call$col.w&quot; &quot;weights for the variables&quot; The object that is created using the function PCA() contains many information found in many different lists and matrices. These values are described in the next section. 26.2 Eigenvalues / Variances As described in previous sections, the eigenvalues measure the amount of variation retained by each principal component. Eigenvalues are large for the first PCs and small for the subsequent PCs. That is, the first PCs corresponds to the directions with the maximum amount of variation in the data set. We examine the eigenvalues to determine the number of principal components to be considered. The eigenvalues and the proportion of variances (i.e., information) retained by the principal components (PCs) can be extracted using the function get_eigenvalue() [factoextra package]. library(factoextra) eig.val &lt;- get_eigenvalue(res.pca) eig.val #&gt; eigenvalue variance.percent cumulative.variance.percent #&gt; Dim.1 4.124 41.24 41.2 #&gt; Dim.2 1.839 18.39 59.6 #&gt; Dim.3 1.239 12.39 72.0 #&gt; Dim.4 0.819 8.19 80.2 #&gt; Dim.5 0.702 7.02 87.2 #&gt; Dim.6 0.423 4.23 91.5 #&gt; Dim.7 0.303 3.03 94.5 #&gt; Dim.8 0.274 2.74 97.2 #&gt; Dim.9 0.155 1.55 98.8 #&gt; Dim.10 0.122 1.22 100.0 The sum of all the eigenvalues give a total variance of 10. The proportion of variation explained by each eigenvalue is given in the second column. For example, 4.124 divided by 10 equals 0.4124, or, about 41.24% of the variation is explained by this first eigenvalue. The cumulative percentage explained is obtained by adding the successive proportions of variation explained to obtain the running total. For instance, 41.242% plus 18.385% equals 59.627%, and so forth. Therefore, about 59.627% of the variation is explained by the first two eigenvalues together. Unfortunately, there is no well-accepted objective way to decide how many principal components are enough. This will depend on the specific field of application and the specific data set. In practice, we tend to look at the first few principal components in order to find interesting patterns in the data. In our analysis, the first three principal components explain 72% of the variation. This is an acceptably large percentage. An alternative method to determine the number of principal components is to look at a Scree Plot, which is the plot of eigenvalues ordered from largest to the smallest. The number of component is determined at the point, beyond which the remaining eigenvalues are all relatively small and of comparable size (Jollife 2002, Peres-Neto, Jackson, and Somers (2005)). The scree plot can be produced using the function fviz_eig() or fviz_screeplot() [factoextra package]. fviz_eig(res.pca, addlabels = TRUE, ylim = c(0, 50)) From the plot above, we might want to stop at the fifth principal component. 87% of the information (variances) contained in the data are retained by the first five principal components. 26.3 Graph of variables Results A simple method to extract the results, for variables, from a PCA output is to use the function get_pca_var() [factoextra package]. This function provides a list of matrices containing all the results for the active variables (coordinates, correlation between variables and axes, squared cosine and contributions) var &lt;- get_pca_var(res.pca) var #&gt; Principal Component Analysis Results for variables #&gt; =================================================== #&gt; Name Description #&gt; 1 &quot;$coord&quot; &quot;Coordinates for the variables&quot; #&gt; 2 &quot;$cor&quot; &quot;Correlations between variables and dimensions&quot; #&gt; 3 &quot;$cos2&quot; &quot;Cos2 for the variables&quot; #&gt; 4 &quot;$contrib&quot; &quot;contributions of the variables&quot; The components of the get_pca_var() can be used in the plot of variables as follow: var$coord: coordinates of variables to create a scatter plot var$cos2: represents the quality of representation for variables on the factor map. It’s calculated as the squared coordinates: var.cos2 = var.coord * var.coord. var$contrib: contains the contributions (in percentage) of the variables to the principal components. The contribution of a variable (var) to a given principal component is (in percentage) : (var.cos2 * 100) / (total cos2 of the component). Note that, it’s possible to plot variables and to color them according to either i) their quality on the factor map (cos2) or ii) their contribution values to the principal components (contrib). The different components can be accessed as follow: # Coordinates head(var$coord) #&gt; Dim.1 Dim.2 Dim.3 Dim.4 Dim.5 #&gt; X100m -0.851 -0.1794 0.302 0.0336 -0.194 #&gt; Long.jump 0.794 0.2809 -0.191 -0.1154 0.233 #&gt; Shot.put 0.734 0.0854 0.518 0.1285 -0.249 #&gt; High.jump 0.610 -0.4652 0.330 0.1446 0.403 #&gt; X400m -0.702 0.2902 0.284 0.4308 0.104 #&gt; X110m.hurdle -0.764 -0.0247 0.449 -0.0169 0.224 # Cos2: quality on the factore map head(var$cos2) #&gt; Dim.1 Dim.2 Dim.3 Dim.4 Dim.5 #&gt; X100m 0.724 0.032184 0.0909 0.001127 0.0378 #&gt; Long.jump 0.631 0.078881 0.0363 0.013315 0.0544 #&gt; Shot.put 0.539 0.007294 0.2679 0.016504 0.0619 #&gt; High.jump 0.372 0.216424 0.1090 0.020895 0.1622 #&gt; X400m 0.492 0.084203 0.0804 0.185611 0.0108 #&gt; X110m.hurdle 0.584 0.000612 0.2015 0.000285 0.0503 # Contributions to the principal components head(var$contrib) #&gt; Dim.1 Dim.2 Dim.3 Dim.4 Dim.5 #&gt; X100m 17.54 1.7505 7.34 0.1376 5.39 #&gt; Long.jump 15.29 4.2904 2.93 1.6249 7.75 #&gt; Shot.put 13.06 0.3967 21.62 2.0141 8.82 #&gt; High.jump 9.02 11.7716 8.79 2.5499 23.12 #&gt; X400m 11.94 4.5799 6.49 22.6509 1.54 #&gt; X110m.hurdle 14.16 0.0333 16.26 0.0348 7.17 In this section, we describe how to visualize variables and draw conclusions about their correlations. Next, we highlight variables according to either i) their quality of representation on the factor map or ii) their contributions to the principal components. 26.4 Correlation circle The correlation between a variable and a principal component (PC) is used as the coordinates of the variable on the PC. The representation of variables differs from the plot of the observations: The observations are represented by their projections, but the variables are represented by their correlations (Abdi and Williams 2010). # Coordinates of variables head(var$coord, 4) #&gt; Dim.1 Dim.2 Dim.3 Dim.4 Dim.5 #&gt; X100m -0.851 -0.1794 0.302 0.0336 -0.194 #&gt; Long.jump 0.794 0.2809 -0.191 -0.1154 0.233 #&gt; Shot.put 0.734 0.0854 0.518 0.1285 -0.249 #&gt; High.jump 0.610 -0.4652 0.330 0.1446 0.403 To plot variables, type this: fviz_pca_var(res.pca, col.var = &quot;black&quot;) The plot above is also known as variable correlation plots. It shows the relationships between all variables. It can be interpreted as follow: Positively correlated variables are grouped together. Negatively correlated variables are positioned on opposite sides of the plot origin (opposed quadrants). The distance between variables and the origin measures the quality of the variables on the factor map. Variables that are away from the origin are well represented on the factor map. 26.5 Quality of representation The quality of representation of the variables on factor map is called cos2 (square cosine, squared coordinates) . You can access to the cos2 as follow: head(var$cos2, 4) #&gt; Dim.1 Dim.2 Dim.3 Dim.4 Dim.5 #&gt; X100m 0.724 0.03218 0.0909 0.00113 0.0378 #&gt; Long.jump 0.631 0.07888 0.0363 0.01331 0.0544 #&gt; Shot.put 0.539 0.00729 0.2679 0.01650 0.0619 #&gt; High.jump 0.372 0.21642 0.1090 0.02089 0.1622 You can visualize the cos2 of variables on all the dimensions using the corrplot package: library(corrplot) #&gt; corrplot 0.84 loaded corrplot(var$cos2, is.corr=FALSE) It’s also possible to create a bar plot of variables cos2 using the function fviz_cos2() [in factoextra]: # Total cos2 of variables on Dim.1 and Dim.2 fviz_cos2(res.pca, choice = &quot;var&quot;, axes = 1:2) Note that, A high cos2 indicates a good representation of the variable on the principal component. In this case the variable is positioned close to the circumference of the correlation circle. A low cos2 indicates that the variable is not perfectly represented by the PCs. In this case the variable is close to the center of the circle. For a given variable, the sum of the cos2 on all the principal components is equal to one. If a variable is perfectly represented by only two principal components (Dim.1 &amp; Dim.2), the sum of the cos2 on these two PCs is equal to one. In this case the variables will be positioned on the circle of correlations. For some of the variables, more than 2 components might be required to perfectly represent the data. In this case the variables are positioned inside the circle of correlations. In summary: The cos2 values are used to estimate the quality of the representation The closer a variable is to the circle of correlations, the better its representation on the factor map (and the more important it is to interpret these components) Variables that are closed to the center of the plot are less important for the first components. It’s possible to color variables by their cos2 values using the argument col.var = “cos2”. This produces a gradient colors. In this case, the argument gradient.cols can be used to provide a custom color. For instance, gradient.cols = c(“white”, “blue”, “red”) means that: variables with low cos2 values will be colored in “white” variables with mid cos2 values will be colored in “blue” variables with high cos2 values will be colored in red # Color by cos2 values: quality on the factor map fviz_pca_var(res.pca, col.var = &quot;cos2&quot;, gradient.cols = c(&quot;#00AFBB&quot;, &quot;#E7B800&quot;, &quot;#FC4E07&quot;), repel = TRUE # Avoid text overlapping ) Note that, it’s also possible to change the transparency of the variables according to their cos2 values using the option alpha.var = “cos2”. For example, type this: # Change the transparency by cos2 values fviz_pca_var(res.pca, alpha.var = &quot;cos2&quot;) 26.6 Contributions of variables to PCs The contributions of variables in accounting for the variability in a given principal component are expressed in percentage. Variables that are correlated with PC1 (i.e., Dim.1) and PC2 (i.e., Dim.2) are the most important in explaining the variability in the data set. Variables that do not correlated with any PC or correlated with the last dimensions are variables with low contribution and might be removed to simplify the overall analysis. The contribution of variables can be extracted as follow : head(var$contrib, 4) #&gt; Dim.1 Dim.2 Dim.3 Dim.4 Dim.5 #&gt; X100m 17.54 1.751 7.34 0.138 5.39 #&gt; Long.jump 15.29 4.290 2.93 1.625 7.75 #&gt; Shot.put 13.06 0.397 21.62 2.014 8.82 #&gt; High.jump 9.02 11.772 8.79 2.550 23.12 The larger the value of the contribution, the more the variable contributes to the component. It’s possible to use the function corrplot() [corrplot package] to highlight the most contributing variables for each dimension: library(&quot;corrplot&quot;) corrplot(var$contrib, is.corr=FALSE) The function fviz_contrib() [factoextra package] can be used to draw a bar plot of variable contributions. If your data contains many variables, you can decide to show only the top contributing variables. The R code below shows the top 10 variables contributing to the principal components: # Contributions of variables to PC1 fviz_contrib(res.pca, choice = &quot;var&quot;, axes = 1, top = 10) # Contributions of variables to PC2 fviz_contrib(res.pca, choice = &quot;var&quot;, axes = 2, top = 10) The total contribution to PC1 and PC2 is obtained with the following R code: fviz_contrib(res.pca, choice = &quot;var&quot;, axes = 1:2, top = 10) The red dashed line on the graph above indicates the expected average contribution. If the contribution of the variables were uniform, the expected value would be 1/length(variables) = 1/10 = 10%. For a given component, a variable with a contribution larger than this cutoff could be considered as important in contributing to the component. Note that, the total contribution of a given variable, on explaining the variations retained by two principal components, say PC1 and PC2, is calculated as contrib = [(C1 * Eig1) + (C2 * Eig2)]/(Eig1 + Eig2), where C1 and C2 are the contributions of the variable on PC1 and PC2, respectively Eig1 and Eig2 are the eigenvalues of PC1 and PC2, respectively. Recall that eigenvalues measure the amount of variation retained by each PC. In this case, the expected average contribution (cutoff) is calculated as follow: As mentioned above, if the contributions of the 10 variables were uniform, the expected average contribution on a given PC would be 1/10 = 10%. The expected average contribution of a variable for PC1 and PC2 is : [(10* Eig1) + (10 * Eig2)]/(Eig1 + Eig2) It can be seen that the variables - X100m, Long.jump and Pole.vault - contribute the most to the dimensions 1 and 2. The most important (or, contributing) variables can be highlighted on the correlation plot as follow: fviz_pca_var(res.pca, col.var = &quot;contrib&quot;, gradient.cols = c(&quot;#00AFBB&quot;, &quot;#E7B800&quot;, &quot;#FC4E07&quot;) ) Note that, it’s also possible to change the transparency of variables according to their contrib values using the option alpha.var = “contrib”. For example, type this: # Change the transparency by contrib values fviz_pca_var(res.pca, alpha.var = &quot;contrib&quot;) 26.7 Color by a custom continuous variable In the previous sections, we showed how to color variables by their contributions and their cos2. Note that, it’s possible to color variables by any custom continuous variable. The coloring variable should have the same length as the number of active variables in the PCA (here n = 10). For example, type this: # Create a random continuous variable of length 10 set.seed(123) my.cont.var &lt;- rnorm(10) # Color variables by the continuous variable fviz_pca_var(res.pca, col.var = my.cont.var, gradient.cols = c(&quot;blue&quot;, &quot;yellow&quot;, &quot;red&quot;), legend.title = &quot;Cont.Var&quot;) 26.8 Color by groups It’s also possible to change the color of variables by groups defined by a qualitative/categorical variable, also called factor in R terminology. As we don’t have any grouping variable in our data sets for classifying variables, we’ll create it. In the following demo example, we start by classifying the variables into 3 groups using the kmeans clustering algorithm. Next, we use the clusters returned by the kmeans algorithm to color variables. # Create a grouping variable using kmeans # Create 3 groups of variables (centers = 3) set.seed(123) res.km &lt;- kmeans(var$coord, centers = 3, nstart = 25) grp &lt;- as.factor(res.km$cluster) # Color variables by groups fviz_pca_var(res.pca, col.var = grp, palette = c(&quot;#0073C2FF&quot;, &quot;#EFC000FF&quot;, &quot;#868686FF&quot;), legend.title = &quot;Cluster&quot;) 26.9 Dimension description In the section (???)(pca-variable-contributions), we described how to highlight variables according to their contributions to the principal components. Note also that, the function dimdesc() [in FactoMineR], for dimension description, can be used to identify the most significantly associated variables with a given principal component . It can be used as follow: res.desc &lt;- dimdesc(res.pca, axes = c(1,2), proba = 0.05) # Description of dimension 1 res.desc$Dim.1 #&gt; $quanti #&gt; correlation p.value #&gt; Long.jump 0.794 6.06e-06 #&gt; Discus 0.743 4.84e-05 #&gt; Shot.put 0.734 6.72e-05 #&gt; High.jump 0.610 1.99e-03 #&gt; Javeline 0.428 4.15e-02 #&gt; X400m -0.702 1.91e-04 #&gt; X110m.hurdle -0.764 2.20e-05 #&gt; X100m -0.851 2.73e-07 # Description of dimension 2 res.desc$Dim.2 #&gt; $quanti #&gt; correlation p.value #&gt; Pole.vault 0.807 3.21e-06 #&gt; X1500m 0.784 9.38e-06 #&gt; High.jump -0.465 2.53e-02 26.10 Graph of individuals Results The results, for individuals can be extracted using the function get_pca_ind() [factoextra package]. Similarly to the get_pca_var(), the function get_pca_ind() provides a list of matrices containing all the results for the individuals (coordinates, correlation between individuals and axes, squared cosine and contributions) ind &lt;- get_pca_ind(res.pca) ind #&gt; Principal Component Analysis Results for individuals #&gt; =================================================== #&gt; Name Description #&gt; 1 &quot;$coord&quot; &quot;Coordinates for the individuals&quot; #&gt; 2 &quot;$cos2&quot; &quot;Cos2 for the individuals&quot; #&gt; 3 &quot;$contrib&quot; &quot;contributions of the individuals&quot; To get access to the different components, use this: # Coordinates of individuals head(ind$coord) #&gt; Dim.1 Dim.2 Dim.3 Dim.4 Dim.5 #&gt; SEBRLE 0.196 1.589 0.642 0.0839 1.1683 #&gt; CLAY 0.808 2.475 -1.387 1.2984 -0.8250 #&gt; BERNARD -1.359 1.648 0.201 -1.9641 0.0842 #&gt; YURKOV -0.889 -0.443 2.530 0.7129 0.4078 #&gt; ZSIVOCZKY -0.108 -2.069 -1.334 -0.1015 -0.2015 #&gt; McMULLEN 0.121 -1.014 -0.863 1.3416 1.6215 # Quality of individuals head(ind$cos2) #&gt; Dim.1 Dim.2 Dim.3 Dim.4 Dim.5 #&gt; SEBRLE 0.00753 0.4975 0.08133 0.00139 0.268903 #&gt; CLAY 0.04870 0.4570 0.14363 0.12579 0.050785 #&gt; BERNARD 0.19720 0.2900 0.00429 0.41182 0.000757 #&gt; YURKOV 0.09611 0.0238 0.77823 0.06181 0.020228 #&gt; ZSIVOCZKY 0.00157 0.5764 0.23975 0.00139 0.005465 #&gt; McMULLEN 0.00218 0.1522 0.11014 0.26649 0.389262 # Contributions of individuals head(ind$contrib) #&gt; Dim.1 Dim.2 Dim.3 Dim.4 Dim.5 #&gt; SEBRLE 0.0403 5.971 1.448 0.0373 8.4589 #&gt; CLAY 0.6881 14.484 6.754 8.9446 4.2179 #&gt; BERNARD 1.9474 6.423 0.141 20.4682 0.0439 #&gt; YURKOV 0.8331 0.463 22.452 2.6966 1.0308 #&gt; ZSIVOCZKY 0.0123 10.122 6.246 0.0547 0.2515 #&gt; McMULLEN 0.0155 2.431 2.610 9.5506 16.2949 26.11 Plots: quality and contribution The fviz_pca_ind() is used to produce the graph of individuals. To create a simple plot, type this: fviz_pca_ind(res.pca) Like variables, it’s also possible to color individuals by their cos2 values: fviz_pca_ind(res.pca, col.ind = &quot;cos2&quot;, gradient.cols = c(&quot;#00AFBB&quot;, &quot;#E7B800&quot;, &quot;#FC4E07&quot;), repel = TRUE # Avoid text overlapping (slow if many points) ) Note that, individuals that are similar are grouped together on the plot. You can also change the point size according the cos2 of the corresponding individuals: fviz_pca_ind(res.pca, pointsize = &quot;cos2&quot;, pointshape = 21, fill = &quot;#E7B800&quot;, repel = TRUE # Avoid text overlapping (slow if many points) ) To change both point size and color by cos2, try this: fviz_pca_ind(res.pca, col.ind = &quot;cos2&quot;, pointsize = &quot;cos2&quot;, gradient.cols = c(&quot;#00AFBB&quot;, &quot;#E7B800&quot;, &quot;#FC4E07&quot;), repel = TRUE # Avoid text overlapping (slow if many points) ) To create a bar plot of the quality of representation (cos2) of individuals on the factor map, you can use the function fviz_cos2() as previously described for variables: fviz_cos2(res.pca, choice = &quot;ind&quot;) To visualize the contribution of individuals to the first two principal components, type this: # Total contribution on PC1 and PC2 fviz_contrib(res.pca, choice = &quot;ind&quot;, axes = 1:2) 26.12 Color by a custom continuous variable As for variables, individuals can be colored by any custom continuous variable by specifying the argument col.ind. For example, type this: # Create a random continuous variable of length 23, # Same length as the number of active individuals in the PCA set.seed(123) my.cont.var &lt;- rnorm(23) # Color individuals by the continuous variable fviz_pca_ind(res.pca, col.ind = my.cont.var, gradient.cols = c(&quot;blue&quot;, &quot;yellow&quot;, &quot;red&quot;), legend.title = &quot;Cont.Var&quot;) 26.13 Color by groups Here, we describe how to color individuals by group. Additionally, we show how to add concentration ellipses and confidence ellipses by groups. For this, we’ll use the iris data as demo data sets. Iris data sets look like this: head(iris, 3) #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width Species #&gt; 1 5.1 3.5 1.4 0.2 setosa #&gt; 2 4.9 3.0 1.4 0.2 setosa #&gt; 3 4.7 3.2 1.3 0.2 setosa The column “Species” will be used as grouping variable. We start by computing principal component analysis as follow: # The variable Species (index = 5) is removed # before PCA analysis iris.pca &lt;- PCA(iris[,-5], graph = FALSE) In the R code below: the argument habillage or col.ind can be used to specify the factor variable for coloring the individuals by groups. To add a concentration ellipse around each group, specify the argument addEllipses = TRUE. The argument palette can be used to change group colors. fviz_pca_ind(iris.pca, geom.ind = &quot;point&quot;, # show points only (nbut not &quot;text&quot;) col.ind = iris$Species, # color by groups palette = c(&quot;#00AFBB&quot;, &quot;#E7B800&quot;, &quot;#FC4E07&quot;), addEllipses = TRUE, # Concentration ellipses legend.title = &quot;Groups&quot; ) To remove the group mean point, specify the argument mean.point = FALSE. If you want confidence ellipses instead of concentration ellipses, use ellipse.type = “confidence”. # Add confidence ellipses fviz_pca_ind(iris.pca, geom.ind = &quot;point&quot;, col.ind = iris$Species, palette = c(&quot;#00AFBB&quot;, &quot;#E7B800&quot;, &quot;#FC4E07&quot;), addEllipses = TRUE, ellipse.type = &quot;confidence&quot;, legend.title = &quot;Groups&quot; ) Note that, allowed values for palette include: “grey” for grey color palettes; brewer palettes e.g. “RdBu”, “Blues”, …; To view all, type this in R: RColorBrewer::display.brewer.all(). custom color palette e.g. c(“blue”, “red”); and scientific journal palettes from ggsci R package, e.g.: “npg”, “aaas”, * “lancet”, “jco”, “ucscgb”, “uchicago”, “simpsons” and “rickandmorty”. For example, to use the jco (journal of clinical oncology) color palette, type this: fviz_pca_ind(iris.pca, label = &quot;none&quot;, # hide individual labels habillage = iris$Species, # color by groups addEllipses = TRUE, # Concentration ellipses palette = &quot;jco&quot; ) 26.14 Graph customization Note that, fviz_pca_ind() and fviz_pca_var() and related functions are wrapper around the core function fviz() [in factoextra]. fviz() is a wrapper around the function ggscatter() [in ggpubr]. Therefore, further arguments, to be passed to the function fviz() and ggscatter(), can be specified in fviz_pca_ind() and fviz_pca_var(). Here, we present some of these additional arguments to customize the PCA graph of variables and individuals. 26.14.1 Dimensions By default, variables/individuals are represented on dimensions 1 and 2. If you want to visualize them on dimensions 2 and 3, for example, you should specify the argument axes = c(2, 3). # Variables on dimensions 2 and 3 fviz_pca_var(res.pca, axes = c(2, 3)) # Individuals on dimensions 2 and 3 fviz_pca_ind(res.pca, axes = c(2, 3)) Plot elements: point, text, arrow The argument geom (for geometry) and derivatives are used to specify the geometry elements or graphical elements to be used for plotting. geom.var: a text specifying the geometry to be used for plotting variables. Allowed values are the combination of c(“point”, “arrow”, “text”). Use geom.var = “point”, to show only points; Use geom.var = “text” to show only text labels; Use geom.var = c(“point”, “text”) to show both points and text labels Use geom.var = c(“arrow”, “text”) to show arrows and labels (default). For example, type this: # Show variable points and text labels fviz_pca_var(res.pca, geom.var = c(&quot;point&quot;, &quot;text&quot;)) # Show individuals text labels only fviz_pca_ind(res.pca, geom.ind = &quot;text&quot;) 26.15 Size and shape of plot elements # Change the size of arrows an labels fviz_pca_var(res.pca, arrowsize = 1, labelsize = 5, repel = TRUE) # Change points size, shape and fill color # Change labelsize fviz_pca_ind(res.pca, pointsize = 3, pointshape = 21, fill = &quot;lightblue&quot;, labelsize = 5, repel = TRUE) 26.16 Ellipses # Add confidence ellipses fviz_pca_ind(iris.pca, geom.ind = &quot;point&quot;, col.ind = iris$Species, # color by groups palette = c(&quot;#00AFBB&quot;, &quot;#E7B800&quot;, &quot;#FC4E07&quot;), addEllipses = TRUE, ellipse.type = &quot;confidence&quot;, legend.title = &quot;Groups&quot; ) # Convex hull fviz_pca_ind(iris.pca, geom.ind = &quot;point&quot;, col.ind = iris$Species, # color by groups palette = c(&quot;#00AFBB&quot;, &quot;#E7B800&quot;, &quot;#FC4E07&quot;), addEllipses = TRUE, ellipse.type = &quot;convex&quot;, legend.title = &quot;Groups&quot; ) 26.17 Group mean points When coloring individuals by groups (section (???)(color-ind-by-groups)), the mean points of groups (barycenters) are also displayed by default. To remove the mean points, use the argument mean.point = FALSE. fviz_pca_ind(iris.pca, geom.ind = &quot;point&quot;, # show points only (but not &quot;text&quot;) group.ind = iris$Species, # color by groups legend.title = &quot;Groups&quot;, mean.point = FALSE) 26.18 Axis lines fviz_pca_var(res.pca, axes.linetype = &quot;blank&quot;) 26.19 Graphical parameters To change easily the graphical of any ggplots, you can use the function ggpar() [ggpubr package] ind.p &lt;- fviz_pca_ind(iris.pca, geom = &quot;point&quot;, col.ind = iris$Species) ggpubr::ggpar(ind.p, title = &quot;Principal Component Analysis&quot;, subtitle = &quot;Iris data set&quot;, caption = &quot;Source: factoextra&quot;, xlab = &quot;PC1&quot;, ylab = &quot;PC2&quot;, legend.title = &quot;Species&quot;, legend.position = &quot;top&quot;, ggtheme = theme_gray(), palette = &quot;jco&quot; ) 26.20 Biplot To make a simple biplot of individuals and variables, type this: fviz_pca_biplot(res.pca, repel = TRUE, col.var = &quot;#2E9FDF&quot;, # Variables color col.ind = &quot;#696969&quot; # Individuals color ) Note that, the biplot might be only useful when there is a low number of variables and individuals in the data set; otherwise the final plot would be unreadable. Note also that, the coordinate of individuals and variables are not constructed on the same space. Therefore, in the biplot, you should mainly focus on the direction of variables but not on their absolute positions on the plot. Roughly speaking a biplot can be interpreted as follow: * an individual that is on the same side of a given variable has a high value for this variable; * an individual that is on the opposite side of a given variable has a low value for this variable. fviz_pca_biplot(iris.pca, col.ind = iris$Species, palette = &quot;jco&quot;, addEllipses = TRUE, label = &quot;var&quot;, col.var = &quot;black&quot;, repel = TRUE, legend.title = &quot;Species&quot;) In the following example, we want to color both individuals and variables by groups. The trick is to use pointshape = 21 for individual points. This particular point shape can be filled by a color using the argument fill.ind. The border line color of individual points is set to “black” using col.ind. To color variable by groups, the argument col.var will be used. To customize individuals and variable colors, we use the helper functions fill_palette() and color_palette() [in ggpubr package]. fviz_pca_biplot(iris.pca, # Fill individuals by groups geom.ind = &quot;point&quot;, pointshape = 21, pointsize = 2.5, fill.ind = iris$Species, col.ind = &quot;black&quot;, # Color variable by groups col.var = factor(c(&quot;sepal&quot;, &quot;sepal&quot;, &quot;petal&quot;, &quot;petal&quot;)), legend.title = list(fill = &quot;Species&quot;, color = &quot;Clusters&quot;), repel = TRUE # Avoid label overplotting )+ ggpubr::fill_palette(&quot;jco&quot;)+ # Indiviual fill color ggpubr::color_palette(&quot;npg&quot;) # Variable colors Another complex example is to color individuals by groups (discrete color) and variables by their contributions to the principal components (gradient colors). Additionally, we’ll change the transparency of variables by their contributions using the argument alpha.var. fviz_pca_biplot(iris.pca, # Individuals geom.ind = &quot;point&quot;, fill.ind = iris$Species, col.ind = &quot;black&quot;, pointshape = 21, pointsize = 2, palette = &quot;jco&quot;, addEllipses = TRUE, # Variables alpha.var =&quot;contrib&quot;, col.var = &quot;contrib&quot;, gradient.cols = &quot;RdYlBu&quot;, legend.title = list(fill = &quot;Species&quot;, color = &quot;Contrib&quot;, alpha = &quot;Contrib&quot;) ) 26.21 Supplementary elements Definition and types As described above (section (???)(pca-data-format)), the decathlon2 data sets contain supplementary continuous variables (quanti.sup, columns 11:12), supplementary qualitative variables (quali.sup, column 13) and supplementary individuals (ind.sup, rows 24:27). Supplementary variables and individuals are not used for the determination of the principal components. Their coordinates are predicted using only the information provided by the performed principal component analysis on active variables/individuals. Specification in PCA To specify supplementary individuals and variables, the function PCA() can be used as follow: res.pca &lt;- PCA(decathlon2, ind.sup = 24:27, quanti.sup = 11:12, quali.sup = 13, graph=FALSE) 26.22 Quantitative variables Predicted results (coordinates, correlation and cos2) for the supplementary quantitative variables: res.pca$quanti.sup #&gt; $coord #&gt; Dim.1 Dim.2 Dim.3 Dim.4 Dim.5 #&gt; Rank -0.701 -0.2452 -0.183 0.0558 -0.0738 #&gt; Points 0.964 0.0777 0.158 -0.1662 -0.0311 #&gt; #&gt; $cor #&gt; Dim.1 Dim.2 Dim.3 Dim.4 Dim.5 #&gt; Rank -0.701 -0.2452 -0.183 0.0558 -0.0738 #&gt; Points 0.964 0.0777 0.158 -0.1662 -0.0311 #&gt; #&gt; $cos2 #&gt; Dim.1 Dim.2 Dim.3 Dim.4 Dim.5 #&gt; Rank 0.492 0.06012 0.0336 0.00311 0.00545 #&gt; Points 0.929 0.00603 0.0250 0.02763 0.00097 Visualize all variables (active and supplementary ones): fviz_pca_var(res.pca) Note that, by default, supplementary quantitative variables are shown in blue color and dashed lines. Further arguments to customize the plot: # Change color of variables fviz_pca_var(res.pca, col.var = &quot;black&quot;, # Active variables col.quanti.sup = &quot;red&quot; # Suppl. quantitative variables ) # Hide active variables on the plot, # show only supplementary variables fviz_pca_var(res.pca, invisible = &quot;var&quot;) # Hide supplementary variables fviz_pca_var(res.pca, invisible = &quot;quanti.sup&quot;) Using the fviz_pca_var(), the quantitative supplementary variables are displayed automatically on the correlation circle plot. Note that, you can add the quanti.sup variables manually, using the fviz_add() function, for further customization. An example is shown below. # Plot of active variables p &lt;- fviz_pca_var(res.pca, invisible = &quot;quanti.sup&quot;) # Add supplementary active variables fviz_add(p, res.pca$quanti.sup$coord, geom = c(&quot;arrow&quot;, &quot;text&quot;), color = &quot;red&quot;) 26.23 Individuals Predicted results for the supplementary individuals (ind.sup): res.pca$ind.sup #&gt; $coord #&gt; Dim.1 Dim.2 Dim.3 Dim.4 Dim.5 #&gt; KARPOV 0.795 0.7795 -1.633 1.724 -0.7507 #&gt; WARNERS -0.386 -0.1216 -1.739 -0.706 -0.0323 #&gt; Nool -0.559 1.9775 -0.483 -2.278 -0.2546 #&gt; Drews -1.109 0.0174 -3.049 -1.534 -0.3264 #&gt; #&gt; $cos2 #&gt; Dim.1 Dim.2 Dim.3 Dim.4 Dim.5 #&gt; KARPOV 0.0510 4.91e-02 0.2155 0.2403 0.045549 #&gt; WARNERS 0.0242 2.40e-03 0.4904 0.0809 0.000169 #&gt; Nool 0.0290 3.62e-01 0.0216 0.4811 0.006008 #&gt; Drews 0.0921 2.27e-05 0.6956 0.1762 0.007974 #&gt; #&gt; $dist #&gt; KARPOV WARNERS Nool Drews #&gt; 3.52 2.48 3.28 3.66 Visualize all individuals (active and supplementary ones). On the graph, you can add also the supplementary qualitative variables (quali.sup), which coordinates is accessible using res.pca\\(quali.supp\\)coord. p &lt;- fviz_pca_ind(res.pca, col.ind.sup = &quot;blue&quot;, repel = TRUE) p &lt;- fviz_add(p, res.pca$quali.sup$coord, color = &quot;red&quot;) p Supplementary individuals are shown in blue. The levels of the supplementary qualitative variable are shown in red color. 26.24 Qualitative variables In the previous section, we showed that you can add the supplementary qualitative variables on individuals plot using fviz_add(). Note that, the supplementary qualitative variables can be also used for coloring individuals by groups. This can help to interpret the data. The data sets decathlon2 contain a supplementary qualitative variable at columns 13 corresponding to the type of competitions. The results concerning the supplementary qualitative variable are: res.pca$quali #&gt; $coord #&gt; Dim.1 Dim.2 Dim.3 Dim.4 Dim.5 #&gt; Decastar -1.34 0.122 -0.0379 0.181 0.134 #&gt; OlympicG 1.23 -0.112 0.0347 -0.166 -0.123 #&gt; #&gt; $cos2 #&gt; Dim.1 Dim.2 Dim.3 Dim.4 Dim.5 #&gt; Decastar 0.905 0.00744 0.00072 0.0164 0.00905 #&gt; OlympicG 0.905 0.00744 0.00072 0.0164 0.00905 #&gt; #&gt; $v.test #&gt; Dim.1 Dim.2 Dim.3 Dim.4 Dim.5 #&gt; Decastar -2.97 0.403 -0.153 0.897 0.72 #&gt; OlympicG 2.97 -0.403 0.153 -0.897 -0.72 #&gt; #&gt; $dist #&gt; Decastar OlympicG #&gt; 1.41 1.29 #&gt; #&gt; $eta2 #&gt; Dim.1 Dim.2 Dim.3 Dim.4 Dim.5 #&gt; Competition 0.401 0.0074 0.00106 0.0366 0.0236 To color individuals by a supplementary qualitative variable, the argument habillage is used to specify the index of the supplementary qualitative variable. Historically, this argument name comes from the FactoMineR package. It’s a french word meaning “dressing” in english. To keep consistency between FactoMineR and factoextra, we decided to keep the same argument name fviz_pca_ind(res.pca, habillage = 13, addEllipses =TRUE, ellipse.type = &quot;confidence&quot;, palette = &quot;jco&quot;, repel = TRUE) Recall that, to remove the mean points of groups, specify the argument mean.point = FALSE. 26.25 Filtering results If you have many individuals/variable, it’s possible to visualize only some of them using the arguments select.ind and select.var. # Visualize variable with cos2 &gt;= 0.6 fviz_pca_var(res.pca, select.var = list(cos2 = 0.6)) # Top 5 active variables with the highest cos2 fviz_pca_var(res.pca, select.var= list(cos2 = 5)) # Select by names name &lt;- list(name = c(&quot;Long.jump&quot;, &quot;High.jump&quot;, &quot;X100m&quot;)) fviz_pca_var(res.pca, select.var = name) # top 5 contributing individuals and variable fviz_pca_biplot(res.pca, select.ind = list(contrib = 5), select.var = list(contrib = 5), ggtheme = theme_minimal()) When the selection is done according to the contribution values, supplementary individuals/variables are not shown because they don’t contribute to the construction of the axes. 26.26 Exporting results Export plots to PDF/PNG files The factoextra package produces a ggplot2-based graphs. To save any ggplots, the standard R code is as follow: # Print the plot to a pdf file pdf(&quot;myplot.pdf&quot;) print(myplot) dev.off() In the following examples, we’ll show you how to save the different graphs into pdf or png files. The first step is to create the plots you want as an R object: # Scree plot scree.plot &lt;- fviz_eig(res.pca) # Plot of individuals ind.plot &lt;- fviz_pca_ind(res.pca) # Plot of variables var.plot &lt;- fviz_pca_var(res.pca) pdf(file.path(data_out_dir, &quot;PCA.pdf&quot;)) # Create a new pdf device print(scree.plot) print(ind.plot) print(var.plot) dev.off() # Close the pdf device #&gt; png #&gt; 2 Note that, using the above R code will create the PDF file into your current working directory. To see the path of your current working directory, type getwd() in the R console. To print each plot to specific png file, the R code looks like this: # Print scree plot to a png file png(file.path(data_out_dir, &quot;pca-scree-plot.png&quot;)) print(scree.plot) dev.off() #&gt; png #&gt; 2 # Print individuals plot to a png file png(file.path(data_out_dir, &quot;pca-variables.png&quot;)) print(var.plot) dev.off() #&gt; png #&gt; 2 # Print variables plot to a png file png(file.path(data_out_dir, &quot;pca-individuals.png&quot;)) print(ind.plot) dev.off() #&gt; png #&gt; 2 Another alternative, to export ggplots, is to use the function ggexport() [in ggpubr package]. We like ggexport(), because it’s very simple. With one line R code, it allows us to export individual plots to a file (pdf, eps or png) (one plot per page). It can also arrange the plots (2 plot per page, for example) before exporting them. The examples below demonstrates how to export ggplots using ggexport(). Export individual plots to a pdf file (one plot per page): library(ggpubr) #&gt; Loading required package: magrittr ggexport(plotlist = list(scree.plot, ind.plot, var.plot), filename = file.path(data_out_dir, &quot;PCA.pdf&quot;)) #&gt; file saved to /home/datascience/repos/machine-learning-rsuite/export/PCA.pdf Arrange and export. Specify nrow and ncol to display multiple plots on the same page: ggexport(plotlist = list(scree.plot, ind.plot, var.plot), nrow = 2, ncol = 2, filename = file.path(data_out_dir, &quot;PCA.pdf&quot;)) #&gt; file saved to /home/datascience/repos/machine-learning-rsuite/export/PCA.pdf Export plots to png files. If you specify a list of plots, then multiple png files will be automatically created to hold each plot. ggexport(plotlist = list(scree.plot, ind.plot, var.plot), filename = file.path(data_out_dir, &quot;PCA.png&quot;)) #&gt; [1] &quot;/home/datascience/repos/machine-learning-rsuite/export/PCA%03d.png&quot; #&gt; file saved to /home/datascience/repos/machine-learning-rsuite/export/PCA%03d.png 26.27 Export results to txt/csv files All the outputs of the PCA (individuals/variables coordinates, contributions, etc) can be exported at once, into a TXT/CSV file, using the function write.infile() [in FactoMineR] package: # Export into a TXT file write.infile(res.pca, file.path(data_out_dir, &quot;pca.txt&quot;), sep = &quot;\\t&quot;) # Export into a CSV file write.infile(res.pca, file.path(data_out_dir, &quot;pca.csv&quot;), sep = &quot;;&quot;) 26.28 Summary In conclusion, we described how to perform and interpret principal component analysis (PCA). We computed PCA using the PCA() function [FactoMineR]. Next, we used the factoextra R package to produce ggplot2-based visualization of the PCA results. There are other functions [packages] to compute PCA in R: Using prcomp() [stats] res.pca &lt;- prcomp(iris[, -5], scale. = TRUE) res.pca &lt;- princomp(iris[, -5], cor = TRUE) Using dudi.pca() [ade4] library(ade4) #&gt; #&gt; Attaching package: &#39;ade4&#39; #&gt; The following object is masked from &#39;package:FactoMineR&#39;: #&gt; #&gt; reconst res.pca &lt;- dudi.pca(iris[, -5], scannf = FALSE, nf = 5) Using epPCA() [ExPosition] library(ExPosition) #&gt; Loading required package: prettyGraphs res.pca &lt;- epPCA(iris[, -5], graph = FALSE) No matter what functions you decide to use, in the list above, the factoextra package can handle the output for creating beautiful plots similar to what we described in the previous sections for FactoMineR: fviz_eig(res.pca) # Scree plot fviz_pca_ind(res.pca) # Graph of individuals fviz_pca_var(res.pca) # Graph of variables "],
["biplot-of-the-iris-data-set.html", "Chapter 27 Biplot of the Iris data set 27.1 Iris: underlying principal components 27.2 Iris. Compute the eigenvectors and eigenvalues", " Chapter 27 Biplot of the Iris data set # devtools::install_github(&quot;vqv/ggbiplot&quot;) library(ggbiplot) #&gt; Loading required package: ggplot2 #&gt; Registered S3 methods overwritten by &#39;ggplot2&#39;: #&gt; method from #&gt; [.quosures rlang #&gt; c.quosures rlang #&gt; print.quosures rlang #&gt; Loading required package: plyr #&gt; Loading required package: scales #&gt; Loading required package: grid iris.pca &lt;- prcomp(iris[, 1:4], center = TRUE, scale = TRUE) print(iris.pca) #&gt; Standard deviations (1, .., p=4): #&gt; [1] 1.708 0.956 0.383 0.144 #&gt; #&gt; Rotation (n x k) = (4 x 4): #&gt; PC1 PC2 PC3 PC4 #&gt; Sepal.Length 0.521 -0.3774 0.720 0.261 #&gt; Sepal.Width -0.269 -0.9233 -0.244 -0.124 #&gt; Petal.Length 0.580 -0.0245 -0.142 -0.801 #&gt; Petal.Width 0.565 -0.0669 -0.634 0.524 summary(iris.pca) #&gt; Importance of components: #&gt; PC1 PC2 PC3 PC4 #&gt; Standard deviation 1.71 0.956 0.3831 0.14393 #&gt; Proportion of Variance 0.73 0.229 0.0367 0.00518 #&gt; Cumulative Proportion 0.73 0.958 0.9948 1.00000 g &lt;- ggbiplot(iris.pca, obs.scale = 1, var.scale = 1, groups = iris$Species, ellipse = TRUE, circle = TRUE) + scale_color_discrete(name = &quot;&quot;) + theme(legend.direction = &quot;horizontal&quot;, legend.position = &quot;top&quot;) print(g) The PC1 axis explains 0.730 of the variance, while the PC2 axis explains 0.229 of the variance. 27.1 Iris: underlying principal components # Run PCA here with prcomp () iris.pca &lt;- prcomp(iris[, 1:4], center = TRUE, scale = TRUE) print(iris.pca) #&gt; Standard deviations (1, .., p=4): #&gt; [1] 1.708 0.956 0.383 0.144 #&gt; #&gt; Rotation (n x k) = (4 x 4): #&gt; PC1 PC2 PC3 PC4 #&gt; Sepal.Length 0.521 -0.3774 0.720 0.261 #&gt; Sepal.Width -0.269 -0.9233 -0.244 -0.124 #&gt; Petal.Length 0.580 -0.0245 -0.142 -0.801 #&gt; Petal.Width 0.565 -0.0669 -0.634 0.524 # Now, compute the new dataset aligned to the PCs by # using the predict() function . df.new &lt;- predict(iris.pca, iris[, 1:4]) head(df.new) #&gt; PC1 PC2 PC3 PC4 #&gt; [1,] -2.26 -0.478 0.1273 0.02409 #&gt; [2,] -2.07 0.672 0.2338 0.10266 #&gt; [3,] -2.36 0.341 -0.0441 0.02828 #&gt; [4,] -2.29 0.595 -0.0910 -0.06574 #&gt; [5,] -2.38 -0.645 -0.0157 -0.03580 #&gt; [6,] -2.07 -1.484 -0.0269 0.00659 # Show the PCA model’s sdev values are the square root # of the projected variances, which are along the diagonal # of the covariance matrix of the projected data. iris.pca$sdev^2 #&gt; [1] 2.9185 0.9140 0.1468 0.0207 # # Compute covariance matrix for new dataset. # Recall that the standard deviation is the square root of the variance. round(cov(df.new), 5) #&gt; PC1 PC2 PC3 PC4 #&gt; PC1 2.92 0.000 0.000 0.0000 #&gt; PC2 0.00 0.914 0.000 0.0000 #&gt; PC3 0.00 0.000 0.147 0.0000 #&gt; PC4 0.00 0.000 0.000 0.0207 27.2 Iris. Compute the eigenvectors and eigenvalues # Scale and center the data. df.scaled &lt;- scale(iris[, 1:4], center = TRUE, scale = TRUE) # Compute the covariance matrix. cov.df.scaled &lt;- cov(df.scaled) # Compute the eigenvectors and eigen values. # Each eigenvector (column) is a principal component. # Each eigenvalue is the variance explained by the # associated eigenvector. eigenInformation &lt;- eigen(cov.df.scaled) print(eigenInformation) #&gt; eigen() decomposition #&gt; $values #&gt; [1] 2.9185 0.9140 0.1468 0.0207 #&gt; #&gt; $vectors #&gt; [,1] [,2] [,3] [,4] #&gt; [1,] 0.521 -0.3774 0.720 0.261 #&gt; [2,] -0.269 -0.9233 -0.244 -0.124 #&gt; [3,] 0.580 -0.0245 -0.142 -0.801 #&gt; [4,] 0.565 -0.0669 -0.634 0.524 # Now, compute the new dataset aligned to the PCs by # multiplying the eigenvector and data matrices. # Create transposes in preparation for matrix multiplication eigenvectors.t &lt;- t(eigenInformation$vectors) # 4x4 df.scaled.t &lt;- t(df.scaled) # 4x150 # Perform matrix multiplication. df.new &lt;- eigenvectors.t %*% df.scaled.t # 4x150 # Create new data frame. First take transpose and # then add column names. df.new.t &lt;- t(df.new) # 150x4 colnames(df.new.t) &lt;- c(&quot;PC1&quot;, &quot;PC2&quot;, &quot;PC3&quot;, &quot;PC4&quot;) head(df.new.t) #&gt; PC1 PC2 PC3 PC4 #&gt; [1,] -2.26 -0.478 0.1273 0.02409 #&gt; [2,] -2.07 0.672 0.2338 0.10266 #&gt; [3,] -2.36 0.341 -0.0441 0.02828 #&gt; [4,] -2.29 0.595 -0.0910 -0.06574 #&gt; [5,] -2.38 -0.645 -0.0157 -0.03580 #&gt; [6,] -2.07 -1.484 -0.0269 0.00659 # Compute covariance matrix for new dataset round(cov(df.new.t), 5) #&gt; PC1 PC2 PC3 PC4 #&gt; PC1 2.92 0.000 0.000 0.0000 #&gt; PC2 0.00 0.914 0.000 0.0000 #&gt; PC3 0.00 0.000 0.147 0.0000 #&gt; PC4 0.00 0.000 0.000 0.0207 "],
["logistic-regression-diabetes.html", "Chapter 28 Logistic Regression. Diabetes 28.1 Introduction 28.2 Exploring the data 28.3 Logistic regression with R 28.4 A second model 28.5 Classification rate and confusion matrix 28.6 Plots and decision boundaries", " Chapter 28 Logistic Regression. Diabetes 28.1 Introduction Source: https://github.com/AntoineGuillot2/Logistic-Regression-R/blob/master/script.R Source: http://enhancedatascience.com/2017/04/26/r-basics-logistic-regression-with-r/ Data: https://www.kaggle.com/uciml/pima-indians-diabetes-database The goal of logistic regression is to predict whether an outcome will be positive (aka 1) or negative (i.e: 0). Some real life example could be: Will Emmanuel Macron win the French Presidential election or will he lose? Does Mr.X has this illness or not? Will this visitor click on my link or not? So, logistic regression can be used in a lot of binary classification cases and will often be run before more advanced methods. For this tutorial, we will use the diabetes detection dataset from Kaggle. This dataset contains data from Pima Indians Women such as the number of pregnancies, the blood pressure, the skin thickness, … the goal of the tutorial is to be able to detect diabetes using only these measures. 28.2 Exploring the data As usual, first, let’s take a look at our data. You can download the data here then please put the file diabetes.csv in your working directory. With the summary function, we can easily summarise the different variables: library(ggplot2) #&gt; Registered S3 methods overwritten by &#39;ggplot2&#39;: #&gt; method from #&gt; [.quosures rlang #&gt; c.quosures rlang #&gt; print.quosures rlang library(data.table) DiabetesData &lt;- data.table(read.csv(file.path(data_raw_dir, &#39;diabetes.csv&#39;))) # Quick data summary summary(DiabetesData) #&gt; Pregnancies Glucose BloodPressure SkinThickness #&gt; Min. : 0.00 Min. : 0 Min. : 0.0 Min. : 0.0 #&gt; 1st Qu.: 1.00 1st Qu.: 99 1st Qu.: 62.0 1st Qu.: 0.0 #&gt; Median : 3.00 Median :117 Median : 72.0 Median :23.0 #&gt; Mean : 3.85 Mean :121 Mean : 69.1 Mean :20.5 #&gt; 3rd Qu.: 6.00 3rd Qu.:140 3rd Qu.: 80.0 3rd Qu.:32.0 #&gt; Max. :17.00 Max. :199 Max. :122.0 Max. :99.0 #&gt; Insulin BMI DiabetesPedigreeFunction Age #&gt; Min. : 0 Min. : 0.0 Min. :0.078 Min. :21.0 #&gt; 1st Qu.: 0 1st Qu.:27.3 1st Qu.:0.244 1st Qu.:24.0 #&gt; Median : 30 Median :32.0 Median :0.372 Median :29.0 #&gt; Mean : 80 Mean :32.0 Mean :0.472 Mean :33.2 #&gt; 3rd Qu.:127 3rd Qu.:36.6 3rd Qu.:0.626 3rd Qu.:41.0 #&gt; Max. :846 Max. :67.1 Max. :2.420 Max. :81.0 #&gt; Outcome #&gt; Min. :0.000 #&gt; 1st Qu.:0.000 #&gt; Median :0.000 #&gt; Mean :0.349 #&gt; 3rd Qu.:1.000 #&gt; Max. :1.000 The mean of the outcome is 0.35 which shows an imbalance between the classes. However, the imbalance should not be too strong to be a problem. To understand the relationship between variables, a Scatter Plot Matrix will be used. To plot it, the GGally package was used. # Scatter plot matrix library(GGally) #&gt; Registered S3 method overwritten by &#39;GGally&#39;: #&gt; method from #&gt; +.gg ggplot2 ggpairs(DiabetesData, lower = list(continuous=&#39;smooth&#39;)) The correlations between explanatory variables do not seem too strong. Hence the model is not likely to suffer from multicollinearity. All explanatory variable are correlated with the Outcome. At first sight, glucose rate is the most important factor to detect the outcome. 28.3 Logistic regression with R After variable exploration, a first model can be fitted using the glm function. With stargazer, it is easy to get nice output in ASCII or even Latex. # first model: all features glm1 = glm(Outcome~., DiabetesData, family = binomial(link=&quot;logit&quot;)) summary(glm1) #&gt; #&gt; Call: #&gt; glm(formula = Outcome ~ ., family = binomial(link = &quot;logit&quot;), #&gt; data = DiabetesData) #&gt; #&gt; Deviance Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -2.557 -0.727 -0.416 0.727 2.930 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) -8.404696 0.716636 -11.73 &lt; 2e-16 *** #&gt; Pregnancies 0.123182 0.032078 3.84 0.00012 *** #&gt; Glucose 0.035164 0.003709 9.48 &lt; 2e-16 *** #&gt; BloodPressure -0.013296 0.005234 -2.54 0.01107 * #&gt; SkinThickness 0.000619 0.006899 0.09 0.92852 #&gt; Insulin -0.001192 0.000901 -1.32 0.18607 #&gt; BMI 0.089701 0.015088 5.95 2.8e-09 *** #&gt; DiabetesPedigreeFunction 0.945180 0.299147 3.16 0.00158 ** #&gt; Age 0.014869 0.009335 1.59 0.11119 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for binomial family taken to be 1) #&gt; #&gt; Null deviance: 993.48 on 767 degrees of freedom #&gt; Residual deviance: 723.45 on 759 degrees of freedom #&gt; AIC: 741.4 #&gt; #&gt; Number of Fisher Scoring iterations: 5 require(stargazer) #&gt; Loading required package: stargazer #&gt; #&gt; Please cite as: #&gt; Hlavac, Marek (2018). stargazer: Well-Formatted Regression and Summary Statistics Tables. #&gt; R package version 5.2.2. https://CRAN.R-project.org/package=stargazer stargazer(glm1,type=&#39;text&#39;) #&gt; #&gt; ==================================================== #&gt; Dependent variable: #&gt; --------------------------- #&gt; Outcome #&gt; ---------------------------------------------------- #&gt; Pregnancies 0.123*** #&gt; (0.032) #&gt; #&gt; Glucose 0.035*** #&gt; (0.004) #&gt; #&gt; BloodPressure -0.013** #&gt; (0.005) #&gt; #&gt; SkinThickness 0.001 #&gt; (0.007) #&gt; #&gt; Insulin -0.001 #&gt; (0.001) #&gt; #&gt; BMI 0.090*** #&gt; (0.015) #&gt; #&gt; DiabetesPedigreeFunction 0.945*** #&gt; (0.299) #&gt; #&gt; Age 0.015 #&gt; (0.009) #&gt; #&gt; Constant -8.400*** #&gt; (0.717) #&gt; #&gt; ---------------------------------------------------- #&gt; Observations 768 #&gt; Log Likelihood -362.000 #&gt; Akaike Inf. Crit. 741.000 #&gt; ==================================================== #&gt; Note: *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01 The overall model is significant. As expected the glucose rate has the lowest p-value of all the variables. However, Age, Insulin and Skin Thickness are not good predictors of Diabetes. 28.4 A second model Since some variables are not significant, removing them is a good way to improve model robustness. In the second model, SkinThickness, Insulin, and Age are removed. # second model: selected features glm2 = glm(Outcome~., data = DiabetesData[,c(1:3,6:7,9), with=F], family = binomial(link=&quot;logit&quot;)) summary(glm2) #&gt; #&gt; Call: #&gt; glm(formula = Outcome ~ ., family = binomial(link = &quot;logit&quot;), #&gt; data = DiabetesData[, c(1:3, 6:7, 9), with = F]) #&gt; #&gt; Deviance Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -2.793 -0.736 -0.419 0.725 2.955 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) -7.95495 0.67582 -11.77 &lt; 2e-16 *** #&gt; Pregnancies 0.15349 0.02784 5.51 3.5e-08 *** #&gt; Glucose 0.03466 0.00339 10.21 &lt; 2e-16 *** #&gt; BloodPressure -0.01201 0.00503 -2.39 0.017 * #&gt; BMI 0.08483 0.01412 6.01 1.9e-09 *** #&gt; DiabetesPedigreeFunction 0.91063 0.29403 3.10 0.002 ** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for binomial family taken to be 1) #&gt; #&gt; Null deviance: 993.48 on 767 degrees of freedom #&gt; Residual deviance: 728.56 on 762 degrees of freedom #&gt; AIC: 740.6 #&gt; #&gt; Number of Fisher Scoring iterations: 5 28.5 Classification rate and confusion matrix Now that we have our model, let’s access its performance. # Correctly classified observations mean((glm2$fitted.values&gt;0.5)==DiabetesData$Outcome) #&gt; [1] 0.775 Around 77.4% of all observations are correctly classified. Due to class imbalance, we need to go further with a confusion matrix. ###Confusion matrix count RP=sum((glm2$fitted.values&gt;=0.5)==DiabetesData$Outcome &amp; DiabetesData$Outcome==1) FP=sum((glm2$fitted.values&gt;=0.5)!=DiabetesData$Outcome &amp; DiabetesData$Outcome==0) RN=sum((glm2$fitted.values&gt;=0.5)==DiabetesData$Outcome &amp; DiabetesData$Outcome==0) FN=sum((glm2$fitted.values&gt;=0.5)!=DiabetesData$Outcome &amp; DiabetesData$Outcome==1) confMat&lt;-matrix(c(RP,FP,FN,RN),ncol = 2) colnames(confMat)&lt;-c(&quot;Pred Diabetes&quot;,&#39;Pred no diabetes&#39;) rownames(confMat)&lt;-c(&quot;Real Diabetes&quot;,&#39;Real no diabetes&#39;) confMat #&gt; Pred Diabetes Pred no diabetes #&gt; Real Diabetes 154 114 #&gt; Real no diabetes 59 441 The model is good to detect people who do not have diabetes. However, its performance on ill people is not great (only 154 out of 268 have been correctly classified). You can also get the percentage of Real/False Positive/Negative: # Confusion matrix proportion RPR=RP/sum(DiabetesData$Outcome==1)*100 FNR=FN/sum(DiabetesData$Outcome==1)*100 FPR=FP/sum(DiabetesData$Outcome==0)*100 RNR=RN/sum(DiabetesData$Outcome==0)*100 confMat&lt;-matrix(c(RPR,FPR,FNR,RNR),ncol = 2) colnames(confMat)&lt;-c(&quot;Pred Diabetes&quot;,&#39;Pred no diabetes&#39;) rownames(confMat)&lt;-c(&quot;Real Diabetes&quot;,&#39;Real no diabetes&#39;) confMat #&gt; Pred Diabetes Pred no diabetes #&gt; Real Diabetes 57.5 42.5 #&gt; Real no diabetes 11.8 88.2 And here is the matrix, 57.5% of people with diabetes are correctly classified. A way to improve the false negative rate would lower the detection threshold. However, as a consequence, the false positive rate would increase. 28.6 Plots and decision boundaries The two strongest predictors of the outcome are Glucose rate and BMI. High glucose rate and high BMI are strong indicators of Diabetes. # Plot and decision boundaries DiabetesData$Predicted &lt;- glm2$fitted.values ggplot(DiabetesData, aes(x = BMI, y = Glucose, color = Predicted &gt; 0.5)) + geom_point(size=2, alpha=0.5) ggplot(DiabetesData, aes(x=BMI, y = Glucose, color=Outcome == (Predicted &gt; 0.5))) + geom_point(size=2, alpha=0.5) We can also plot both BMI and glucose against the outcomes, the other variables are taken at their mean level. range(DiabetesData$BMI) #&gt; [1] 0.0 67.1 # BMI vs predicted BMI_plot = data.frame(BMI = ((min(DiabetesData$BMI-2)*100): (max(DiabetesData$BMI+2)*100))/100, Glucose = mean(DiabetesData$Glucose), Pregnancies = mean(DiabetesData$Pregnancies), BloodPressure = mean(DiabetesData$BloodPressure), DiabetesPedigreeFunction = mean(DiabetesData$DiabetesPedigreeFunction)) BMI_plot$Predicted = predict(glm2, newdata = BMI_plot, type = &#39;response&#39;) ggplot(BMI_plot, aes(x = BMI, y = Predicted)) + geom_line() range(BMI_plot$BMI) #&gt; [1] -2.0 69.1 range(DiabetesData$Glucose) #&gt; [1] 0 199 # Glucose vs predicted Glucose_plot=data.frame(Glucose = ((min(DiabetesData$Glucose-2)*100): (max(DiabetesData$Glucose+2)*100))/100, BMI=mean(DiabetesData$BMI), Pregnancies=mean(DiabetesData$Pregnancies), BloodPressure=mean(DiabetesData$BloodPressure), DiabetesPedigreeFunction=mean(DiabetesData$DiabetesPedigreeFunction)) Glucose_plot$Predicted = predict(glm2, newdata = Glucose_plot, type = &#39;response&#39;) ggplot(Glucose_plot, aes(x = Glucose, y = Predicted)) + geom_line() range(Glucose_plot$Glucose) #&gt; [1] -2 201 "],
["sensitivity-analysis-for-neural-networks.html", "Chapter 29 Sensitivity analysis for neural networks 29.1 Introduction 29.2 The Lek profile function 29.3 Getting a dataframe from lek 29.4 The lek function works with lm 29.5 lek function works with RSNNS", " Chapter 29 Sensitivity analysis for neural networks 29.1 Introduction https://beckmw.wordpress.com/tag/nnet/ I’ve made quite a few blog posts about neural networks and some of the diagnostic tools that can be used to ‘demystify’ the information contained in these models. Frankly, I’m kind of sick of writing about neural networks but I wanted to share one last tool I’ve implemented in R. I’m a strong believer that supervised neural networks can be used for much more than prediction, as is the common assumption by most researchers. I hope that my collection of posts, including this one, has shown the versatility of these models to develop inference into causation. To date, I’ve authored posts on visualizing neural networks, animating neural networks, and determining importance of model inputs. This post will describe a function for a sensitivity analysis of a neural network. Specifically, I will describe an approach to evaluate the form of the relationship of a response variable with the explanatory variables used in the model. The general goal of a sensitivity analysis is similar to evaluating relative importance of explanatory variables, with a few important distinctions. For both analyses, we are interested in the relationships between explanatory and response variables as described by the model in the hope that the neural network has explained some real-world phenomenon. Using Garson’s algorithm,1 we can get an idea of the magnitude and sign of the relationship between variables relative to each other. Conversely, the sensitivity analysis allows us to obtain information about the form of the relationship between variables rather than a categorical description, such as variable x is positively and strongly related to y. For example, how does a response variable change in relation to increasing or decreasing values of a given explanatory variable? Is it a linear response, non-linear, uni-modal, no response, etc.? Furthermore, how does the form of the response change given values of the other explanatory variables in the model? We might expect that the relationship between a response and explanatory variable might differ given the context of the other explanatory variables (i.e., an interaction may be present). The sensitivity analysis can provide this information. As with most of my posts, I’ve created the sensitivity analysis function using ideas from other people that are much more clever than me. I’ve simply converted these ideas into a useful form in R. Ultimate credit for the sensitivity analysis goes to Sovan Lek (and colleagues), who developed the approach in the mid-1990s. The ‘Lek-profile method’ is described briefly in Lek et al. 19962 and in more detail in Gevrey et al. 2003.3 I’ll provide a brief summary here since the method is pretty simple. In fact, the profile method can be extended to any statistical model and is not specific to neural networks, although it is one of few methods used to evaluate the latter. For any statistical model where multiple response variables are related to multiple explanatory variables, we choose one response and one explanatory variable. We obtain predictions of the response variable across the range of values for the given explanatory variable. All other explanatory variables are held constant at a given set of respective values (e.g., minimum, 20th percentile, maximum). The final product is a set of response curves for one response variable across the range of values for one explanatory variable, while holding all other explanatory variables constant. This is implemented in R by creating a matrix of values for explanatory variables where the number of rows is the number of observations and the number of columns is the number of explanatory variables. All explanatory variables are held at their mean (or other constant value) while the variable of interest is sequenced from its minimum to maximum value across the range of observations. This matrix (actually a data frame) is then used to predict values of the response variable from a fitted model object. This is repeated for different variables. I’ll illustrate the function using simulated data, as I’ve done in previous posts. The exception here is that I’ll be using two response variables instead of one. The two response variables are linear combinations of eight explanatory variables, with random error components taken from a normal distribution. The relationships between the variables are determined by the arbitrary set of parameters (parms1 and parms2). The explanatory variables are partially correlated and taken from a multivariate normal distribution. require(clusterGeneration) #&gt; Loading required package: clusterGeneration #&gt; Loading required package: MASS require(nnet) #&gt; Loading required package: nnet #define number of variables and observations set.seed(2) num.vars&lt;-8 num.obs&lt;-10000 #define correlation matrix for explanatory variables #define actual parameter values cov.mat&lt;-genPositiveDefMat(num.vars,covMethod=c(&quot;unifcorrmat&quot;))$Sigma rand.vars&lt;-mvrnorm(num.obs,rep(0,num.vars),Sigma=cov.mat) parms1&lt;-runif(num.vars,-10,10) y1&lt;-rand.vars %*% matrix(parms1) + rnorm(num.obs,sd=20) parms2&lt;-runif(num.vars,-10,10) y2&lt;-rand.vars %*% matrix(parms2) + rnorm(num.obs,sd=20) #prep data and create neural network rand.vars&lt;-data.frame(rand.vars) resp&lt;-apply(cbind(y1,y2),2, function(y) (y-min(y))/(max(y)-min(y))) resp&lt;-data.frame(resp) names(resp)&lt;-c(&#39;Y1&#39;,&#39;Y2&#39;) mod1 &lt;- nnet(rand.vars,resp,size=8,linout=T) #&gt; # weights: 90 #&gt; initial value 30121.205794 #&gt; iter 10 value 130.537462 #&gt; iter 20 value 57.187090 #&gt; iter 30 value 47.285919 #&gt; iter 40 value 42.778564 #&gt; iter 50 value 39.837784 #&gt; iter 60 value 36.694632 #&gt; iter 70 value 35.140948 #&gt; iter 80 value 34.268819 #&gt; iter 90 value 33.772282 #&gt; iter 100 value 33.472654 #&gt; final value 33.472654 #&gt; stopped after 100 iterations #import the function from Github library(devtools) # source_url(&#39;https://gist.githubusercontent.com/fawda123/7471137/raw/466c1474d0a505ff044412703516c34f1a4684a5/nnet_plot_update.r&#39;) source(&quot;nnet_plot_update.r&quot;) #plot each model plot.nnet(mod1) #&gt; Loading required package: scales #&gt; Loading required package: reshape 29.2 The Lek profile function We’ve created a neural network that hopefully describes the relationship of two response variables with eight explanatory variables. The sensitivity analysis lets us visualize these relationships. The Lek profile function can be used once we have a neural network model in our workspace. The function is imported and used as follows: # source(&#39;https://gist.githubusercontent.com/fawda123/6860630/raw/b8bf4a6c88d6b392b1bfa6ef24759ae98f31877c/lek_fun.r&#39;) source(&quot;lek_fun.r&quot;) lek.fun(mod1) #&gt; Loading required package: ggplot2 #&gt; Registered S3 methods overwritten by &#39;ggplot2&#39;: #&gt; method from #&gt; [.quosures rlang #&gt; c.quosures rlang #&gt; print.quosures rlang Fig: Sensitivity analysis of the two response variables in the neural network model to individual explanatory variables. Splits represent the quantile values at which the remaining explanatory variables were held constant. The function can be obtained here By default, the function runs a sensitivity analysis for all variables. This creates a busy plot so we may want to look at specific variables of interest. Maybe we want to evaluate different quantile values as well. These options can be changed using the arguments. lek.fun(mod1,var.sens=c(&#39;X2&#39;,&#39;X5&#39;),split.vals=seq(0,1,by=0.05)) Fig: Sensitivity analysis of the two response variables in relation to explanatory variables X2 and X5 and different quantile values for the remaining variables. The function also returns a ggplot2 object that can be further modified. You may prefer a different theme, color, or line type, for example. p1&lt;-lek.fun(mod1) class(p1) #&gt; [1] &quot;gg&quot; &quot;ggplot&quot; # [1] &quot;gg&quot; &quot;ggplot&quot; p1 + theme_bw() + scale_colour_brewer(palette=&quot;PuBu&quot;) + scale_linetype_manual(values=rep(&#39;dashed&#39;,6)) + scale_size_manual(values=rep(1,6)) #&gt; Scale for &#39;linetype&#39; is already present. Adding another scale for #&gt; &#39;linetype&#39;, which will replace the existing scale. #&gt; Scale for &#39;size&#39; is already present. Adding another scale for &#39;size&#39;, #&gt; which will replace the existing scale. 29.3 Getting a dataframe from lek Finally, the actual values from the sensitivity analysis can be returned if you’d prefer that instead. The output is a data frame in long form that was created using melt.list from the reshape package for compatibility with ggplot2. The six columns indicate values for explanatory variables on the x-axes, names of the response variables, predicted values of the response variables, quantiles at which other explanatory variables were held constant, and names of the explanatory variables on the x-axes. head(lek.fun(mod1,val.out = TRUE)) #&gt; Explanatory resp.name Response Splits exp.name #&gt; 1 -9.58 Y1 0.466 0 X1 #&gt; 2 -9.39 Y1 0.466 0 X1 #&gt; 3 -9.19 Y1 0.467 0 X1 #&gt; 4 -9.00 Y1 0.467 0 X1 #&gt; 5 -8.81 Y1 0.468 0 X1 #&gt; 6 -8.62 Y1 0.468 0 X1 29.4 The lek function works with lm I mentioned earlier that the function is not unique to neural networks and can work with other models created in R. I haven’t done an extensive test of the function, but I’m fairly certain that it will work if the model object has a predict method (e.g., predict.lm). Here’s an example using the function to evaluate a multiple linear regression for one of the response variables. mod2 &lt;-lm(Y1 ~ ., data = cbind(resp[,&#39;Y1&#39;, drop = F], rand.vars)) lek.fun(mod2) This function has little relevance for conventional models like linear regression since a wealth of diagnostic tools are already available (e.g., effects plots, add/drop procedures, outlier tests, etc.). The application of the function to neural networks provides insight into the relationships described by the models, insights that to my knowledge, cannot be obtained using current tools in R. This post concludes my contribution of diagnostic tools for neural networks in R and I hope that they have been useful to some of you. I have spent the last year or so working with neural networks and my opinion of their utility is mixed. I see advantages in the use of highly flexible computer-based algorithms, although in most cases similar conclusions can be made using more conventional analyses. I suggest that neural networks only be used if there is an extremely high sample size and other methods have proven inconclusive. Feel free to voice your opinions or suggestions in the comments. 29.5 lek function works with RSNNS require(clusterGeneration) require(RSNNS) #&gt; Loading required package: RSNNS #&gt; Loading required package: Rcpp require(devtools) #define number of variables and observations set.seed(2) num.vars&lt;-8 num.obs&lt;-10000 #define correlation matrix for explanatory variables #define actual parameter values cov.mat &lt;-genPositiveDefMat(num.vars,covMethod=c(&quot;unifcorrmat&quot;))$Sigma rand.vars &lt;-mvrnorm(num.obs,rep(0,num.vars),Sigma=cov.mat) parms1 &lt;-runif(num.vars,-10,10) y1 &lt;-rand.vars %*% matrix(parms1) + rnorm(num.obs,sd=20) parms2 &lt;-runif(num.vars,-10,10) y2 &lt;-rand.vars %*% matrix(parms2) + rnorm(num.obs,sd=20) #prep data and create neural network rand.vars &lt;- data.frame(rand.vars) resp &lt;- apply(cbind(y1,y2),2, function(y) (y-min(y))/(max(y)-min(y))) resp &lt;- data.frame(resp) names(resp)&lt;-c(&#39;Y1&#39;,&#39;Y2&#39;) tibble::as_tibble(rand.vars) #&gt; # A tibble: 10,000 x 8 #&gt; X1 X2 X3 X4 X5 X6 X7 X8 #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1.61 2.13 2.13 3.97 -1.34 2.00 3.11 -2.55 #&gt; 2 -1.25 3.07 -0.325 1.61 -0.484 2.28 2.98 -1.71 #&gt; 3 -3.17 -1.29 -1.77 -1.66 -0.549 -3.19 1.07 1.81 #&gt; 4 -2.39 3.28 -3.42 -0.160 -1.52 2.67 7.05 -1.14 #&gt; 5 -1.55 -0.181 -1.14 2.27 -1.68 -1.67 3.08 0.334 #&gt; 6 0.0690 -1.54 -2.98 2.84 1.42 1.31 1.82 2.07 #&gt; # … with 9,994 more rows tibble::as_tibble(resp) #&gt; # A tibble: 10,000 x 2 #&gt; Y1 Y2 #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.461 0.500 #&gt; 2 0.416 0.509 #&gt; 3 0.534 0.675 #&gt; 4 0.548 0.619 #&gt; 5 0.519 0.659 #&gt; 6 0.389 0.622 #&gt; # … with 9,994 more rows # create neural network model mod2 &lt;- mlp(rand.vars, resp, size = 8, linOut = T) #import sensitivity analysis function source_url(&#39;https://gist.githubusercontent.com/fawda123/6860630/raw/b8bf4a6c88d6b392b1bfa6ef24759ae98f31877c/lek_fun.r&#39;) #&gt; SHA-1 hash of file is 4a2d33b94a08f46a94518207a4ae7cc412845222 #sensitivity analsyis, note &#39;exp.in&#39; argument lek.fun(mod2, exp.in = rand.vars) "],
["references.html", "Chapter 30 References", " Chapter 30 References 1 Garson GD. 1991. Interpreting neural network connection weights. Artificial Intelligence Expert. 6:46–51. 2 Lek S, Delacoste M, Baran P, Dimopoulos I, Lauga J, Aulagnier S. 1996. Application of neural networks to modelling nonlinear relationships in Ecology. Ecological Modelling. 90:39-52. 3 Gevrey M, Dimopoulos I, Lek S. 2003. Review and comparison of methods to study the contribution of variables in artificial neural network models. Ecological Modelling. 160:249-264. "],
["what-is-hat-in-regression-output.html", "Chapter 31 What is .hat in regression output", " Chapter 31 What is .hat in regression output https://stats.stackexchange.com/a/256364/154908 Q. The augment() function in the broom package for R creates a dataframe of predicted values from a regression model. Columns created include the fitted values, the standard error of the fit and Cook’s distance. They also include something with which I’m not familar and that is the column .hat. library(broom) data(mtcars) m1 &lt;- lm(mpg ~ wt, data = mtcars) head(augment(m1)) #&gt; # A tibble: 6 x 10 #&gt; .rownames mpg wt .fitted .se.fit .resid .hat .sigma .cooksd #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Mazda RX4 21 2.62 23.3 0.634 -2.28 0.0433 3.07 1.33e-2 #&gt; 2 Mazda RX… 21 2.88 21.9 0.571 -0.920 0.0352 3.09 1.72e-3 #&gt; 3 Datsun 7… 22.8 2.32 24.9 0.736 -2.09 0.0584 3.07 1.54e-2 #&gt; 4 Hornet 4… 21.4 3.22 20.1 0.538 1.30 0.0313 3.09 3.02e-3 #&gt; 5 Hornet S… 18.7 3.44 18.9 0.553 -0.200 0.0329 3.10 7.60e-5 #&gt; 6 Valiant 18.1 3.46 18.8 0.555 -0.693 0.0332 3.10 9.21e-4 #&gt; # … with 1 more variable: .std.resid &lt;dbl&gt; # .hat vector augment(m1)$.hat #&gt; [1] 0.0433 0.0352 0.0584 0.0313 0.0329 0.0332 0.0354 0.0313 0.0314 0.0329 #&gt; [11] 0.0329 0.0558 0.0401 0.0419 0.1705 0.1953 0.1838 0.0661 0.1177 0.0956 #&gt; [21] 0.0503 0.0343 0.0328 0.0443 0.0445 0.0866 0.0704 0.1291 0.0313 0.0380 #&gt; [31] 0.0354 0.0377 Can anyone explain what this value is, and is it different between linear regression and logistic regression? A. Those would be the diagonal elements of the hat-matrix which describe the leverage each point has on its fitted values. If one fits: \\[\\vec{Y} = \\mathbf{X} \\vec {\\beta} + \\vec {\\epsilon}\\] then: \\[\\mathbf{H} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\] In this example: \\[ \\begin{pmatrix}Y_1\\\\ \\vdots\\\\ Y_{32}\\end{pmatrix} = \\begin{pmatrix} 1 &amp; 2.620\\\\ \\vdots\\\\ 1 &amp; 2.780 \\end{pmatrix} \\cdot \\begin{pmatrix} \\beta_0\\\\ \\beta_1 \\end{pmatrix} + \\begin{pmatrix}\\epsilon_1\\\\ \\vdots\\\\ \\epsilon_{32}\\end{pmatrix} \\] Then calculating this \\(\\mathbf{H}\\) matrix results in: library(MASS) wt &lt;- mtcars[, 6] X &lt;- matrix(cbind(rep(1, length(wt)), wt), ncol=2) H &lt;- X %*% ginv(t(X) %*% X) %*% t(X) Where this last matrix is a \\(32 \\times 32\\) matrix and contains these hat values on the diagonal. X 32x2 t(X) 2x32 X %*% t(X) 32x32 t(X) %*% X 2x2 ginv(t(X) %*% X) 2x2 ginv(t(X) %*% X) %*% t(X) 2x32 X %*% ginv(t(X) %*% X) 32x2 dim(ginv(t(X) %*% X) %*% t(X)) #&gt; [1] 2 32 x1 &lt;- X %*% ginv(t(X) %*% X) dim(x1) #&gt; [1] 32 2 dim(x1 %*% t(X)) #&gt; [1] 32 32 x2 &lt;- ginv(t(X) %*% X) %*% t(X) dim(x2) #&gt; [1] 2 32 dim(X %*% x2) #&gt; [1] 32 32 # this last matrix is a 32×32 matrix and contains these hat values on the diagonal. diag(H) #&gt; [1] 0.0433 0.0352 0.0584 0.0313 0.0329 0.0332 0.0354 0.0313 0.0314 0.0329 #&gt; [11] 0.0329 0.0558 0.0401 0.0419 0.1705 0.1953 0.1838 0.0661 0.1177 0.0956 #&gt; [21] 0.0503 0.0343 0.0328 0.0443 0.0445 0.0866 0.0704 0.1291 0.0313 0.0380 #&gt; [31] 0.0354 0.0377 "],
["q-q-normal-to-compare-data-to-distributions.html", "Chapter 32 Q-Q normal to compare data to distributions 32.1 Introduction 32.2 Why we want to compare emprirical vs theoretical distributions 32.3 The normal q-q plot 32.4 Using R’s built-in functions 32.5 Using the ggplot2 plotting environment", " Chapter 32 Q-Q normal to compare data to distributions 32.1 Introduction https://mgimond.github.io/ES218/Week06a.html Thus far, we have used the quantile-quantile plots to compare the distributions between two empirical (i.e. observational) datasets. This is sometimes referred to as an empirical Q-Q plot. We can also use the q-q plot to compare an empirical observation to a theoretical observation (i.e. one defined mathematically). Such a plot is usually referred to as a theoretical Q-Q plot. Examples of popular theoretical observations are the normal distribution (aka the Gaussian distribution), the chi-square distribution, and the exponential distribution just to name a few. #&gt; Registered S3 methods overwritten by &#39;ggplot2&#39;: #&gt; method from #&gt; [.quosures rlang #&gt; c.quosures rlang #&gt; print.quosures rlang 32.2 Why we want to compare emprirical vs theoretical distributions There are many reasons we might want to compare empirical data to theoretical distributions: A theoretical distribution is easy to parameterize. For example, if the shape of the distribution of a batch of numbers can be approximated by a normal distribution we can reduce the complexity of our data to just two values: the mean and the standard deviation. If data can be approximated by certain theoretical distributions, then many mainstream statistical procedures can be applied to the data. In inferential statistics, knowing that a sample was derived from a population whose distribution follows a theoretical distribution allows us to derive certain properties of the population from the sample. For example, if we know that a sample comes from a normally distributed population, we can define confidence intervals for the sample mean using a t-distribution. Modeling the distribution of the observed data can provide insight into the underlying process that generated the data. But very few empirical datasets follow any theoretical distributions exactly. So the questions usually ends up being “how well does theoretical distribution X fit my data?” The theoretical quantile-quantile plot is a tool to explore how a batch of numbers deviates from a theoretical distribution and to visually assess whether the difference is significant for the purpose of the analysis. In the following examples, we will compare empirical data to the normal distribution using the normal quantile-quantile plot. 32.3 The normal q-q plot The normal q-q plot is just a special case of the empirical q-q plot we’ve explored so far; the difference being that we assign the normal distribution quantiles to the x-axis. 32.3.1 Drawing a normal q-q plot from scratch In the following example, we’ll compare the Alto 1 group to a normal distribution. First, we’ll extract the Alto 1 height values and save them as an atomic vector object using dplyr’s piping operations. However, dplyr’s operations will return a dataframe–even if a single column is selected. To force the output to an atomic vector, we’ll pipe the subset to pull(height) which will extract the height column into a plain vector element. library(dplyr) #&gt; #&gt; Attaching package: &#39;dplyr&#39; #&gt; The following object is masked from &#39;package:gridExtra&#39;: #&gt; #&gt; combine #&gt; The following objects are masked from &#39;package:stats&#39;: #&gt; #&gt; filter, lag #&gt; The following objects are masked from &#39;package:base&#39;: #&gt; #&gt; intersect, setdiff, setequal, union df &lt;- lattice::singer alto &lt;- df %&gt;% filter(voice.part == &quot;Alto 1&quot;) %&gt;% arrange(height) %&gt;% pull(height) %&gt;% print #&gt; [1] 60 61 61 61 61 62 62 62 63 63 63 63 64 64 64 65 65 65 65 66 66 66 66 #&gt; [24] 66 66 66 67 67 67 67 68 68 69 70 72 Next, we need to find the matching normal distribution quantiles. We first find the f-values for alto, then use qnorm to find the matching normal distribution values from those same f-values i &lt;- 1:length(alto) fi &lt;- (i - 0.5) / length(alto) fi #&gt; [1] 0.0143 0.0429 0.0714 0.1000 0.1286 0.1571 0.1857 0.2143 0.2429 0.2714 #&gt; [11] 0.3000 0.3286 0.3571 0.3857 0.4143 0.4429 0.4714 0.5000 0.5286 0.5571 #&gt; [21] 0.5857 0.6143 0.6429 0.6714 0.7000 0.7286 0.7571 0.7857 0.8143 0.8429 #&gt; [31] 0.8714 0.9000 0.9286 0.9571 0.9857 x.norm &lt;- qnorm(fi) x.norm #&gt; [1] -2.1893 -1.7185 -1.4652 -1.2816 -1.1332 -1.0063 -0.8938 -0.7916 #&gt; [9] -0.6971 -0.6085 -0.5244 -0.4439 -0.3661 -0.2905 -0.2165 -0.1437 #&gt; [17] -0.0717 0.0000 0.0717 0.1437 0.2165 0.2905 0.3661 0.4439 #&gt; [25] 0.5244 0.6085 0.6971 0.7916 0.8938 1.0063 1.1332 1.2816 #&gt; [33] 1.4652 1.7185 2.1893 plot(x.norm) Now we can plot the sorted alto values against the normal values. plot( alto ~ x.norm, type=&quot;p&quot;, xlab=&quot;Normal quantiles&quot;, pch=20) When comparing a batch of numbers to a theoretical distribution on a q-q plot, we are looking for significant deviation from a straight line. To make it easier to judge straightness, we can fit a line to the points. Note that we are not creating a 45° (or x=y) slope; the range of values between both sets of numbers do not match. Here, we are only seeking the straightness of the points. There are many ways one can fit a line to the data, Cleveland opts to fit a line to the first and third quartile of the q-q plot. The following chunk of code identifies the quantiles for both the alto dataset and the theoretical normal distribution. It then computes the slope and intercept from these coordinates. # Find 1st and 3rd quartile for the Alto 1 data y &lt;- quantile(alto, c(0.25, 0.75), type=5) y #&gt; 25% 75% #&gt; 63.0 66.8 # Find the 1st and 3rd quartile of the normal distribution x &lt;- qnorm( c(0.25, 0.75)) x #&gt; [1] -0.674 0.674 # Now we can compute the intercept and slope of the line that passes # through these points slope &lt;- diff(y) / diff(x) int &lt;- y[1] - slope * x[1] Next, we add the line to the plot. plot( alto ~ x.norm, type=&quot;p&quot;, xlab=&quot;Normal quantiles&quot;, pch=20) abline(a=int, b=slope ) 32.4 Using R’s built-in functions R has two built-in functions that facilitate the plot building task when comparing a batch to a normal distribution: qqnorm and qqline. Note that the function qqline allows the user to define the quantile method via the qtype= parameter. Here, we set it to 5 to match our choice of f-value calculation. qqnorm(alto) # plot the points qqline(alto, qtype=5) # plot the line That’s it. Just two lines of code! 32.5 Using the ggplot2 plotting environment We can take advantage of the stat_qq() function to plot the points, but the equation for the line must be computed manually (as was done earlier). Those steps will be repeated here. # normal distribution library(ggplot2) # Find the slope and intercept of the line that passes through the 1st and 3rd # quartile of the normal q-q plot y &lt;- quantile(alto, c(0.25, 0.75), type=5) # Find the 1st and 3rd quartiles x &lt;- qnorm( c(0.25, 0.75)) # Find the matching normal values on the x-axis slope &lt;- diff(y) / diff(x) # Compute the line slope int &lt;- y[1] - slope * x[1] # Compute the line intercept # Generate normal q-q plot ggplot() + aes(sample=alto) + stat_qq(distribution=qnorm) + geom_abline(intercept=int, slope=slope) + ylab(&quot;Height&quot;) qq_any &lt;- function(var, f) { # Find the slope and intercept of the line that passes through the 1st and 3rd # quartile of the normal q-q plot y &lt;- quantile(var, c(0.25, 0.75), type=5) # Find the 1st and 3rd quartiles x &lt;- f( c(0.25, 0.75)) # Find the matching normal values x-axis slope &lt;- diff(y) / diff(x) # Compute the line slope int &lt;- y[1] - slope * x[1] # Compute the line intercept ggplot() + aes(sample = var) + stat_qq(distribution = f) + geom_abline(intercept=int, slope=slope) } # two function only, for the moment qq_any(alto, qexp) qq_any(alto, qnorm) We can, of course, make use of ggplot’s faceting function to generate trellised plots. For example, the following plot replicates Cleveland’s figure 2.11 (except for the layout which we’ll setup as a single row of plots instead). But first, we will need to compute the slopes for each singer group. We’ll use dplyr’s piping operations to create a new dataframe with singer group name, slope and intercept. library(dplyr) intsl &lt;- df %&gt;% group_by(voice.part) %&gt;% summarize(q25 = quantile(height,0.25, type=5), q75 = quantile(height,0.75, type=5), norm25 = qnorm( 0.25), norm75 = qnorm( 0.75), slope = (q25 - q75) / (norm25 - norm75), int = q25 - slope * norm25) %&gt;% select(voice.part, slope, int) %&gt;% print #&gt; # A tibble: 8 x 3 #&gt; voice.part slope int #&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Bass 2 2.97 72 #&gt; 2 Bass 1 2.22 70.5 #&gt; 3 Tenor 2 1.48 70 #&gt; 4 Tenor 1 3.89 68.6 #&gt; 5 Alto 2 2.22 65.5 #&gt; 6 Alto 1 2.78 64.9 #&gt; # … with 2 more rows It’s important that the voice.part names match those in df letter-for-letter so that when ggplot is called, it will know which facet to assign the slope and intercept values to via geom_abline. ggplot(df, aes(sample = height)) + stat_qq(distribution = qnorm) + geom_abline(data=intsl, aes(intercept=int, slope=slope), col=&quot;blue&quot;) + facet_wrap(~voice.part, nrow=1) + ylab(&quot;Height&quot;) "],
["qq-and-pp-plots.html", "Chapter 33 QQ and PP Plots 33.1 QQ Plot 33.2 Some Examples 33.3 Calibrating the Variability 33.4 Scalability 33.5 Comparing Two Distributions 33.6 PP Plots 33.7 Plots For Assessing Model Fit", " Chapter 33 QQ and PP Plots https://homepage.divms.uiowa.edu/~luke/classes/STAT4580/qqpp.html 33.1 QQ Plot One way to assess how well a particular theoretical model describes a data distribution is to plot data quantiles against theoretical quantiles. Base graphics provides qqnorm, lattice has qqmath, and ggplot2 has geom_qq. The default theoretical distribution used in these is a standard normal, but, except for qqnorm, these allow you to specify an alternative. For a large sample from the theoretical distribution the plot should be a straight line through the origin with slope 1: library(ggplot2) #&gt; Registered S3 methods overwritten by &#39;ggplot2&#39;: #&gt; method from #&gt; [.quosures rlang #&gt; c.quosures rlang #&gt; print.quosures rlang n &lt;- 10000 ggplot() + geom_qq(aes(sample = rnorm(n))) If the plot is a straight line with a different slope or intercept, then the data distribution corresponds to a location-scale transformation of the theoretical distribution. The slope is the scale and the intercept is the location: ggplot() + geom_qq(aes(sample = rnorm(n, 10, 4))) + geom_abline(intercept = 10, slope = 4, color = &quot;red&quot;, size = 1.5, alpha = 0.8) The QQ plot can be constructed directly as a scatterplot of the sorted sample \\(i = 1, \\dots, n\\) against quantiles for \\[p_i = \\frac{i}{n} - \\frac{1}{2n}\\] p &lt;- (1 : n) / n - 0.5 / n y &lt;- rnorm(n, 10, 4) ggplot() + geom_point(aes(x = qnorm(p), y = sort(y))) 33.2 Some Examples The histograms and density estimates for the duration variable in the geyser data set showed that the distribution is far from a normal distribution, and the normal QQ plot shows this as well: library(MASS) ggplot(geyser) + geom_qq(aes(sample = duration)) Except for rounding the parent heights in the Galton data seemed not too fat from normally distributed: library(psych) ggplot(galton) + geom_qq(aes(sample = parent)) Rounding interferes more with this visualization than with a histogram or a density plot. Rounding is more visible with this visualization than with a histogram or a density plot. Another Gatlton dataset available in the UsingR package with less rounding is father.son: library(UsingR) ggplot(father.son) + geom_qq(aes(sample = fheight)) The middle seems to be fairly straight, but the ends are somewhat wiggly. How can you calibrate your judgment? 33.3 Calibrating the Variability One approach is to use simulation, sometimes called a graphical bootstrap. The nboot function will simulate R samples from a normal distribution that match a variable x on sample size, sample mean, and sample SD. The result is returned in a dataframe suitable for plotting: nsim &lt;- function(n, m = 0, s = 1) { z &lt;- rnorm(n) m + s * ((z - mean(z)) / sd(z)) } nboot &lt;- function(x, R) { n &lt;- length(x) m &lt;- mean(x) s &lt;- sd(x) do.call(rbind, lapply(1 : R, function(i) { xx &lt;- sort(nsim(n, m, s)) p &lt;- seq_along(x) / n - 0.5 / n data.frame(x = xx, p = p, sim = i) })) } Plotting these as lines shows the variability in shapes we can expect when sampling from the theoretical normal distribution: gb &lt;- nboot(father.son$fheight, 50) tibble::as_tibble(gb) #&gt; # A tibble: 53,900 x 3 #&gt; x p sim #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 59.8 0.000464 1 #&gt; 2 59.9 0.00139 1 #&gt; 3 59.9 0.00232 1 #&gt; 4 60.8 0.00325 1 #&gt; 5 60.8 0.00417 1 #&gt; 6 60.9 0.00510 1 #&gt; # … with 5.389e+04 more rows ggplot() + geom_line(aes(x = qnorm(p), y = x, group = sim), color = &quot;gray&quot;, data = gb) We can then insert this simulation behind our data to help calibrate the visualization: ggplot(father.son) + geom_line(aes(x = qnorm(p), y = x, group = sim), color = &quot;gray&quot;, data = gb) + geom_qq(aes(sample = fheight)) 33.4 Scalability For large sample sizes overplotting will occur: ggplot(diamonds) + geom_qq(aes(sample = price)) This can be alleviated by using a grid of quantiles: nq &lt;- 100 p &lt;- (1 : nq) / nq - 0.5 / nq ggplot() + geom_point(aes(x = qnorm(p), y = quantile(diamonds$price, p))) A more reasonable model might be an exponential distribution: ggplot() + geom_point(aes(x = qexp(p), y = quantile(diamonds$price, p))) 33.5 Comparing Two Distributions The QQ plot can also be used to compare two distributions based on a sample from each. If the samples are the same size then this is just a plot of the ordered sample values against each other. Choosing a fixed set of quantiles allows samples of unequal size to be compared. Using a small set of quantiles we can compare the distributions of waiting times between eruptions of Old Faithful from the two different data sets we have looked at: nq &lt;- 31 # user defined nq &lt;- min(length(geyser$waiting), length(faithful$waiting)) # or take the minimum p &lt;- (1 : nq) / nq - 0.5 / nq wg &lt;- geyser$waiting wf &lt;- faithful$waiting ggplot() + geom_point(aes(x = quantile(wg, p), y = quantile(wf, p))) 33.6 PP Plots The PP plot for comparing a sample to a theoretical model plots the theoretical proportion less than or equal to each observed value against the actual proportion. For a theoretical cumulative distribution function F this means plotting \\[F(x(i))∼pi\\] For the fheight variable in the father.son data: m &lt;- mean(father.son$fheight) s &lt;- sd(father.son$fheight) n &lt;- nrow(father.son) p &lt;- (1 : n) / n - 0.5 / n ggplot(father.son) + geom_point(aes(x = p, y = sort(pnorm(fheight, m, s)))) The values on the vertical axis are the probability integral transform of the data for the theoretical distribution. If the data are a sample from the theoretical distribution then these transforms would be uniformly distributed on [0,1]. The PP plot is a QQ plot of these transformed values against a uniform distribution. The PP plot goes through the points (0,0) and (1,1) and so is much less variable in the tails: pp &lt;- ggplot() + geom_line(aes(x = p, y = pnorm(x, m, s), group = sim), color = &quot;gray&quot;, data = gb) pp Adding the data: pp + geom_point(aes(x = p, y = sort(pnorm(fheight, m, s))), data = (father.son)) The PP plot is also less sensitive to deviations in the tails. A compromise between the QQ and PP plots uses the arcsine square root variance-stabilizing transformation, which makes the variability approximately constant across the range of the plot: vpp &lt;- ggplot() + geom_line(aes(x = asin(sqrt(p)), y = asin(sqrt(pnorm(x, m, s))), group = sim), color = &quot;gray&quot;, data = gb) vpp Adding the data: vpp + geom_point(aes(x = asin(sqrt(p)), y = sort(asin(sqrt(pnorm(fheight, m, s))))), data = (father.son)) 33.7 Plots For Assessing Model Fit Both QQ and PP plots can be used to asses how well a theoretical family of models fits your data, or your residuals. To use a PP plot you have to estimate the parameters first. For a location-scale family, like the normal distribution family, you can use a QQ plot with a standard member of the family. Some other families can use other transformations that lead to straight lines for family members: The Weibull family is widely used in reliability modeling; its CDF is \\[F(t) = 1 - \\exp\\left\\{-\\left(\\frac{t}{b}\\right)^a\\right\\}\\] The logarithms of Weibull random variables form a location-scale family. Special paper used to be available for Weibull probability plots. A Weibull QQ plot for price in the diamonds data: n &lt;- nrow(diamonds) p &lt;- (1 : n) / n - 0.5 / n ggplot(diamonds) + geom_point(aes(x = log10(qweibull(p, 1, 1)), y = log10(sort(price)))) The lower tail does not match a Weibull distribution. Is this important? In engineering applications it often is. In selecting a reasonable model to capture the shape of this distribution it may not be. QQ plots are helpful for understanding departures from a theoretical model. No data will fit a theoretical model perfectly. Case-specific judgment is needed to decide whether departures are important. George Box: All models are wrong but some are useful. "],
["data-visualization-working-with-models.html", "Chapter 34 Data Visualization: Working with models 34.1 Introduction 34.2 Show several fits at once, with a legend 34.3 Look inside model objects 34.4 Get model-based graphics right 34.5 Generate predictions to graph 34.6 Tidy model objects with broom 34.7 Grouped analysis and list-columns 34.8 Plot marginal effects 34.9 Plots from complex surveys 34.10 Where to go next", " Chapter 34 Data Visualization: Working with models 34.1 Introduction Source: https://socviz.co/modeling.html Data visualization is about more than generating figures that display the raw numbers from a table of data. Right from the beginning, it involves summarizing or transforming parts of the data, and then plotting the results. Statistical models are a central part of that process. In this Chapter, we will begin by looking briefly at how ggplot can use various modeling techniques directly within geoms. Then we will see how to use the broom and margins libraries to tidily extract and plot estimates from models that we fit ourselves. # load libraries library(ggplot2) #&gt; Registered S3 methods overwritten by &#39;ggplot2&#39;: #&gt; method from #&gt; [.quosures rlang #&gt; c.quosures rlang #&gt; print.quosures rlang library(dplyr) #&gt; #&gt; Attaching package: &#39;dplyr&#39; #&gt; The following objects are masked from &#39;package:stats&#39;: #&gt; #&gt; filter, lag #&gt; The following objects are masked from &#39;package:base&#39;: #&gt; #&gt; intersect, setdiff, setequal, union library(tidyr) library(purrr) library(socviz) # devtools::install_github(&quot;kjhealy/socviz&quot;) library(gapminder) # plot two lines p &lt;- ggplot(data = gapminder, mapping = aes(x = log(gdpPercap), y = lifeExp)) p + geom_point(alpha=0.1) + geom_smooth(color = &quot;tomato&quot;, fill=&quot;tomato&quot;, method = MASS::rlm) + geom_smooth(color = &quot;steelblue&quot;, fill=&quot;steelblue&quot;, method = &quot;lm&quot;) # plot spline p + geom_point(alpha=0.1) + geom_smooth(color = &quot;tomato&quot;, method = &quot;lm&quot;, size = 1.2, formula = y ~ splines::bs(x, 3), se = FALSE) p + geom_point(alpha=0.1) + geom_quantile(color = &quot;tomato&quot;, size = 1.2, method = &quot;rqss&quot;, lambda = 1, quantiles = c(0.20, 0.5, 0.85)) #&gt; Loading required package: SparseM #&gt; #&gt; Attaching package: &#39;SparseM&#39; #&gt; The following object is masked from &#39;package:base&#39;: #&gt; #&gt; backsolve #&gt; Smoothing formula not specified. Using: y ~ qss(x, lambda = 1) #&gt; Warning in rq.fit.sfn(x, y, tau = tau, rhs = rhs, control = control, ...): tiny diagonals replaced with Inf when calling blkfct Histograms, density plots, boxplots, and other geoms compute either single numbers or new variables before plotting them. As we saw in Section 4.4, these calculations are done by stat_ functions, each of which works hand-in-hand with its default geom_ function, and vice versa. Moreover, from the smoothing lines we drew from almost the very first plots we made, we have seen that stat_ functions can do a fair amount of calculation and even model estimation on the fly. The geom_smooth() function can take a range of method arguments to fit LOESS, OLS, and robust regression lines, amongst others. Both the geom_smooth() and geom_quantile() functions can also be instructed to use different formulas to produce their fits. In the top panel of Figure 6.1, we access the MASS library’s rlm function to fit a robust regression line. In the second panel, the bs function is invoked directly from the splines library in the same way, to fit a polynominal curve to the data. This is the same approach to directly accessing functions without loading a whole library that we have already used several times when using functions from the scales library. The geom_quantile() function, meanwhile, is like a specialized version of geom_smooth() that can fit quantile regression lines using a variety of methods. The quantiles argument takes a vector specifying the quantiles at which to fit the lines. 34.2 Show several fits at once, with a legend As we just saw in the first panel of Figure 6.1, where we plotted both an OLS and a robust regression line, we can look at several fits at once on the same plot by layering on new smoothers with geom_smooth(). As long as we set the color and fill aesthetics to different values for each fit, we can easily distinguish them visually. However, ggplot will not draw a legend that guides us about which fit is which. This is because the smoothers are not logically connected to one another. They exist as separate layers. What if we are comparing several different fits and want a legend describing them? As it turns out, geom_smooth() can do this via the slightly unusual route of mapping the color and fill aesthetics to a string describing the model we are fitting, and then using scale_color_manual() and scale_fill_manual() to create the legend. First we use brewer.pal() from the RColorBrewer library to extract three qualitatively different colors from a larger palette. The colors are represented as hex values. As before use the :: convention to use the function without loading the whole library: model_colors &lt;- RColorBrewer::brewer.pal(3, &quot;Set1&quot;) model_colors #&gt; [1] &quot;#E41A1C&quot; &quot;#377EB8&quot; &quot;#4DAF4A&quot; Then we create a plot with three different smoothers, mapping the color and fill within the aes() function as the name of the smoother: p0 &lt;- ggplot(data = gapminder, mapping = aes(x = log(gdpPercap), y = lifeExp)) p1 &lt;- p0 + geom_point(alpha = 0.2) + geom_smooth(method = &quot;lm&quot;, aes(color = &quot;OLS&quot;, fill = &quot;OLS&quot;)) + geom_smooth(method = &quot;lm&quot;, formula = y ~ splines::bs(x, df = 3), aes(color = &quot;Cubic Spline&quot;, fill = &quot;Cubic Spline&quot;)) + geom_smooth(method = &quot;loess&quot;, aes(color = &quot;LOESS&quot;, fill = &quot;LOESS&quot;)) p1 + scale_color_manual(name = &quot;Models&quot;, values = model_colors) + scale_fill_manual(name = &quot;Models&quot;, values = model_colors) + theme(legend.position = &quot;top&quot;) In a way we have cheated a little here to make the plot work. Until now, we have always mapped aesthetics to the names of variables, not to strings like “OLS” or “Cubic Splines”. In Chapter 3, when we discussed mapping versus setting aesthetics, we saw what happened when we tried to change the color of the points in a scatterplot by setting them to “purple” inside the aes()function. The result was that the points turned red instead, as ggplot in effect created a new variable and labeled it with the word “purple”. We learned there that the aes() function was for mapping variables to aesthetics. Here we take advantage of that behavior, creating a new single-value variable for the name of each of our models. Ggplot will properly construct the relevant guide if we call scale_color_manual() and scale_fill_manual(). Remember that we have to call two scale functions because we have two mappings. The result is a single plot containing not just our three smoothers, but also an appropriate legend to guide the reader. These model-fitting features make ggplot very useful for exploratory work, and make it straightforward to generate and compare model-based trends and other summaries as part of the process of descriptive data visualization. The various stat_ functions are a flexible way to add summary estimates of various kinds to plots. But we will also want more than this, including presenting results from models we fit ourselves. 34.3 Look inside model objects Covering the details of fitting statistical models in R is beyond the scope of this book. For a comprehensive, modern introduction to that topic you should work your way through (Gelman &amp; Hill, 2018). (Harrell, 2016) is also very good on the many practical connections between modeling and graphing data. Similarly, (Gelman, 2004) provides a detailed discussion of the use of graphics as a tool in model-checking and validation. Here we will discuss some ways to take the models that you fit and extract information that is easy to work with in ggplot. Our goal, as always, is to get from however the object is stored to a tidy table of numbers that we can plot. Most classes of statistical model in R will contain the information we need, or will have a special set of functions, or methods, designed to extract it. We can start by learning a little more about how the output of models is stored in R. Remember, we are always working with objects, and objects have an internal structure consisting of named pieces. Sometimes these are single numbers, sometimes vectors, and sometimes lists of things like vectors, matrices, or formulas. We have been working extensively with tibbles and data frames. These store tables of data with named columns, perhaps consisting of different classes of variable, such as integers, characters, dates, or factors. Model objects are a little more complicated again. gapminder #&gt; # A tibble: 1,704 x 6 #&gt; country continent year lifeExp pop gdpPercap #&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 Afghanistan Asia 1952 28.8 8425333 779. #&gt; 2 Afghanistan Asia 1957 30.3 9240934 821. #&gt; 3 Afghanistan Asia 1962 32.0 10267083 853. #&gt; 4 Afghanistan Asia 1967 34.0 11537966 836. #&gt; 5 Afghanistan Asia 1972 36.1 13079460 740. #&gt; 6 Afghanistan Asia 1977 38.4 14880372 786. #&gt; # … with 1,698 more rows Remember, we can use the str() function to learn more about the internal structure of any object. For example, we can get some information on what class (or classes) of object gapminder is, how large it is, and what components it has. The output from str(gapminder) is somewhat dense: str(gapminder) #&gt; Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 1704 obs. of 6 variables: #&gt; $ country : Factor w/ 142 levels &quot;Afghanistan&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... #&gt; $ continent: Factor w/ 5 levels &quot;Africa&quot;,&quot;Americas&quot;,..: 3 3 3 3 3 3 3 3 3 3 ... #&gt; $ year : int 1952 1957 1962 1967 1972 1977 1982 1987 1992 1997 ... #&gt; $ lifeExp : num 28.8 30.3 32 34 36.1 ... #&gt; $ pop : int 8425333 9240934 10267083 11537966 13079460 14880372 12881816 13867957 16317921 22227415 ... #&gt; $ gdpPercap: num 779 821 853 836 740 ... There is a lot of information here about the object as a whole and each variable in it. In the same way, statistical models in R have an internal structure. But because models are more complex entities than data tables, their structure is correspondingly more complicated. There are more pieces of information, and more kinds of information, that we might want to use. All of this information is generally stored in or is computable from parts of a model object. We can create a linear model, an ordinary OLS regression, using the gapminder data. This dataset has a country-year structure that makes an OLS specification like this the wrong one to use. But never mind that for now. We use the lm() function to run the model, and store it in an object called out: out &lt;- lm(formula = lifeExp ~ gdpPercap + pop + continent, data = gapminder) The first argument is the formula for the model. lifeExp is the dependent variable and the tilde ~ operator is used to designate the left- and right-hand sides of a model (including in cases, as we saw with facet_wrap() where the model just has a right-hand side.) Let’s look at the results by asking R to print a summary of the model. summary(out) #&gt; #&gt; Call: #&gt; lm(formula = lifeExp ~ gdpPercap + pop + continent, data = gapminder) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -49.16 -4.49 0.30 5.11 25.17 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 4.78e+01 3.40e-01 140.82 &lt;2e-16 *** #&gt; gdpPercap 4.50e-04 2.35e-05 19.16 &lt;2e-16 *** #&gt; pop 6.57e-09 1.98e-09 3.33 9e-04 *** #&gt; continentAmericas 1.35e+01 6.00e-01 22.46 &lt;2e-16 *** #&gt; continentAsia 8.19e+00 5.71e-01 14.34 &lt;2e-16 *** #&gt; continentEurope 1.75e+01 6.25e-01 27.97 &lt;2e-16 *** #&gt; continentOceania 1.81e+01 1.78e+00 10.15 &lt;2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 8.37 on 1697 degrees of freedom #&gt; Multiple R-squared: 0.582, Adjusted R-squared: 0.581 #&gt; F-statistic: 394 on 6 and 1697 DF, p-value: &lt;2e-16 When we use the summary() function on out, we are not getting a simple feed of what’s in the model object. Instead, like any function, summary() takes its input, performs some actions, and produces output. In this case, what is printed to the console is partly information that is stored inside the model object, and partly information that the summary() function has calculated and formated for display on the screen. Behind the scenes, summary() gets help from other functions. Objects of different classes have default methods associated with them, so that when the generic summary() function is applied to a linear model object, the function knows to pass the work on to a more specialized function that does a bunch of calculations and formatting appropriate to a linear model object. We use the same generic summary() function on data frames, as in summary(gapminder), but in that case a different default method is applied. Schematic view of a linear model object.Figure 6.3: Schematic view of a linear model object. The output from summary() gives a precis of the model, but we can’t really do any further analysis with it directly. For example, what if we want to plot something from the model? The information necessary to make plots is inside the out object, but it is not obvious how to use it. If we take a look at the structure of the model object with str(out) we will find that there is a lot of information in there. Like most complex objects in R, out is organized as a list of components or elements. Several of these elements are themselves lists. Figure 6.3 gives you a schematic view of the contents of a linear model object. In this list of items, elements are single values, some are data frames, and some are additional lists of simpler items. Again, remember our earlier discussion where we said objects could be thought of as being organized like a filing system: cabinets contain drawers, and drawer may contain which may contain pages of information, whole documents, or groups of folders with more documents inside. As an alternative analogy, and sticking with the image of a list, you can think of a master to-do list for a project, where the top-level headings lead to contain additional lists of tasks of different kinds. The out object created by lm contains several different named elements. Some, like the residual degrees of freedom in the model, are just a single number. Try out$df.residual at the console. Others are much larger entities, such as the data frame used to fit the model, which is retained by default. Try out$model, but be prepared for a lot of stuff to be printed at the console. Other elements have been computed by R and then stored, such as the coefficients of the model and other quantities. You can try out$coefficients, out$residuals, and out$fitted.values, for instance. Others are lists themselves (like qr). So you can see that the summary() function is selecting and printing only a small amount of core information, in comparison to what is stored in the model object. Just like the tables of data we saw earlier in Section A.1.3, the output of summary() is presented in a way that is compact and efficient in terms of getting information across, but also untidy when considered from the point of view of further manipulation. There is a table of coefficients, but the variable names are in the rows. The column names are awkward, and some information (e.g. at the bottom of the output) has been calculated and printed out, but is not stored in the model object. 34.4 Get model-based graphics right Figures based on statistical models face all the ordinary challenges of effective data visualization, and then some. This is because model results usually carry a considerable extra burden of interpretation and necessary background knowledge. The more complex the model, the trickier it becomes to convey this information effectively, and the easier it becomes to lead one’s audience or oneself into error. Within the social sciences, our ability to clearly and honestly present model-based graphics has greatly improved over the past ten or fifteen years. Over the same period, it has become clearer that some kinds of models are quite tricky to understand, even ones that had previously been seen as straightforward elements of the modeling toolkit (Ai &amp; Norton, 2003; Brambor, Clark, &amp; Golder, 2006). Plotting model estimates is closely connected to properly estimating models in the first place. This means there is no substitute for learning the statistics. You should not use graphical methods as a substitute for understanding the model used to produce them. While this book cannot teach you that material, we can make a few general points about what good model-based graphics look like, and work through some examples of how ggplot and some additional libraries can make it easier to get good results. 34.4.1 Present your findings in substantive terms Useful model-based plots show results in ways that are substantively meaningful and directly interpretable with respect to the questions the analysis is trying to answer. This means showing results in a context where other variables in the analysis are held at sensible values, such as their means or medians. With continuous variables, it can often be useful to generate predicted values that cover some substantively meaningful move across the distribution, such as from the 25th to the 75th percentile, rather than a single-unit increment in the variable of interest. For unordered categorical variables, predicted values might be presented with respect to the modal category in the data, or for a particular category of theoretical interest. Presenting substantively interpretable findings often also means using (and sometimes converting to) a scale that readers can easily understand. If your model reports results in log-odds, for example, converting the estimates to predicted probabilities will make it easier to interpret. All of this advice is quite general. Each of these points applies equally well to the presentation of summary results in a table rather than a graph. There is nothing distinctively graphical about putting the focus on the substantive meaning of your findings. 34.4.2 Show your degree of confidence Much the same applies to presenting the degree of uncertainty or confidence you have in your results. Model estimates come with various measures of precision, confidence, credence, or significance. Presenting and interpreting these measures is notoriously prone to misinterpretation, or over-interpretation, as researchers and audiences both demand more from things like confidence intervals and p-values than these statistics can deliver. At a minimum, having decided on an appropriate measure of model fit or the right assessment of confidence, you should show their range when you present your results. A family of related ggplot geoms allow you to show a range or interval defined by position on the x-axis and then a ymin and ymax range on the y-axis. These geoms include geom_pointrange() and geom_errorbar(), which we will see in action shortly. A related geom, geom_ribbon() uses the same arguments to draw filled areas, and is useful for plotting ranges of y-axis values along some continuously varying x-axis. 34.4.3 Show your data when you can Plotting the results from a multivariate model generally means one of two things. First, we can show what is in effect a table of coefficients with associated measures of confidence, perhaps organizing the coefficients into meaningful groups, or by the size of the predicted association, or both. Second, we can show the predicted values of some variables (rather than just a model’s coefficients) across some range of interest. The latter approach lets us show the original data points if we wish. The way ggplot builds graphics layer by layer allows us to easily combine model estimates (e.g. a regression line and an associated range) and the underlying data. In effect these are manually-constructed versions of the automatically-generated plots that we have been producing with geom_smooth() since the beginning of this book. 34.5 Generate predictions to graph Having fitted a model, then, we might want to get a picture of the estimates it produces over the range of some particular variable, holding other covariates constant at some sensible values. The predict() function is a generic way of using model objects to produce this kind of prediction. In R, “generic” functions take their inputs and pass them along to more specific functions behind the scenes, ones that are suited to working with the particular kind of model object we have. The details of getting predicted values from a OLS model, for instance, will be somewhat different from getting predictions out of a logistic regression. But in each case we can use the same predict() function, taking care to check the documentation to see what form the results are returned in for the kind of model we are working with. Many of the most commonly-used functions in R are generic in this way. The summary() function, for example, works on objects of many different classes, from vectors to data frames and statistical models, producing appropriate output in each case by way of a class-specific function in the background. For predict() to calculate the new values for us, it needs some new data to fit the model to. We will generate a new data frame whose columns have the same names as the variables in the model’s original data, but where the rows have new values. A very useful function called expand.grid() will help us do this. We will give it a list of variables, specifying the range of values we want each variable to take. Then expand.grid() will generate the will multiply out the full range of values for all combinations of the values we give it, thus creating a new data frame with the new data we need. In the following bit of code, we use min() and max() to get the minimum and maximum values for per capita GDP, and then create a vector with one hundred evenly-spaced elements between the minimum and the maximum. We hold population constant at its median, and we let continent take all of its five available values. min_gdp &lt;- min(gapminder$gdpPercap) max_gdp &lt;- max(gapminder$gdpPercap) med_pop &lt;- median(gapminder$pop) pred_df &lt;- expand.grid(gdpPercap = (seq(from = min_gdp, to = max_gdp, length.out = 100)), pop = med_pop, continent = c(&quot;Africa&quot;, &quot;Americas&quot;, &quot;Asia&quot;, &quot;Europe&quot;, &quot;Oceania&quot;)) dim(pred_df) #&gt; [1] 500 3 head(pred_df) #&gt; gdpPercap pop continent #&gt; 1 241 7023596 Africa #&gt; 2 1385 7023596 Africa #&gt; 3 2530 7023596 Africa #&gt; 4 3674 7023596 Africa #&gt; 5 4818 7023596 Africa #&gt; 6 5962 7023596 Africa Now we can use predict(). If we give the function our new data and model, without any further argument, it will calculate the fitted values for every row in the data frame. If we specify interval = 'predict' as an argument, it will calculate 95% prediction intervals in addition to the point estimate. pred_out &lt;- predict(object = out, newdata = pred_df, interval = &quot;predict&quot;) head(pred_out) #&gt; fit lwr upr #&gt; 1 48.0 31.5 64.4 #&gt; 2 48.5 32.1 64.9 #&gt; 3 49.0 32.6 65.4 #&gt; 4 49.5 33.1 65.9 #&gt; 5 50.0 33.6 66.4 #&gt; 6 50.5 34.1 67.0 Because we know that, by construction, the cases in pred_df and pred_out correspond row for row, we can bind the two data frames together by column. This method of joining or merging tables is definitely not recommended when you are dealing with data. pred_df &lt;- cbind(pred_df, pred_out) head(pred_df) #&gt; gdpPercap pop continent fit lwr upr #&gt; 1 241 7023596 Africa 48.0 31.5 64.4 #&gt; 2 1385 7023596 Africa 48.5 32.1 64.9 #&gt; 3 2530 7023596 Africa 49.0 32.6 65.4 #&gt; 4 3674 7023596 Africa 49.5 33.1 65.9 #&gt; 5 4818 7023596 Africa 50.0 33.6 66.4 #&gt; 6 5962 7023596 Africa 50.5 34.1 67.0 The end result is a tidy data frame, containing the predicted values from the model for the range of values we specified. Now we can plot the results. Because we produced a full range of predicted values, we can decide whether or not to use all of them. Here we further subset the predictions to just those for Europe and Africa. p &lt;- ggplot(data = subset(pred_df, continent %in% c(&quot;Europe&quot;, &quot;Africa&quot;)), aes(x = gdpPercap, y = fit, ymin = lwr, ymax = upr, color = continent, fill = continent, group = continent)) p + geom_point(data = subset(gapminder, continent %in% c(&quot;Europe&quot;, &quot;Africa&quot;)), aes(x = gdpPercap, y = lifeExp, color = continent), alpha = 0.5, inherit.aes = FALSE) + geom_line() + geom_ribbon(alpha = 0.2, color = FALSE) + scale_x_log10(labels = scales::dollar) We use a new geom here to draw the area covered by the prediction intervals: geom_ribbon(). It takes an x argument like a line, but a ymin and ymax argument as specified in the ggplot() aesthetic mapping. This defines the lower and upper limits of the prediction interval. In practice, you may not use predict() directly all that often. Instead, you might write code using additional libraries that encapsulate the process of producing predictions and plots from models. These are especially useful when your model is a little more complex and the interpretation of coefficients becomes trickier. This happens, for instance, when you have a binary outcome variable and need to convert the results of a logistic regression into predicted probabilities, or when you have interaction terms amongst your predictions. We will discuss some of these helper libraries in the next few sections. However, bear in mind that predict() and its ability to work safely with different classes of model underpins many of those libraries. So it’s useful to see it in action first hand in order to understand what it is doing. 34.6 Tidy model objects with broom The predict method is very useful, but there are a lot of other things we might want to do with our model output. We will use David Robinson’s broom package to help us out. It is a library of functions that help us get from the model results that R generates to numbers that we can plot. It will take model objects and turn pieces of them into data frames that you can use easily with ggplot. library(broom) Broom takes ggplot’s approach to tidy data and extends it to the model objects that R produces. Its methods can tidily extract three kinds of information. First, we can see component-level information about aspects of the model itself, such as coefficients and t-statistics. Second, we can obtain observation-level information about the model’s connection to the underlying data. This includes the fitted values and residuals for each observation in the data. And finally we can get model-level information that summarizes the fit as a whole, such as an F-statistic, the model deviance, or the r-squared. There is a broom function for each of these tasks. 34.6.1 Get component-level statistics with tidy() The tidy() function takes a model object and returns a data frame of component-level information. We can work with this to make plots in a familiar way, and much more easily than fishing inside the model object to extract the various terms. Here is an example, using the default results as just returned. For a more convenient display of the results, we will pipe the object we create with tidy() through a function that rounds the numeric columns of the data frame to two decimal places. This doesn’t change anything about the object itself, of course. out_comp &lt;- tidy(out) out_comp %&gt;% round_df() #&gt; # A tibble: 7 x 5 #&gt; term estimate std.error statistic p.value #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 (Intercept) 47.8 0.34 141. 0 #&gt; 2 gdpPercap 0 0 19.2 0 #&gt; 3 pop 0 0 3.33 0 #&gt; 4 continentAmericas 13.5 0.6 22.5 0 #&gt; 5 continentAsia 8.19 0.570 14.3 0 #&gt; 6 continentEurope 17.5 0.62 28.0 0 #&gt; # … with 1 more row We are now able to treat this dataframe just like all the other data that we have seen so far. p &lt;- ggplot(out_comp, mapping = aes(x = term, y = estimate)) p + geom_point() + coord_flip() We can extend and clean up this plot in a variety of ways. For example, we can tell tidy() to calculate confidence intervals for the estimates, using R’s confint() function. out_conf &lt;- tidy(out, conf.int = TRUE) out_conf %&gt;% round_df() #&gt; # A tibble: 7 x 7 #&gt; term estimate std.error statistic p.value conf.low conf.high #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 (Intercept) 47.8 0.34 141. 0 47.2 48.5 #&gt; 2 gdpPercap 0 0 19.2 0 0 0 #&gt; 3 pop 0 0 3.33 0 0 0 #&gt; 4 continentAmericas 13.5 0.6 22.5 0 12.3 14.6 #&gt; 5 continentAsia 8.19 0.570 14.3 0 7.07 9.31 #&gt; 6 continentEurope 17.5 0.62 28.0 0 16.2 18.7 #&gt; # … with 1 more row The convenience “not in” operator %nin% is available via the socviz library. It does the opposite of %in% and selects only the items in a first vector of characters that are not in the second. We’ll use it to drop the intercept term from the table. We also want to something about the labels. When fitting a model with categorical variables, R will create coefficient names based on the variable name and the category name, like continentAmericas. Normally we like to clean these up before plotting. Most commonly, we just want to strip away the variable name at the beginning of the coefficient label. For this we can use prefix_strip(), a convenience function in the socviz library. We tell it which prefixes to drop, using it to create a new column variable in out_conf that corresponds to the terms column, but that has nicer labels. out_conf &lt;- subset(out_conf, term %nin% &quot;(Intercept)&quot;) out_conf$nicelabs &lt;- prefix_strip(out_conf$term, &quot;continent&quot;) Now we can use geom_pointrange()to make a figure that displays some information about our confidence in the variable estimates, as opposed to just the coefficients. As with the boxplots earlier, we use reorder() to sort the names of the model’s terms by the estimate variable, thus arranging our plot of effects from largest to smallest in magnitude. p &lt;- ggplot(out_conf, mapping = aes(x = reorder(nicelabs, estimate), y = estimate, ymin = conf.low, ymax = conf.high)) p + geom_pointrange() + coord_flip() + labs(x=&quot;&quot;, y=&quot;OLS Estimate&quot;) Dotplots of this kind can be very compact. The vertical axis can often be compressed quite a bit, with no loss in comprehension. In fact, they are often easier to read with much less room between the rows than given by a default square shape. 34.6.2 Get observation-level statistics with augment() The values returned by augment() are all statistics calculated at the level of the original observations. As such, they can be added on to the data frame that the model is based on. Working from a call to augment() will return a data frame with all the original observations used in the estimation of the model, together with columns like the following: .fitted — The fitted values of the model. .se.fit — The standard errors of the fitted values. .resid — The residuals. .hat — The diagonal of the hat matrix. .sigma — An estimate of residual standard deviation when the corresponding observation is dropped from the model. .cooksd — Cook’s distance, a common regression diagnostic; and .std.resid — The standardized residuals. Each of these variables is named with a leading dot, for example .hat rather than hat, and so on. This is to guard against accidentally confusing it with (or accidentally overwriting) an existing variable in your data with this name. The columns of values return will differ slightly depending on the class of model being fitted. out_aug &lt;- augment(out) head(out_aug) %&gt;% round_df() #&gt; # A tibble: 6 x 11 #&gt; lifeExp gdpPercap pop continent .fitted .se.fit .resid .hat .sigma #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 28.8 779. 8.43e6 Asia 56.4 0.47 -27.6 0 8.34 #&gt; 2 30.3 821. 9.24e6 Asia 56.4 0.47 -26.1 0 8.34 #&gt; 3 32 853. 1.03e7 Asia 56.5 0.47 -24.5 0 8.35 #&gt; 4 34.0 836. 1.15e7 Asia 56.5 0.47 -22.4 0 8.35 #&gt; 5 36.1 740. 1.31e7 Asia 56.4 0.47 -20.3 0 8.35 #&gt; 6 38.4 786. 1.49e7 Asia 56.5 0.47 -18.0 0 8.36 #&gt; # … with 2 more variables: .cooksd &lt;dbl&gt;, .std.resid &lt;dbl&gt; By default, augment() will extract the available data from the model object. This will usually include the variables used in the model itself, but not any additional ones contained in the original data frame. Sometimes it is useful to have these. We can add them by specifying the data argument: out_aug &lt;- augment(out, data = gapminder) head(out_aug) %&gt;% round_df() #&gt; # A tibble: 6 x 13 #&gt; country continent year lifeExp pop gdpPercap .fitted .se.fit .resid #&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Afghan… Asia 1952 28.8 8.43e6 779. 56.4 0.47 -27.6 #&gt; 2 Afghan… Asia 1957 30.3 9.24e6 821. 56.4 0.47 -26.1 #&gt; 3 Afghan… Asia 1962 32 1.03e7 853. 56.5 0.47 -24.5 #&gt; 4 Afghan… Asia 1967 34.0 1.15e7 836. 56.5 0.47 -22.4 #&gt; 5 Afghan… Asia 1972 36.1 1.31e7 740. 56.4 0.47 -20.3 #&gt; 6 Afghan… Asia 1977 38.4 1.49e7 786. 56.5 0.47 -18.0 #&gt; # … with 4 more variables: .hat &lt;dbl&gt;, .sigma &lt;dbl&gt;, .cooksd &lt;dbl&gt;, #&gt; # .std.resid &lt;dbl&gt; If some rows containing missing data were dropped to fit the model, then these will not be carried over to the augmented dataframe. The new columns created by augment() can be used to create some standard regression plots. For example, we can plot the residuals versus the fitted values. Figure 6.7 suggests, unsurprisingly, that our country-year data has rather more structure than is captured by our OLS model. p &lt;- ggplot(data = out_aug, mapping = aes(x = .fitted, y = .resid)) p + geom_point() 34.6.3 Get model-level statistics with glance() This function organizes the information typically presented at the bottom of a model’s summary() output. By itself, it usually just returns a table with a single row in it. But as we shall see in a moment, the real power of broom’s approach is the way that it can scale up to cases where we are grouping or subsampling our data. glance(out) %&gt;% round_df() #&gt; # A tibble: 1 x 11 #&gt; r.squared adj.r.squared sigma statistic p.value df logLik AIC #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.580 0.580 8.37 394. 0 7 -6034. 12084. #&gt; # … with 3 more variables: BIC &lt;dbl&gt;, deviance &lt;dbl&gt;, df.residual &lt;dbl&gt; Broom is able to tidy (and augment, and glance at) a wide range of model types. Not all functions are available for all classes of model. Consult broom’s documentation for more details on what is available. For example, here is a plot created from the tidied output of an event-history analysis. First we generate a Cox proportional hazards model of some survival data. library(survival) #&gt; #&gt; Attaching package: &#39;survival&#39; #&gt; The following object is masked from &#39;package:quantreg&#39;: #&gt; #&gt; untangle.specials out_cph &lt;- coxph(Surv(time, status) ~ age + sex, data = lung) out_surv &lt;- survfit(out_cph) The details of the fit are not important here, but in the first step the Surv() function creates the response or outcome variable for the proportional hazards model that is then fitted by the coxph()function. Then the survfit() function creates the survival curve from the model, much like we used predict() to generate predicted values earlier. Try summary(out_cph) to see the model, and summary(out_surv) to see the table of predicted values that will form the basis for our plot. Next we tidy out_surv to get a data frame, and plot it. # Figure 6.8: A Kaplan-Meier plot. out_tidy &lt;- tidy(out_surv) p &lt;- ggplot(data = out_tidy, mapping = aes(time, estimate)) p + geom_line() + geom_ribbon(mapping = aes(ymin = conf.low, ymax = conf.high), alpha = .2) 34.7 Grouped analysis and list-columns Broom makes it possible to quickly fit models to different subsets of your data and get consistent and usable tables of results out the other end. For example, let’s say we wanted to look at the gapminder data by examining the relationship between life expectancy and GDP by continent, for each year in the data. The gapminder data is at bottom organized by country-years. That is the unit of observation in the rows. If we wanted, we could take a slice of the data manually, such as “all countries observed in Asia, in 1962” or “all in Africa, 2002”. Here is “Europe, 1977”: eu77 &lt;- gapminder %&gt;% filter(continent == &quot;Europe&quot;, year == 1977) We could then see what the relationship between life expectancy and GDP looked like for that continent-year group: fit &lt;- lm(lifeExp ~ log(gdpPercap), data = eu77) summary(fit) #&gt; #&gt; Call: #&gt; lm(formula = lifeExp ~ log(gdpPercap), data = eu77) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -7.496 -1.031 0.093 1.176 3.712 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 29.489 7.161 4.12 0.00031 *** #&gt; log(gdpPercap) 4.488 0.756 5.94 2.2e-06 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 2.11 on 28 degrees of freedom #&gt; Multiple R-squared: 0.557, Adjusted R-squared: 0.541 #&gt; F-statistic: 35.2 on 1 and 28 DF, p-value: 2.17e-06 With dplyr and broom we can do this for every continent-year slice of the data in a compact and tidy way. We start with our table of data, and then (%&gt;%) group the countries by continent and year using the group_by() function. We introduced this grouping operation in Chapter 4. Our data is reorganized first by continent, and within continent by year. Here we will take one further step and nest the data that make up each group: out_le &lt;- gapminder %&gt;% group_by(continent, year) %&gt;% nest() out_le #&gt; # A tibble: 60 x 3 #&gt; continent year data #&gt; &lt;fct&gt; &lt;int&gt; &lt;list&gt; #&gt; 1 Asia 1952 &lt;tibble [33 × 4]&gt; #&gt; 2 Asia 1957 &lt;tibble [33 × 4]&gt; #&gt; 3 Asia 1962 &lt;tibble [33 × 4]&gt; #&gt; 4 Asia 1967 &lt;tibble [33 × 4]&gt; #&gt; 5 Asia 1972 &lt;tibble [33 × 4]&gt; #&gt; 6 Asia 1977 &lt;tibble [33 × 4]&gt; #&gt; # … with 54 more rows Think of what nest() does as a more intensive version what group_by() does. The resulting object is has the tabular form we expect (it is a tibble) but it looks a little unusual. The first two columns are the familiar continent and year. But we now also have a new column, data, that contains a small table of data corresponding to each continent-year group. This is a list-column, something we have not seen before. It turns out to be very useful for bundling together complex objects (structured, in this case, as a list of tibbles, each being a 33x4 table of data) within the rows of our data (which remains tabular). Our “Europe 1977” fit is in there. We can look at it, if we like, by filtering the data and then unnesting the list column. out_le %&gt;% filter(continent == &quot;Europe&quot; &amp; year == 1977) %&gt;% unnest() #&gt; # A tibble: 30 x 6 #&gt; continent year country lifeExp pop gdpPercap #&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 Europe 1977 Albania 68.9 2509048 3533. #&gt; 2 Europe 1977 Austria 72.2 7568430 19749. #&gt; 3 Europe 1977 Belgium 72.8 9821800 19118. #&gt; 4 Europe 1977 Bosnia and Herzegovina 69.9 4086000 3528. #&gt; 5 Europe 1977 Bulgaria 70.8 8797022 7612. #&gt; 6 Europe 1977 Croatia 70.6 4318673 11305. #&gt; # … with 24 more rows List-columns are useful because we can act on them in a compact and tidy way. In particular, we can pass functions along to each row of the list-column and make something happen. For example, a moment ago we ran a regression of life expectancy and logged GDP for European countries in 1977. We can do that for every continent-year combination in the data. We first create a convenience function called fit_ols() that takes a single argument, df (for data frame) and that fits the linear model we are interested in. Then we map that function to each of our list-column rows in turn. Recall from Chapter 4 that mutate creates new variables or columns on the fly within a pipeline. The map action is an important idea in functional programming. If you have written code in other, more imperative languages you can think of it as a compact alternative to writing for … next loops. You can of course write loops like this in R. Computationally they are often not any less efficient than their functional alternatives. But mapping functions to arrays is more easily integrated into a sequence of data transformations. fit_ols &lt;- function(df) { lm(lifeExp ~ log(gdpPercap), data = df) } out_le &lt;- gapminder %&gt;% group_by(continent, year) %&gt;% nest() %&gt;% mutate(model = map(data, fit_ols)) out_le #&gt; # A tibble: 60 x 4 #&gt; continent year data model #&gt; &lt;fct&gt; &lt;int&gt; &lt;list&gt; &lt;list&gt; #&gt; 1 Asia 1952 &lt;tibble [33 × 4]&gt; &lt;lm&gt; #&gt; 2 Asia 1957 &lt;tibble [33 × 4]&gt; &lt;lm&gt; #&gt; 3 Asia 1962 &lt;tibble [33 × 4]&gt; &lt;lm&gt; #&gt; 4 Asia 1967 &lt;tibble [33 × 4]&gt; &lt;lm&gt; #&gt; 5 Asia 1972 &lt;tibble [33 × 4]&gt; &lt;lm&gt; #&gt; 6 Asia 1977 &lt;tibble [33 × 4]&gt; &lt;lm&gt; #&gt; # … with 54 more rows Before starting the pipeline we create a new function: It is a convenience function whose only job is to estimate a particular OLS model on some data. Like almost everything in R, functions are a kind of object. To make a new one, we use the slightly special function() function. (Nerds love that sort of thing.) There is a little more detail on creating functions in the Appendix. To see what fit_ols() looks like once it is created, type fit_ols without parentheses at the Console. To see what it does, try fit_ols(df = gapminder), or summary(fit_ols(gapminder)). Now we have two list-columns: data, and model. The latter was created by mapping the fit_ols() function to each row of data. Inside each element of model is a linear model for that continent-year. So we now have sixty OLS fits, one for every continent-year grouping. Having the models inside the list column is not much use to us in and of itself. But we can extract the information we want while keeping things in a tidy tabular form. For clarity we will run the pipeline from the beginning again, this time adding a few new steps. First we extract summary statistics from each model by mapping the tidy() function from broom to the model list column. Then we unnest the result, dropping the other columns in the process. Finally, we filter out all the Intercept terms, and also drop all observations from Oceania. In the case of the Intercepts we do this just out of convenience. Oceania we drop just because there are so few observations. We put the results in an object called out_tidy. fit_ols &lt;- function(df) { lm(lifeExp ~ log(gdpPercap), data = df) } out_tidy &lt;- gapminder %&gt;% group_by(continent, year) %&gt;% nest() %&gt;% mutate(model = map(data, fit_ols), tidied = map(model, tidy)) %&gt;% unnest(tidied, .drop = TRUE) %&gt;% filter(term %nin% &quot;(Intercept)&quot; &amp; continent %nin% &quot;Oceania&quot;) out_tidy %&gt;% sample_n(5) #&gt; # A tibble: 5 x 7 #&gt; continent year term estimate std.error statistic p.value #&gt; &lt;fct&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Americas 1992 log(gdpPercap) 6.06 0.895 6.77 0.000000664 #&gt; 2 Europe 2002 log(gdpPercap) 3.74 0.445 8.40 0.00000000391 #&gt; 3 Asia 2007 log(gdpPercap) 5.16 0.694 7.43 0.0000000226 #&gt; 4 Americas 1952 log(gdpPercap) 10.4 2.72 3.84 0.000827 #&gt; 5 Americas 1957 log(gdpPercap) 10.3 2.40 4.31 0.000261 We now have tidy regression output with an estimate of the association between log GDP per capita and life expectancy for each year, within continents. We can plot these estimates in a way that takes advantage of their groupiness. # Figure 6.9: Yearly estimates of the association between GDP and Life Expectancy, pooled by continent. p &lt;- ggplot(data = out_tidy, mapping = aes(x = year, y = estimate, ymin = estimate - 2*std.error, ymax = estimate + 2*std.error, group = continent, color = continent)) p + geom_pointrange(position = position_dodge(width = 1)) + scale_x_continuous(breaks = unique(gapminder$year)) + theme(legend.position = &quot;top&quot;) + labs(x = &quot;Year&quot;, y = &quot;Estimate&quot;, color = &quot;Continent&quot;) The call to position_dodge() within geom_pointrange() allows the point ranges for each continent to be near each other within years, instead of being plotted right on top of one another. We could have faceted the results by continent, but doing it this way lets us see differences in the yearly estimates much more easily. This technique is very useful not just for cases like this, but also when you want to compare the coefficients given by different kinds of statistical model. This sometimes happens when we’re interested in seeing how, say, OLS performs against some other model specification. 34.8 Plot marginal effects Our earlier discussion of predict() was about obtaining estimates of the average effect of some coefficient, net of the other terms in the model. Over the past decade, estimating and plotting partial or marginal effects from a model has become an increasingly common way of presenting accurate and interpretively useful predictions. Interest in marginal effects plots was stimulated by the realization that the interpretation of terms in logistic regression models, in particular, was trickier than it seemed—especially when there were interaction terms in the model (Ai &amp; Norton, 2003). Thomas Leeper’s margins package can make these plots for us. library(margins) To see it in action, we’ll take another look at the General Social Survey data in gss_sm, this time focusing on the binary variable, obama.As is common with retrospective questions on elections, rather more people claim to have voted for Obama than is consistent with the vote share he received in the election. It is coded 1 if the respondent said they voted for Barack Obama in the 2012 presidential election, and 0 otherwise. In this case, mostly for convenience here, the zero code includes all other answers to the question, including those who said they voted for Mitt Romney, those who said they did not vote, those who refused to answer, and those who said they didn’t know who they voted for. We will fit a logistic regression on obama, with age, polviews, race, and sex as the predictors. The age variable is the respondent’s age in years. The sex variable is coded as “Male” or “Female” with “Male” as the reference category. The race variable is coded as “White”, “Black”, or “Other” with “White” as the reference category. The polviews measure is a self-reported scale of the respondent’s political orientation from “Extremely Conservative” through “Extremely Liberal”, with “Moderate” in the middle. We take polviews and create a new variable, polviews_m, using the relevel() function to recode “Moderate” to be the reference category. We fit the model with the glm() function, and specify an interaction between race and sex. gss_sm$polviews_m &lt;- relevel(gss_sm$polviews, ref = &quot;Moderate&quot;) out_bo &lt;- glm(obama ~ polviews_m + sex*race, family = &quot;binomial&quot;, data = gss_sm) summary(out_bo) #&gt; #&gt; Call: #&gt; glm(formula = obama ~ polviews_m + sex * race, family = &quot;binomial&quot;, #&gt; data = gss_sm) #&gt; #&gt; Deviance Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -2.905 -0.554 0.177 0.542 2.244 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) 0.29649 0.13409 2.21 0.0270 * #&gt; polviews_mExtremely Liberal 2.37295 0.52504 4.52 6.2e-06 *** #&gt; polviews_mLiberal 2.60003 0.35667 7.29 3.1e-13 *** #&gt; polviews_mSlightly Liberal 1.29317 0.24843 5.21 1.9e-07 *** #&gt; polviews_mSlightly Conservative -1.35528 0.18129 -7.48 7.7e-14 *** #&gt; polviews_mConservative -2.34746 0.20038 -11.71 &lt; 2e-16 *** #&gt; polviews_mExtremely Conservative -2.72738 0.38721 -7.04 1.9e-12 *** #&gt; sexFemale 0.25487 0.14537 1.75 0.0796 . #&gt; raceBlack 3.84953 0.50132 7.68 1.6e-14 *** #&gt; raceOther -0.00214 0.43576 0.00 0.9961 #&gt; sexFemale:raceBlack -0.19751 0.66007 -0.30 0.7648 #&gt; sexFemale:raceOther 1.57483 0.58766 2.68 0.0074 ** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for binomial family taken to be 1) #&gt; #&gt; Null deviance: 2247.9 on 1697 degrees of freedom #&gt; Residual deviance: 1345.9 on 1686 degrees of freedom #&gt; (1169 observations deleted due to missingness) #&gt; AIC: 1370 #&gt; #&gt; Number of Fisher Scoring iterations: 6 The summary reports the coefficients and other information. We can now graph the data in any one of several ways. Using margins() we calculate the marginal effects for each variable: bo_m &lt;- margins(out_bo) summary(bo_m) #&gt; factor AME SE z p lower #&gt; polviews_mConservative -0.4119 0.0283 -14.5394 0.0000 -0.4674 #&gt; polviews_mExtremely Conservative -0.4538 0.0420 -10.7971 0.0000 -0.5361 #&gt; polviews_mExtremely Liberal 0.2681 0.0295 9.0996 0.0000 0.2103 #&gt; polviews_mLiberal 0.2768 0.0229 12.0736 0.0000 0.2319 #&gt; polviews_mSlightly Conservative -0.2658 0.0330 -8.0596 0.0000 -0.3304 #&gt; polviews_mSlightly Liberal 0.1933 0.0303 6.3896 0.0000 0.1340 #&gt; raceBlack 0.4032 0.0173 23.3568 0.0000 0.3694 #&gt; raceOther 0.1247 0.0386 3.2297 0.0012 0.0490 #&gt; sexFemale 0.0443 0.0177 2.5073 0.0122 0.0097 #&gt; upper #&gt; -0.3564 #&gt; -0.3714 #&gt; 0.3258 #&gt; 0.3218 #&gt; -0.2011 #&gt; 0.2526 #&gt; 0.4371 #&gt; 0.2005 #&gt; 0.0789 The margins library comes with several plot methods of its own. If you wish, at this point you can just try plot(bo_m) to see a plot of the average marginal effects, produced with the general look of a Stata graphic. Other plot methods in the margins library include cplot(), which visualizes marginal effects conditional on a second variable, and image(), which shows predictions or marginal effects as a filled heatmap or contour plot. Alternatively, we can take results from margins() and plot them ourselves. To clean up the summary a little a little, we convert it to a tibble, then use prefix_strip() and prefix_replace() to tidy the labels. We want to strip the polviews_m and sex prefixes, and (to avoid ambiguity about “Other”), adjust the race prefix. bo_gg &lt;- as_tibble(summary(bo_m)) prefixes &lt;- c(&quot;polviews_m&quot;, &quot;sex&quot;) bo_gg$factor &lt;- prefix_strip(bo_gg$factor, prefixes) bo_gg$factor &lt;- prefix_replace(bo_gg$factor, &quot;race&quot;, &quot;Race: &quot;) bo_gg %&gt;% select(factor, AME, lower, upper) #&gt; # A tibble: 9 x 4 #&gt; factor AME lower upper #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Conservative -0.412 -0.467 -0.356 #&gt; 2 Extremely Conservative -0.454 -0.536 -0.371 #&gt; 3 Extremely Liberal 0.268 0.210 0.326 #&gt; 4 Liberal 0.277 0.232 0.322 #&gt; 5 Slightly Conservative -0.266 -0.330 -0.201 #&gt; 6 Slightly Liberal 0.193 0.134 0.253 #&gt; # … with 3 more rows Now we have a table that we can plot as we have learned: p &lt;- ggplot(data = bo_gg, aes(x = reorder(factor, AME), y = AME, ymin = lower, ymax = upper)) p + geom_hline(yintercept = 0, color = &quot;gray80&quot;) + geom_pointrange() + coord_flip() + labs(x = NULL, y = &quot;Average Marginal Effect&quot;) If we are just interested in getting conditional effects for a particular variable, then conveniently we can ask the plot methods in the margins library to do the work calculating effects for us but without drawing their plot. Instead, they can return the results in a format we can easily use in ggplot, and with less need for clean up, for the clean-up. For example, with cplot(): pv_cp &lt;- cplot(out_bo, x = &quot;sex&quot;, draw = FALSE) #&gt; xvals yvals upper lower #&gt; 1 Male 0.574 0.638 0.509 #&gt; 2 Female 0.634 0.689 0.580 p &lt;- ggplot(data = pv_cp, aes(x = reorder(xvals, yvals), y = yvals, ymin = lower, ymax = upper)) p + geom_hline(yintercept = 0, color = &quot;gray80&quot;) + geom_pointrange() + coord_flip() + labs(x = NULL, y = &quot;Conditional Effect&quot;) The margins package is under active development. It can do much more than described here. The vignettes that come with the package provide more extensive discussion and numerous examples. 34.9 Plots from complex surveys Social scientists often work with data collected using a complex survey design. Survey instruments may be stratified by region or some other characteristic, contain replicate weights to make them comparable to a reference population, have a clustered structure, and so on. In Chapter 4 we learned how calculate and then plot frequency tables of categorical variables, using some data from the General Social Survey (GSS). However, if we want accurate estimates of US households from the GSS, we will need to take the survey’s design into account, and use the survey weights provided in the dataset. Thomas Lumley’s survey library provides a comprehensive set of tools for addressing these issues. The tools and the theory behind them are discussed in detail in Lumley (2010), and an overview of the package is provided in Lumley (2004). While the functions in the survey package are straightforward to use and return results in a generally tidy form, the package predates the tidyverse and its conventions by several years. This means we cannot use survey functions directly with dplyr. However, Greg Freedman Ellis has written a helper package, srvyr, that solves this problem for us, and lets us use the survey library’s functions within a data analysis pipeline in a familiar way. For example, the gss_lon data contains a small subset of measures from every wave of the GSS since its inception in 1972. It also contains several variables that describe the design of the survey and provide replicate weights for observations in various years. These technical details are described in the GSS documentation. Similar information is typically provided by other complex surveys. Here we will use this design information to calculate weighted estimates of the distribution of educational attainment by race, for selected survey years from 1976 to 2016. To begin, we load the survey and srvyr libraries. library(survey) #&gt; Loading required package: grid #&gt; Loading required package: Matrix #&gt; #&gt; Attaching package: &#39;Matrix&#39; #&gt; The following object is masked from &#39;package:tidyr&#39;: #&gt; #&gt; expand #&gt; #&gt; Attaching package: &#39;survey&#39; #&gt; The following object is masked from &#39;package:graphics&#39;: #&gt; #&gt; dotchart library(srvyr) #&gt; #&gt; Attaching package: &#39;srvyr&#39; #&gt; The following object is masked from &#39;package:stats&#39;: #&gt; #&gt; filter Next, we take our gss_lon dataset and use the survey tools to create a new object that contains the data, as before, but with some additional information about the survey’s design: options(survey.lonely.psu = &quot;adjust&quot;) options(na.action=&quot;na.pass&quot;) gss_wt &lt;- subset(gss_lon, year &gt; 1974) %&gt;% mutate(stratvar = interaction(year, vstrat)) %&gt;% as_survey_design(ids = vpsu, strata = stratvar, weights = wtssall, nest = TRUE) The two options set at the beginning provide some information to the survey library about how to behave. You should consult Lumley (2010) and the survey package documentation for details. The subsequent operations create gss_wt, an object with one additional column (stratvar), describing the yearly sampling strata. We use the interaction() function to do this. It multiplies the vstrat variable by the year variable to get a vector of stratum information for each year. We have to do this because of the way the GSS codes its stratum information. In the next step, we use the as_survey_design() function to add the key pieces of information about the survey design. It adds information about the sampling identifiers (ids), the strata (strata), and the replicate weights (weights). With those in place we can take advantage of a large number of specialized functions in the survey library that allow us to calculate properly weighted survey means or estimate models with the correct sampling specification. For example, we can easily calculate the distribution of education by race for a series of years from 1976 to 2016. We use survey_mean() to do this: out_grp &lt;- gss_wt %&gt;% filter(year %in% seq(1976, 2016, by = 4)) %&gt;% group_by(year, race, degree) %&gt;% summarize(prop = survey_mean(na.rm = TRUE)) #&gt; Warning: Factor `degree` contains implicit NA, consider using #&gt; `forcats::fct_explicit_na` out_grp #&gt; # A tibble: 150 x 5 #&gt; year race degree prop prop_se #&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1976 White Lt High School 0.328 0.0160 #&gt; 2 1976 White High School 0.518 0.0162 #&gt; 3 1976 White Junior College 0.0129 0.00298 #&gt; 4 1976 White Bachelor 0.101 0.00960 #&gt; 5 1976 White Graduate 0.0393 0.00644 #&gt; 6 1976 Black Lt High School 0.562 0.0611 #&gt; # … with 144 more rows The results returned in out_grp include standard errors. We can also ask survey_mean() to calculate confidence intervals for us, if we wish. Grouping with group_by() lets us calculate counts or means for the innermost variable, grouped by the next variable “up” or “out”, in this case, degree by race, such that the proportions for degree will sum to one for each group in race, and this will be done separately for each value of year. If we want the marginal frequencies, such that the values for all combinations of race and degree sum to one within each year, we first have to interact the variables we are cross-classifying. Then we group by the new interacted variable and do the calculation as before: out_mrg &lt;- gss_wt %&gt;% filter(year %in% seq(1976, 2016, by = 4)) %&gt;% mutate(racedeg = interaction(race, degree)) %&gt;% group_by(year, racedeg) %&gt;% summarize(prop = survey_mean(na.rm = TRUE)) #&gt; Warning: Factor `racedeg` contains implicit NA, consider using #&gt; `forcats::fct_explicit_na` out_mrg #&gt; # A tibble: 150 x 4 #&gt; year racedeg prop prop_se #&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1976 White.Lt High School 0.298 0.0146 #&gt; 2 1976 Black.Lt High School 0.0471 0.00840 #&gt; 3 1976 Other.Lt High School 0.00195 0.00138 #&gt; 4 1976 White.High School 0.471 0.0160 #&gt; 5 1976 Black.High School 0.0283 0.00594 #&gt; 6 1976 Other.High School 0.00325 0.00166 #&gt; # … with 144 more rows This gives us the numbers that we want and returns them in a tidy data frame. The interaction() function produces variable labels that are a compound of the two variables we interacted, with each combination of categories separated by a period, (such as White.Graduate. However, perhaps we would like to see these categories as two separate columns, one for race and one for education, as before. Because the variable labels are organized in a predictable way, we can use one of the convenient functions in the tidyverse’s tidyr library to separate the single variable into two columns while correctly preserving the row values. Appropriately, this function is called separate(). out_mrg &lt;- gss_wt %&gt;% filter(year %in% seq(1976, 2016, by = 4)) %&gt;% mutate(racedeg = interaction(race, degree)) %&gt;% group_by(year, racedeg) %&gt;% summarize(prop = survey_mean(na.rm = TRUE)) %&gt;% separate(racedeg, sep = &quot;\\\\.&quot;, into = c(&quot;race&quot;, &quot;degree&quot;)) #&gt; Warning: Factor `racedeg` contains implicit NA, consider using #&gt; `forcats::fct_explicit_na` out_mrg #&gt; # A tibble: 150 x 5 #&gt; year race degree prop prop_se #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1976 White Lt High School 0.298 0.0146 #&gt; 2 1976 Black Lt High School 0.0471 0.00840 #&gt; 3 1976 Other Lt High School 0.00195 0.00138 #&gt; 4 1976 White High School 0.471 0.0160 #&gt; 5 1976 Black High School 0.0283 0.00594 #&gt; 6 1976 Other High School 0.00325 0.00166 #&gt; # … with 144 more rows The call to separate() says to take the racedeg column, split each value when it sees a period, and reorganize the results into two columns, race and degree. This gives us a tidy table much like out_grp, but for the marginal frequencies. Reasonable people can disagree over how best to plot a small multiple of a frequency table while faceting by year, especially when there is some measure of uncertainty attached. A barplot is the obvious approach for a single case, but when there are many years it can become difficult to compare bars across panels. This is especially the case when standard errors or confidence intervals are used in conjunction with bars.Sometimes it may be preferable to show that the underlying variable is categorical, as a bar chart makes clear, and not continuous, as a line graph suggests. Here the trade-off is in favor of the line graphs as the bars are very hard to compare across facets. This is sometimes called a “dynamite plot”, not because it looks amazing but because the t-shaped error bars on the tops of the columns make them look like cartoon dynamite plungers. An alternative is to use a line graph to join up the time observations, faceting on educational categories instead of year. Figure 6.12 shows the results for our GSS data in dynamite-plot form, where the error bars are defined as twice the standard error in either direction around the point estimate. p &lt;- ggplot(data = subset(out_grp, race %nin% &quot;Other&quot;), mapping = aes(x = degree, y = prop, ymin = prop - 2*prop_se, ymax = prop + 2*prop_se, fill = race, color = race, group = race)) dodge &lt;- position_dodge(width=0.9) p + geom_col(position = dodge, alpha = 0.2) + geom_errorbar(position = dodge, width = 0.2) + scale_x_discrete(labels = scales::wrap_format(10)) + scale_y_continuous(labels = scales::percent) + scale_color_brewer(type = &quot;qual&quot;, palette = &quot;Dark2&quot;) + scale_fill_brewer(type = &quot;qual&quot;, palette = &quot;Dark2&quot;) + labs(title = &quot;Educational Attainment by Race&quot;, subtitle = &quot;GSS 1976-2016&quot;, fill = &quot;Race&quot;, color = &quot;Race&quot;, x = NULL, y = &quot;Percent&quot;) + facet_wrap(~ year, ncol = 2) + theme(legend.position = &quot;top&quot;) This plot has a few cosmetic details and adjustments that we will learn more about in Chapter 8. As before, I encourage you to peel back the plot from the bottom, one instruction at a time, to see what changes. One useful adjustment to notice is the new call to the scales library to adjust the labels on the x-axis. The adjustment on the y-axis is familiar, scales::percent to convert the proportion to a percentage. On the x-axis, the issue is that several of the labels are rather long. If we do not adjust them they will print over one another. The scales::wrap_format() function will break long labels into lines. It takes a single numerical argument (here 10) that is the maximum length a string can be before it is wrapped onto a new line. Faceting by education instead.Figure 6.13: Faceting by education instead. A graph like this is true to the categorical nature of the data, while showing the breakdown of groups within each year. But you should experiment with some alternatives. For example, we might decide that it is better to facet by degree category instead, and put the year on the x-axis within each panel. If we do that, then we can use geom_line() to show a time trend, which is more natural, and geom_ribbon() to show the error range. This is perhaps a better way to show the data, especially as it brings out the time trends within each degree category, and allows us to see the similarities and differences by racial classification at the same time. p &lt;- ggplot(data = subset(out_grp, race %nin% &quot;Other&quot;), mapping = aes(x = year, y = prop, ymin = prop - 2*prop_se, ymax = prop + 2*prop_se, fill = race, color = race, group = race)) p + geom_ribbon(alpha = 0.3, aes(color = NULL)) + geom_line() + facet_wrap(~ degree, ncol = 1) + scale_y_continuous(labels = scales::percent) + scale_color_brewer(type = &quot;qual&quot;, palette = &quot;Dark2&quot;) + scale_fill_brewer(type = &quot;qual&quot;, palette = &quot;Dark2&quot;) + labs(title = &quot;Educational Attainment\\nby Race&quot;, subtitle = &quot;GSS 1976-2016&quot;, fill = &quot;Race&quot;, color = &quot;Race&quot;, x = NULL, y = &quot;Percent&quot;) + theme(legend.position = &quot;top&quot;) 34.10 Where to go next In general, when you estimate models and want to plot the results, the difficult step is not the plotting but rather calculating and extracting the right numbers. Generating predicted values and measures of confidence or uncertainty from models requires that you understand the model you are fitting, and the function you use to fit it, especially when it involves interactions, cross-level effects, or transformations of the predictor or response scales. The details can vary substantially from model type to model type, and also with the goals of any particular analysis. It is unwise to approach them mechanically. That said, several tools exist to help you work with model objects and produce a default set of plots from them. 34.10.1 Default plots for models Just as model objects in R usually have a default summary() method, printing out an overview tailored to the type of model it is, they will usually have a default plot() method, too. Figures produced by plot() are typically not generated via ggplot, but it is usually worth exploring them. They typically make use of either R’s base graphics or the lattice library (Sarkar, 2008). These are two plotting systems that we do not cover in this book. Default plot methods are easy to examine. Let’s take a look again at our simple OLS model. out &lt;- lm(formula = lifeExp ~ log(gdpPercap) + pop + continent, data = gapminder) To look at some of R’s default plots for this model, use the plot() function. # Plot not shown plot(out, which = c(1,2), ask=FALSE) The which() statement here selects the first two of four default plots for this kind of model. If you want to easily reproduce base R’s default model graphics using ggplot, the ggfortify library is worth examining. It is in some ways similar to broom, in that it tidies the output of model objects, but it focuses on producing a standard plot (or group of plots) for a wide variety of model types. It does this by defining a function called autoplot(). The idea is to be able to use autoplot() with the output of many different kinds of model. A second option worth looking at is the coefplot library. It provides a quick way to produce good-quality plots of point estimates and confidence intervals. It has the advantage of managing the estimation of interaction effects and other occasionally tricky calculations. library(coefplot) out &lt;- lm(formula = lifeExp ~ log(gdpPercap) + log(pop) + continent, data = gapminder) coefplot(out, sort = &quot;magnitude&quot;, intercept = FALSE) 34.10.2 Tools in development Tidyverse tools for modeling and model exploration are being actively developed. The broom and margins libraries continue to get more and more useful. There are also other projects worth paying attention to. The infer packageinfer.netlify.com is in its early stages but can already do useful things in a pipeline-friendly way. You can install it from CRAN with install.packages(&quot;infer&quot;). 34.10.3 Extensions to ggplot The GGally package provides a suite of functions designed to make producing standard but somewhat complex plots a little easier. For instance, it can produce generalized pairs plots, a useful way of quickly examining possible relationships between several different variables at once. This sort of plot is like the visual version of a correlation matrix. It shows a bivariate plot for all pairs of variables in the data. This is relatively straightforward when all the variables are continuous measures. Things get more complex when, as is often the case in the social sciences, some or all variables are categorical or otherwise limited in the range of values they can take. A generalized pairs plot can handle these cases. For example, Figure ?? shows a generalized pairs plot for five variables from the organdata dataset. library(GGally) organdata_sm &lt;- organdata %&gt;% select(donors, pop_dens, pubhealth, roads, consent_law) ggpairs(data = organdata_sm, mapping = aes(color = consent_law), upper = list(continuous = wrap(&quot;density&quot;), combo = &quot;box_no_facet&quot;), lower = list(continuous = wrap(&quot;points&quot;), combo = wrap(&quot;dot_no_facet&quot;))) Multi-panel plots like this are intrinsically very rich in information. When combined with several within-panel types of representation, or any more than a modest number of variables, they can become quite complex. They should be used less for the presentation of finished work, although it is possible. More often they are a useful tool for the working researcher to quickly investigate aspects of a dataset. The goal is not to pithily summarize a single point one already knows, but to open things up for further exploration. "],
["visualizing-residuals.html", "Chapter 35 Visualizing residuals 35.1 Simple Linear Regression 35.2 Step 4: use residuals to adjust", " Chapter 35 Visualizing residuals Source: https://www.r-bloggers.com/visualising-residuals/ fit &lt;- lm(mpg ~ hp, data = mtcars) # Fit the model summary(fit) # Report the results #&gt; #&gt; Call: #&gt; lm(formula = mpg ~ hp, data = mtcars) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -5.712 -2.112 -0.885 1.582 8.236 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 30.0989 1.6339 18.42 &lt; 2e-16 *** #&gt; hp -0.0682 0.0101 -6.74 1.8e-07 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 3.86 on 30 degrees of freedom #&gt; Multiple R-squared: 0.602, Adjusted R-squared: 0.589 #&gt; F-statistic: 45.5 on 1 and 30 DF, p-value: 1.79e-07 par(mfrow = c(2, 2)) # Split the plotting panel into a 2 x 2 grid plot(fit) # Plot the model information par(mfrow = c(1, 1)) # Return plotting panel to 1 section 35.1 Simple Linear Regression d &lt;- mtcars fit &lt;- lm(mpg ~ hp, data = d) d$predicted &lt;- predict(fit) # Save the predicted values d$residuals &lt;- residuals(fit) # Save the residual values # Quick look at the actual, predicted, and residual values library(dplyr) #&gt; #&gt; Attaching package: &#39;dplyr&#39; #&gt; The following objects are masked from &#39;package:stats&#39;: #&gt; #&gt; filter, lag #&gt; The following objects are masked from &#39;package:base&#39;: #&gt; #&gt; intersect, setdiff, setequal, union d %&gt;% select(mpg, predicted, residuals) %&gt;% head() #&gt; mpg predicted residuals #&gt; Mazda RX4 21.0 22.6 -1.594 #&gt; Mazda RX4 Wag 21.0 22.6 -1.594 #&gt; Datsun 710 22.8 23.8 -0.954 #&gt; Hornet 4 Drive 21.4 22.6 -1.194 #&gt; Hornet Sportabout 18.7 18.2 0.541 #&gt; Valiant 18.1 22.9 -4.835 35.1.1 Step 3: plot the actual and predicted values plot first the actual data library(ggplot2) #&gt; Registered S3 methods overwritten by &#39;ggplot2&#39;: #&gt; method from #&gt; [.quosures rlang #&gt; c.quosures rlang #&gt; print.quosures rlang ggplot(d, aes(x = hp, y = mpg)) + # Set up canvas with outcome variable on y-axis geom_point() # Plot the actual points Next, we plot the predicted values in a way that they’re distinguishable from the actual values. For example, let’s change their shape: ggplot(d, aes(x = hp, y = mpg)) + geom_point() + geom_point(aes(y = predicted), shape = 1) # Add the predicted values This is on track, but it’s difficult to see how our actual and predicted values are related. Let’s connect the actual data points with their corresponding predicted value using geom_segment(): ggplot(d, aes(x = hp, y = mpg)) + geom_segment(aes(xend = hp, yend = predicted)) + geom_point() + geom_point(aes(y = predicted), shape = 1) We’ll make a few final adjustments: * Clean up the overall look with theme_bw(). * Fade out connection lines by adjusting their alpha. * Add the regression slope with geom_smooth(): library(ggplot2) ggplot(d, aes(x = hp, y = mpg)) + geom_smooth(method = &quot;lm&quot;, se = FALSE, color = &quot;lightgrey&quot;) + # Plot regression slope geom_segment(aes(xend = hp, yend = predicted), alpha = .2) + # alpha to fade lines geom_point() + geom_point(aes(y = predicted), shape = 1) + theme_bw() # Add theme for cleaner look 35.2 Step 4: use residuals to adjust Finally, we want to make an adjustment to highlight the size of the residual. There are MANY options. To make comparisons easy, I’ll make adjustments to the actual values, but you could just as easily apply these, or other changes, to the predicted values. Here are a few examples building on the previous plot: # ALPHA # Changing alpha of actual values based on absolute value of residuals ggplot(d, aes(x = hp, y = mpg)) + geom_smooth(method = &quot;lm&quot;, se = FALSE, color = &quot;lightgrey&quot;) + geom_segment(aes(xend = hp, yend = predicted), alpha = .2) + # &gt; Alpha adjustments made here... geom_point(aes(alpha = abs(residuals))) + # Alpha mapped to abs(residuals) guides(alpha = FALSE) + # Alpha legend removed # &lt; geom_point(aes(y = predicted), shape = 1) + theme_bw() # COLOR # High residuals (in abolsute terms) made more red on actual values. ggplot(d, aes(x = hp, y = mpg)) + geom_smooth(method = &quot;lm&quot;, se = FALSE, color = &quot;lightgrey&quot;) + geom_segment(aes(xend = hp, yend = predicted), alpha = .2) + # &gt; Color adjustments made here... geom_point(aes(color = abs(residuals))) + # Color mapped to abs(residuals) scale_color_continuous(low = &quot;black&quot;, high = &quot;red&quot;) + # Colors to use here guides(color = FALSE) + # Color legend removed # &lt; geom_point(aes(y = predicted), shape = 1) + theme_bw() # SIZE AND COLOR # Same coloring as above, size corresponding as well ggplot(d, aes(x = hp, y = mpg)) + geom_smooth(method = &quot;lm&quot;, se = FALSE, color = &quot;lightgrey&quot;) + geom_segment(aes(xend = hp, yend = predicted), alpha = .2) + # &gt; Color AND size adjustments made here... geom_point(aes(color = abs(residuals), size = abs(residuals))) + # size also mapped scale_color_continuous(low = &quot;black&quot;, high = &quot;red&quot;) + guides(color = FALSE, size = FALSE) + # Size legend also removed # &lt; geom_point(aes(y = predicted), shape = 1) + theme_bw() # COLOR UNDER/OVER # Color mapped to residual with sign taken into account. # i.e., whether actual value is greater or less than predicted ggplot(d, aes(x = hp, y = mpg)) + geom_smooth(method = &quot;lm&quot;, se = FALSE, color = &quot;lightgrey&quot;) + geom_segment(aes(xend = hp, yend = predicted), alpha = .2) + # &gt; Color adjustments made here... geom_point(aes(color = residuals)) + # Color mapped here scale_color_gradient2(low = &quot;blue&quot;, mid = &quot;white&quot;, high = &quot;red&quot;) + # Colors to use here guides(color = FALSE) + # &lt; geom_point(aes(y = predicted), shape = 1) + theme_bw() I particularly like this last example, because the colours nicely help to identify non-linearity in the data. For example, we can see that there is more red for extreme values of hp where the actual values are greater than what is being predicted. There is more blue in the centre, however, indicating that the actual values are less than what is being predicted. Together, this suggests that the relationship between the variables is non-linear, and might be better modelled by including a quadratic term in the regression equation. "],
["building-deep-neural-nets-with-h2o-that-predict-arrhythmia-of-the-heart.html", "Chapter 36 Building deep neural nets with h2o that predict arrhythmia of the heart 36.1 Introduction 36.2 Arrhythmia data 36.3 Converting the dataframe to a h2o object 36.4 Training, test and validation data 36.5 Modeling 36.6 Model performance 36.7 Test data 36.8 Final conclusions: How useful is the model?", " Chapter 36 Building deep neural nets with h2o that predict arrhythmia of the heart 36.1 Introduction 27 February 2017 This week, I am showing how to build feed-forward deep neural networks or multilayer perceptrons. The models in this example are built to classify ECG data into being either from healthy hearts or from someone suffering from arrhythmia. I will show how to prepare a dataset for modeling, setting weights and other modeling parameters, and finally, how to evaluate model performance with the h2o package. 36.1.1 Deep learning with neural networks Deep learning with neural networks is arguably one of the most rapidly growing applications of machine learning and AI today. They allow building complex models that consist of multiple hidden layers within artificial networks and are able to find non-linear patterns in unstructured data. Deep neural networks are usually feed-forward, which means that each layer feeds its output to subsequent layers, but recurrent or feed-back neural networks can also be built. Feed-forward neural networks are also called multilayer perceptrons (MLPs). 36.1.2 H2O The R package h2o provides a convenient interface to H2O, which is an open-source machine learning and deep learning platform. H2O distributes a wide range of common machine learning algorithms for classification, regression and deep learning. 36.1.3 Preparing the R session First, we need to load the packages. library(dplyr) library(h2o) library(ggplot2) library(ggrepel) library(h2o) h2o.init() #&gt; #&gt; H2O is not running yet, starting it now... #&gt; #&gt; Note: In case of errors look at the following log files: #&gt; /tmp/RtmpocQ3a3/h2o_datascience_started_from_r.out #&gt; /tmp/RtmpocQ3a3/h2o_datascience_started_from_r.err #&gt; #&gt; #&gt; Starting H2O JVM and connecting: . Connection successful! #&gt; #&gt; R is connected to the H2O cluster: #&gt; H2O cluster uptime: 1 seconds 385 milliseconds #&gt; H2O cluster timezone: America/Chicago #&gt; H2O data parsing timezone: UTC #&gt; H2O cluster version: 3.22.1.1 #&gt; H2O cluster version age: 8 months and 22 days !!! #&gt; H2O cluster name: H2O_started_from_R_datascience_mwl453 #&gt; H2O cluster total nodes: 1 #&gt; H2O cluster total memory: 6.96 GB #&gt; H2O cluster total cores: 8 #&gt; H2O cluster allowed cores: 8 #&gt; H2O cluster healthy: TRUE #&gt; H2O Connection ip: localhost #&gt; H2O Connection port: 54321 #&gt; H2O Connection proxy: NA #&gt; H2O Internal Security: FALSE #&gt; H2O API Extensions: XGBoost, Algos, AutoML, Core V3, Core V4 #&gt; R Version: R version 3.6.0 (2019-04-26) #&gt; Warning in h2o.clusterInfo(): #&gt; Your H2O cluster version is too old (8 months and 22 days)! #&gt; Please download and install the latest version from http://h2o.ai/download/ my_theme &lt;- function(base_size = 12, base_family = &quot;sans&quot;){ theme_minimal(base_size = base_size, base_family = base_family) + theme( axis.text = element_text(size = 12), axis.title = element_text(size = 14), panel.grid.major = element_line(color = &quot;grey&quot;), panel.grid.minor = element_blank(), panel.background = element_rect(fill = &quot;aliceblue&quot;), strip.background = element_rect(fill = &quot;darkgrey&quot;, color = &quot;grey&quot;, size = 1), strip.text = element_text(face = &quot;bold&quot;, size = 12, color = &quot;white&quot;), legend.position = &quot;right&quot;, legend.justification = &quot;top&quot;, panel.border = element_rect(color = &quot;grey&quot;, fill = NA, size = 0.5) ) } 36.2 Arrhythmia data The data I am using to demonstrate the building of neural nets is the arrhythmia dataset from UC Irvine’s machine learning database. It contains 279 features from ECG heart rhythm diagnostics and one output column. I am not going to rename the feature columns because they are too many and the descriptions are too complex. Also, we don’t need to know specifically which features we are looking at for building the models. For a description of each feature, see https://archive.ics.uci.edu/ml/machine-learning-databases/arrhythmia/arrhythmia.names. The output column defines 16 classes: class 1 samples are from healthy ECGs, the remaining classes belong to different types of arrhythmia, with class 16 being all remaining arrhythmia cases that didn’t fit into distinct classes. arrhythmia &lt;- read.table(file.path(data_raw_dir, &quot;arrhythmia.data.txt&quot;), sep = &quot;,&quot;) arrhythmia[arrhythmia == &quot;?&quot;] &lt;- NA # making sure, that all feature columns are numeric arrhythmia[-280] &lt;- lapply(arrhythmia[-280], as.character) arrhythmia[-280] &lt;- lapply(arrhythmia[-280], as.numeric) # renaming output column and converting to factor colnames(arrhythmia)[280] &lt;- &quot;class&quot; arrhythmia$class &lt;- as.factor(arrhythmia$class) As usual, I want to get acquainted with the data and explore it’s properties before I am building any model. So, I am first going to look at the distribution of classes and of healthy and arrhythmia samples. p1 &lt;- ggplot(arrhythmia, aes(x = class)) + geom_bar(fill = &quot;navy&quot;, alpha = 0.7) + my_theme() Because I am interested in distinguishing healthy from arrhythmia ECGs, I am converting the output to binary format by combining all arrhythmia cases into one class. # all arrhythmia cases into one class arrhythmia$diagnosis &lt;- ifelse(arrhythmia$class == 1, &quot;healthy&quot;, &quot;arrhythmia&quot;) arrhythmia$diagnosis &lt;- as.factor(arrhythmia$diagnosis) p2 &lt;- ggplot(arrhythmia, aes(x = diagnosis)) + geom_bar(fill = &quot;navy&quot;, alpha = 0.7) + my_theme() library(gridExtra) #&gt; #&gt; Attaching package: &#39;gridExtra&#39; #&gt; The following object is masked from &#39;package:dplyr&#39;: #&gt; #&gt; combine library(grid) grid.arrange(p1, p2, ncol = 2) With binary classification, we have almost the same numbers of healthy and arrhythmia cases in our dataset. I am also interested in how much the normal and arrhythmia cases cluster in a Principal Component Analysis (PCA). I am first preparing the PCA plotting function and then run it on the feature data. library(pcaGoPromoter) #&gt; Warning: replacing previous import &#39;BiocGenerics::boxplot&#39; by #&gt; &#39;graphics::boxplot&#39; when loading &#39;Biostrings&#39; #&gt; Warning: replacing previous import &#39;BiocGenerics::image&#39; by #&gt; &#39;graphics::image&#39; when loading &#39;Biostrings&#39; #&gt; Warning: replacing previous import &#39;S4Vectors::na.exclude&#39; by #&gt; &#39;stats::na.exclude&#39; when loading &#39;Biostrings&#39; #&gt; Warning: replacing previous import &#39;IRanges::smoothEnds&#39; by #&gt; &#39;stats::smoothEnds&#39; when loading &#39;Biostrings&#39; #&gt; Warning: replacing previous import &#39;BiocGenerics::density&#39; by #&gt; &#39;stats::density&#39; when loading &#39;Biostrings&#39; #&gt; Warning: replacing previous import &#39;IRanges::mad&#39; by &#39;stats::mad&#39; when #&gt; loading &#39;Biostrings&#39; #&gt; Warning: replacing previous import &#39;S4Vectors::na.omit&#39; by &#39;stats::na.omit&#39; #&gt; when loading &#39;Biostrings&#39; #&gt; Warning: replacing previous import &#39;S4Vectors::complete.cases&#39; by #&gt; &#39;stats::complete.cases&#39; when loading &#39;Biostrings&#39; #&gt; Warning: replacing previous import &#39;IRanges::runmed&#39; by &#39;stats::runmed&#39; #&gt; when loading &#39;Biostrings&#39; #&gt; Warning: replacing previous import &#39;IRanges::start&#39; by &#39;stats::start&#39; when #&gt; loading &#39;Biostrings&#39; #&gt; Warning: replacing previous import &#39;IRanges::window&lt;-&#39; by &#39;stats::window&lt;-&#39; #&gt; when loading &#39;Biostrings&#39; #&gt; Warning: replacing previous import &#39;S4Vectors::window&#39; by &#39;stats::window&#39; #&gt; when loading &#39;Biostrings&#39; #&gt; Warning: replacing previous import &#39;S4Vectors::aggregate&#39; by #&gt; &#39;stats::aggregate&#39; when loading &#39;Biostrings&#39; #&gt; Warning: replacing previous import &#39;BiocGenerics::weights&#39; by #&gt; &#39;stats::weights&#39; when loading &#39;Biostrings&#39; #&gt; Warning: replacing previous import &#39;IRanges::cor&#39; by &#39;stats::cor&#39; when #&gt; loading &#39;Biostrings&#39; #&gt; Warning: replacing previous import &#39;IRanges::cov&#39; by &#39;stats::cov&#39; when #&gt; loading &#39;Biostrings&#39; #&gt; Warning: replacing previous import &#39;IRanges::quantile&#39; by &#39;stats::quantile&#39; #&gt; when loading &#39;Biostrings&#39; #&gt; Warning: replacing previous import &#39;IRanges::end&#39; by &#39;stats::end&#39; when #&gt; loading &#39;Biostrings&#39; #&gt; Warning: replacing previous import &#39;BiocGenerics::residuals&#39; by #&gt; &#39;stats::residuals&#39; when loading &#39;Biostrings&#39; #&gt; Warning: replacing previous import &#39;IRanges::median&#39; by &#39;stats::median&#39; #&gt; when loading &#39;Biostrings&#39; #&gt; Warning: replacing previous import &#39;IRanges::sd&#39; by &#39;stats::sd&#39; when #&gt; loading &#39;Biostrings&#39; #&gt; Warning: replacing previous import &#39;IRanges::var&#39; by &#39;stats::var&#39; when #&gt; loading &#39;Biostrings&#39; #&gt; Warning: replacing previous import &#39;S4Vectors::xtabs&#39; by &#39;stats::xtabs&#39; #&gt; when loading &#39;Biostrings&#39; #&gt; Warning: replacing previous import &#39;IRanges::IQR&#39; by &#39;stats::IQR&#39; when #&gt; loading &#39;Biostrings&#39; #&gt; Warning: replacing previous import &#39;S4Vectors::tail&#39; by &#39;utils::tail&#39; when #&gt; loading &#39;Biostrings&#39; #&gt; Warning: replacing previous import &#39;IRanges::stack&#39; by &#39;utils::stack&#39; when #&gt; loading &#39;Biostrings&#39; #&gt; Warning: replacing previous import &#39;XVector::relist&#39; by &#39;utils::relist&#39; #&gt; when loading &#39;Biostrings&#39; #&gt; Warning: replacing previous import &#39;S4Vectors::head&#39; by &#39;utils::head&#39; when #&gt; loading &#39;Biostrings&#39; pca_func &lt;- function(pcaOutput2, group_name){ centroids &lt;- aggregate(cbind(PC1, PC2) ~ groups, pcaOutput2, mean) conf.rgn &lt;- do.call(rbind, lapply(unique(pcaOutput2$groups), function(t) data.frame(groups = as.character(t), ellipse(cov(pcaOutput2[pcaOutput2$groups == t, 1:2]), centre = as.matrix(centroids[centroids$groups == t, 2:3]), level = 0.95), stringsAsFactors = FALSE))) plot &lt;- ggplot(data = pcaOutput2, aes(x = PC1, y = PC2, group = groups, color = groups)) + geom_polygon(data = conf.rgn, aes(fill = groups), alpha = 0.2) + geom_point(size = 2, alpha = 0.5) + labs(color = paste(group_name), fill = paste(group_name), x = paste0(&quot;PC1: &quot;, round(pcaOutput$pov[1], digits = 2) * 100, &quot;% variance&quot;), y = paste0(&quot;PC2: &quot;, round(pcaOutput$pov[2], digits = 2) * 100, &quot;% variance&quot;)) + my_theme() return(plot) } # Find what columns have NAs and the quantity for (col in names(arrhythmia)) { n_nas &lt;- length(which(is.na(arrhythmia[, col]))) if (n_nas &gt; 0) cat(col, n_nas, &quot;\\n&quot;) } #&gt; V11 8 #&gt; V12 22 #&gt; V13 1 #&gt; V14 376 #&gt; V15 1 # Replace NAs with zeros arrhythmia[is.na(arrhythmia)] &lt;- 0 Find and plot the PCAs. pcaOutput &lt;- pca(t(arrhythmia[-c(280, 281)]), printDropped=FALSE, scale=TRUE, center = TRUE) pcaOutput2 &lt;- as.data.frame(pcaOutput$scores) pcaOutput2$groups &lt;- arrhythmia$class p1 &lt;- pca_func(pcaOutput2, group_name = &quot;class&quot;) pcaOutput2$groups &lt;- arrhythmia$diagnosis p2 &lt;- pca_func(pcaOutput2, group_name = &quot;diagnosis&quot;) grid.arrange(p1, p2, ncol = 2) The PCA shows that there is a big overlap between healthy and arrhythmia samples, i.e. there does not seem to be major global differences in all features. The class that is most distinct from all others seems to be class 9. I want to give the arrhythmia cases that are very different from the rest a stronger weight in the neural network, so I define a weight column where every sample outside the central PCA cluster will get a “2”, they will in effect be used twice in the model. weights &lt;- ifelse(pcaOutput2$PC1 &lt; -5 &amp; abs(pcaOutput2$PC2) &gt; 10, 2, 1) I also want to know what the variance is within features. library(matrixStats) #&gt; #&gt; Attaching package: &#39;matrixStats&#39; #&gt; The following object is masked from &#39;package:dplyr&#39;: #&gt; #&gt; count colvars &lt;- data.frame(feature = colnames(arrhythmia[-c(280, 281)]), variance = colVars(as.matrix(arrhythmia[-c(280, 281)]))) subset(colvars, variance &gt; 50) %&gt;% mutate(feature = factor(feature, levels = colnames(arrhythmia[-c(280, 281)]))) %&gt;% ggplot(aes(x = feature, y = variance)) + geom_bar(stat = &quot;identity&quot;, fill = &quot;navy&quot;, alpha = 0.7) + my_theme() + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) Features with low variance are less likely to strongly contribute to a differentiation between healthy and arrhythmia cases, so I am going to remove them. I am also concatenating the weights column: arrhythmia_subset &lt;- cbind(weights, arrhythmia[, c(281, 280, which(colvars$variance &gt; 50))]) 36.3 Converting the dataframe to a h2o object Now that I have my final data frame for modeling, for working with h2o functions, the data needs to be converted from a DataFrame to an H2O Frame. This is done with the as_h2o_frame() function. #as_h2o_frame(arrhythmia_subset) arrhythmia_hf &lt;- as.h2o(arrhythmia_subset, key=&quot;arrhtythmia.hex&quot;) #&gt; | | | 0% | |=================================================================| 100% We can now access all functions from the h2o package that are built to work on h2o Frames. A useful such function is h2o.describe(). It is similar to base R’s summary() function but outputs many more descriptive measures for our data. To get a good overview about these measures, I am going to plot them. library(tidyr) # for gathering #&gt; #&gt; Attaching package: &#39;tidyr&#39; #&gt; The following object is masked from &#39;package:S4Vectors&#39;: #&gt; #&gt; expand h2o.describe(arrhythmia_hf[, -1]) %&gt;% # excluding the weights column gather(x, y, Zeros:Sigma) %&gt;% mutate(group = ifelse( x %in% c(&quot;Min&quot;, &quot;Max&quot;, &quot;Mean&quot;), &quot;min, mean, max&quot;, ifelse(x %in% c(&quot;NegInf&quot;, &quot;PosInf&quot;), &quot;Inf&quot;, &quot;sigma, zeros&quot;))) %&gt;% # separating them into facets makes them easier to see mutate(Label = factor(Label, levels = colnames(arrhythmia_hf[, -1]))) %&gt;% ggplot(aes(x = Label, y = as.numeric(y), color = x)) + geom_point(size = 4, alpha = 0.6) + scale_color_brewer(palette = &quot;Set1&quot;) + my_theme() + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) + facet_grid(group ~ ., scales = &quot;free&quot;) + labs(x = &quot;Feature&quot;, y = &quot;Value&quot;, color = &quot;&quot;) #&gt; Warning: Removed 2 rows containing missing values (geom_point). I am also interested in the correlation between features and the output. We can use the h2o.cor() function to calculate the correlation matrix. It is again much easier to understand the data when we visualize it, so I am going to create another plot. library(reshape2) # for melting #&gt; #&gt; Attaching package: &#39;reshape2&#39; #&gt; The following object is masked from &#39;package:tidyr&#39;: #&gt; #&gt; smiths # diagnosis is now a characer column and we need to convert it again arrhythmia_hf[, 2] &lt;- h2o.asfactor(arrhythmia_hf[, 2]) arrhythmia_hf[, 3] &lt;- h2o.asfactor(arrhythmia_hf[, 3]) # same for class cor &lt;- h2o.cor(arrhythmia_hf[, -c(1, 3)]) rownames(cor) &lt;- colnames(cor) melt(cor) %&gt;% mutate(Var2 = rep(rownames(cor), nrow(cor))) %&gt;% mutate(Var2 = factor(Var2, levels = colnames(cor))) %&gt;% mutate(variable = factor(variable, levels = colnames(cor))) %&gt;% ggplot(aes(x = variable, y = Var2, fill = value)) + geom_tile(width = 0.9, height = 0.9) + scale_fill_gradient2(low = &quot;white&quot;, high = &quot;red&quot;, name = &quot;Cor.&quot;) + my_theme() + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) + labs(x = &quot;&quot;, y = &quot;&quot;) #&gt; No id variables; using all as measure variables 36.4 Training, test and validation data Now we can use the h2o.splitFrame() function to split the data into training, validation and test data. Here, I am using 70% for training and 15% each for validation and testing. We could also just split the data into two sections, a training and test set but when we have sufficient samples, it is a good idea to evaluate model performance on an independent test set on top of training with a validation set. Because we can easily overfit a model, we want to get an idea about how generalizable it is - this we can only assess by looking at how well it works on previously unknown data. I am also defining response, features and weights column names now. splits &lt;- h2o.splitFrame(arrhythmia_hf, ratios = c(0.7, 0.15), seed = 1) train &lt;- splits[[1]] valid &lt;- splits[[2]] test &lt;- splits[[3]] response &lt;- &quot;diagnosis&quot; weights &lt;- &quot;weights&quot; features &lt;- setdiff(colnames(train), c(response, weights, &quot;class&quot;)) summary(train$diagnosis, exact_quantiles = TRUE) #&gt; diagnosis #&gt; healthy :163 #&gt; arrhythmia:155 summary(valid$diagnosis, exact_quantiles = TRUE) #&gt; diagnosis #&gt; healthy :43 #&gt; arrhythmia:25 summary(test$diagnosis, exact_quantiles = TRUE) #&gt; diagnosis #&gt; healthy :39 #&gt; arrhythmia:27 If we had more categorical features, we could use the h2o.interaction() function to define interaction terms, but since we only have numeric features here, we don’t need this. We can also run a PCA on the training data, using the h2o.prcomp() function to calculate the singular value decomposition of the Gram matrix with the power method. pca &lt;- h2o.prcomp(training_frame = train, x = features, validation_frame = valid, transform = &quot;NORMALIZE&quot;, k = 3, seed = 42) #&gt; | | | 0% | |========================== | 40% | |=================================================================| 100% #&gt; Warning in doTryCatch(return(expr), name, parentenv, handler): _train: #&gt; Dataset used may contain fewer number of rows due to removal of rows with #&gt; NA/missing values. If this is not desirable, set impute_missing argument in #&gt; pca call to TRUE/True/true/... depending on the client language. pca #&gt; Model Details: #&gt; ============== #&gt; #&gt; H2ODimReductionModel: pca #&gt; Model ID: PCA_model_R_1569006233870_1 #&gt; Importance of components: #&gt; pc1 pc2 pc3 #&gt; Standard deviation 0.582620 0.507796 0.421869 #&gt; Proportion of Variance 0.164697 0.125110 0.086351 #&gt; Cumulative Proportion 0.164697 0.289808 0.376159 #&gt; #&gt; #&gt; H2ODimReductionMetrics: pca #&gt; #&gt; No model metrics available for PCA #&gt; H2ODimReductionMetrics: pca #&gt; #&gt; No model metrics available for PCA eigenvec &lt;- as.data.frame(pca@model$eigenvectors) eigenvec$label &lt;- features ggplot(eigenvec, aes(x = pc1, y = pc2, label = label)) + geom_point(color = &quot;navy&quot;, alpha = 0.7) + geom_text_repel() + my_theme() 36.5 Modeling Now, we can build a deep neural network model. We can specify quite a few parameters, like Cross-validation: Cross validation can tell us the training and validation errors for each model. The final model will be overwritten with the best model, if we don’t specify otherwise. Adaptive learning rate: For deep learning with h2o, we by default use stochastic gradient descent optimization with an an adaptive learning rate. The two corresponding parameters rho and epsilon help us find global (or near enough) optima. Activation function: The activation function defines the node output relative to a given set of inputs. We want our activation function to be non-linear and continuously differentiable. Hidden nodes: Defines the number of hidden layers and the number of nodes per layer. Epochs: Increasing the number of epochs (one full training cycle on all training samples) can increase model performance, but we also run the risk of overfitting. To determine the optimal number of epochs, we need to use early stopping. Early stopping: By default, early stopping is enabled. This means that training will be stopped when we reach a certain validation error to prevent overfitting. Of course, you need quite a bit of experience and intuition to hit on a good combination of parameters. That’s why it usually makes sense to do a grid search for hyper-parameter tuning. Here, I want to focus on building and evaluating deep learning models, though. I will cover grid search in next week’s post. # this will take some time and all CPUs dl_model &lt;- h2o.deeplearning(x = features, y = response, weights_column = weights, model_id = &quot;dl_model&quot;, training_frame = train, validation_frame = valid, nfolds = 15, # 10x cross validation keep_cross_validation_fold_assignment = TRUE, fold_assignment = &quot;Stratified&quot;, activation = &quot;RectifierWithDropout&quot;, score_each_iteration = TRUE, hidden = c(200, 200, 200, 200, 200), # 5 hidden layers, each of 200 neurons epochs = 100, variable_importances = TRUE, export_weights_and_biases = TRUE, seed = 42) #&gt; | | | 0% | |== | 2% | |====== | 9% | |====== | 10% | |========= | 14% | |========== | 16% | |============= | 19% | |============== | 21% | |=============== | 23% | |================ | 25% | |================= | 26% | |================= | 27% | |================== | 28% | |=================== | 29% | |==================== | 31% | |====================== | 34% | |======================= | 36% | |======================== | 36% | |======================== | 38% | |========================= | 39% | |=========================== | 42% | |============================ | 43% | |============================= | 44% | |============================= | 45% | |============================== | 46% | |============================== | 47% | |================================ | 49% | |================================= | 51% | |================================== | 52% | |=================================== | 54% | |==================================== | 55% | |===================================== | 57% | |====================================== | 58% | |======================================= | 59% | |======================================= | 61% | |========================================= | 63% | |========================================= | 64% | |=========================================== | 67% | |============================================ | 68% | |============================================= | 69% | |============================================== | 71% | |=============================================== | 72% | |================================================ | 74% | |================================================= | 75% | |================================================== | 76% | |================================================== | 78% | |=================================================== | 79% | |=================================================================| 100% Because training can take a while, depending on how many samples, features, nodes and hidden layers you are training on, it is a good idea to save your model. # if file exists, overwrite it h2o.saveModel(dl_model, path = file.path(data_out_dir, &quot;dl_model&quot;), force = TRUE) #&gt; [1] &quot;/home/datascience/repos/machine-learning-rsuite/export/dl_model/dl_model&quot; We can then re-load the model again any time to check the model quality and make predictions on new data. dl_model &lt;- h2o.loadModel(file.path(data_out_dir, &quot;dl_model/dl_model&quot;)) 36.6 Model performance We now want to know how our model performed on the validation data. The summary() function will give us a detailed overview of our model. I am not showing the output here, because it is quite extensive. sum_model &lt;- summary(dl_model) #&gt; Model Details: #&gt; ============== #&gt; #&gt; H2OBinomialModel: deeplearning #&gt; Model Key: dl_model #&gt; Status of Neuron Layers: predicting diagnosis, 2-class classification, bernoulli distribution, CrossEntropy loss, 179,402 weights/biases, 2.1 MB, 34,090 training samples, mini-batch size 1 #&gt; layer units type dropout l1 l2 mean_rate #&gt; 1 1 90 Input 0.00 % NA NA NA #&gt; 2 2 200 RectifierDropout 50.00 % 0.000000 0.000000 0.003940 #&gt; 3 3 200 RectifierDropout 50.00 % 0.000000 0.000000 0.006008 #&gt; 4 4 200 RectifierDropout 50.00 % 0.000000 0.000000 0.008890 #&gt; 5 5 200 RectifierDropout 50.00 % 0.000000 0.000000 0.008181 #&gt; 6 6 200 RectifierDropout 50.00 % 0.000000 0.000000 0.026456 #&gt; 7 7 2 Softmax NA 0.000000 0.000000 0.002482 #&gt; rate_rms momentum mean_weight weight_rms mean_bias bias_rms #&gt; 1 NA NA NA NA NA NA #&gt; 2 0.003262 0.000000 0.003762 0.096540 0.416030 0.059538 #&gt; 3 0.003521 0.000000 -0.009085 0.074949 0.946218 0.053690 #&gt; 4 0.004511 0.000000 -0.008754 0.072650 0.959295 0.030155 #&gt; 5 0.004010 0.000000 -0.007750 0.071214 0.965323 0.036717 #&gt; 6 0.057193 0.000000 -0.010064 0.070802 0.947907 0.037711 #&gt; 7 0.001305 0.000000 -0.041059 0.375035 -0.000047 0.029590 #&gt; #&gt; H2OBinomialMetrics: deeplearning #&gt; ** Reported on training data. ** #&gt; ** Metrics reported on full training frame ** #&gt; #&gt; MSE: 0.0162 #&gt; RMSE: 0.127 #&gt; LogLoss: 0.0712 #&gt; Mean Per-Class Error: 0.00915 #&gt; AUC: 0.989 #&gt; pr_auc: 0.973 #&gt; Gini: 0.978 #&gt; #&gt; Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold: #&gt; arrhythmia healthy Error Rate #&gt; arrhythmia 161 3 0.018293 =3/164 #&gt; healthy 0 163 0.000000 =0/163 #&gt; Totals 161 166 0.009174 =3/327 #&gt; #&gt; Maximum Metrics: Maximum metrics at their respective thresholds #&gt; metric threshold value idx #&gt; 1 max f1 0.165665 0.990881 165 #&gt; 2 max f2 0.165665 0.996333 165 #&gt; 3 max f0point5 0.165665 0.985490 165 #&gt; 4 max accuracy 0.165665 0.990826 165 #&gt; 5 max precision 0.993766 1.000000 0 #&gt; 6 max recall 0.165665 1.000000 165 #&gt; 7 max specificity 0.993766 1.000000 0 #&gt; 8 max absolute_mcc 0.165665 0.981818 165 #&gt; 9 max min_per_class_accuracy 0.564953 0.981707 163 #&gt; 10 max mean_per_class_accuracy 0.165665 0.990854 165 #&gt; #&gt; Gains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)` #&gt; H2OBinomialMetrics: deeplearning #&gt; ** Reported on validation data. ** #&gt; ** Metrics reported on full validation frame ** #&gt; #&gt; MSE: 0.228 #&gt; RMSE: 0.478 #&gt; LogLoss: 1.22 #&gt; Mean Per-Class Error: 0.183 #&gt; AUC: 0.862 #&gt; pr_auc: 0.883 #&gt; Gini: 0.725 #&gt; #&gt; Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold: #&gt; arrhythmia healthy Error Rate #&gt; arrhythmia 17 8 0.320000 =8/25 #&gt; healthy 2 41 0.046512 =2/43 #&gt; Totals 19 49 0.147059 =10/68 #&gt; #&gt; Maximum Metrics: Maximum metrics at their respective thresholds #&gt; metric threshold value idx #&gt; 1 max f1 0.000560 0.891304 48 #&gt; 2 max f2 0.000054 0.937500 51 #&gt; 3 max f0point5 0.000560 0.857741 48 #&gt; 4 max accuracy 0.000560 0.852941 48 #&gt; 5 max precision 0.993865 1.000000 0 #&gt; 6 max recall 0.000001 1.000000 58 #&gt; 7 max specificity 0.993865 1.000000 0 #&gt; 8 max absolute_mcc 0.000560 0.680719 48 #&gt; 9 max min_per_class_accuracy 0.336530 0.760000 38 #&gt; 10 max mean_per_class_accuracy 0.000560 0.816744 48 #&gt; #&gt; Gains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)` #&gt; H2OBinomialMetrics: deeplearning #&gt; ** Reported on cross-validation data. ** #&gt; ** 15-fold cross-validation on training data (Metrics computed for combined holdout predictions) ** #&gt; #&gt; MSE: 0.178 #&gt; RMSE: 0.422 #&gt; LogLoss: 0.61 #&gt; Mean Per-Class Error: 0.235 #&gt; AUC: 0.845 #&gt; pr_auc: 0.797 #&gt; Gini: 0.69 #&gt; #&gt; Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold: #&gt; arrhythmia healthy Error Rate #&gt; arrhythmia 103 61 0.371951 =61/164 #&gt; healthy 16 147 0.098160 =16/163 #&gt; Totals 119 208 0.235474 =77/327 #&gt; #&gt; Maximum Metrics: Maximum metrics at their respective thresholds #&gt; metric threshold value idx #&gt; 1 max f1 0.243823 0.792453 207 #&gt; 2 max f2 0.004894 0.883315 264 #&gt; 3 max f0point5 0.840755 0.782361 134 #&gt; 4 max accuracy 0.668136 0.773700 170 #&gt; 5 max precision 0.991154 1.000000 0 #&gt; 6 max recall 0.000239 1.000000 296 #&gt; 7 max specificity 0.991154 1.000000 0 #&gt; 8 max absolute_mcc 0.319483 0.552269 202 #&gt; 9 max min_per_class_accuracy 0.727822 0.766871 162 #&gt; 10 max mean_per_class_accuracy 0.647896 0.773792 172 #&gt; #&gt; Gains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)` #&gt; Cross-Validation Metrics Summary: #&gt; mean sd cv_1_valid cv_2_valid #&gt; accuracy 0.83517987 0.06588336 0.71428573 0.7058824 #&gt; auc 0.8612211 0.077477224 0.6737968 0.65 #&gt; err 0.1648201 0.06588336 0.2857143 0.29411766 #&gt; err_count 3.9333334 1.9402176 8.0 5.0 #&gt; f0point5 0.80713534 0.08969009 0.6321839 0.5405405 #&gt; f1 0.8503437 0.06640498 0.73333335 0.61538464 #&gt; f2 0.9055896 0.046776082 0.8730159 0.71428573 #&gt; lift_top_group 1.4293057 0.6512436 0.0 0.0 #&gt; logloss 0.5747075 0.1731897 0.825226 1.0893124 #&gt; max_per_class_error 0.31434393 0.1261734 0.47058824 0.33333334 #&gt; mcc 0.6943212 0.1145281 0.5536258 0.42600644 #&gt; mean_per_class_accuracy 0.8289644 0.065515384 0.7647059 0.73333335 #&gt; mean_per_class_error 0.1710356 0.065515384 0.23529412 0.26666668 #&gt; mse 0.1679895 0.045410696 0.27646768 0.27095565 #&gt; precision 0.78330284 0.1044274 0.57894737 0.5 #&gt; r2 0.29303393 0.20241839 -0.15909448 -0.3051031 #&gt; recall 0.95181817 0.04828302 1.0 0.8 #&gt; rmse 0.40217802 0.055867482 0.52580196 0.52053404 #&gt; specificity 0.7061106 0.14323944 0.5294118 0.6666667 #&gt; cv_3_valid cv_4_valid cv_5_valid cv_6_valid #&gt; accuracy 0.93333334 0.82608694 0.9444444 0.9375 #&gt; auc 0.9444444 0.85714287 0.9480519 0.984375 #&gt; err 0.06666667 0.17391305 0.055555556 0.0625 #&gt; err_count 1.0 4.0 1.0 1.0 #&gt; f0point5 0.9183673 0.81395346 0.98039216 0.90909094 #&gt; f1 0.94736844 0.875 0.95238096 0.9411765 #&gt; f2 0.9782609 0.9459459 0.9259259 0.9756098 #&gt; lift_top_group 1.6666666 1.6428572 1.6363636 2.0 #&gt; logloss 0.2557896 0.42208487 0.3682161 0.2906219 #&gt; max_per_class_error 0.16666667 0.44444445 0.09090909 0.125 #&gt; mcc 0.8660254 0.6573422 0.8918826 0.8819171 #&gt; mean_per_class_accuracy 0.9166667 0.7777778 0.95454544 0.9375 #&gt; mean_per_class_error 0.083333336 0.22222222 0.045454547 0.0625 #&gt; mse 0.07362311 0.1387199 0.12175192 0.0882786 #&gt; precision 0.9 0.7777778 1.0 0.8888889 #&gt; r2 0.69323707 0.41759658 0.48769322 0.6468856 #&gt; recall 1.0 1.0 0.90909094 1.0 #&gt; rmse 0.27133578 0.37245122 0.34892967 0.29711714 #&gt; specificity 0.8333333 0.5555556 1.0 0.875 #&gt; cv_7_valid cv_8_valid cv_9_valid cv_10_valid #&gt; accuracy 0.95454544 0.90909094 0.7692308 0.73333335 #&gt; auc 0.9464286 0.96428573 0.77124184 0.8348214 #&gt; err 0.045454547 0.09090909 0.23076923 0.26666668 #&gt; err_count 1.0 1.0 6.0 8.0 #&gt; f0point5 0.9722222 0.8974359 0.7798165 0.6862745 #&gt; f1 0.93333334 0.93333334 0.85 0.7777778 #&gt; f2 0.8974359 0.9722222 0.93406594 0.8974359 #&gt; lift_top_group 2.75 1.5714285 1.5294118 2.142857 #&gt; logloss 0.48026484 0.35876134 0.63326705 0.76694065 #&gt; max_per_class_error 0.125 0.25 0.6666667 0.5 #&gt; mcc 0.9036961 0.81009257 0.49636358 0.56407607 #&gt; mean_per_class_accuracy 0.9375 0.875 0.6666667 0.75 #&gt; mean_per_class_error 0.0625 0.125 0.33333334 0.25 #&gt; mse 0.12494385 0.09713111 0.20564677 0.22267836 #&gt; precision 1.0 0.875 0.73913044 0.6363636 #&gt; r2 0.46006408 0.58025485 0.09139072 0.10531017 #&gt; recall 0.875 1.0 1.0 1.0 #&gt; rmse 0.35347396 0.31165865 0.45348293 0.47188807 #&gt; specificity 1.0 0.75 0.33333334 0.5 #&gt; cv_11_valid cv_12_valid cv_13_valid cv_14_valid #&gt; accuracy 0.8636364 0.71428573 0.75 0.83870965 #&gt; auc 0.9338843 0.7090909 0.8359375 0.8833333 #&gt; err 0.13636364 0.2857143 0.25 0.16129032 #&gt; err_count 3.0 6.0 8.0 5.0 #&gt; f0point5 0.88235295 0.6756757 0.72115386 0.8152174 #&gt; f1 0.85714287 0.7692308 0.7894737 0.85714287 #&gt; f2 0.8333333 0.89285713 0.872093 0.90361446 #&gt; lift_top_group 2.0 0.0 2.0 0.0 #&gt; logloss 0.48772183 0.8426437 0.901343 0.52144325 #&gt; max_per_class_error 0.18181819 0.54545456 0.4375 0.26666668 #&gt; mcc 0.73029673 0.5330018 0.53935987 0.6882605 #&gt; mean_per_class_accuracy 0.8636364 0.72727275 0.75 0.8354167 #&gt; mean_per_class_error 0.13636364 0.27272728 0.25 0.16458334 #&gt; mse 0.15514173 0.24412373 0.21544832 0.15732217 #&gt; precision 0.9 0.625 0.6818182 0.7894737 #&gt; r2 0.37943313 0.021285798 0.13820672 0.3700558 #&gt; recall 0.8181818 1.0 0.9375 0.9375 #&gt; rmse 0.39388034 0.49408877 0.4641641 0.3966386 #&gt; specificity 0.90909094 0.45454547 0.5625 0.73333335 #&gt; cv_15_valid #&gt; accuracy 0.93333334 #&gt; auc 0.9814815 #&gt; err 0.06666667 #&gt; err_count 1.0 #&gt; f0point5 0.88235295 #&gt; f1 0.9230769 #&gt; f2 0.9677419 #&gt; lift_top_group 2.5 #&gt; logloss 0.3769763 #&gt; max_per_class_error 0.11111111 #&gt; mcc 0.8728716 #&gt; mean_per_class_accuracy 0.9444444 #&gt; mean_per_class_error 0.055555556 #&gt; mse 0.12760974 #&gt; precision 0.85714287 #&gt; r2 0.4682927 #&gt; recall 1.0 #&gt; rmse 0.35722506 #&gt; specificity 0.8888889 #&gt; #&gt; Scoring History: #&gt; timestamp duration training_speed epochs #&gt; 1 2019-09-20 14:05:49 0.000 sec NA 0.00000 #&gt; 2 2019-09-20 14:05:49 1 min 38.092 sec 5368 obs/sec 10.72013 #&gt; 3 2019-09-20 14:05:50 1 min 38.833 sec 5122 obs/sec 21.44025 #&gt; 4 2019-09-20 14:05:51 1 min 39.469 sec 5323 obs/sec 32.16038 #&gt; 5 2019-09-20 14:05:51 1 min 40.087 sec 5474 obs/sec 42.88050 #&gt; 6 2019-09-20 14:05:52 1 min 40.712 sec 5546 obs/sec 53.60063 #&gt; 7 2019-09-20 14:05:53 1 min 41.343 sec 5594 obs/sec 64.32075 #&gt; 8 2019-09-20 14:05:53 1 min 42.084 sec 5484 obs/sec 75.04088 #&gt; 9 2019-09-20 14:05:54 1 min 42.717 sec 5532 obs/sec 85.76101 #&gt; 10 2019-09-20 14:05:55 1 min 43.352 sec 5558 obs/sec 96.48113 #&gt; 11 2019-09-20 14:05:55 1 min 44.034 sec 5542 obs/sec 107.20126 #&gt; iterations samples training_rmse training_logloss training_r2 #&gt; 1 0 0.000000 NA NA NA #&gt; 2 1 3409.000000 0.36763 0.50295 0.45940 #&gt; 3 2 6818.000000 0.34486 0.40354 0.52429 #&gt; 4 3 10227.000000 0.29519 0.30818 0.65144 #&gt; 5 4 13636.000000 0.26966 0.25560 0.70914 #&gt; 6 5 17045.000000 0.24679 0.22589 0.75637 #&gt; 7 6 20454.000000 0.23407 0.21965 0.78084 #&gt; 8 7 23863.000000 0.21579 0.18456 0.81374 #&gt; 9 8 27272.000000 0.20427 0.14319 0.83309 #&gt; 10 9 30681.000000 0.15428 0.09524 0.90478 #&gt; 11 10 34090.000000 0.12717 0.07116 0.93531 #&gt; training_auc training_pr_auc training_lift #&gt; 1 NA NA NA #&gt; 2 0.90233 0.86784 2.00613 #&gt; 3 0.92690 0.89425 2.00613 #&gt; 4 0.94647 0.92121 2.00613 #&gt; 5 0.95784 0.93482 2.00613 #&gt; 6 0.96611 0.94838 2.00613 #&gt; 7 0.97198 0.95185 2.00613 #&gt; 8 0.98070 0.96656 2.00613 #&gt; 9 0.98541 0.97474 2.00613 #&gt; 10 0.98958 0.97823 2.00613 #&gt; 11 0.98919 0.97311 2.00613 #&gt; training_classification_error validation_rmse validation_logloss #&gt; 1 NA NA NA #&gt; 2 0.16208 0.41635 0.67358 #&gt; 3 0.12538 0.45089 0.74106 #&gt; 4 0.09480 0.38720 0.56451 #&gt; 5 0.08869 0.43941 0.74275 #&gt; 6 0.07034 0.41206 0.73015 #&gt; 7 0.05810 0.41964 0.83731 #&gt; 8 0.04893 0.43850 1.03958 #&gt; 9 0.04587 0.40487 0.77129 #&gt; 10 0.02446 0.45292 1.16543 #&gt; 11 0.00917 0.47776 1.22234 #&gt; validation_r2 validation_auc validation_pr_auc validation_lift #&gt; 1 NA NA NA NA #&gt; 2 0.25436 0.84186 0.84164 1.58140 #&gt; 3 0.12551 0.85209 0.86126 1.58140 #&gt; 4 0.35511 0.86977 0.87816 1.58140 #&gt; 5 0.16948 0.85023 0.86600 1.58140 #&gt; 6 0.26964 0.85581 0.85473 1.58140 #&gt; 7 0.24253 0.86884 0.87915 1.58140 #&gt; 8 0.17290 0.85395 0.87379 1.58140 #&gt; 9 0.29492 0.86698 0.87436 1.58140 #&gt; 10 0.11761 0.85023 0.86318 1.58140 #&gt; 11 0.01818 0.86233 0.88300 1.58140 #&gt; validation_classification_error #&gt; 1 NA #&gt; 2 0.16176 #&gt; 3 0.16176 #&gt; 4 0.14706 #&gt; 5 0.17647 #&gt; 6 0.19118 #&gt; 7 0.16176 #&gt; 8 0.17647 #&gt; 9 0.13235 #&gt; 10 0.16176 #&gt; 11 0.14706 #&gt; #&gt; Variable Importances: (Extract with `h2o.varimp`) #&gt; ================================================= #&gt; #&gt; Variable Importances: #&gt; variable relative_importance scaled_importance percentage #&gt; 1 V169 1.000000 1.000000 0.013786 #&gt; 2 V15 0.972648 0.972648 0.013409 #&gt; 3 V103 0.947561 0.947561 0.013063 #&gt; 4 V112 0.923257 0.923257 0.012728 #&gt; 5 V239 0.920690 0.920690 0.012693 #&gt; #&gt; --- #&gt; variable relative_importance scaled_importance percentage #&gt; 85 V138 0.720881 0.720881 0.009938 #&gt; 86 V188 0.718535 0.718535 0.009906 #&gt; 87 V259 0.716001 0.716001 0.009871 #&gt; 88 V179 0.714563 0.714563 0.009851 #&gt; 89 V218 0.710374 0.710374 0.009793 #&gt; 90 V33 0.676190 0.676190 0.009322 One performance metric we are usually interested in is the mean per class error for training and validation data. h2o.mean_per_class_error(dl_model, train = TRUE, valid = TRUE, xval = TRUE) #&gt; train valid xval #&gt; 0.00915 0.18326 0.23506 The confusion matrix tells us, how many classes have been predicted correctly and how many predictions were accurate. Here, we see the errors in predictions on validation data. h2o.confusionMatrix(dl_model, valid = TRUE) #&gt; Confusion Matrix (vertical: actual; across: predicted) for max f1 @ threshold = 0.000560086240199644: #&gt; arrhythmia healthy Error Rate #&gt; arrhythmia 17 8 0.320000 =8/25 #&gt; healthy 2 41 0.046512 =2/43 #&gt; Totals 19 49 0.147059 =10/68 We can also plot the classification error over all epochs or samples. plot(dl_model, timestep = &quot;epochs&quot;, metric = &quot;classification_error&quot;) plot(dl_model, timestep = &quot;samples&quot;, metric = &quot;classification_error&quot;) Next to the classification error, we are usually interested in the logistic loss (negative log-likelihood or log loss). It describes the sum of errors for each sample in the training or validation data or the negative logarithm of the likelihood of error for a given prediction/ classification. Simply put, the lower the loss, the better the model (if we ignore potential overfitting). plot(dl_model, timestep = &quot;epochs&quot;, metric = &quot;logloss&quot;) We can also plot the mean squared error (MSE). The MSE tells us the average of the prediction errors squared, i.e. the estimator’s variance and bias. The closer to zero, the better a model. plot(dl_model, timestep = &quot;epochs&quot;, metric = &quot;rmse&quot;) Next, we want to know the area under the curve (AUC). AUC is an important metric for measuring binary classification model performances. It gives the area under the curve, i.e. the integral, of true positive vs false positive rates. The closer to 1, the better a model. h2o.auc(dl_model, train = TRUE) #&gt; [1] 0.989 h2o.auc(dl_model, valid = TRUE) #&gt; [1] 0.862 h2o.auc(dl_model, xval = TRUE) #&gt; [1] 0.845 The weights for connecting two adjacent layers and per-neuron biases that we specified the model to save, can be accessed with: w &lt;- h2o.weights(dl_model, matrix_id = 1) b &lt;- h2o.biases(dl_model, vector_id = 1) Variable importance can be extracted as well (but keep in mind, that variable importance in deep neural networks is difficult to assess and should be considered only as rough estimates). h2o.varimp(dl_model) #&gt; Variable Importances: #&gt; variable relative_importance scaled_importance percentage #&gt; 1 V169 1.000000 1.000000 0.013786 #&gt; 2 V15 0.972648 0.972648 0.013409 #&gt; 3 V103 0.947561 0.947561 0.013063 #&gt; 4 V112 0.923257 0.923257 0.012728 #&gt; 5 V239 0.920690 0.920690 0.012693 #&gt; #&gt; --- #&gt; variable relative_importance scaled_importance percentage #&gt; 85 V138 0.720881 0.720881 0.009938 #&gt; 86 V188 0.718535 0.718535 0.009906 #&gt; 87 V259 0.716001 0.716001 0.009871 #&gt; 88 V179 0.714563 0.714563 0.009851 #&gt; 89 V218 0.710374 0.710374 0.009793 #&gt; 90 V33 0.676190 0.676190 0.009322 h2o.varimp_plot(dl_model) 36.7 Test data Now that we have a good idea about model performance on validation data, we want to know how it performed on unseen test data. A good model should find an optimal balance between accuracy on training and test data. A model that has 0% error on the training data but 40% error on the test data is in effect useless. It overfit on the training data and is thus not able to generalize to unknown data. perf &lt;- h2o.performance(dl_model, test) perf #&gt; H2OBinomialMetrics: deeplearning #&gt; #&gt; MSE: 0.236 #&gt; RMSE: 0.485 #&gt; LogLoss: 1.78 #&gt; Mean Per-Class Error: 0.303 #&gt; AUC: 0.806 #&gt; pr_auc: 0.823 #&gt; Gini: 0.613 #&gt; #&gt; Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold: #&gt; arrhythmia healthy Error Rate #&gt; arrhythmia 12 15 0.555556 =15/27 #&gt; healthy 2 37 0.051282 =2/39 #&gt; Totals 14 52 0.257576 =17/66 #&gt; #&gt; Maximum Metrics: Maximum metrics at their respective thresholds #&gt; metric threshold value idx #&gt; 1 max f1 0.000004 0.813187 51 #&gt; 2 max f2 0.000000 0.902778 59 #&gt; 3 max f0point5 0.134882 0.811518 37 #&gt; 4 max accuracy 0.134882 0.772727 37 #&gt; 5 max precision 0.992380 1.000000 0 #&gt; 6 max recall 0.000000 1.000000 59 #&gt; 7 max specificity 0.992380 1.000000 0 #&gt; 8 max absolute_mcc 0.134882 0.532837 37 #&gt; 9 max min_per_class_accuracy 0.456912 0.740741 35 #&gt; 10 max mean_per_class_accuracy 0.134882 0.767806 37 #&gt; #&gt; Gains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)` Plotting the test performance’s AUC plot shows us approximately how good the predictions are. plot(perf) We also want to know the log loss, MSE and AUC values, as well as other model metrics for the test data: h2o.logloss(perf) #&gt; [1] 1.78 h2o.mse(perf) #&gt; [1] 0.236 h2o.auc(perf) #&gt; [1] 0.806 head(h2o.metric(perf)) #&gt; Metrics for Thresholds: Binomial metrics as a function of classification thresholds #&gt; threshold f1 f2 f0point5 accuracy precision recall #&gt; 1 0.992380 0.050000 0.031847 0.116279 0.424242 1.000000 0.025641 #&gt; 2 0.992376 0.097561 0.063291 0.212766 0.439394 1.000000 0.051282 #&gt; 3 0.991993 0.142857 0.094340 0.294118 0.454545 1.000000 0.076923 #&gt; 4 0.991957 0.186047 0.125000 0.363636 0.469697 1.000000 0.102564 #&gt; 5 0.991921 0.227273 0.155280 0.423729 0.484848 1.000000 0.128205 #&gt; 6 0.991158 0.266667 0.185185 0.476190 0.500000 1.000000 0.153846 #&gt; specificity absolute_mcc min_per_class_accuracy mean_per_class_accuracy #&gt; 1 1.000000 0.103203 0.025641 0.512821 #&gt; 2 1.000000 0.147087 0.051282 0.525641 #&gt; 3 1.000000 0.181568 0.076923 0.538462 #&gt; 4 1.000000 0.211341 0.102564 0.551282 #&gt; 5 1.000000 0.238215 0.128205 0.564103 #&gt; 6 1.000000 0.263117 0.153846 0.576923 #&gt; tns fns fps tps tnr fnr fpr tpr idx #&gt; 1 27 38 0 1 1.000000 0.974359 0.000000 0.025641 0 #&gt; 2 27 37 0 2 1.000000 0.948718 0.000000 0.051282 1 #&gt; 3 27 36 0 3 1.000000 0.923077 0.000000 0.076923 2 #&gt; 4 27 35 0 4 1.000000 0.897436 0.000000 0.102564 3 #&gt; 5 27 34 0 5 1.000000 0.871795 0.000000 0.128205 4 #&gt; 6 27 33 0 6 1.000000 0.846154 0.000000 0.153846 5 The confusion matrix alone can be seen with the h2o.confusionMatrix() function, but is is also part of the performance summary. h2o.confusionMatrix(dl_model, test) #&gt; Confusion Matrix (vertical: actual; across: predicted) for max f1 @ threshold = 4.46418270603299e-06: #&gt; arrhythmia healthy Error Rate #&gt; arrhythmia 12 15 0.555556 =15/27 #&gt; healthy 2 37 0.051282 =2/39 #&gt; Totals 14 52 0.257576 =17/66 The final predictions with probabilities can be extracted with the h2o.predict() function. Beware though, that the number of correct and wrong classifications can be slightly different from the confusion matrix above. Here, I combine the predictions with the actual test diagnoses and classes into a data frame. For plotting I also want to have a column, that tells me whether the predictions were correct. By default, a prediction probability above 0.5 will get scored as a prediction for the respective category. I find it often makes sense to be more stringent with this, though and set a higher threshold. Therefore, I am creating another column with stringent predictions, where I only count predictions that were made with more than 80% probability. Everything that does not fall within this range gets scored as “uncertain”. For these stringent predictions, I am also creating a column that tells me whether they were accurate. finalRf_predictions &lt;- data.frame(class = as.vector(test$class), actual = as.vector(test$diagnosis), as.data.frame(h2o.predict(object = dl_model, newdata = test))) #&gt; | | | 0% | |=================================================================| 100% finalRf_predictions$accurate &lt;- ifelse( finalRf_predictions$actual == finalRf_predictions$predict, &quot;yes&quot;, &quot;no&quot;) finalRf_predictions$predict_stringent &lt;- ifelse( finalRf_predictions$arrhythmia &gt; 0.8, &quot;arrhythmia&quot;, ifelse(finalRf_predictions$healthy &gt; 0.8, &quot;healthy&quot;, &quot;uncertain&quot;)) finalRf_predictions$accurate_stringent &lt;- ifelse( finalRf_predictions$actual == finalRf_predictions$predict_stringent, &quot;yes&quot;, ifelse(finalRf_predictions$predict_stringent == &quot;uncertain&quot;, &quot;na&quot;, &quot;no&quot;)) finalRf_predictions %&gt;% group_by(actual, predict) %&gt;% summarise(n = n()) #&gt; # A tibble: 4 x 3 #&gt; # Groups: actual [2] #&gt; actual predict n #&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 arrhythmia arrhythmia 16 #&gt; 2 arrhythmia healthy 11 #&gt; 3 healthy arrhythmia 7 #&gt; 4 healthy healthy 32 finalRf_predictions %&gt;% group_by(actual, predict_stringent) %&gt;% summarise(n = n()) #&gt; # A tibble: 6 x 3 #&gt; # Groups: actual [2] #&gt; actual predict_stringent n #&gt; &lt;fct&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 arrhythmia arrhythmia 20 #&gt; 2 arrhythmia healthy 6 #&gt; 3 arrhythmia uncertain 1 #&gt; 4 healthy arrhythmia 9 #&gt; 5 healthy healthy 27 #&gt; 6 healthy uncertain 3 To get a better overview, I am going to plot the predictions (default and stringent): p1 &lt;- finalRf_predictions %&gt;% ggplot(aes(x = actual, fill = accurate)) + geom_bar(position = &quot;dodge&quot;) + scale_fill_brewer(palette = &quot;Set1&quot;) + my_theme() + labs(fill = &quot;Were\\npredictions\\naccurate?&quot;, title = &quot;Default predictions&quot;) p2 &lt;- finalRf_predictions %&gt;% subset(accurate_stringent != &quot;na&quot;) %&gt;% ggplot(aes(x = actual, fill = accurate_stringent)) + geom_bar(position = &quot;dodge&quot;) + scale_fill_brewer(palette = &quot;Set1&quot;) + my_theme() + labs(fill = &quot;Were\\npredictions\\naccurate?&quot;, title = &quot;Stringent predictions&quot;) grid.arrange(p1, p2, ncol = 2) Being more stringent with the prediction threshold slightly reduced the number of errors but not by much. I also want to know whether there are certain classes of arrhythmia that are especially prone to being misclassified: p1 &lt;- subset(finalRf_predictions, actual == &quot;arrhythmia&quot;) %&gt;% ggplot(aes(x = predict, fill = class)) + geom_bar(position = &quot;dodge&quot;) + my_theme() + labs(title = &quot;Prediction accuracy of arrhythmia cases&quot;, subtitle = &quot;Default predictions&quot;, x = &quot;predicted to be&quot;) p2 &lt;- subset(finalRf_predictions, actual == &quot;arrhythmia&quot;) %&gt;% ggplot(aes(x = predict_stringent, fill = class)) + geom_bar(position = &quot;dodge&quot;) + my_theme() + labs(title = &quot;Prediction accuracy of arrhythmia cases&quot;, subtitle = &quot;Stringent predictions&quot;, x = &quot;predicted to be&quot;) grid.arrange(p1, p2, ncol = 2) There are no obvious biases towards some classes but with the small number of samples for most classes, this is difficult to assess. 36.8 Final conclusions: How useful is the model? Most samples were classified correctly, but the total error was not particularly good. Moreover, when evaluating the usefulness of a specific model, we need to keep in mind what we want to achieve with it and which questions we want to answer. If we wanted to deploy this model in a clinical setting, it should assist with diagnosing patients. So, we need to think about what the consequences of wrong classifications would be. Would it be better to optimize for high sensitivity, in this example as many arrhythmia cases as possible get detected - with the drawback that we probably also diagnose a few healthy people? Or do we want to maximize precision, meaning that we could be confident that a patient who got predicted to have arrhythmia does indeed have it, while accepting that a few arrhythmia cases would remain undiagnosed? When we consider stringent predictions, this model correctly classified 19 out of 27 arrhythmia cases, but 6 were misdiagnosed. This would mean that some patients who were actually sick, wouldn’t have gotten the correct treatment (if decided solely based on this model). For real-life application, this is obviously not sufficient! Next week, I’ll be trying to improve the model by doing a grid search for hyper-parameter tuning. So, stay tuned… (sorry, couldn’t resist ;-)) sessionInfo() #&gt; R version 3.6.0 (2019-04-26) #&gt; Platform: x86_64-pc-linux-gnu (64-bit) #&gt; Running under: Ubuntu 18.04.3 LTS #&gt; #&gt; Matrix products: default #&gt; BLAS/LAPACK: /usr/lib/x86_64-linux-gnu/libopenblasp-r0.2.20.so #&gt; #&gt; locale: #&gt; [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C #&gt; [3] LC_TIME=en_US.UTF-8 LC_COLLATE=en_US.UTF-8 #&gt; [5] LC_MONETARY=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8 #&gt; [7] LC_PAPER=en_US.UTF-8 LC_NAME=C #&gt; [9] LC_ADDRESS=C LC_TELEPHONE=C #&gt; [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C #&gt; #&gt; attached base packages: #&gt; [1] stats4 parallel grid stats graphics grDevices utils #&gt; [8] datasets methods base #&gt; #&gt; other attached packages: #&gt; [1] reshape2_1.4.3 tidyr_0.8.3 matrixStats_0.54.0 #&gt; [4] pcaGoPromoter_1.28.0 Biostrings_2.52.0 XVector_0.24.0 #&gt; [7] IRanges_2.18.0 S4Vectors_0.22.0 BiocGenerics_0.30.0 #&gt; [10] ellipse_0.4.1 gridExtra_2.3 ggrepel_0.8.1 #&gt; [13] ggplot2_3.1.1 h2o_3.22.1.1 dplyr_0.8.0.1 #&gt; [16] logging_0.9-107 #&gt; #&gt; loaded via a namespace (and not attached): #&gt; [1] Rcpp_1.0.1 assertthat_0.2.1 zeallot_0.1.0 #&gt; [4] rprojroot_1.3-2 digest_0.6.18 utf8_1.1.4 #&gt; [7] R6_2.4.0 plyr_1.8.4 backports_1.1.4 #&gt; [10] RSQLite_2.1.1 evaluate_0.13 pillar_1.4.0 #&gt; [13] zlibbioc_1.30.0 rlang_0.3.4 lazyeval_0.2.2 #&gt; [16] rstudioapi_0.10 blob_1.1.1 rmarkdown_1.12 #&gt; [19] labeling_0.3 stringr_1.4.0 RCurl_1.95-4.12 #&gt; [22] bit_1.1-14 munsell_0.5.0 compiler_3.6.0 #&gt; [25] xfun_0.6 pkgconfig_2.0.2 htmltools_0.3.6 #&gt; [28] tidyselect_0.2.5 tibble_2.1.1 bookdown_0.10 #&gt; [31] fansi_0.4.0 crayon_1.3.4 withr_2.1.2 #&gt; [34] bitops_1.0-6 jsonlite_1.6 gtable_0.3.0 #&gt; [37] DBI_1.0.0 magrittr_1.5 scales_1.0.0 #&gt; [40] cli_1.1.0 stringi_1.4.3 vctrs_0.1.0 #&gt; [43] RColorBrewer_1.1-2 tools_3.6.0 bit64_0.9-7 #&gt; [46] Biobase_2.44.0 glue_1.3.1 purrr_0.3.2 #&gt; [49] yaml_2.2.0 AnnotationDbi_1.46.0 colorspace_1.4-1 #&gt; [52] memoise_1.1.0 knitr_1.22 "],
["credit-scoring.html", "Chapter 37 Credit Scoring 37.1 Introduction 37.2 Motivation 37.3 load the data 37.4 Objective 37.5 Steps 37.6 Test the neural network", " Chapter 37 Credit Scoring 37.1 Introduction Source: https://www.r-bloggers.com/using-neural-networks-for-credit-scoring-a-simple-example/ 37.2 Motivation Credit scoring is the practice of analysing a persons background and credit application in order to assess the creditworthiness of the person. One can take numerous approaches on analysing this creditworthiness. In the end it basically comes down to first selecting the correct independent variables (e.g. income, age, gender) that lead to a given level of creditworthiness. In other words: creditworthiness = f(income, age, gender, …). A creditscoring system can be represented by linear regression, logistic regression, machine learning or a combination of these. Neural networks are situated in the domain of machine learning. The following is an strongly simplified example. The actual procedure of building a credit scoring system is much more complex and the resulting model will most likely not consist of solely or even a neural network. If you’re unsure on what a neural network exactly is, I find this a good place to start. For this example the R package neuralnet is used, for a more in-depth view on the exact workings of the package see neuralnet: Training of Neural Networks by F. Günther and S. Fritsch. 37.3 load the data Dataset downloaded: https://gist.github.com/Bart6114/8675941#file-creditset-csv set.seed(1234567890) library(neuralnet) dataset &lt;- read.csv(file.path(data_raw_dir, &quot;creditset.csv&quot;)) head(dataset) #&gt; clientid income age loan LTI default10yr #&gt; 1 1 66156 59.0 8106.5 0.122537 0 #&gt; 2 2 34415 48.1 6564.7 0.190752 0 #&gt; 3 3 57317 63.1 8021.0 0.139940 0 #&gt; 4 4 42710 45.8 6103.6 0.142911 0 #&gt; 5 5 66953 18.6 8770.1 0.130989 1 #&gt; 6 6 24904 57.5 15.5 0.000622 0 names(dataset) #&gt; [1] &quot;clientid&quot; &quot;income&quot; &quot;age&quot; &quot;loan&quot; &quot;LTI&quot; #&gt; [6] &quot;default10yr&quot; summary(dataset) #&gt; clientid income age loan #&gt; Min. : 1 Min. :20014 Min. :18.1 Min. : 1 #&gt; 1st Qu.: 501 1st Qu.:32796 1st Qu.:29.1 1st Qu.: 1940 #&gt; Median :1000 Median :45789 Median :41.4 Median : 3975 #&gt; Mean :1000 Mean :45332 Mean :40.9 Mean : 4444 #&gt; 3rd Qu.:1500 3rd Qu.:57791 3rd Qu.:52.6 3rd Qu.: 6432 #&gt; Max. :2000 Max. :69996 Max. :64.0 Max. :13766 #&gt; LTI default10yr #&gt; Min. :0.0000 Min. :0.000 #&gt; 1st Qu.:0.0479 1st Qu.:0.000 #&gt; Median :0.0994 Median :0.000 #&gt; Mean :0.0984 Mean :0.142 #&gt; 3rd Qu.:0.1476 3rd Qu.:0.000 #&gt; Max. :0.1999 Max. :1.000 # distribution of defaults table(dataset$default10yr) #&gt; #&gt; 0 1 #&gt; 1717 283 min(dataset$LTI) #&gt; [1] 4.91e-05 plot(jitter(dataset$default10yr, 1) ~ jitter(dataset$LTI, 2)) # convert LTI continuous variable to categorical dataset$LTIrng &lt;- cut(dataset$LTI, breaks = 10) unique(dataset$LTIrng) #&gt; [1] (0.12,0.14] (0.18,0.2] (0.14,0.16] (-0.000151,0.02] #&gt; [5] (0.1,0.12] (0.04,0.06] (0.06,0.08] (0.08,0.1] #&gt; [9] (0.16,0.18] (0.02,0.04] #&gt; 10 Levels: (-0.000151,0.02] (0.02,0.04] (0.04,0.06] ... (0.18,0.2] plot(dataset$LTIrng, dataset$default10yr) # what age and LTI is more likely to default library(ggplot2) #&gt; Registered S3 methods overwritten by &#39;ggplot2&#39;: #&gt; method from #&gt; [.quosures rlang #&gt; c.quosures rlang #&gt; print.quosures rlang ggplot(dataset, aes(x = age, y = LTI, col = default10yr)) + geom_point() # what age and loan size is more likely to default library(ggplot2) ggplot(dataset, aes(x = age, y = loan, col = default10yr)) + geom_point() 37.4 Objective The dataset contains information on different clients who received a loan at least 10 years ago. The variables income (yearly), age, loan (size in euros) and LTI (the loan to yearly income ratio) are available. Our goal is to devise a model which predicts, based on the input variables LTI and age, whether or not a default will occur within 10 years. 37.5 Steps The dataset will be split up in a subset used for training the neural network and another set used for testing. As the ordering of the dataset is completely random, we do not have to extract random rows and can just take the first x rows. ## extract a set to train the NN trainset &lt;- dataset[1:800, ] ## select the test set testset &lt;- dataset[801:2000, ] 37.5.1 Build the neural network Now we’ll build a neural network with 4 hidden nodes (a neural network is comprised of an input, hidden and output nodes). The number of nodes is chosen here without a clear method, however there are some rules of thumb. The lifesign option refers to the verbosity. The ouput is not linear and we will use a threshold value of 10%. The neuralnet package uses resilient backpropagation with weight backtracking as its standard algorithm. ## build the neural network (NN) creditnet &lt;- neuralnet(default10yr ~ LTI + age, trainset, hidden = 4, lifesign = &quot;minimal&quot;, linear.output = FALSE, threshold = 0.1) #&gt; hidden: 4 thresh: 0.1 rep: 1/1 steps: 44487 error: 0.20554 time: 9.6 secs The neuralnet package also has the possibility to visualize the generated model and show the found weights. ## plot the NN plot(creditnet, rep = &quot;best&quot;) 37.6 Test the neural network Once we’ve trained the neural network we are ready to test it. We use the testset subset for this. The compute function is applied for computing the outputs based on the LTI and age inputs from the testset. ## test the resulting output temp_test &lt;- subset(testset, select = c(&quot;LTI&quot;, &quot;age&quot;)) creditnet.results &lt;- compute(creditnet, temp_test) The temp dataset contains only the columns LTI and age of the train set. Only these variables are used for input. The set looks as follows: head(temp_test) #&gt; LTI age #&gt; 801 0.0231 25.9 #&gt; 802 0.1373 40.8 #&gt; 803 0.1046 32.5 #&gt; 804 0.1599 53.2 #&gt; 805 0.1116 46.5 #&gt; 806 0.1149 47.1 Let’s have a look at what the neural network produced: results &lt;- data.frame(actual = testset$default10yr, prediction = creditnet.results$net.result) results[100:115, ] #&gt; actual prediction #&gt; 900 0 7.29e-32 #&gt; 901 0 8.17e-11 #&gt; 902 0 4.33e-45 #&gt; 903 1 1.00e+00 #&gt; 904 0 8.06e-04 #&gt; 905 0 3.54e-40 #&gt; 906 0 1.48e-24 #&gt; 907 1 1.00e+00 #&gt; 908 0 1.11e-02 #&gt; 909 0 8.05e-44 #&gt; 910 0 6.72e-07 #&gt; 911 1 1.00e+00 #&gt; 912 0 9.97e-59 #&gt; 913 1 1.00e+00 #&gt; 914 0 3.39e-37 #&gt; 915 0 1.18e-07 We can round to the nearest integer to improve readability: results$prediction &lt;- round(results$prediction) results[100:115, ] #&gt; actual prediction #&gt; 900 0 0 #&gt; 901 0 0 #&gt; 902 0 0 #&gt; 903 1 1 #&gt; 904 0 0 #&gt; 905 0 0 #&gt; 906 0 0 #&gt; 907 1 1 #&gt; 908 0 0 #&gt; 909 0 0 #&gt; 910 0 0 #&gt; 911 1 1 #&gt; 912 0 0 #&gt; 913 1 1 #&gt; 914 0 0 #&gt; 915 0 0 As you can see it is pretty close! As already stated, this is a strongly simplified example. But it might serve as a basis for you to play around with your first neural network. # how many predictions were wrong indices &lt;- which(results$actual != results$prediction) indices #&gt; [1] 330 1008 # what are the predictions that failed results[indices,] #&gt; actual prediction #&gt; 1130 0 1 #&gt; 1808 1 0 "],
["build-a-fully-connected-neural-network-from-scratch.html", "Chapter 38 Build a fully connected neural network from scratch 38.1 Introduction", " Chapter 38 Build a fully connected neural network from scratch 38.1 Introduction http://www.parallelr.com/r-deep-neural-network-from-scratch/ library(neuralnet) # Copyright 2016: www.ParallelR.com # Parallel Blog : R For Deep Learning (I): Build Fully Connected Neural Network From Scratch # Classification by 2-layers DNN and tested by iris dataset # Author: Peng Zhao, patric.zhao@gmail.com # Prediction predict.dnn &lt;- function(model, data = X.test) { # new data, transfer to matrix new.data &lt;- data.matrix(data) # Feed Forwad hidden.layer &lt;- sweep(new.data %*% model$W1 ,2, model$b1, &#39;+&#39;) # neurons : Rectified Linear hidden.layer &lt;- pmax(hidden.layer, 0) score &lt;- sweep(hidden.layer %*% model$W2, 2, model$b2, &#39;+&#39;) # Loss Function: softmax score.exp &lt;- exp(score) probs &lt;-sweep(score.exp, 1, rowSums(score.exp), &#39;/&#39;) # select max possiblity labels.predicted &lt;- max.col(probs) return(labels.predicted) } # Train: build and train a 2-layers neural network train.dnn &lt;- function(x, y, traindata=data, testdata=NULL, model = NULL, # set hidden layers and neurons # currently, only support 1 hidden layer hidden=c(6), # max iteration steps maxit=2000, # delta loss abstol=1e-2, # learning rate lr = 1e-2, # regularization rate reg = 1e-3, # show results every &#39;display&#39; step display = 100, random.seed = 1) { # to make the case reproducible. set.seed(random.seed) # total number of training set N &lt;- nrow(traindata) # extract the data and label # don&#39;t need atribute X &lt;- unname(data.matrix(traindata[,x])) # correct categories represented by integer Y &lt;- traindata[,y] if(is.factor(Y)) { Y &lt;- as.integer(Y) } # create index for both row and col # create index for both row and col Y.len &lt;- length(unique(Y)) Y.set &lt;- sort(unique(Y)) Y.index &lt;- cbind(1:N, match(Y, Y.set)) # create model or get model from parameter if(is.null(model)) { # number of input features D &lt;- ncol(X) # number of categories for classification K &lt;- length(unique(Y)) H &lt;- hidden # create and init weights and bias W1 &lt;- 0.01*matrix(rnorm(D*H), nrow=D, ncol=H) b1 &lt;- matrix(0, nrow=1, ncol=H) W2 &lt;- 0.01*matrix(rnorm(H*K), nrow=H, ncol=K) b2 &lt;- matrix(0, nrow=1, ncol=K) } else { D &lt;- model$D K &lt;- model$K H &lt;- model$H W1 &lt;- model$W1 b1 &lt;- model$b1 W2 &lt;- model$W2 b2 &lt;- model$b2 } # use all train data to update weights since it&#39;s a small dataset batchsize &lt;- N # init loss to a very big value loss &lt;- 100000 # Training the network i &lt;- 0 while(i &lt; maxit &amp;&amp; loss &gt; abstol ) { # iteration index i &lt;- i +1 # forward .... # 1 indicate row, 2 indicate col hidden.layer &lt;- sweep(X %*% W1 ,2, b1, &#39;+&#39;) # neurons : ReLU hidden.layer &lt;- pmax(hidden.layer, 0) score &lt;- sweep(hidden.layer %*% W2, 2, b2, &#39;+&#39;) # softmax score.exp &lt;- exp(score) # debug probs &lt;- score.exp/rowSums(score.exp) # compute the loss corect.logprobs &lt;- -log(probs[Y.index]) data.loss &lt;- sum(corect.logprobs)/batchsize reg.loss &lt;- 0.5*reg* (sum(W1*W1) + sum(W2*W2)) loss &lt;- data.loss + reg.loss # display results and update model if( i %% display == 0) { if(!is.null(testdata)) { model &lt;- list( D = D, H = H, K = K, # weights and bias W1 = W1, b1 = b1, W2 = W2, b2 = b2) labs &lt;- predict.dnn(model, testdata[,-y]) accuracy &lt;- mean(as.integer(testdata[,y]) == Y.set[labs]) cat(i, loss, accuracy, &quot;\\n&quot;) } else { cat(i, loss, &quot;\\n&quot;) } } # backward .... dscores &lt;- probs dscores[Y.index] &lt;- dscores[Y.index] -1 dscores &lt;- dscores / batchsize dW2 &lt;- t(hidden.layer) %*% dscores db2 &lt;- colSums(dscores) dhidden &lt;- dscores %*% t(W2) dhidden[hidden.layer &lt;= 0] &lt;- 0 dW1 &lt;- t(X) %*% dhidden db1 &lt;- colSums(dhidden) # update .... dW2 &lt;- dW2 + reg*W2 dW1 &lt;- dW1 + reg*W1 W1 &lt;- W1 - lr * dW1 b1 &lt;- b1 - lr * db1 W2 &lt;- W2 - lr * dW2 b2 &lt;- b2 - lr * db2 } # final results # creat list to store learned parameters # you can add more parameters for debug and visualization # such as residuals, fitted.values ... model &lt;- list( D = D, H = H, K = K, # weights and bias W1= W1, b1= b1, W2= W2, b2= b2) return(model) } ######################################################################## # testing ####################################################################### set.seed(1) # 0. EDA summary(iris) #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width #&gt; Min. :4.30 Min. :2.00 Min. :1.00 Min. :0.1 #&gt; 1st Qu.:5.10 1st Qu.:2.80 1st Qu.:1.60 1st Qu.:0.3 #&gt; Median :5.80 Median :3.00 Median :4.35 Median :1.3 #&gt; Mean :5.84 Mean :3.06 Mean :3.76 Mean :1.2 #&gt; 3rd Qu.:6.40 3rd Qu.:3.30 3rd Qu.:5.10 3rd Qu.:1.8 #&gt; Max. :7.90 Max. :4.40 Max. :6.90 Max. :2.5 #&gt; Species #&gt; setosa :50 #&gt; versicolor:50 #&gt; virginica :50 #&gt; #&gt; #&gt; plot(iris) # 1. split data into test/train samp &lt;- c(sample(1:50,25), sample(51:100,25), sample(101:150,25)) # 2. train model ir.model &lt;- train.dnn(x=1:4, y=5, traindata=iris[samp,], testdata=iris[-samp,], hidden=10, maxit=2000, display=50) #&gt; 50 1.1 0.333 #&gt; 100 1.1 0.333 #&gt; 150 1.09 0.333 #&gt; 200 1.08 0.333 #&gt; 250 1.05 0.333 #&gt; 300 1 0.333 #&gt; 350 0.933 0.667 #&gt; 400 0.855 0.667 #&gt; 450 0.775 0.667 #&gt; 500 0.689 0.667 #&gt; 550 0.611 0.68 #&gt; 600 0.552 0.693 #&gt; 650 0.507 0.747 #&gt; 700 0.473 0.84 #&gt; 750 0.445 0.88 #&gt; 800 0.421 0.92 #&gt; 850 0.399 0.947 #&gt; 900 0.379 0.96 #&gt; 950 0.36 0.96 #&gt; 1000 0.341 0.973 #&gt; 1050 0.324 0.973 #&gt; 1100 0.307 0.973 #&gt; 1150 0.292 0.973 #&gt; 1200 0.277 0.973 #&gt; 1250 0.263 0.973 #&gt; 1300 0.25 0.973 #&gt; 1350 0.238 0.973 #&gt; 1400 0.227 0.973 #&gt; 1450 0.216 0.973 #&gt; 1500 0.207 0.973 #&gt; 1550 0.198 0.973 #&gt; 1600 0.19 0.973 #&gt; 1650 0.183 0.973 #&gt; 1700 0.176 0.973 #&gt; 1750 0.17 0.973 #&gt; 1800 0.164 0.973 #&gt; 1850 0.158 0.973 #&gt; 1900 0.153 0.973 #&gt; 1950 0.149 0.973 #&gt; 2000 0.144 0.973 # ir.model &lt;- train.dnn(x=1:4, y=5, traindata=iris[samp,], hidden=6, maxit=2000, display=50) # 3. prediction # NOTE: if the predict is factor, we need to transfer the number into class manually. # To make the code clear, I don&#39;t write this change into predict.dnn function. labels.dnn &lt;- predict.dnn(ir.model, iris[-samp, -5]) # 4. verify the results table(iris[-samp,5], labels.dnn) #&gt; labels.dnn #&gt; 1 2 3 #&gt; setosa 25 0 0 #&gt; versicolor 0 23 2 #&gt; virginica 0 0 25 # labels.dnn # 1 2 3 #setosa 25 0 0 #versicolor 0 24 1 #virginica 0 0 25 #accuracy mean(as.integer(iris[-samp, 5]) == labels.dnn) #&gt; [1] 0.973 # 0.98 # 5. compare with nnet library(nnet) ird &lt;- data.frame(rbind(iris3[,,1], iris3[,,2], iris3[,,3]), species = factor(c(rep(&quot;s&quot;,50), rep(&quot;c&quot;, 50), rep(&quot;v&quot;, 50)))) ir.nn2 &lt;- nnet(species ~ ., data = ird, subset = samp, size = 6, rang = 0.1, decay = 1e-2, maxit = 2000) #&gt; # weights: 51 #&gt; initial value 82.293110 #&gt; iter 10 value 29.196376 #&gt; iter 20 value 5.446284 #&gt; iter 30 value 4.782022 #&gt; iter 40 value 4.379729 #&gt; iter 50 value 4.188725 #&gt; iter 60 value 4.120587 #&gt; iter 70 value 4.091706 #&gt; iter 80 value 4.086017 #&gt; iter 90 value 4.081664 #&gt; iter 100 value 4.074111 #&gt; iter 110 value 4.072894 #&gt; iter 120 value 4.069011 #&gt; iter 130 value 4.067690 #&gt; iter 140 value 4.067633 #&gt; final value 4.067633 #&gt; converged labels.nnet &lt;- predict(ir.nn2, ird[-samp,], type=&quot;class&quot;) table(ird$species[-samp], labels.nnet) #&gt; labels.nnet #&gt; c s v #&gt; c 23 0 2 #&gt; s 0 25 0 #&gt; v 0 0 25 # labels.nnet # c s v #c 22 0 3 #s 0 25 0 #v 3 0 22 # accuracy mean(ird$species[-samp] == labels.nnet) #&gt; [1] 0.973 # 0.96 # Visualization # the output from screen, copy and paste here. data1 &lt;- (&quot;i loss accuracy 50 1.098421 0.3333333 100 1.098021 0.3333333 150 1.096843 0.3333333 200 1.093393 0.3333333 250 1.084069 0.3333333 300 1.063278 0.3333333 350 1.027273 0.3333333 400 0.9707605 0.64 450 0.8996356 0.6666667 500 0.8335469 0.6666667 550 0.7662386 0.6666667 600 0.6914156 0.6666667 650 0.6195753 0.68 700 0.5620381 0.68 750 0.5184008 0.7333333 800 0.4844815 0.84 850 0.4568258 0.8933333 900 0.4331083 0.92 950 0.4118948 0.9333333 1000 0.392368 0.96 1050 0.3740457 0.96 1100 0.3566594 0.96 1150 0.3400993 0.9866667 1200 0.3243276 0.9866667 1250 0.3093422 0.9866667 1300 0.2951787 0.9866667 1350 0.2818472 0.9866667 1400 0.2693641 0.9866667 1450 0.2577245 0.9866667 1500 0.2469068 0.9866667 1550 0.2368819 0.9866667 1600 0.2276124 0.9866667 1650 0.2190535 0.9866667 1700 0.2111565 0.9866667 1750 0.2038719 0.9866667 1800 0.1971507 0.9866667 1850 0.1909452 0.9866667 1900 0.1852105 0.9866667 1950 0.1799045 0.9866667 2000 0.1749881 0.9866667 &quot;) data.v &lt;- read.table(text=data1, header=T) par(mar=c(5.1, 4.1, 4.1, 4.1)) plot(x=data.v$i, y=data.v$loss, type=&quot;o&quot;, col=&quot;blue&quot;, pch=16, main=&quot;IRIS loss and accuracy by 2-layers DNN&quot;, ylim=c(0, 1.2), xlab=&quot;&quot;, ylab=&quot;&quot;, axe =F) lines(x=data.v$i, y=data.v$accuracy, type=&quot;o&quot;, col=&quot;red&quot;, pch=1) box() axis(1, at=seq(0,2000,by=200)) axis(4, at=seq(0,1.0,by=0.1)) axis(2, at=seq(0,1.2,by=0.1)) mtext(&quot;training step&quot;, 1, line=3) mtext(&quot;loss of training set&quot;, 2, line=2.5) mtext(&quot;accuracy of testing set&quot;, 4, line=2) legend(&quot;bottomleft&quot;, legend = c(&quot;loss&quot;, &quot;accuracy&quot;), pch = c(16,1), col = c(&quot;blue&quot;,&quot;red&quot;), lwd=c(1,1) ) "],
["wine-with-neuralnet.html", "Chapter 39 Wine with neuralnet 39.1 The dataset 39.2 Preprocessing 39.3 Fitting the model with neuralnet 39.4 Cross validating the classifier", " Chapter 39 Wine with neuralnet Source: https://www.r-bloggers.com/multilabel-classification-with-neuralnet-package/ The neuralnet package is perhaps not the best option in R for using neural networks. If you ask why, for starters it does not recognize the typical formula y~., it does not support factors, it does not provide a lot of models other than a standard MLP, and it has great competitors in the nnet package that seems to be better integrated in R and can be used with the caret package, and in the MXnet package that is a high level deep learning library which provides a wide variety of neural networks. But still, I think there is some value in the ease of use of the neuralnet package, especially for a beginner, therefore I’ll be using it. I’m going to be using both the neuralnet and, curiously enough, the nnet package. Let’s load them: # load libs require(neuralnet) #&gt; Loading required package: neuralnet require(nnet) #&gt; Loading required package: nnet require(ggplot2) #&gt; Loading required package: ggplot2 #&gt; Registered S3 methods overwritten by &#39;ggplot2&#39;: #&gt; method from #&gt; [.quosures rlang #&gt; c.quosures rlang #&gt; print.quosures rlang set.seed(10) 39.1 The dataset I looked in the UCI Machine Learning Repository1 and found the wine dataset. This dataset contains the results of a chemical analysis on 3 different kind of wines. The target variable is the label of the wine which is a factor with 3 (unordered) levels. The predictors are all continuous and represent 13 variables obtained as a result of chemical measurements. # get the data file from the package location wine_dataset_path &lt;- file.path(data_raw_dir, &quot;wine.data&quot;) wine_dataset_path #&gt; [1] &quot;/home/datascience/repos/machine-learning-rsuite/import/wine.data&quot; wines &lt;- read.csv(wine_dataset_path) wines #&gt; X1 X14.23 X1.71 X2.43 X15.6 X127 X2.8 X3.06 X.28 X2.29 X5.64 X1.04 #&gt; 1 1 13.2 1.78 2.14 11.2 100 2.65 2.76 0.26 1.28 4.38 1.050 #&gt; 2 1 13.2 2.36 2.67 18.6 101 2.80 3.24 0.30 2.81 5.68 1.030 #&gt; 3 1 14.4 1.95 2.50 16.8 113 3.85 3.49 0.24 2.18 7.80 0.860 #&gt; 4 1 13.2 2.59 2.87 21.0 118 2.80 2.69 0.39 1.82 4.32 1.040 #&gt; 5 1 14.2 1.76 2.45 15.2 112 3.27 3.39 0.34 1.97 6.75 1.050 #&gt; 6 1 14.4 1.87 2.45 14.6 96 2.50 2.52 0.30 1.98 5.25 1.020 #&gt; 7 1 14.1 2.15 2.61 17.6 121 2.60 2.51 0.31 1.25 5.05 1.060 #&gt; 8 1 14.8 1.64 2.17 14.0 97 2.80 2.98 0.29 1.98 5.20 1.080 #&gt; 9 1 13.9 1.35 2.27 16.0 98 2.98 3.15 0.22 1.85 7.22 1.010 #&gt; 10 1 14.1 2.16 2.30 18.0 105 2.95 3.32 0.22 2.38 5.75 1.250 #&gt; 11 1 14.1 1.48 2.32 16.8 95 2.20 2.43 0.26 1.57 5.00 1.170 #&gt; 12 1 13.8 1.73 2.41 16.0 89 2.60 2.76 0.29 1.81 5.60 1.150 #&gt; 13 1 14.8 1.73 2.39 11.4 91 3.10 3.69 0.43 2.81 5.40 1.250 #&gt; 14 1 14.4 1.87 2.38 12.0 102 3.30 3.64 0.29 2.96 7.50 1.200 #&gt; 15 1 13.6 1.81 2.70 17.2 112 2.85 2.91 0.30 1.46 7.30 1.280 #&gt; 16 1 14.3 1.92 2.72 20.0 120 2.80 3.14 0.33 1.97 6.20 1.070 #&gt; 17 1 13.8 1.57 2.62 20.0 115 2.95 3.40 0.40 1.72 6.60 1.130 #&gt; 18 1 14.2 1.59 2.48 16.5 108 3.30 3.93 0.32 1.86 8.70 1.230 #&gt; 19 1 13.6 3.10 2.56 15.2 116 2.70 3.03 0.17 1.66 5.10 0.960 #&gt; 20 1 14.1 1.63 2.28 16.0 126 3.00 3.17 0.24 2.10 5.65 1.090 #&gt; 21 1 12.9 3.80 2.65 18.6 102 2.41 2.41 0.25 1.98 4.50 1.030 #&gt; 22 1 13.7 1.86 2.36 16.6 101 2.61 2.88 0.27 1.69 3.80 1.110 #&gt; 23 1 12.8 1.60 2.52 17.8 95 2.48 2.37 0.26 1.46 3.93 1.090 #&gt; 24 1 13.5 1.81 2.61 20.0 96 2.53 2.61 0.28 1.66 3.52 1.120 #&gt; 25 1 13.1 2.05 3.22 25.0 124 2.63 2.68 0.47 1.92 3.58 1.130 #&gt; 26 1 13.4 1.77 2.62 16.1 93 2.85 2.94 0.34 1.45 4.80 0.920 #&gt; 27 1 13.3 1.72 2.14 17.0 94 2.40 2.19 0.27 1.35 3.95 1.020 #&gt; 28 1 13.9 1.90 2.80 19.4 107 2.95 2.97 0.37 1.76 4.50 1.250 #&gt; 29 1 14.0 1.68 2.21 16.0 96 2.65 2.33 0.26 1.98 4.70 1.040 #&gt; 30 1 13.7 1.50 2.70 22.5 101 3.00 3.25 0.29 2.38 5.70 1.190 #&gt; 31 1 13.6 1.66 2.36 19.1 106 2.86 3.19 0.22 1.95 6.90 1.090 #&gt; 32 1 13.7 1.83 2.36 17.2 104 2.42 2.69 0.42 1.97 3.84 1.230 #&gt; 33 1 13.8 1.53 2.70 19.5 132 2.95 2.74 0.50 1.35 5.40 1.250 #&gt; 34 1 13.5 1.80 2.65 19.0 110 2.35 2.53 0.29 1.54 4.20 1.100 #&gt; 35 1 13.5 1.81 2.41 20.5 100 2.70 2.98 0.26 1.86 5.10 1.040 #&gt; 36 1 13.3 1.64 2.84 15.5 110 2.60 2.68 0.34 1.36 4.60 1.090 #&gt; 37 1 13.1 1.65 2.55 18.0 98 2.45 2.43 0.29 1.44 4.25 1.120 #&gt; 38 1 13.1 1.50 2.10 15.5 98 2.40 2.64 0.28 1.37 3.70 1.180 #&gt; 39 1 14.2 3.99 2.51 13.2 128 3.00 3.04 0.20 2.08 5.10 0.890 #&gt; 40 1 13.6 1.71 2.31 16.2 117 3.15 3.29 0.34 2.34 6.13 0.950 #&gt; 41 1 13.4 3.84 2.12 18.8 90 2.45 2.68 0.27 1.48 4.28 0.910 #&gt; 42 1 13.9 1.89 2.59 15.0 101 3.25 3.56 0.17 1.70 5.43 0.880 #&gt; 43 1 13.2 3.98 2.29 17.5 103 2.64 2.63 0.32 1.66 4.36 0.820 #&gt; 44 1 13.1 1.77 2.10 17.0 107 3.00 3.00 0.28 2.03 5.04 0.880 #&gt; 45 1 14.2 4.04 2.44 18.9 111 2.85 2.65 0.30 1.25 5.24 0.870 #&gt; 46 1 14.4 3.59 2.28 16.0 102 3.25 3.17 0.27 2.19 4.90 1.040 #&gt; 47 1 13.9 1.68 2.12 16.0 101 3.10 3.39 0.21 2.14 6.10 0.910 #&gt; 48 1 14.1 2.02 2.40 18.8 103 2.75 2.92 0.32 2.38 6.20 1.070 #&gt; 49 1 13.9 1.73 2.27 17.4 108 2.88 3.54 0.32 2.08 8.90 1.120 #&gt; 50 1 13.1 1.73 2.04 12.4 92 2.72 3.27 0.17 2.91 7.20 1.120 #&gt; 51 1 13.8 1.65 2.60 17.2 94 2.45 2.99 0.22 2.29 5.60 1.240 #&gt; 52 1 13.8 1.75 2.42 14.0 111 3.88 3.74 0.32 1.87 7.05 1.010 #&gt; 53 1 13.8 1.90 2.68 17.1 115 3.00 2.79 0.39 1.68 6.30 1.130 #&gt; 54 1 13.7 1.67 2.25 16.4 118 2.60 2.90 0.21 1.62 5.85 0.920 #&gt; 55 1 13.6 1.73 2.46 20.5 116 2.96 2.78 0.20 2.45 6.25 0.980 #&gt; 56 1 14.2 1.70 2.30 16.3 118 3.20 3.00 0.26 2.03 6.38 0.940 #&gt; 57 1 13.3 1.97 2.68 16.8 102 3.00 3.23 0.31 1.66 6.00 1.070 #&gt; 58 1 13.7 1.43 2.50 16.7 108 3.40 3.67 0.19 2.04 6.80 0.890 #&gt; 59 2 12.4 0.94 1.36 10.6 88 1.98 0.57 0.28 0.42 1.95 1.050 #&gt; 60 2 12.3 1.10 2.28 16.0 101 2.05 1.09 0.63 0.41 3.27 1.250 #&gt; 61 2 12.6 1.36 2.02 16.8 100 2.02 1.41 0.53 0.62 5.75 0.980 #&gt; 62 2 13.7 1.25 1.92 18.0 94 2.10 1.79 0.32 0.73 3.80 1.230 #&gt; 63 2 12.4 1.13 2.16 19.0 87 3.50 3.10 0.19 1.87 4.45 1.220 #&gt; 64 2 12.2 1.45 2.53 19.0 104 1.89 1.75 0.45 1.03 2.95 1.450 #&gt; 65 2 12.4 1.21 2.56 18.1 98 2.42 2.65 0.37 2.08 4.60 1.190 #&gt; 66 2 13.1 1.01 1.70 15.0 78 2.98 3.18 0.26 2.28 5.30 1.120 #&gt; 67 2 12.4 1.17 1.92 19.6 78 2.11 2.00 0.27 1.04 4.68 1.120 #&gt; 68 2 13.3 0.94 2.36 17.0 110 2.53 1.30 0.55 0.42 3.17 1.020 #&gt; 69 2 12.2 1.19 1.75 16.8 151 1.85 1.28 0.14 2.50 2.85 1.280 #&gt; 70 2 12.3 1.61 2.21 20.4 103 1.10 1.02 0.37 1.46 3.05 0.906 #&gt; 71 2 13.9 1.51 2.67 25.0 86 2.95 2.86 0.21 1.87 3.38 1.360 #&gt; 72 2 13.5 1.66 2.24 24.0 87 1.88 1.84 0.27 1.03 3.74 0.980 #&gt; 73 2 13.0 1.67 2.60 30.0 139 3.30 2.89 0.21 1.96 3.35 1.310 #&gt; 74 2 12.0 1.09 2.30 21.0 101 3.38 2.14 0.13 1.65 3.21 0.990 #&gt; 75 2 11.7 1.88 1.92 16.0 97 1.61 1.57 0.34 1.15 3.80 1.230 #&gt; 76 2 13.0 0.90 1.71 16.0 86 1.95 2.03 0.24 1.46 4.60 1.190 #&gt; 77 2 11.8 2.89 2.23 18.0 112 1.72 1.32 0.43 0.95 2.65 0.960 #&gt; 78 2 12.3 0.99 1.95 14.8 136 1.90 1.85 0.35 2.76 3.40 1.060 #&gt; 79 2 12.7 3.87 2.40 23.0 101 2.83 2.55 0.43 1.95 2.57 1.190 #&gt; 80 2 12.0 0.92 2.00 19.0 86 2.42 2.26 0.30 1.43 2.50 1.380 #&gt; 81 2 12.7 1.81 2.20 18.8 86 2.20 2.53 0.26 1.77 3.90 1.160 #&gt; 82 2 12.1 1.13 2.51 24.0 78 2.00 1.58 0.40 1.40 2.20 1.310 #&gt; 83 2 13.1 3.86 2.32 22.5 85 1.65 1.59 0.61 1.62 4.80 0.840 #&gt; 84 2 11.8 0.89 2.58 18.0 94 2.20 2.21 0.22 2.35 3.05 0.790 #&gt; 85 2 12.7 0.98 2.24 18.0 99 2.20 1.94 0.30 1.46 2.62 1.230 #&gt; 86 2 12.2 1.61 2.31 22.8 90 1.78 1.69 0.43 1.56 2.45 1.330 #&gt; 87 2 11.7 1.67 2.62 26.0 88 1.92 1.61 0.40 1.34 2.60 1.360 #&gt; 88 2 11.6 2.06 2.46 21.6 84 1.95 1.69 0.48 1.35 2.80 1.000 #&gt; 89 2 12.1 1.33 2.30 23.6 70 2.20 1.59 0.42 1.38 1.74 1.070 #&gt; 90 2 12.1 1.83 2.32 18.5 81 1.60 1.50 0.52 1.64 2.40 1.080 #&gt; 91 2 12.0 1.51 2.42 22.0 86 1.45 1.25 0.50 1.63 3.60 1.050 #&gt; 92 2 12.7 1.53 2.26 20.7 80 1.38 1.46 0.58 1.62 3.05 0.960 #&gt; 93 2 12.3 2.83 2.22 18.0 88 2.45 2.25 0.25 1.99 2.15 1.150 #&gt; 94 2 11.6 1.99 2.28 18.0 98 3.02 2.26 0.17 1.35 3.25 1.160 #&gt; 95 2 12.5 1.52 2.20 19.0 162 2.50 2.27 0.32 3.28 2.60 1.160 #&gt; 96 2 11.8 2.12 2.74 21.5 134 1.60 0.99 0.14 1.56 2.50 0.950 #&gt; 97 2 12.3 1.41 1.98 16.0 85 2.55 2.50 0.29 1.77 2.90 1.230 #&gt; 98 2 12.4 1.07 2.10 18.5 88 3.52 3.75 0.24 1.95 4.50 1.040 #&gt; 99 2 12.3 3.17 2.21 18.0 88 2.85 2.99 0.45 2.81 2.30 1.420 #&gt; 100 2 12.1 2.08 1.70 17.5 97 2.23 2.17 0.26 1.40 3.30 1.270 #&gt; 101 2 12.6 1.34 1.90 18.5 88 1.45 1.36 0.29 1.35 2.45 1.040 #&gt; 102 2 12.3 2.45 2.46 21.0 98 2.56 2.11 0.34 1.31 2.80 0.800 #&gt; 103 2 11.8 1.72 1.88 19.5 86 2.50 1.64 0.37 1.42 2.06 0.940 #&gt; 104 2 12.5 1.73 1.98 20.5 85 2.20 1.92 0.32 1.48 2.94 1.040 #&gt; 105 2 12.4 2.55 2.27 22.0 90 1.68 1.84 0.66 1.42 2.70 0.860 #&gt; 106 2 12.2 1.73 2.12 19.0 80 1.65 2.03 0.37 1.63 3.40 1.000 #&gt; 107 2 12.7 1.75 2.28 22.5 84 1.38 1.76 0.48 1.63 3.30 0.880 #&gt; 108 2 12.2 1.29 1.94 19.0 92 2.36 2.04 0.39 2.08 2.70 0.860 #&gt; 109 2 11.6 1.35 2.70 20.0 94 2.74 2.92 0.29 2.49 2.65 0.960 #&gt; 110 2 11.5 3.74 1.82 19.5 107 3.18 2.58 0.24 3.58 2.90 0.750 #&gt; 111 2 12.5 2.43 2.17 21.0 88 2.55 2.27 0.26 1.22 2.00 0.900 #&gt; 112 2 11.8 2.68 2.92 20.0 103 1.75 2.03 0.60 1.05 3.80 1.230 #&gt; 113 2 11.4 0.74 2.50 21.0 88 2.48 2.01 0.42 1.44 3.08 1.100 #&gt; 114 2 12.1 1.39 2.50 22.5 84 2.56 2.29 0.43 1.04 2.90 0.930 #&gt; 115 2 11.0 1.51 2.20 21.5 85 2.46 2.17 0.52 2.01 1.90 1.710 #&gt; 116 2 11.8 1.47 1.99 20.8 86 1.98 1.60 0.30 1.53 1.95 0.950 #&gt; 117 2 12.4 1.61 2.19 22.5 108 2.00 2.09 0.34 1.61 2.06 1.060 #&gt; 118 2 12.8 3.43 1.98 16.0 80 1.63 1.25 0.43 0.83 3.40 0.700 #&gt; 119 2 12.0 3.43 2.00 19.0 87 2.00 1.64 0.37 1.87 1.28 0.930 #&gt; 120 2 11.4 2.40 2.42 20.0 96 2.90 2.79 0.32 1.83 3.25 0.800 #&gt; 121 2 11.6 2.05 3.23 28.5 119 3.18 5.08 0.47 1.87 6.00 0.930 #&gt; 122 2 12.4 4.43 2.73 26.5 102 2.20 2.13 0.43 1.71 2.08 0.920 #&gt; 123 2 13.1 5.80 2.13 21.5 86 2.62 2.65 0.30 2.01 2.60 0.730 #&gt; 124 2 11.9 4.31 2.39 21.0 82 2.86 3.03 0.21 2.91 2.80 0.750 #&gt; 125 2 12.1 2.16 2.17 21.0 85 2.60 2.65 0.37 1.35 2.76 0.860 #&gt; 126 2 12.4 1.53 2.29 21.5 86 2.74 3.15 0.39 1.77 3.94 0.690 #&gt; 127 2 11.8 2.13 2.78 28.5 92 2.13 2.24 0.58 1.76 3.00 0.970 #&gt; 128 2 12.4 1.63 2.30 24.5 88 2.22 2.45 0.40 1.90 2.12 0.890 #&gt; 129 2 12.0 4.30 2.38 22.0 80 2.10 1.75 0.42 1.35 2.60 0.790 #&gt; 130 3 12.9 1.35 2.32 18.0 122 1.51 1.25 0.21 0.94 4.10 0.760 #&gt; 131 3 12.9 2.99 2.40 20.0 104 1.30 1.22 0.24 0.83 5.40 0.740 #&gt; 132 3 12.8 2.31 2.40 24.0 98 1.15 1.09 0.27 0.83 5.70 0.660 #&gt; 133 3 12.7 3.55 2.36 21.5 106 1.70 1.20 0.17 0.84 5.00 0.780 #&gt; 134 3 12.5 1.24 2.25 17.5 85 2.00 0.58 0.60 1.25 5.45 0.750 #&gt; 135 3 12.6 2.46 2.20 18.5 94 1.62 0.66 0.63 0.94 7.10 0.730 #&gt; 136 3 12.2 4.72 2.54 21.0 89 1.38 0.47 0.53 0.80 3.85 0.750 #&gt; 137 3 12.5 5.51 2.64 25.0 96 1.79 0.60 0.63 1.10 5.00 0.820 #&gt; 138 3 13.5 3.59 2.19 19.5 88 1.62 0.48 0.58 0.88 5.70 0.810 #&gt; 139 3 12.8 2.96 2.61 24.0 101 2.32 0.60 0.53 0.81 4.92 0.890 #&gt; 140 3 12.9 2.81 2.70 21.0 96 1.54 0.50 0.53 0.75 4.60 0.770 #&gt; 141 3 13.4 2.56 2.35 20.0 89 1.40 0.50 0.37 0.64 5.60 0.700 #&gt; 142 3 13.5 3.17 2.72 23.5 97 1.55 0.52 0.50 0.55 4.35 0.890 #&gt; 143 3 13.6 4.95 2.35 20.0 92 2.00 0.80 0.47 1.02 4.40 0.910 #&gt; 144 3 12.2 3.88 2.20 18.5 112 1.38 0.78 0.29 1.14 8.21 0.650 #&gt; 145 3 13.2 3.57 2.15 21.0 102 1.50 0.55 0.43 1.30 4.00 0.600 #&gt; 146 3 13.9 5.04 2.23 20.0 80 0.98 0.34 0.40 0.68 4.90 0.580 #&gt; 147 3 12.9 4.61 2.48 21.5 86 1.70 0.65 0.47 0.86 7.65 0.540 #&gt; 148 3 13.3 3.24 2.38 21.5 92 1.93 0.76 0.45 1.25 8.42 0.550 #&gt; 149 3 13.1 3.90 2.36 21.5 113 1.41 1.39 0.34 1.14 9.40 0.570 #&gt; 150 3 13.5 3.12 2.62 24.0 123 1.40 1.57 0.22 1.25 8.60 0.590 #&gt; 151 3 12.8 2.67 2.48 22.0 112 1.48 1.36 0.24 1.26 10.80 0.480 #&gt; 152 3 13.1 1.90 2.75 25.5 116 2.20 1.28 0.26 1.56 7.10 0.610 #&gt; 153 3 13.2 3.30 2.28 18.5 98 1.80 0.83 0.61 1.87 10.52 0.560 #&gt; 154 3 12.6 1.29 2.10 20.0 103 1.48 0.58 0.53 1.40 7.60 0.580 #&gt; 155 3 13.2 5.19 2.32 22.0 93 1.74 0.63 0.61 1.55 7.90 0.600 #&gt; 156 3 13.8 4.12 2.38 19.5 89 1.80 0.83 0.48 1.56 9.01 0.570 #&gt; 157 3 12.4 3.03 2.64 27.0 97 1.90 0.58 0.63 1.14 7.50 0.670 #&gt; 158 3 14.3 1.68 2.70 25.0 98 2.80 1.31 0.53 2.70 13.00 0.570 #&gt; 159 3 13.5 1.67 2.64 22.5 89 2.60 1.10 0.52 2.29 11.75 0.570 #&gt; 160 3 12.4 3.83 2.38 21.0 88 2.30 0.92 0.50 1.04 7.65 0.560 #&gt; 161 3 13.7 3.26 2.54 20.0 107 1.83 0.56 0.50 0.80 5.88 0.960 #&gt; 162 3 12.8 3.27 2.58 22.0 106 1.65 0.60 0.60 0.96 5.58 0.870 #&gt; 163 3 13.0 3.45 2.35 18.5 106 1.39 0.70 0.40 0.94 5.28 0.680 #&gt; 164 3 13.8 2.76 2.30 22.0 90 1.35 0.68 0.41 1.03 9.58 0.700 #&gt; 165 3 13.7 4.36 2.26 22.5 88 1.28 0.47 0.52 1.15 6.62 0.780 #&gt; 166 3 13.4 3.70 2.60 23.0 111 1.70 0.92 0.43 1.46 10.68 0.850 #&gt; 167 3 12.8 3.37 2.30 19.5 88 1.48 0.66 0.40 0.97 10.26 0.720 #&gt; 168 3 13.6 2.58 2.69 24.5 105 1.55 0.84 0.39 1.54 8.66 0.740 #&gt; 169 3 13.4 4.60 2.86 25.0 112 1.98 0.96 0.27 1.11 8.50 0.670 #&gt; 170 3 12.2 3.03 2.32 19.0 96 1.25 0.49 0.40 0.73 5.50 0.660 #&gt; 171 3 12.8 2.39 2.28 19.5 86 1.39 0.51 0.48 0.64 9.90 0.570 #&gt; 172 3 14.2 2.51 2.48 20.0 91 1.68 0.70 0.44 1.24 9.70 0.620 #&gt; 173 3 13.7 5.65 2.45 20.5 95 1.68 0.61 0.52 1.06 7.70 0.640 #&gt; 174 3 13.4 3.91 2.48 23.0 102 1.80 0.75 0.43 1.41 7.30 0.700 #&gt; 175 3 13.3 4.28 2.26 20.0 120 1.59 0.69 0.43 1.35 10.20 0.590 #&gt; 176 3 13.2 2.59 2.37 20.0 120 1.65 0.68 0.53 1.46 9.30 0.600 #&gt; 177 3 14.1 4.10 2.74 24.5 96 2.05 0.76 0.56 1.35 9.20 0.610 #&gt; X3.92 X1065 #&gt; 1 3.40 1050 #&gt; 2 3.17 1185 #&gt; 3 3.45 1480 #&gt; 4 2.93 735 #&gt; 5 2.85 1450 #&gt; 6 3.58 1290 #&gt; 7 3.58 1295 #&gt; 8 2.85 1045 #&gt; 9 3.55 1045 #&gt; 10 3.17 1510 #&gt; 11 2.82 1280 #&gt; 12 2.90 1320 #&gt; 13 2.73 1150 #&gt; 14 3.00 1547 #&gt; 15 2.88 1310 #&gt; 16 2.65 1280 #&gt; 17 2.57 1130 #&gt; 18 2.82 1680 #&gt; 19 3.36 845 #&gt; 20 3.71 780 #&gt; 21 3.52 770 #&gt; 22 4.00 1035 #&gt; 23 3.63 1015 #&gt; 24 3.82 845 #&gt; 25 3.20 830 #&gt; 26 3.22 1195 #&gt; 27 2.77 1285 #&gt; 28 3.40 915 #&gt; 29 3.59 1035 #&gt; 30 2.71 1285 #&gt; 31 2.88 1515 #&gt; 32 2.87 990 #&gt; 33 3.00 1235 #&gt; 34 2.87 1095 #&gt; 35 3.47 920 #&gt; 36 2.78 880 #&gt; 37 2.51 1105 #&gt; 38 2.69 1020 #&gt; 39 3.53 760 #&gt; 40 3.38 795 #&gt; 41 3.00 1035 #&gt; 42 3.56 1095 #&gt; 43 3.00 680 #&gt; 44 3.35 885 #&gt; 45 3.33 1080 #&gt; 46 3.44 1065 #&gt; 47 3.33 985 #&gt; 48 2.75 1060 #&gt; 49 3.10 1260 #&gt; 50 2.91 1150 #&gt; 51 3.37 1265 #&gt; 52 3.26 1190 #&gt; 53 2.93 1375 #&gt; 54 3.20 1060 #&gt; 55 3.03 1120 #&gt; 56 3.31 970 #&gt; 57 2.84 1270 #&gt; 58 2.87 1285 #&gt; 59 1.82 520 #&gt; 60 1.67 680 #&gt; 61 1.59 450 #&gt; 62 2.46 630 #&gt; 63 2.87 420 #&gt; 64 2.23 355 #&gt; 65 2.30 678 #&gt; 66 3.18 502 #&gt; 67 3.48 510 #&gt; 68 1.93 750 #&gt; 69 3.07 718 #&gt; 70 1.82 870 #&gt; 71 3.16 410 #&gt; 72 2.78 472 #&gt; 73 3.50 985 #&gt; 74 3.13 886 #&gt; 75 2.14 428 #&gt; 76 2.48 392 #&gt; 77 2.52 500 #&gt; 78 2.31 750 #&gt; 79 3.13 463 #&gt; 80 3.12 278 #&gt; 81 3.14 714 #&gt; 82 2.72 630 #&gt; 83 2.01 515 #&gt; 84 3.08 520 #&gt; 85 3.16 450 #&gt; 86 2.26 495 #&gt; 87 3.21 562 #&gt; 88 2.75 680 #&gt; 89 3.21 625 #&gt; 90 2.27 480 #&gt; 91 2.65 450 #&gt; 92 2.06 495 #&gt; 93 3.30 290 #&gt; 94 2.96 345 #&gt; 95 2.63 937 #&gt; 96 2.26 625 #&gt; 97 2.74 428 #&gt; 98 2.77 660 #&gt; 99 2.83 406 #&gt; 100 2.96 710 #&gt; 101 2.77 562 #&gt; 102 3.38 438 #&gt; 103 2.44 415 #&gt; 104 3.57 672 #&gt; 105 3.30 315 #&gt; 106 3.17 510 #&gt; 107 2.42 488 #&gt; 108 3.02 312 #&gt; 109 3.26 680 #&gt; 110 2.81 562 #&gt; 111 2.78 325 #&gt; 112 2.50 607 #&gt; 113 2.31 434 #&gt; 114 3.19 385 #&gt; 115 2.87 407 #&gt; 116 3.33 495 #&gt; 117 2.96 345 #&gt; 118 2.12 372 #&gt; 119 3.05 564 #&gt; 120 3.39 625 #&gt; 121 3.69 465 #&gt; 122 3.12 365 #&gt; 123 3.10 380 #&gt; 124 3.64 380 #&gt; 125 3.28 378 #&gt; 126 2.84 352 #&gt; 127 2.44 466 #&gt; 128 2.78 342 #&gt; 129 2.57 580 #&gt; 130 1.29 630 #&gt; 131 1.42 530 #&gt; 132 1.36 560 #&gt; 133 1.29 600 #&gt; 134 1.51 650 #&gt; 135 1.58 695 #&gt; 136 1.27 720 #&gt; 137 1.69 515 #&gt; 138 1.82 580 #&gt; 139 2.15 590 #&gt; 140 2.31 600 #&gt; 141 2.47 780 #&gt; 142 2.06 520 #&gt; 143 2.05 550 #&gt; 144 2.00 855 #&gt; 145 1.68 830 #&gt; 146 1.33 415 #&gt; 147 1.86 625 #&gt; 148 1.62 650 #&gt; 149 1.33 550 #&gt; 150 1.30 500 #&gt; 151 1.47 480 #&gt; 152 1.33 425 #&gt; 153 1.51 675 #&gt; 154 1.55 640 #&gt; 155 1.48 725 #&gt; 156 1.64 480 #&gt; 157 1.73 880 #&gt; 158 1.96 660 #&gt; 159 1.78 620 #&gt; 160 1.58 520 #&gt; 161 1.82 680 #&gt; 162 2.11 570 #&gt; 163 1.75 675 #&gt; 164 1.68 615 #&gt; 165 1.75 520 #&gt; 166 1.56 695 #&gt; 167 1.75 685 #&gt; 168 1.80 750 #&gt; 169 1.92 630 #&gt; 170 1.83 510 #&gt; 171 1.63 470 #&gt; 172 1.71 660 #&gt; 173 1.74 740 #&gt; 174 1.56 750 #&gt; 175 1.56 835 #&gt; 176 1.62 840 #&gt; 177 1.60 560 names(wines) &lt;- c(&quot;label&quot;, &quot;Alcohol&quot;, &quot;Malic_acid&quot;, &quot;Ash&quot;, &quot;Alcalinity_of_ash&quot;, &quot;Magnesium&quot;, &quot;Total_phenols&quot;, &quot;Flavanoids&quot;, &quot;Nonflavanoid_phenols&quot;, &quot;Proanthocyanins&quot;, &quot;Color_intensity&quot;, &quot;Hue&quot;, &quot;OD280_OD315_of_diluted_wines&quot;, &quot;Proline&quot;) head(wines) #&gt; label Alcohol Malic_acid Ash Alcalinity_of_ash Magnesium Total_phenols #&gt; 1 1 13.2 1.78 2.14 11.2 100 2.65 #&gt; 2 1 13.2 2.36 2.67 18.6 101 2.80 #&gt; 3 1 14.4 1.95 2.50 16.8 113 3.85 #&gt; 4 1 13.2 2.59 2.87 21.0 118 2.80 #&gt; 5 1 14.2 1.76 2.45 15.2 112 3.27 #&gt; 6 1 14.4 1.87 2.45 14.6 96 2.50 #&gt; Flavanoids Nonflavanoid_phenols Proanthocyanins Color_intensity Hue #&gt; 1 2.76 0.26 1.28 4.38 1.05 #&gt; 2 3.24 0.30 2.81 5.68 1.03 #&gt; 3 3.49 0.24 2.18 7.80 0.86 #&gt; 4 2.69 0.39 1.82 4.32 1.04 #&gt; 5 3.39 0.34 1.97 6.75 1.05 #&gt; 6 2.52 0.30 1.98 5.25 1.02 #&gt; OD280_OD315_of_diluted_wines Proline #&gt; 1 3.40 1050 #&gt; 2 3.17 1185 #&gt; 3 3.45 1480 #&gt; 4 2.93 735 #&gt; 5 2.85 1450 #&gt; 6 3.58 1290 plt1 &lt;- ggplot(wines, aes(x = Alcohol, y = Magnesium, colour = as.factor(label))) + geom_point(size=3) + ggtitle(&quot;Wines&quot;) plt1 plt2 &lt;- ggplot(wines, aes(x = Alcohol, y = Proline, colour = as.factor(label))) + geom_point(size=3) + ggtitle(&quot;Wines&quot;) plt2 39.2 Preprocessing During the preprocessing phase, I have to do at least the following two things: Encode the categorical variables. Standardize the predictors. First of all, let’s encode our target variable. The encoding of the categorical variables is needed when using neuralnet since it does not like factors at all. It will shout at you if you try to feed in a factor (I am told nnet likes factors though). In the wine dataset the variable label contains three different labels: 1,2 and 3. The usual practice, as far as I know, is to encode categorical variables as a “one hot” vector. For instance, if I had three classes, like in this case, I’d need to replace the label variable with three variables like these: # l1,l2,l3 # 1,0,0 # 0,0,1 # ... In this case the first observation would be labelled as a 1, the second would be labelled as a 2, and so on. Ironically, the nnet package provides a function to perform this encoding in a painless way: # Encode as a one hot vector multilabel data train &lt;- cbind(wines[, 2:14], class.ind(as.factor(wines$label))) # Set labels name names(train) &lt;- c(names(wines)[2:14],&quot;l1&quot;,&quot;l2&quot;,&quot;l3&quot;) By the way, since the predictors are all continuous, you do not need to encode any of them, however, in case you needed to, you could apply the same strategy applied above to all the categorical predictors. Unless of course you’d like to try some other kind of custom encoding. Now let’s standardize the predictors in the [0−1]&quot;&gt;[0−1] interval by leveraging the lapply function: # Scale data scl &lt;- function(x) { (x - min(x))/(max(x) - min(x)) } train[, 1:13] &lt;- data.frame(lapply(train[, 1:13], scl)) head(train) #&gt; Alcohol Malic_acid Ash Alcalinity_of_ash Magnesium Total_phenols #&gt; 1 0.571 0.206 0.417 0.0309 0.326 0.576 #&gt; 2 0.561 0.320 0.701 0.4124 0.337 0.628 #&gt; 3 0.879 0.239 0.610 0.3196 0.467 0.990 #&gt; 4 0.582 0.366 0.807 0.5361 0.522 0.628 #&gt; 5 0.834 0.202 0.583 0.2371 0.457 0.790 #&gt; 6 0.884 0.223 0.583 0.2062 0.283 0.524 #&gt; Flavanoids Nonflavanoid_phenols Proanthocyanins Color_intensity Hue #&gt; 1 0.511 0.245 0.274 0.265 0.463 #&gt; 2 0.612 0.321 0.757 0.375 0.447 #&gt; 3 0.665 0.208 0.558 0.556 0.309 #&gt; 4 0.496 0.491 0.445 0.259 0.455 #&gt; 5 0.643 0.396 0.492 0.467 0.463 #&gt; 6 0.460 0.321 0.495 0.339 0.439 #&gt; OD280_OD315_of_diluted_wines Proline l1 l2 l3 #&gt; 1 0.780 0.551 1 0 0 #&gt; 2 0.696 0.647 1 0 0 #&gt; 3 0.799 0.857 1 0 0 #&gt; 4 0.608 0.326 1 0 0 #&gt; 5 0.579 0.836 1 0 0 #&gt; 6 0.846 0.722 1 0 0 39.3 Fitting the model with neuralnet Now it is finally time to fit the model. As you might remember from the old post I wrote, neuralnet does not like the formula y~.. Fear not, you can build the formula to be used in a simple step: # Set up formula n &lt;- names(train) f &lt;- as.formula(paste(&quot;l1 + l2 + l3 ~&quot;, paste(n[!n %in% c(&quot;l1&quot;,&quot;l2&quot;,&quot;l3&quot;)], collapse = &quot; + &quot;))) f #&gt; l1 + l2 + l3 ~ Alcohol + Malic_acid + Ash + Alcalinity_of_ash + #&gt; Magnesium + Total_phenols + Flavanoids + Nonflavanoid_phenols + #&gt; Proanthocyanins + Color_intensity + Hue + OD280_OD315_of_diluted_wines + #&gt; Proline Note that the characters in the vector are not pasted to the right of the “~” symbol. Just remember to check that the formula is indeed correct and then you are good to go. Let’s train the neural network with the full dataset. It should take very little time to converge. If you did not standardize the predictors it could take a lot more though. nn &lt;- neuralnet(f, data = train, hidden = c(13, 10, 3), act.fct = &quot;logistic&quot;, linear.output = FALSE, lifesign = &quot;minimal&quot;) #&gt; hidden: 13, 10, 3 thresh: 0.01 rep: 1/1 steps: 88 error: 0.03039 time: 0.05 secs Note that I set the argument linear.output to FALSE in order to tell the model that I want to apply the activation function act.fct and that I am not doing a regression task. Then I set the activation function to logistic (which by the way is the default option) in order to apply the logistic function. The other available option is tanh but the model seems to perform a little worse with it so I opted for the default option. As far as I know these two are the only two available options, there is no “relu” function available although it seems to be a common activation function in other packages. As far as the number of hidden neurons, I tried some combination and the one used seems to perform slightly better than the others (around 1% of accuracy difference in cross validation score). By using the in-built plot method you can get a visual take on what is actually happening inside the model, however the plot is not that helpful I think plot(nn) Let’s have a look at the accuracy on the training set: # Compute predictions pr.nn &lt;- compute(nn, train[, 1:13]) # Extract results pr.nn_ &lt;- pr.nn$net.result head(pr.nn_) #&gt; [,1] [,2] [,3] #&gt; [1,] 0.990 0.00317 6.99e-06 #&gt; [2,] 0.991 0.00233 8.69e-06 #&gt; [3,] 0.991 0.00210 8.65e-06 #&gt; [4,] 0.986 0.00442 8.74e-06 #&gt; [5,] 0.992 0.00212 8.32e-06 #&gt; [6,] 0.992 0.00214 8.34e-06 # Accuracy (training set) original_values &lt;- max.col(train[, 14:16]) pr.nn_2 &lt;- max.col(pr.nn_) mean(pr.nn_2 == original_values) #&gt; [1] 1 100% not bad! But wait, this may be because our model over fitted the data, furthermore evaluating accuracy on the training set is kind of cheating since the model already “knows” (or should know) the answers. In order to assess the “true accuracy” of the model you need to perform some kind of cross validation. 39.4 Cross validating the classifier Let’s crossvalidate the model using the evergreen 10 fold cross validation with the following train and test split: 95% of the dataset will be used as training set while the remaining 5% as test set. Just out of curiosity I decided to run a LOOCV round too. In case you’d like to run this cross validation technique, just set the proportion variable to 0.995: this will select just one observation for as test set and leave all the other observations as training set. Running LOOCV you should get similar results to the 10 fold cross validation. # Set seed for reproducibility purposes set.seed(500) # 10 fold cross validation k &lt;- 10 # Results from cv outs &lt;- NULL # Train test split proportions proportion &lt;- 0.95 # Set to 0.995 for LOOCV # Crossvalidate, go! for(i in 1:k) { index &lt;- sample(1:nrow(train), round(proportion*nrow(train))) train_cv &lt;- train[index, ] test_cv &lt;- train[-index, ] nn_cv &lt;- neuralnet(f, data = train_cv, hidden = c(13, 10, 3), act.fct = &quot;logistic&quot;, linear.output = FALSE) # Compute predictions pr.nn &lt;- compute(nn_cv, test_cv[, 1:13]) # Extract results pr.nn_ &lt;- pr.nn$net.result # Accuracy (test set) original_values &lt;- max.col(test_cv[, 14:16]) pr.nn_2 &lt;- max.col(pr.nn_) outs[i] &lt;- mean(pr.nn_2 == original_values) } mean(outs) #&gt; [1] 0.978 98.8%, awesome! Next time when you are invited to a relaxing evening that includes a wine tasting competition I think you should definitely bring your laptop as a contestant! Aside from that poor taste joke, (I made it again!), indeed this dataset is not the most challenging, I think with some more tweaking a better cross validation score could be achieved. Nevertheless I hope you found this tutorial useful. A gist with the entire code for this tutorial can be found here. Thank you for reading this article, please feel free to leave a comment if you have any questions or suggestions and share the post with others if you find it useful. Notes: "],
["classification-and-regression-with-h2o-deep-learning.html", "Chapter 40 Classification and Regression with H2O Deep Learning 40.1 Introduction 40.2 H2O R Package 40.3 Start H2O 40.4 Let’s have some fun first: Decision Boundaries 40.5 Cover Type Dataset 40.6 Regression and Binary Classification 40.7 Unsupervised Anomaly detection 40.8 H2O Deep Learning Tips &amp; Tricks 40.9 All done, shutdown H2O", " Chapter 40 Classification and Regression with H2O Deep Learning 40.1 Introduction Source: http://docs.h2o.ai/h2o-tutorials/latest-stable/tutorials/deeplearning/index.html Repo: https://github.com/h2oai/h2o-tutorials This tutorial shows how a H2O Deep Learning model can be used to do supervised classification and regression. A great tutorial about Deep Learning is given by Quoc Le here and here. This tutorial covers usage of H2O from R. A python version of this tutorial will be available as well in a separate document. This file is available in plain R, R markdown and regular markdown formats, and the plots are available as PDF files. All documents are available on Github. If run from plain R, execute R in the directory of this script. If run from RStudio, be sure to setwd() to the location of this script.h2o.init() starts H2O in R’s current working directory. h2o.importFile() looks for files from the perspective of where H2O was started. More examples and explanations can be found in our H2O Deep Learning booklet and on our H2O Github Repository. The PDF slide deck can be found on Github. 40.2 H2O R Package Load the H2O R package: Source: http://docs.h2o.ai/h2o-tutorials/latest-stable/tutorials/deeplearning/index.html ## R installation instructions are at http://h2o.ai/download library(h2o) #&gt; #&gt; ---------------------------------------------------------------------- #&gt; #&gt; Your next step is to start H2O: #&gt; &gt; h2o.init() #&gt; #&gt; For H2O package documentation, ask for help: #&gt; &gt; ??h2o #&gt; #&gt; After starting H2O, you can use the Web UI at http://localhost:54321 #&gt; For more information visit http://docs.h2o.ai #&gt; #&gt; ---------------------------------------------------------------------- #&gt; #&gt; Attaching package: &#39;h2o&#39; #&gt; The following objects are masked from &#39;package:stats&#39;: #&gt; #&gt; cor, sd, var #&gt; The following objects are masked from &#39;package:base&#39;: #&gt; #&gt; &amp;&amp;, %*%, %in%, ||, apply, as.factor, as.numeric, colnames, #&gt; colnames&lt;-, ifelse, is.character, is.factor, is.numeric, log, #&gt; log10, log1p, log2, round, signif, trunc 40.3 Start H2O Start up a 1-node H2O server on your local machine, and allow it to use all CPU cores and up to 2GB of memory: h2o.init(nthreads=-1, max_mem_size=&quot;2G&quot;) #&gt; #&gt; H2O is not running yet, starting it now... #&gt; #&gt; Note: In case of errors look at the following log files: #&gt; /tmp/RtmpTwh2wZ/h2o_datascience_started_from_r.out #&gt; /tmp/RtmpTwh2wZ/h2o_datascience_started_from_r.err #&gt; #&gt; #&gt; Starting H2O JVM and connecting: . Connection successful! #&gt; #&gt; R is connected to the H2O cluster: #&gt; H2O cluster uptime: 1 seconds 553 milliseconds #&gt; H2O cluster timezone: America/Chicago #&gt; H2O data parsing timezone: UTC #&gt; H2O cluster version: 3.22.1.1 #&gt; H2O cluster version age: 8 months and 23 days !!! #&gt; H2O cluster name: H2O_started_from_R_datascience_mwl453 #&gt; H2O cluster total nodes: 1 #&gt; H2O cluster total memory: 1.78 GB #&gt; H2O cluster total cores: 8 #&gt; H2O cluster allowed cores: 8 #&gt; H2O cluster healthy: TRUE #&gt; H2O Connection ip: localhost #&gt; H2O Connection port: 54321 #&gt; H2O Connection proxy: NA #&gt; H2O Internal Security: FALSE #&gt; H2O API Extensions: XGBoost, Algos, AutoML, Core V3, Core V4 #&gt; R Version: R version 3.6.0 (2019-04-26) #&gt; Warning in h2o.clusterInfo(): #&gt; Your H2O cluster version is too old (8 months and 23 days)! #&gt; Please download and install the latest version from http://h2o.ai/download/ h2o.removeAll() ## clean slate - just in case the cluster was already running #&gt; [1] 0 The h2o.deeplearning function fits H2O’s Deep Learning models from within R. We can run the example from the man page using the example function, or run a longer demonstration from the h2o package using the demo function:: args(h2o.deeplearning) #&gt; function (x, y, training_frame, model_id = NULL, validation_frame = NULL, #&gt; nfolds = 0, keep_cross_validation_models = TRUE, keep_cross_validation_predictions = FALSE, #&gt; keep_cross_validation_fold_assignment = FALSE, fold_assignment = c(&quot;AUTO&quot;, #&gt; &quot;Random&quot;, &quot;Modulo&quot;, &quot;Stratified&quot;), fold_column = NULL, #&gt; ignore_const_cols = TRUE, score_each_iteration = FALSE, weights_column = NULL, #&gt; offset_column = NULL, balance_classes = FALSE, class_sampling_factors = NULL, #&gt; max_after_balance_size = 5, max_hit_ratio_k = 0, checkpoint = NULL, #&gt; pretrained_autoencoder = NULL, overwrite_with_best_model = TRUE, #&gt; use_all_factor_levels = TRUE, standardize = TRUE, activation = c(&quot;Tanh&quot;, #&gt; &quot;TanhWithDropout&quot;, &quot;Rectifier&quot;, &quot;RectifierWithDropout&quot;, #&gt; &quot;Maxout&quot;, &quot;MaxoutWithDropout&quot;), hidden = c(200, 200), #&gt; epochs = 10, train_samples_per_iteration = -2, target_ratio_comm_to_comp = 0.05, #&gt; seed = -1, adaptive_rate = TRUE, rho = 0.99, epsilon = 1e-08, #&gt; rate = 0.005, rate_annealing = 1e-06, rate_decay = 1, momentum_start = 0, #&gt; momentum_ramp = 1e+06, momentum_stable = 0, nesterov_accelerated_gradient = TRUE, #&gt; input_dropout_ratio = 0, hidden_dropout_ratios = NULL, l1 = 0, #&gt; l2 = 0, max_w2 = 3.4028235e+38, initial_weight_distribution = c(&quot;UniformAdaptive&quot;, #&gt; &quot;Uniform&quot;, &quot;Normal&quot;), initial_weight_scale = 1, initial_weights = NULL, #&gt; initial_biases = NULL, loss = c(&quot;Automatic&quot;, &quot;CrossEntropy&quot;, #&gt; &quot;Quadratic&quot;, &quot;Huber&quot;, &quot;Absolute&quot;, &quot;Quantile&quot;), distribution = c(&quot;AUTO&quot;, #&gt; &quot;bernoulli&quot;, &quot;multinomial&quot;, &quot;gaussian&quot;, &quot;poisson&quot;, &quot;gamma&quot;, #&gt; &quot;tweedie&quot;, &quot;laplace&quot;, &quot;quantile&quot;, &quot;huber&quot;), quantile_alpha = 0.5, #&gt; tweedie_power = 1.5, huber_alpha = 0.9, score_interval = 5, #&gt; score_training_samples = 10000, score_validation_samples = 0, #&gt; score_duty_cycle = 0.1, classification_stop = 0, regression_stop = 1e-06, #&gt; stopping_rounds = 5, stopping_metric = c(&quot;AUTO&quot;, &quot;deviance&quot;, #&gt; &quot;logloss&quot;, &quot;MSE&quot;, &quot;RMSE&quot;, &quot;MAE&quot;, &quot;RMSLE&quot;, &quot;AUC&quot;, &quot;lift_top_group&quot;, #&gt; &quot;misclassification&quot;, &quot;mean_per_class_error&quot;, &quot;custom&quot;, #&gt; &quot;custom_increasing&quot;), stopping_tolerance = 0, max_runtime_secs = 0, #&gt; score_validation_sampling = c(&quot;Uniform&quot;, &quot;Stratified&quot;), diagnostics = TRUE, #&gt; fast_mode = TRUE, force_load_balance = TRUE, variable_importances = TRUE, #&gt; replicate_training_data = TRUE, single_node_mode = FALSE, #&gt; shuffle_training_data = FALSE, missing_values_handling = c(&quot;MeanImputation&quot;, #&gt; &quot;Skip&quot;), quiet_mode = FALSE, autoencoder = FALSE, sparse = FALSE, #&gt; col_major = FALSE, average_activation = 0, sparsity_beta = 0, #&gt; max_categorical_features = 2147483647, reproducible = FALSE, #&gt; export_weights_and_biases = FALSE, mini_batch_size = 1, categorical_encoding = c(&quot;AUTO&quot;, #&gt; &quot;Enum&quot;, &quot;OneHotInternal&quot;, &quot;OneHotExplicit&quot;, &quot;Binary&quot;, #&gt; &quot;Eigen&quot;, &quot;LabelEncoder&quot;, &quot;SortByResponse&quot;, &quot;EnumLimited&quot;), #&gt; elastic_averaging = FALSE, elastic_averaging_moving_rate = 0.9, #&gt; elastic_averaging_regularization = 0.001, export_checkpoints_dir = NULL, #&gt; verbose = FALSE) #&gt; NULL if (interactive()) help(h2o.deeplearning) example(h2o.deeplearning) #&gt; #&gt; h2.dpl&gt; ## No test: #&gt; h2.dpl&gt; ##D library(h2o) #&gt; h2.dpl&gt; ##D h2o.init() #&gt; h2.dpl&gt; ##D iris_hf &lt;- as.h2o(iris) #&gt; h2.dpl&gt; ##D iris_dl &lt;- h2o.deeplearning(x = 1:4, y = 5, training_frame = iris_hf, seed=123456) #&gt; h2.dpl&gt; ##D #&gt; h2.dpl&gt; ##D # now make a prediction #&gt; h2.dpl&gt; ##D predictions &lt;- h2o.predict(iris_dl, iris_hf) #&gt; h2.dpl&gt; ## End(No test) #&gt; h2.dpl&gt; #&gt; h2.dpl&gt; #&gt; h2.dpl&gt; if (interactive()) demo(h2o.deeplearning) #requires user interaction While H2O Deep Learning has many parameters, it was designed to be just as easy to use as the other supervised training methods in H2O. Early stopping, automatic data standardization and handling of categorical variables and missing values and adaptive learning rates (per weight) reduce the amount of parameters the user has to specify. Often, it’s just the number and sizes of hidden layers, the number of epochs and the activation function and maybe some regularization techniques. 40.4 Let’s have some fun first: Decision Boundaries We start with a small dataset representing red and black dots on a plane, arranged in the shape of two nested spirals. Then we task H2O’s machine learning methods to separate the red and black dots, i.e., recognize each spiral as such by assigning each point in the plane to one of the two spirals. We visualize the nature of H2O Deep Learning (DL), H2O’s tree methods (GBM/DRF) and H2O’s generalized linear modeling (GLM) by plotting the decision boundary between the red and black spirals: # setwd(&quot;~/h2o-tutorials/tutorials/deeplearning&quot;) ##For RStudio spiral &lt;- h2o.importFile(path = normalizePath(file.path(data_raw_dir, &quot;spiral.csv&quot;))) #&gt; | | | 0% | |=================================================================| 100% grid &lt;- h2o.importFile(path = normalizePath(file.path(data_raw_dir, &quot;grid.csv&quot;))) #&gt; | | | 0% | |==== | 6% | |=================================================================| 100% # Define helper to plot contours plotC &lt;- function(name, model, data=spiral, g=grid) { data &lt;- as.data.frame(data) #get data from into R pred &lt;- as.data.frame(h2o.predict(model, g)) n=0.5*(sqrt(nrow(g))-1); d &lt;- 1.5; h &lt;- d*(-n:n)/n plot(data[,-3],pch=19,col=data[,3],cex=0.5, xlim=c(-d,d),ylim=c(-d,d),main=name) contour(h,h,z=array(ifelse(pred[,1]==&quot;Red&quot;,0,1), dim=c(2*n+1,2*n+1)),col=&quot;blue&quot;,lwd=2,add=T) } We build a few different models: #dev.new(noRStudioGD=FALSE) #direct plotting output to a new window par(mfrow=c(2,2)) #set up the canvas for 2x2 plots plotC( &quot;DL&quot;, h2o.deeplearning(1:2,3,spiral,epochs=1e3)) plotC(&quot;GBM&quot;, h2o.gbm (1:2,3,spiral)) plotC(&quot;DRF&quot;, h2o.randomForest(1:2,3,spiral)) plotC(&quot;GLM&quot;, h2o.glm (1:2,3,spiral,family=&quot;binomial&quot;)) Let’s investigate some more Deep Learning models. First, we explore the evolution over training time (number of passes over the data), and we use checkpointing to continue training the same model: #dev.new(noRStudioGD=FALSE) #direct plotting output to a new window par(mfrow=c(2,2)) #set up the canvas for 2x2 plots ep &lt;- c(1,250,500,750) plotC(paste0(&quot;DL &quot;,ep[1],&quot; epochs&quot;), h2o.deeplearning(1:2,3,spiral,epochs=ep[1], model_id=&quot;dl_1&quot;)) plotC(paste0(&quot;DL &quot;,ep[2],&quot; epochs&quot;), h2o.deeplearning(1:2,3,spiral,epochs=ep[2], checkpoint=&quot;dl_1&quot;,model_id=&quot;dl_2&quot;)) plotC(paste0(&quot;DL &quot;,ep[3],&quot; epochs&quot;), h2o.deeplearning(1:2,3,spiral,epochs=ep[3], checkpoint=&quot;dl_2&quot;,model_id=&quot;dl_3&quot;)) plotC(paste0(&quot;DL &quot;,ep[4],&quot; epochs&quot;), h2o.deeplearning(1:2,3,spiral,epochs=ep[4], checkpoint=&quot;dl_3&quot;,model_id=&quot;dl_4&quot;)) You can see how the network learns the structure of the spirals with enough training time. We explore different network architectures next: #dev.new(noRStudioGD=FALSE) #direct plotting output to a new window par(mfrow=c(2,2)) #set up the canvas for 2x2 plots for (hidden in list(c(11,13,17,19),c(42,42,42),c(200,200),c(1000))) { plotC(paste0(&quot;DL hidden=&quot;,paste0(hidden, collapse=&quot;x&quot;)), h2o.deeplearning(1:2,3 ,spiral, hidden=hidden, epochs=500)) } It is clear that different configurations can achieve similar performance, and that tuning will be required for optimal performance. Next, we compare between different activation functions, including one with 50% dropout regularization in the hidden layers: #dev.new(noRStudioGD=FALSE) #direct plotting output to a new window par(mfrow=c(2,2)) #set up the canvas for 2x2 plots for (act in c(&quot;Tanh&quot;, &quot;Maxout&quot;, &quot;Rectifier&quot;, &quot;RectifierWithDropout&quot;)) { plotC(paste0(&quot;DL &quot;,act,&quot; activation&quot;), h2o.deeplearning(1:2,3, spiral, activation = act, hidden = c(100,100), epochs = 1000)) } Clearly, the dropout rate was too high or the number of epochs was too low for the last configuration, which often ends up performing the best on larger datasets where generalization is important. More information about the parameters can be found in the H2O Deep Learning booklet. 40.5 Cover Type Dataset We important the full cover type dataset (581k rows, 13 columns, 10 numerical, 3 categorical). We also split the data 3 ways: 60% for training, 20% for validation (hyper parameter tuning) and 20% for final testing. df &lt;- h2o.importFile(path = normalizePath(file.path(data_raw_dir, &quot;covtype.full.csv&quot;))) #&gt; | | | 0% | |=========================================== | 66% | |=================================================================| 100% dim(df) #&gt; [1] 581012 13 df #&gt; Elevation Aspect Slope Horizontal_Distance_To_Hydrology #&gt; 1 3066 124 5 0 #&gt; 2 3136 32 20 450 #&gt; 3 2655 28 14 42 #&gt; 4 3191 45 19 323 #&gt; 5 3217 80 13 30 #&gt; 6 3119 293 13 30 #&gt; Vertical_Distance_To_Hydrology Horizontal_Distance_To_Roadways #&gt; 1 0 1533 #&gt; 2 -38 1290 #&gt; 3 8 1890 #&gt; 4 88 3932 #&gt; 5 1 3901 #&gt; 6 10 4810 #&gt; Hillshade_9am Hillshade_Noon Hillshade_3pm #&gt; 1 229 236 141 #&gt; 2 211 193 111 #&gt; 3 214 209 128 #&gt; 4 221 195 100 #&gt; 5 237 217 109 #&gt; 6 182 237 194 #&gt; Horizontal_Distance_To_Fire_Points Wilderness_Area Soil_Type Cover_Type #&gt; 1 459 area_0 type_22 class_1 #&gt; 2 1112 area_0 type_28 class_1 #&gt; 3 1001 area_2 type_9 class_2 #&gt; 4 2919 area_0 type_39 class_2 #&gt; 5 2859 area_0 type_22 class_7 #&gt; 6 1200 area_0 type_21 class_1 #&gt; #&gt; [581012 rows x 13 columns] splits &lt;- h2o.splitFrame(df, c(0.6, 0.2), seed=1234) train &lt;- h2o.assign(splits[[1]], &quot;train.hex&quot;) # 60% valid &lt;- h2o.assign(splits[[2]], &quot;valid.hex&quot;) # 20% test &lt;- h2o.assign(splits[[3]], &quot;test.hex&quot;) # 20% Here’s a scalable way to do scatter plots via binning (works for categorical and numeric columns) to get more familiar with the dataset. #dev.new(noRStudioGD=FALSE) #direct plotting output to a new window par(mfrow=c(1,1)) # reset canvas plot(h2o.tabulate(df, &quot;Elevation&quot;, &quot;Cover_Type&quot;)) #&gt; Registered S3 methods overwritten by &#39;ggplot2&#39;: #&gt; method from #&gt; [.quosures rlang #&gt; c.quosures rlang #&gt; print.quosures rlang plot(h2o.tabulate(df, &quot;Horizontal_Distance_To_Roadways&quot;, &quot;Cover_Type&quot;)) plot(h2o.tabulate(df, &quot;Soil_Type&quot;, &quot;Cover_Type&quot;)) plot(h2o.tabulate(df, &quot;Horizontal_Distance_To_Roadways&quot;, &quot;Elevation&quot; )) 40.5.1 First Run of H2O Deep Learning Let’s run our first Deep Learning model on the covtype dataset. We want to predict the Cover_Type column, a categorical feature with 7 levels, and the Deep Learning model will be tasked to perform (multi-class) classification. It uses the other 12 predictors of the dataset, of which 10 are numerical, and 2 are categorical with a total of 44 levels. We can expect the Deep Learning model to have 56 input neurons (after automatic one-hot encoding). response &lt;- &quot;Cover_Type&quot; predictors &lt;- setdiff(names(df), response) predictors #&gt; [1] &quot;Elevation&quot; #&gt; [2] &quot;Aspect&quot; #&gt; [3] &quot;Slope&quot; #&gt; [4] &quot;Horizontal_Distance_To_Hydrology&quot; #&gt; [5] &quot;Vertical_Distance_To_Hydrology&quot; #&gt; [6] &quot;Horizontal_Distance_To_Roadways&quot; #&gt; [7] &quot;Hillshade_9am&quot; #&gt; [8] &quot;Hillshade_Noon&quot; #&gt; [9] &quot;Hillshade_3pm&quot; #&gt; [10] &quot;Horizontal_Distance_To_Fire_Points&quot; #&gt; [11] &quot;Wilderness_Area&quot; #&gt; [12] &quot;Soil_Type&quot; train_df &lt;- as.data.frame(train) str(train_df) #&gt; &#39;data.frame&#39;: 349015 obs. of 13 variables: #&gt; $ Elevation : int 3136 3217 3119 2679 3261 2885 3227 2843 2853 2883 ... #&gt; $ Aspect : int 32 80 293 48 322 26 32 12 124 177 ... #&gt; $ Slope : int 20 13 13 7 13 9 6 18 12 9 ... #&gt; $ Horizontal_Distance_To_Hydrology : int 450 30 30 150 30 192 108 335 30 426 ... #&gt; $ Vertical_Distance_To_Hydrology : int -38 1 10 24 5 38 13 50 -5 126 ... #&gt; $ Horizontal_Distance_To_Roadways : int 1290 3901 4810 1588 5701 3271 5542 2642 1485 2139 ... #&gt; $ Hillshade_9am : int 211 237 182 223 186 216 219 199 240 225 ... #&gt; $ Hillshade_Noon : int 193 217 237 224 226 220 227 201 231 246 ... #&gt; $ Hillshade_3pm : int 111 109 194 136 180 140 145 135 119 153 ... #&gt; $ Horizontal_Distance_To_Fire_Points: int 1112 2859 1200 6265 769 2643 765 1719 2497 713 ... #&gt; $ Wilderness_Area : Factor w/ 4 levels &quot;area_0&quot;,&quot;area_1&quot;,..: 1 1 1 1 1 1 1 3 3 3 ... #&gt; $ Soil_Type : Factor w/ 40 levels &quot;type_0&quot;,&quot;type_1&quot;,..: 22 16 15 4 15 22 15 27 12 25 ... #&gt; $ Cover_Type : Factor w/ 7 levels &quot;class_1&quot;,&quot;class_2&quot;,..: 1 7 1 2 1 2 1 2 1 2 ... valid_df &lt;- as.data.frame(valid) str(valid_df) #&gt; &#39;data.frame&#39;: 116018 obs. of 13 variables: #&gt; $ Elevation : int 3066 2655 2902 2994 2697 2990 3237 2884 2972 2696 ... #&gt; $ Aspect : int 124 28 304 61 93 59 135 71 100 169 ... #&gt; $ Slope : int 5 14 22 9 9 12 14 9 4 10 ... #&gt; $ Horizontal_Distance_To_Hydrology : int 0 42 511 391 306 108 240 459 175 323 ... #&gt; $ Vertical_Distance_To_Hydrology : int 0 8 18 57 -2 10 -11 141 13 149 ... #&gt; $ Horizontal_Distance_To_Roadways : int 1533 1890 1273 4286 553 2190 1189 1214 5031 2452 ... #&gt; $ Hillshade_9am : int 229 214 155 227 234 229 241 231 227 228 ... #&gt; $ Hillshade_Noon : int 236 209 223 222 227 215 233 222 234 244 ... #&gt; $ Hillshade_3pm : int 141 128 206 128 125 117 118 124 142 148 ... #&gt; $ Horizontal_Distance_To_Fire_Points: int 459 1001 1347 1928 1716 1048 2748 1355 6198 1044 ... #&gt; $ Wilderness_Area : Factor w/ 4 levels &quot;area_0&quot;,&quot;area_1&quot;,..: 1 3 3 1 1 3 1 3 1 3 ... #&gt; $ Soil_Type : Factor w/ 39 levels &quot;type_0&quot;,&quot;type_1&quot;,..: 15 39 25 4 4 25 14 25 11 23 ... #&gt; $ Cover_Type : Factor w/ 7 levels &quot;class_1&quot;,&quot;class_2&quot;,..: 1 2 2 2 2 2 1 2 1 3 ... To keep it fast, we only run for one epoch (one pass over the training data). m1 &lt;- h2o.deeplearning( model_id=&quot;dl_model_first&quot;, training_frame = train, validation_frame = valid, ## validation dataset: used for scoring and early stopping x = predictors, y = response, #activation=&quot;Rectifier&quot;, ## default #hidden=c(200,200), ## default: 2 hidden layers with 200 neurons each epochs = 1, variable_importances=T ## not enabled by default ) #&gt; | | | 0% | |====== | 10% | |============= | 20% | |=================== | 30% | |========================== | 40% | |================================ | 50% | |======================================= | 60% | |============================================= | 70% | |==================================================== | 80% | |========================================================== | 90% | |=================================================================| 100% summary(m1) #&gt; Model Details: #&gt; ============== #&gt; #&gt; H2OMultinomialModel: deeplearning #&gt; Model Key: dl_model_first #&gt; Status of Neuron Layers: predicting Cover_Type, 7-class classification, multinomial distribution, CrossEntropy loss, 53,007 weights/biases, 634.3 KB, 349,080 training samples, mini-batch size 1 #&gt; layer units type dropout l1 l2 mean_rate rate_rms #&gt; 1 1 56 Input 0.00 % NA NA NA NA #&gt; 2 2 200 Rectifier 0.00 % 0.000000 0.000000 0.061056 0.230335 #&gt; 3 3 200 Rectifier 0.00 % 0.000000 0.000000 0.009885 0.008117 #&gt; 4 4 7 Softmax NA 0.000000 0.000000 0.129550 0.313866 #&gt; momentum mean_weight weight_rms mean_bias bias_rms #&gt; 1 NA NA NA NA NA #&gt; 2 0.000000 -0.010486 0.117157 0.000462 0.119399 #&gt; 3 0.000000 -0.024478 0.116401 0.686478 0.399755 #&gt; 4 0.000000 -0.346361 0.489526 -0.476492 0.102529 #&gt; #&gt; H2OMultinomialMetrics: deeplearning #&gt; ** Reported on training data. ** #&gt; ** Metrics reported on temporary training frame with 10041 samples ** #&gt; #&gt; Training Set Metrics: #&gt; ===================== #&gt; #&gt; MSE: (Extract with `h2o.mse`) 0.135 #&gt; RMSE: (Extract with `h2o.rmse`) 0.367 #&gt; Logloss: (Extract with `h2o.logloss`) 0.433 #&gt; Mean Per-Class Error: 0.303 #&gt; Confusion Matrix: Extract with `h2o.confusionMatrix(&lt;model&gt;,train = TRUE)`) #&gt; ========================================================================= #&gt; Confusion Matrix: Row labels: Actual class; Column labels: Predicted class #&gt; class_1 class_2 class_3 class_4 class_5 class_6 class_7 Error #&gt; class_1 3008 587 3 0 17 2 30 0.1752 #&gt; class_2 605 4178 97 0 48 28 3 0.1575 #&gt; class_3 0 13 572 6 0 14 0 0.0545 #&gt; class_4 0 0 20 24 0 0 0 0.4545 #&gt; class_5 2 62 6 0 93 1 0 0.4329 #&gt; class_6 0 25 153 0 0 110 0 0.6181 #&gt; class_7 73 4 0 0 0 0 257 0.2305 #&gt; Totals 3688 4869 851 30 158 155 290 0.1792 #&gt; Rate #&gt; class_1 = 639 / 3,647 #&gt; class_2 = 781 / 4,959 #&gt; class_3 = 33 / 605 #&gt; class_4 = 20 / 44 #&gt; class_5 = 71 / 164 #&gt; class_6 = 178 / 288 #&gt; class_7 = 77 / 334 #&gt; Totals = 1,799 / 10,041 #&gt; #&gt; Hit Ratio Table: Extract with `h2o.hit_ratio_table(&lt;model&gt;,train = TRUE)` #&gt; ======================================================================= #&gt; Top-7 Hit Ratios: #&gt; k hit_ratio #&gt; 1 1 0.820835 #&gt; 2 2 0.985858 #&gt; 3 3 0.997411 #&gt; 4 4 0.999303 #&gt; 5 5 1.000000 #&gt; 6 6 1.000000 #&gt; 7 7 1.000000 #&gt; #&gt; #&gt; H2OMultinomialMetrics: deeplearning #&gt; ** Reported on validation data. ** #&gt; ** Metrics reported on full validation frame ** #&gt; #&gt; Validation Set Metrics: #&gt; ===================== #&gt; #&gt; Extract validation frame with `h2o.getFrame(&quot;valid.hex&quot;)` #&gt; MSE: (Extract with `h2o.mse`) 0.137 #&gt; RMSE: (Extract with `h2o.rmse`) 0.371 #&gt; Logloss: (Extract with `h2o.logloss`) 0.439 #&gt; Mean Per-Class Error: 0.316 #&gt; Confusion Matrix: Extract with `h2o.confusionMatrix(&lt;model&gt;,valid = TRUE)`) #&gt; ========================================================================= #&gt; Confusion Matrix: Row labels: Actual class; Column labels: Predicted class #&gt; class_1 class_2 class_3 class_4 class_5 class_6 class_7 Error #&gt; class_1 35056 6891 22 0 131 34 366 0.1752 #&gt; class_2 7170 47060 1019 1 681 385 64 0.1653 #&gt; class_3 0 185 6737 71 6 144 0 0.0568 #&gt; class_4 0 0 258 290 0 14 0 0.4840 #&gt; class_5 29 785 78 0 954 24 0 0.4898 #&gt; class_6 4 240 1910 26 0 1284 0 0.6293 #&gt; class_7 843 34 0 0 0 0 3222 0.2140 #&gt; Totals 43102 55195 10024 388 1772 1885 3652 0.1846 #&gt; Rate #&gt; class_1 = 7,444 / 42,500 #&gt; class_2 = 9,320 / 56,380 #&gt; class_3 = 406 / 7,143 #&gt; class_4 = 272 / 562 #&gt; class_5 = 916 / 1,870 #&gt; class_6 = 2,180 / 3,464 #&gt; class_7 = 877 / 4,099 #&gt; Totals = 21,415 / 116,018 #&gt; #&gt; Hit Ratio Table: Extract with `h2o.hit_ratio_table(&lt;model&gt;,valid = TRUE)` #&gt; ======================================================================= #&gt; Top-7 Hit Ratios: #&gt; k hit_ratio #&gt; 1 1 0.815417 #&gt; 2 2 0.981925 #&gt; 3 3 0.997552 #&gt; 4 4 0.999431 #&gt; 5 5 0.999992 #&gt; 6 6 1.000000 #&gt; 7 7 1.000000 #&gt; #&gt; #&gt; #&gt; #&gt; Scoring History: #&gt; timestamp duration training_speed epochs iterations #&gt; 1 2019-09-20 14:08:08 0.000 sec NA 0.00000 0 #&gt; 2 2019-09-20 14:08:12 6.206 sec 7781 obs/sec 0.10021 1 #&gt; 3 2019-09-20 14:08:29 22.499 sec 10838 obs/sec 0.59961 6 #&gt; 4 2019-09-20 14:08:40 33.932 sec 11924 obs/sec 1.00019 10 #&gt; samples training_rmse training_logloss training_r2 #&gt; 1 0.000000 NA NA NA #&gt; 2 34976.000000 0.46173 0.66223 0.88643 #&gt; 3 209272.000000 0.39068 0.48783 0.91869 #&gt; 4 349080.000000 0.36728 0.43254 0.92814 #&gt; training_classification_error validation_rmse validation_logloss #&gt; 1 NA NA NA #&gt; 2 0.28742 0.46230 0.66253 #&gt; 3 0.20635 0.39204 0.49126 #&gt; 4 0.17917 0.37066 0.43934 #&gt; validation_r2 validation_classification_error #&gt; 1 NA NA #&gt; 2 0.89045 0.28826 #&gt; 3 0.92122 0.21111 #&gt; 4 0.92958 0.18458 #&gt; #&gt; Variable Importances: (Extract with `h2o.varimp`) #&gt; ================================================= #&gt; #&gt; Variable Importances: #&gt; variable relative_importance scaled_importance #&gt; 1 Wilderness_Area.area_0 1.000000 1.000000 #&gt; 2 Horizontal_Distance_To_Roadways 0.851341 0.851341 #&gt; 3 Elevation 0.808990 0.808990 #&gt; 4 Horizontal_Distance_To_Fire_Points 0.803482 0.803482 #&gt; 5 Wilderness_Area.area_1 0.692076 0.692076 #&gt; percentage #&gt; 1 0.035292 #&gt; 2 0.030046 #&gt; 3 0.028551 #&gt; 4 0.028357 #&gt; 5 0.024425 #&gt; #&gt; --- #&gt; variable relative_importance scaled_importance #&gt; 51 Soil_Type.type_7 0.391090 0.391090 #&gt; 52 Slope 0.383533 0.383533 #&gt; 53 Hillshade_3pm 0.336801 0.336801 #&gt; 54 Aspect 0.280140 0.280140 #&gt; 55 Soil_Type.missing(NA) 0.000000 0.000000 #&gt; 56 Wilderness_Area.missing(NA) 0.000000 0.000000 #&gt; percentage #&gt; 51 0.013803 #&gt; 52 0.013536 #&gt; 53 0.011887 #&gt; 54 0.009887 #&gt; 55 0.000000 #&gt; 56 0.000000 Inspect the model in Flow for more information about model building etc. by issuing a cell with the content getModel “dl_model_first”, and pressing Ctrl-Enter. 40.5.2 Variable Importances Variable importances for Neural Network models are notoriously difficult to compute, and there are many pitfalls. H2O Deep Learning has implemented the method of Gedeon, and returns relative variable importances in descending order of importance. head(as.data.frame(h2o.varimp(m1))) #&gt; variable relative_importance scaled_importance #&gt; 1 Wilderness_Area.area_0 1.000 1.000 #&gt; 2 Horizontal_Distance_To_Roadways 0.851 0.851 #&gt; 3 Elevation 0.809 0.809 #&gt; 4 Horizontal_Distance_To_Fire_Points 0.803 0.803 #&gt; 5 Wilderness_Area.area_1 0.692 0.692 #&gt; 6 Wilderness_Area.area_3 0.688 0.688 #&gt; percentage #&gt; 1 0.0353 #&gt; 2 0.0300 #&gt; 3 0.0286 #&gt; 4 0.0284 #&gt; 5 0.0244 #&gt; 6 0.0243 40.5.3 Early Stopping Now we run another, smaller network, and we let it stop automatically once the misclassification rate converges (specifically, if the moving average of length 2 does not improve by at least 1% for 2 consecutive scoring events). We also sample the validation set to 10,000 rows for faster scoring. m2 &lt;- h2o.deeplearning( model_id=&quot;dl_model_faster&quot;, training_frame=train, validation_frame=valid, x=predictors, y=response, hidden=c(32,32,32), ## small network, runs faster epochs=1000000, ## hopefully converges earlier... score_validation_samples=10000, ## sample the validation dataset (faster) stopping_rounds=2, stopping_metric=&quot;misclassification&quot;, ## could be &quot;MSE&quot;,&quot;logloss&quot;,&quot;r2&quot; stopping_tolerance=0.01 ) #&gt; | | | 0% | |=================================================================| 100% summary(m2) #&gt; Model Details: #&gt; ============== #&gt; #&gt; H2OMultinomialModel: deeplearning #&gt; Model Key: dl_model_faster #&gt; Status of Neuron Layers: predicting Cover_Type, 7-class classification, multinomial distribution, CrossEntropy loss, 4,167 weights/biases, 59.0 KB, 4,801,487 training samples, mini-batch size 1 #&gt; layer units type dropout l1 l2 mean_rate rate_rms #&gt; 1 1 56 Input 0.00 % NA NA NA NA #&gt; 2 2 32 Rectifier 0.00 % 0.000000 0.000000 0.048217 0.211403 #&gt; 3 3 32 Rectifier 0.00 % 0.000000 0.000000 0.000359 0.000192 #&gt; 4 4 32 Rectifier 0.00 % 0.000000 0.000000 0.000822 0.000738 #&gt; 5 5 7 Softmax NA 0.000000 0.000000 0.147508 0.351749 #&gt; momentum mean_weight weight_rms mean_bias bias_rms #&gt; 1 NA NA NA NA NA #&gt; 2 0.000000 -0.003315 0.270473 0.197707 0.280024 #&gt; 3 0.000000 -0.057896 0.381432 0.600188 0.719822 #&gt; 4 0.000000 0.126392 0.611872 1.042345 1.523591 #&gt; 5 0.000000 -4.139887 3.172857 -2.218202 0.572794 #&gt; #&gt; H2OMultinomialMetrics: deeplearning #&gt; ** Reported on training data. ** #&gt; ** Metrics reported on temporary training frame with 10035 samples ** #&gt; #&gt; Training Set Metrics: #&gt; ===================== #&gt; #&gt; MSE: (Extract with `h2o.mse`) 0.117 #&gt; RMSE: (Extract with `h2o.rmse`) 0.342 #&gt; Logloss: (Extract with `h2o.logloss`) 0.39 #&gt; Mean Per-Class Error: 0.243 #&gt; Confusion Matrix: Extract with `h2o.confusionMatrix(&lt;model&gt;,train = TRUE)`) #&gt; ========================================================================= #&gt; Confusion Matrix: Row labels: Actual class; Column labels: Predicted class #&gt; class_1 class_2 class_3 class_4 class_5 class_6 class_7 Error #&gt; class_1 2990 661 0 0 10 3 10 0.1862 #&gt; class_2 356 4369 55 0 60 43 4 0.1060 #&gt; class_3 0 18 525 8 0 40 0 0.1117 #&gt; class_4 0 0 19 40 0 1 0 0.3333 #&gt; class_5 7 44 3 0 114 0 0 0.3214 #&gt; class_6 0 16 95 0 3 180 0 0.3878 #&gt; class_7 83 8 0 0 0 0 270 0.2521 #&gt; Totals 3436 5116 697 48 187 267 284 0.1542 #&gt; Rate #&gt; class_1 = 684 / 3,674 #&gt; class_2 = 518 / 4,887 #&gt; class_3 = 66 / 591 #&gt; class_4 = 20 / 60 #&gt; class_5 = 54 / 168 #&gt; class_6 = 114 / 294 #&gt; class_7 = 91 / 361 #&gt; Totals = 1,547 / 10,035 #&gt; #&gt; Hit Ratio Table: Extract with `h2o.hit_ratio_table(&lt;model&gt;,train = TRUE)` #&gt; ======================================================================= #&gt; Top-7 Hit Ratios: #&gt; k hit_ratio #&gt; 1 1 0.845840 #&gt; 2 2 0.986148 #&gt; 3 3 0.997907 #&gt; 4 4 0.999900 #&gt; 5 5 1.000000 #&gt; 6 6 1.000000 #&gt; 7 7 1.000000 #&gt; #&gt; #&gt; H2OMultinomialMetrics: deeplearning #&gt; ** Reported on validation data. ** #&gt; ** Metrics reported on temporary validation frame with 10002 samples ** #&gt; #&gt; Validation Set Metrics: #&gt; ===================== #&gt; #&gt; MSE: (Extract with `h2o.mse`) 0.12 #&gt; RMSE: (Extract with `h2o.rmse`) 0.346 #&gt; Logloss: (Extract with `h2o.logloss`) 0.398 #&gt; Mean Per-Class Error: 0.268 #&gt; Confusion Matrix: Extract with `h2o.confusionMatrix(&lt;model&gt;,valid = TRUE)`) #&gt; ========================================================================= #&gt; Confusion Matrix: Row labels: Actual class; Column labels: Predicted class #&gt; class_1 class_2 class_3 class_4 class_5 class_6 class_7 Error #&gt; class_1 2963 669 1 0 9 5 17 0.1913 #&gt; class_2 391 4331 39 0 57 50 1 0.1105 #&gt; class_3 0 26 520 1 2 46 0 0.1261 #&gt; class_4 0 0 14 28 0 2 0 0.3636 #&gt; class_5 6 48 5 0 109 0 0 0.3512 #&gt; class_6 1 17 106 0 2 167 0 0.4300 #&gt; class_7 107 4 0 0 0 0 258 0.3008 #&gt; Totals 3468 5095 685 29 179 270 276 0.1626 #&gt; Rate #&gt; class_1 = 701 / 3,664 #&gt; class_2 = 538 / 4,869 #&gt; class_3 = 75 / 595 #&gt; class_4 = 16 / 44 #&gt; class_5 = 59 / 168 #&gt; class_6 = 126 / 293 #&gt; class_7 = 111 / 369 #&gt; Totals = 1,626 / 10,002 #&gt; #&gt; Hit Ratio Table: Extract with `h2o.hit_ratio_table(&lt;model&gt;,valid = TRUE)` #&gt; ======================================================================= #&gt; Top-7 Hit Ratios: #&gt; k hit_ratio #&gt; 1 1 0.837433 #&gt; 2 2 0.986303 #&gt; 3 3 0.997501 #&gt; 4 4 0.999600 #&gt; 5 5 1.000000 #&gt; 6 6 1.000000 #&gt; 7 7 1.000000 #&gt; #&gt; #&gt; #&gt; #&gt; Scoring History: #&gt; timestamp duration training_speed epochs iterations #&gt; 1 2019-09-20 14:08:43 0.000 sec NA 0.00000 0 #&gt; 2 2019-09-20 14:08:44 1.216 sec 90795 obs/sec 0.28564 1 #&gt; 3 2019-09-20 14:08:50 6.925 sec 103321 obs/sec 2.00268 7 #&gt; 4 2019-09-20 14:08:55 12.322 sec 107168 obs/sec 3.72126 13 #&gt; 5 2019-09-20 14:09:01 18.077 sec 112113 obs/sec 5.72752 20 #&gt; 6 2019-09-20 14:09:06 23.803 sec 114807 obs/sec 7.73521 27 #&gt; 7 2019-09-20 14:09:12 29.465 sec 116683 obs/sec 9.74012 34 #&gt; 8 2019-09-20 14:09:18 35.024 sec 118357 obs/sec 11.74843 41 #&gt; 9 2019-09-20 14:09:23 40.557 sec 119630 obs/sec 13.75725 48 #&gt; samples training_rmse training_logloss training_r2 #&gt; 1 0.000000 NA NA NA #&gt; 2 99693.000000 0.43568 0.59761 0.90340 #&gt; 3 698967.000000 0.38272 0.46714 0.92546 #&gt; 4 1298774.000000 0.37066 0.44146 0.93008 #&gt; 5 1998990.000000 0.36280 0.42550 0.93302 #&gt; 6 2699705.000000 0.35085 0.39788 0.93736 #&gt; 7 3399447.000000 0.34842 0.39948 0.93822 #&gt; 8 4100380.000000 0.36889 0.43925 0.93075 #&gt; 9 4801487.000000 0.34150 0.39013 0.94065 #&gt; training_classification_error validation_rmse validation_logloss #&gt; 1 NA NA NA #&gt; 2 0.25690 0.43526 0.59829 #&gt; 3 0.19920 0.38807 0.47756 #&gt; 4 0.18794 0.37362 0.45339 #&gt; 5 0.17708 0.36614 0.43630 #&gt; 6 0.16911 0.35516 0.41119 #&gt; 7 0.16293 0.35522 0.41280 #&gt; 8 0.18535 0.37607 0.45783 #&gt; 9 0.15416 0.34589 0.39790 #&gt; validation_r2 validation_classification_error #&gt; 1 NA NA #&gt; 2 0.90445 0.25235 #&gt; 3 0.92404 0.20556 #&gt; 4 0.92960 0.18686 #&gt; 5 0.93238 0.18006 #&gt; 6 0.93638 0.16917 #&gt; 7 0.93636 0.16877 #&gt; 8 0.92867 0.19166 #&gt; 9 0.93966 0.16257 #&gt; #&gt; Variable Importances: (Extract with `h2o.varimp`) #&gt; ================================================= #&gt; #&gt; Variable Importances: #&gt; variable relative_importance scaled_importance #&gt; 1 Elevation 1.000000 1.000000 #&gt; 2 Wilderness_Area.area_0 0.997877 0.997877 #&gt; 3 Horizontal_Distance_To_Fire_Points 0.957805 0.957805 #&gt; 4 Wilderness_Area.area_1 0.915098 0.915098 #&gt; 5 Wilderness_Area.area_3 0.889562 0.889562 #&gt; percentage #&gt; 1 0.032497 #&gt; 2 0.032428 #&gt; 3 0.031126 #&gt; 4 0.029738 #&gt; 5 0.028908 #&gt; #&gt; --- #&gt; variable relative_importance scaled_importance #&gt; 51 Hillshade_3pm 0.326472 0.326472 #&gt; 52 Vertical_Distance_To_Hydrology 0.276031 0.276031 #&gt; 53 Slope 0.236305 0.236305 #&gt; 54 Aspect 0.087917 0.087917 #&gt; 55 Soil_Type.missing(NA) 0.000000 0.000000 #&gt; 56 Wilderness_Area.missing(NA) 0.000000 0.000000 #&gt; percentage #&gt; 51 0.010609 #&gt; 52 0.008970 #&gt; 53 0.007679 #&gt; 54 0.002857 #&gt; 55 0.000000 #&gt; 56 0.000000 plot(m2) 40.5.4 Adaptive Learning Rate By default, H2O Deep Learning uses an adaptive learning rate (ADADELTA) for its stochastic gradient descent optimization. There are only two tuning parameters for this method: rho and epsilon, which balance the global and local search efficiencies. rho is the similarity to prior weight updates (similar to momentum), and epsilon is a parameter that prevents the optimization to get stuck in local optima. Defaults are rho=0.99 and epsilon=1e-8. For cases where convergence speed is very important, it might make sense to perform a few runs to optimize these two parameters (e.g., with rho in c(0.9,0.95,0.99,0.999) and epsilon in c(1e-10,1e-8,1e-6,1e-4)). Of course, as always with grid searches, caution has to be applied when extrapolating grid search results to a different parameter regime (e.g., for more epochs or different layer topologies or activation functions, etc.). If adaptive_rate is disabled, several manual learning rate parameters become important: rate, rate_annealing, rate_decay, momentum_start, momentum_ramp, momentum_stable and nesterov_accelerated_gradient, the discussion of which we leave to H2O Deep Learning booklet. 40.5.5 Tuning With some tuning, it is possible to obtain less than 10% test set error rate in about one minute. Error rates of below 5% are possible with larger models. Note that deep tree methods can be more effective for this dataset than Deep Learning, as they directly partition the space into sectors, which seems to be needed here. m3 &lt;- h2o.deeplearning( model_id=&quot;dl_model_tuned&quot;, training_frame=train, validation_frame=valid, x=predictors, y=response, overwrite_with_best_model=F, ## Return final model after 10 epochs, even if not the best hidden=c(128,128,128), ## more hidden layers -&gt; more complex interactions epochs=10, ## to keep it short enough score_validation_samples=10000, ## downsample validation set for faster scoring score_duty_cycle=0.025, ## don&#39;t score more than 2.5% of the wall time adaptive_rate=F, ## manually tuned learning rate rate=0.01, rate_annealing=2e-6, momentum_start=0.2, ## manually tuned momentum momentum_stable=0.4, momentum_ramp=1e7, l1=1e-5, ## add some L1/L2 regularization l2=1e-5, max_w2=10 ## helps stability for Rectifier ) #&gt; | | | 0% | |== | 3% | |==== | 6% | |====== | 9% | |======= | 11% | |========= | 14% | |=========== | 17% | |============= | 20% | |=============== | 23% | |================= | 26% | |=================== | 29% | |==================== | 32% | |====================== | 34% | |======================== | 37% | |========================== | 40% | |============================ | 43% | |============================== | 46% | |================================ | 49% | |================================== | 52% | |=================================== | 54% | |===================================== | 57% | |======================================= | 60% | |========================================= | 63% | |=========================================== | 66% | |============================================= | 69% | |=============================================== | 72% | |================================================ | 74% | |================================================== | 77% | |==================================================== | 80% | |====================================================== | 83% | |======================================================== | 86% | |========================================================== | 89% | |============================================================ | 92% | |============================================================= | 95% | |=============================================================== | 97% | |=================================================================| 100% summary(m3) #&gt; Model Details: #&gt; ============== #&gt; #&gt; H2OMultinomialModel: deeplearning #&gt; Model Key: dl_model_tuned #&gt; Status of Neuron Layers: predicting Cover_Type, 7-class classification, multinomial distribution, CrossEntropy loss, 41,223 weights/biases, 334.1 KB, 3,500,367 training samples, mini-batch size 1 #&gt; layer units type dropout l1 l2 mean_rate rate_rms #&gt; 1 1 56 Input 0.00 % NA NA NA NA #&gt; 2 2 128 Rectifier 0.00 % 0.000010 0.000010 0.001250 0.000000 #&gt; 3 3 128 Rectifier 0.00 % 0.000010 0.000010 0.001250 0.000000 #&gt; 4 4 128 Rectifier 0.00 % 0.000010 0.000010 0.001250 0.000000 #&gt; 5 5 7 Softmax NA 0.000010 0.000010 0.001250 0.000000 #&gt; momentum mean_weight weight_rms mean_bias bias_rms #&gt; 1 NA NA NA NA NA #&gt; 2 0.270007 -0.014110 0.315929 0.013443 0.294899 #&gt; 3 0.270007 -0.052447 0.221344 0.859128 0.355188 #&gt; 4 0.270007 -0.065243 0.216111 0.820031 0.199900 #&gt; 5 0.270007 -0.020254 0.270093 0.013140 0.966391 #&gt; #&gt; H2OMultinomialMetrics: deeplearning #&gt; ** Reported on training data. ** #&gt; ** Metrics reported on temporary training frame with 10022 samples ** #&gt; #&gt; Training Set Metrics: #&gt; ===================== #&gt; #&gt; MSE: (Extract with `h2o.mse`) 0.055 #&gt; RMSE: (Extract with `h2o.rmse`) 0.235 #&gt; Logloss: (Extract with `h2o.logloss`) 0.182 #&gt; Mean Per-Class Error: 0.105 #&gt; Confusion Matrix: Extract with `h2o.confusionMatrix(&lt;model&gt;,train = TRUE)`) #&gt; ========================================================================= #&gt; Confusion Matrix: Row labels: Actual class; Column labels: Predicted class #&gt; class_1 class_2 class_3 class_4 class_5 class_6 class_7 Error #&gt; class_1 3424 247 0 0 1 1 12 0.0708 #&gt; class_2 262 4557 20 0 17 20 1 0.0656 #&gt; class_3 0 8 518 8 0 23 0 0.0700 #&gt; class_4 0 0 4 41 0 1 0 0.1087 #&gt; class_5 4 22 7 0 135 3 0 0.2105 #&gt; class_6 0 8 33 1 0 274 0 0.1329 #&gt; class_7 25 2 0 0 0 0 343 0.0730 #&gt; Totals 3715 4844 582 50 153 322 356 0.0728 #&gt; Rate #&gt; class_1 = 261 / 3,685 #&gt; class_2 = 320 / 4,877 #&gt; class_3 = 39 / 557 #&gt; class_4 = 5 / 46 #&gt; class_5 = 36 / 171 #&gt; class_6 = 42 / 316 #&gt; class_7 = 27 / 370 #&gt; Totals = 730 / 10,022 #&gt; #&gt; Hit Ratio Table: Extract with `h2o.hit_ratio_table(&lt;model&gt;,train = TRUE)` #&gt; ======================================================================= #&gt; Top-7 Hit Ratios: #&gt; k hit_ratio #&gt; 1 1 0.927160 #&gt; 2 2 0.996707 #&gt; 3 3 0.999401 #&gt; 4 4 0.999900 #&gt; 5 5 1.000000 #&gt; 6 6 1.000000 #&gt; 7 7 1.000000 #&gt; #&gt; #&gt; H2OMultinomialMetrics: deeplearning #&gt; ** Reported on validation data. ** #&gt; ** Metrics reported on temporary validation frame with 9919 samples ** #&gt; #&gt; Validation Set Metrics: #&gt; ===================== #&gt; #&gt; MSE: (Extract with `h2o.mse`) 0.0605 #&gt; RMSE: (Extract with `h2o.rmse`) 0.246 #&gt; Logloss: (Extract with `h2o.logloss`) 0.204 #&gt; Mean Per-Class Error: 0.135 #&gt; Confusion Matrix: Extract with `h2o.confusionMatrix(&lt;model&gt;,valid = TRUE)`) #&gt; ========================================================================= #&gt; Confusion Matrix: Row labels: Actual class; Column labels: Predicted class #&gt; class_1 class_2 class_3 class_4 class_5 class_6 class_7 Error #&gt; class_1 3402 274 0 0 5 0 22 0.0813 #&gt; class_2 252 4523 10 1 22 15 2 0.0626 #&gt; class_3 0 13 516 7 1 57 0 0.1313 #&gt; class_4 0 0 4 31 0 4 0 0.2051 #&gt; class_5 4 28 6 0 112 2 0 0.2632 #&gt; class_6 0 8 22 4 0 241 0 0.1236 #&gt; class_7 22 4 0 0 0 0 305 0.0785 #&gt; Totals 3680 4850 558 43 140 319 329 0.0795 #&gt; Rate #&gt; class_1 = 301 / 3,703 #&gt; class_2 = 302 / 4,825 #&gt; class_3 = 78 / 594 #&gt; class_4 = 8 / 39 #&gt; class_5 = 40 / 152 #&gt; class_6 = 34 / 275 #&gt; class_7 = 26 / 331 #&gt; Totals = 789 / 9,919 #&gt; #&gt; Hit Ratio Table: Extract with `h2o.hit_ratio_table(&lt;model&gt;,valid = TRUE)` #&gt; ======================================================================= #&gt; Top-7 Hit Ratios: #&gt; k hit_ratio #&gt; 1 1 0.920456 #&gt; 2 2 0.995766 #&gt; 3 3 0.999597 #&gt; 4 4 1.000000 #&gt; 5 5 1.000000 #&gt; 6 6 1.000000 #&gt; 7 7 1.000000 #&gt; #&gt; #&gt; #&gt; #&gt; Scoring History: #&gt; timestamp duration training_speed epochs #&gt; 1 2019-09-20 14:09:24 0.000 sec NA 0.00000 #&gt; 2 2019-09-20 14:09:30 5.922 sec 17850 obs/sec 0.28631 #&gt; 3 2019-09-20 14:09:42 17.752 sec 23214 obs/sec 1.14631 #&gt; 4 2019-09-20 14:09:52 28.089 sec 25547 obs/sec 2.00435 #&gt; 5 2019-09-20 14:10:02 37.975 sec 26946 obs/sec 2.86444 #&gt; 6 2019-09-20 14:10:11 47.573 sec 27959 obs/sec 3.72560 #&gt; 7 2019-09-20 14:10:21 57.520 sec 28432 obs/sec 4.58412 #&gt; 8 2019-09-20 14:10:31 1 min 7.206 sec 28884 obs/sec 5.44330 #&gt; 9 2019-09-20 14:10:39 1 min 15.521 sec 28414 obs/sec 6.01692 #&gt; 10 2019-09-20 14:10:49 1 min 25.534 sec 28661 obs/sec 6.87729 #&gt; 11 2019-09-20 14:10:59 1 min 35.345 sec 28924 obs/sec 7.73868 #&gt; 12 2019-09-20 14:11:09 1 min 45.086 sec 29150 obs/sec 8.59711 #&gt; 13 2019-09-20 14:11:19 1 min 55.010 sec 29290 obs/sec 9.45618 #&gt; 14 2019-09-20 14:11:26 2 min 1.615 sec 29388 obs/sec 10.02927 #&gt; iterations samples training_rmse training_logloss training_r2 #&gt; 1 0 0.000000 NA NA NA #&gt; 2 1 99927.000000 0.42109 0.55795 0.91218 #&gt; 3 4 400079.000000 0.35849 0.40459 0.93635 #&gt; 4 7 699548.000000 0.33478 0.35861 0.94449 #&gt; 5 10 999733.000000 0.30593 0.30189 0.95365 #&gt; 6 13 1300290.000000 0.28786 0.27027 0.95896 #&gt; 7 16 1599925.000000 0.27623 0.24983 0.96221 #&gt; 8 19 1899792.000000 0.27505 0.24491 0.96253 #&gt; 9 21 2099994.000000 0.26154 0.22497 0.96612 #&gt; 10 24 2400278.000000 0.26558 0.23185 0.96507 #&gt; 11 27 2700916.000000 0.24645 0.20130 0.96992 #&gt; 12 30 3000521.000000 0.23921 0.19258 0.97166 #&gt; 13 33 3300348.000000 0.23954 0.18881 0.97158 #&gt; 14 35 3500367.000000 0.23457 0.18181 0.97275 #&gt; training_classification_error validation_rmse validation_logloss #&gt; 1 NA NA NA #&gt; 2 0.24646 0.41788 0.54873 #&gt; 3 0.17741 0.35647 0.40274 #&gt; 4 0.14987 0.33856 0.36805 #&gt; 5 0.12522 0.30739 0.30475 #&gt; 6 0.11106 0.29033 0.27836 #&gt; 7 0.10158 0.28607 0.26733 #&gt; 8 0.10257 0.27794 0.25822 #&gt; 9 0.09160 0.26908 0.24045 #&gt; 10 0.09349 0.27016 0.24311 #&gt; 11 0.08152 0.25695 0.21998 #&gt; 12 0.07424 0.25264 0.21443 #&gt; 13 0.07713 0.25078 0.20800 #&gt; 14 0.07284 0.24597 0.20354 #&gt; validation_r2 validation_classification_error #&gt; 1 NA NA #&gt; 2 0.90634 0.23924 #&gt; 3 0.93184 0.17008 #&gt; 4 0.93852 0.15697 #&gt; 5 0.94932 0.12441 #&gt; 6 0.95479 0.10989 #&gt; 7 0.95611 0.10797 #&gt; 8 0.95857 0.10172 #&gt; 9 0.96117 0.09578 #&gt; 10 0.96085 0.09658 #&gt; 11 0.96459 0.08882 #&gt; 12 0.96577 0.08338 #&gt; 13 0.96627 0.08368 #&gt; 14 0.96755 0.07954 #&gt; #&gt; Variable Importances: (Extract with `h2o.varimp`) #&gt; ================================================= #&gt; #&gt; Variable Importances: #&gt; variable relative_importance scaled_importance #&gt; 1 Elevation 1.000000 1.000000 #&gt; 2 Horizontal_Distance_To_Fire_Points 0.917530 0.917530 #&gt; 3 Horizontal_Distance_To_Roadways 0.871192 0.871192 #&gt; 4 Wilderness_Area.area_0 0.634473 0.634473 #&gt; 5 Horizontal_Distance_To_Hydrology 0.596334 0.596334 #&gt; percentage #&gt; 1 0.048702 #&gt; 2 0.044685 #&gt; 3 0.042429 #&gt; 4 0.030900 #&gt; 5 0.029043 #&gt; #&gt; --- #&gt; variable relative_importance scaled_importance #&gt; 51 Soil_Type.type_24 0.154181 0.154181 #&gt; 52 Soil_Type.type_35 0.153859 0.153859 #&gt; 53 Soil_Type.type_14 0.152899 0.152899 #&gt; 54 Soil_Type.type_13 0.149000 0.149000 #&gt; 55 Soil_Type.missing(NA) 0.000000 0.000000 #&gt; 56 Wilderness_Area.missing(NA) 0.000000 0.000000 #&gt; percentage #&gt; 51 0.007509 #&gt; 52 0.007493 #&gt; 53 0.007446 #&gt; 54 0.007257 #&gt; 55 0.000000 #&gt; 56 0.000000 Let’s compare the training error with the validation and test set errors h2o.performance(m3, train=T) ## sampled training data (from model building) #&gt; H2OMultinomialMetrics: deeplearning #&gt; ** Reported on training data. ** #&gt; ** Metrics reported on temporary training frame with 10022 samples ** #&gt; #&gt; Training Set Metrics: #&gt; ===================== #&gt; #&gt; MSE: (Extract with `h2o.mse`) 0.055 #&gt; RMSE: (Extract with `h2o.rmse`) 0.235 #&gt; Logloss: (Extract with `h2o.logloss`) 0.182 #&gt; Mean Per-Class Error: 0.105 #&gt; Confusion Matrix: Extract with `h2o.confusionMatrix(&lt;model&gt;,train = TRUE)`) #&gt; ========================================================================= #&gt; Confusion Matrix: Row labels: Actual class; Column labels: Predicted class #&gt; class_1 class_2 class_3 class_4 class_5 class_6 class_7 Error #&gt; class_1 3424 247 0 0 1 1 12 0.0708 #&gt; class_2 262 4557 20 0 17 20 1 0.0656 #&gt; class_3 0 8 518 8 0 23 0 0.0700 #&gt; class_4 0 0 4 41 0 1 0 0.1087 #&gt; class_5 4 22 7 0 135 3 0 0.2105 #&gt; class_6 0 8 33 1 0 274 0 0.1329 #&gt; class_7 25 2 0 0 0 0 343 0.0730 #&gt; Totals 3715 4844 582 50 153 322 356 0.0728 #&gt; Rate #&gt; class_1 = 261 / 3,685 #&gt; class_2 = 320 / 4,877 #&gt; class_3 = 39 / 557 #&gt; class_4 = 5 / 46 #&gt; class_5 = 36 / 171 #&gt; class_6 = 42 / 316 #&gt; class_7 = 27 / 370 #&gt; Totals = 730 / 10,022 #&gt; #&gt; Hit Ratio Table: Extract with `h2o.hit_ratio_table(&lt;model&gt;,train = TRUE)` #&gt; ======================================================================= #&gt; Top-7 Hit Ratios: #&gt; k hit_ratio #&gt; 1 1 0.927160 #&gt; 2 2 0.996707 #&gt; 3 3 0.999401 #&gt; 4 4 0.999900 #&gt; 5 5 1.000000 #&gt; 6 6 1.000000 #&gt; 7 7 1.000000 h2o.performance(m3, valid=T) ## sampled validation data (from model building) #&gt; H2OMultinomialMetrics: deeplearning #&gt; ** Reported on validation data. ** #&gt; ** Metrics reported on temporary validation frame with 9919 samples ** #&gt; #&gt; Validation Set Metrics: #&gt; ===================== #&gt; #&gt; MSE: (Extract with `h2o.mse`) 0.0605 #&gt; RMSE: (Extract with `h2o.rmse`) 0.246 #&gt; Logloss: (Extract with `h2o.logloss`) 0.204 #&gt; Mean Per-Class Error: 0.135 #&gt; Confusion Matrix: Extract with `h2o.confusionMatrix(&lt;model&gt;,valid = TRUE)`) #&gt; ========================================================================= #&gt; Confusion Matrix: Row labels: Actual class; Column labels: Predicted class #&gt; class_1 class_2 class_3 class_4 class_5 class_6 class_7 Error #&gt; class_1 3402 274 0 0 5 0 22 0.0813 #&gt; class_2 252 4523 10 1 22 15 2 0.0626 #&gt; class_3 0 13 516 7 1 57 0 0.1313 #&gt; class_4 0 0 4 31 0 4 0 0.2051 #&gt; class_5 4 28 6 0 112 2 0 0.2632 #&gt; class_6 0 8 22 4 0 241 0 0.1236 #&gt; class_7 22 4 0 0 0 0 305 0.0785 #&gt; Totals 3680 4850 558 43 140 319 329 0.0795 #&gt; Rate #&gt; class_1 = 301 / 3,703 #&gt; class_2 = 302 / 4,825 #&gt; class_3 = 78 / 594 #&gt; class_4 = 8 / 39 #&gt; class_5 = 40 / 152 #&gt; class_6 = 34 / 275 #&gt; class_7 = 26 / 331 #&gt; Totals = 789 / 9,919 #&gt; #&gt; Hit Ratio Table: Extract with `h2o.hit_ratio_table(&lt;model&gt;,valid = TRUE)` #&gt; ======================================================================= #&gt; Top-7 Hit Ratios: #&gt; k hit_ratio #&gt; 1 1 0.920456 #&gt; 2 2 0.995766 #&gt; 3 3 0.999597 #&gt; 4 4 1.000000 #&gt; 5 5 1.000000 #&gt; 6 6 1.000000 #&gt; 7 7 1.000000 h2o.performance(m3, newdata=train) ## full training data #&gt; H2OMultinomialMetrics: deeplearning #&gt; #&gt; Test Set Metrics: #&gt; ===================== #&gt; #&gt; MSE: (Extract with `h2o.mse`) 0.0556 #&gt; RMSE: (Extract with `h2o.rmse`) 0.236 #&gt; Logloss: (Extract with `h2o.logloss`) 0.185 #&gt; Mean Per-Class Error: 0.123 #&gt; Confusion Matrix: Extract with `h2o.confusionMatrix(&lt;model&gt;, &lt;data&gt;)`) #&gt; ========================================================================= #&gt; Confusion Matrix: Row labels: Actual class; Column labels: Predicted class #&gt; class_1 class_2 class_3 class_4 class_5 class_6 class_7 Error #&gt; class_1 117687 8708 0 0 85 25 615 0.0742 #&gt; class_2 8539 160120 388 9 624 568 94 0.0600 #&gt; class_3 6 427 19452 210 23 1324 0 0.0928 #&gt; class_4 0 0 229 1349 0 80 0 0.1864 #&gt; class_5 120 1073 118 0 4374 35 0 0.2353 #&gt; class_6 24 281 963 74 2 9089 0 0.1288 #&gt; class_7 874 115 0 0 2 0 11309 0.0806 #&gt; Totals 127250 170724 21150 1642 5110 11121 12018 0.0734 #&gt; Rate #&gt; class_1 = 9,433 / 127,120 #&gt; class_2 = 10,222 / 170,342 #&gt; class_3 = 1,990 / 21,442 #&gt; class_4 = 309 / 1,658 #&gt; class_5 = 1,346 / 5,720 #&gt; class_6 = 1,344 / 10,433 #&gt; class_7 = 991 / 12,300 #&gt; Totals = 25,635 / 349,015 #&gt; #&gt; Hit Ratio Table: Extract with `h2o.hit_ratio_table(&lt;model&gt;, &lt;data&gt;)` #&gt; ======================================================================= #&gt; Top-7 Hit Ratios: #&gt; k hit_ratio #&gt; 1 1 0.926550 #&gt; 2 2 0.996805 #&gt; 3 3 0.999739 #&gt; 4 4 0.999977 #&gt; 5 5 1.000000 #&gt; 6 6 1.000000 #&gt; 7 7 1.000000 h2o.performance(m3, newdata=valid) ## full validation data #&gt; H2OMultinomialMetrics: deeplearning #&gt; #&gt; Test Set Metrics: #&gt; ===================== #&gt; #&gt; MSE: (Extract with `h2o.mse`) 0.0614 #&gt; RMSE: (Extract with `h2o.rmse`) 0.248 #&gt; Logloss: (Extract with `h2o.logloss`) 0.204 #&gt; Mean Per-Class Error: 0.134 #&gt; Confusion Matrix: Extract with `h2o.confusionMatrix(&lt;model&gt;, &lt;data&gt;)`) #&gt; ========================================================================= #&gt; Confusion Matrix: Row labels: Actual class; Column labels: Predicted class #&gt; class_1 class_2 class_3 class_4 class_5 class_6 class_7 Error #&gt; class_1 38957 3283 0 0 30 12 218 0.0834 #&gt; class_2 3067 52672 140 4 251 200 46 0.0658 #&gt; class_3 2 157 6400 95 7 482 0 0.1040 #&gt; class_4 0 0 76 450 0 36 0 0.1993 #&gt; class_5 46 400 43 0 1365 16 0 0.2701 #&gt; class_6 9 108 314 25 2 3006 0 0.1322 #&gt; class_7 313 36 0 0 1 0 3749 0.0854 #&gt; Totals 42394 56656 6973 574 1656 3752 4013 0.0812 #&gt; Rate #&gt; class_1 = 3,543 / 42,500 #&gt; class_2 = 3,708 / 56,380 #&gt; class_3 = 743 / 7,143 #&gt; class_4 = 112 / 562 #&gt; class_5 = 505 / 1,870 #&gt; class_6 = 458 / 3,464 #&gt; class_7 = 350 / 4,099 #&gt; Totals = 9,419 / 116,018 #&gt; #&gt; Hit Ratio Table: Extract with `h2o.hit_ratio_table(&lt;model&gt;, &lt;data&gt;)` #&gt; ======================================================================= #&gt; Top-7 Hit Ratios: #&gt; k hit_ratio #&gt; 1 1 0.918814 #&gt; 2 2 0.995811 #&gt; 3 3 0.999655 #&gt; 4 4 0.999974 #&gt; 5 5 1.000000 #&gt; 6 6 1.000000 #&gt; 7 7 1.000000 h2o.performance(m3, newdata=test) ## full test data #&gt; H2OMultinomialMetrics: deeplearning #&gt; #&gt; Test Set Metrics: #&gt; ===================== #&gt; #&gt; MSE: (Extract with `h2o.mse`) 0.0614 #&gt; RMSE: (Extract with `h2o.rmse`) 0.248 #&gt; Logloss: (Extract with `h2o.logloss`) 0.204 #&gt; Mean Per-Class Error: 0.132 #&gt; Confusion Matrix: Extract with `h2o.confusionMatrix(&lt;model&gt;, &lt;data&gt;)`) #&gt; ========================================================================= #&gt; Confusion Matrix: Row labels: Actual class; Column labels: Predicted class #&gt; class_1 class_2 class_3 class_4 class_5 class_6 class_7 Error #&gt; class_1 38722 3243 0 0 29 5 221 0.0829 #&gt; class_2 3142 52746 146 4 256 236 49 0.0677 #&gt; class_3 1 164 6423 97 12 472 0 0.1041 #&gt; class_4 0 0 73 424 0 30 0 0.1954 #&gt; class_5 40 374 33 0 1437 19 0 0.2449 #&gt; class_6 10 99 335 29 2 2995 0 0.1369 #&gt; class_7 339 30 0 0 0 0 3742 0.0898 #&gt; Totals 42254 56656 7010 554 1736 3757 4012 0.0818 #&gt; Rate #&gt; class_1 = 3,498 / 42,220 #&gt; class_2 = 3,833 / 56,579 #&gt; class_3 = 746 / 7,169 #&gt; class_4 = 103 / 527 #&gt; class_5 = 466 / 1,903 #&gt; class_6 = 475 / 3,470 #&gt; class_7 = 369 / 4,111 #&gt; Totals = 9,490 / 115,979 #&gt; #&gt; Hit Ratio Table: Extract with `h2o.hit_ratio_table(&lt;model&gt;, &lt;data&gt;)` #&gt; ======================================================================= #&gt; Top-7 Hit Ratios: #&gt; k hit_ratio #&gt; 1 1 0.918175 #&gt; 2 2 0.995896 #&gt; 3 3 0.999543 #&gt; 4 4 0.999914 #&gt; 5 5 1.000000 #&gt; 6 6 1.000000 #&gt; 7 7 1.000000 To confirm that the reported confusion matrix on the validation set (here, the test set) was correct, we make a prediction on the test set and compare the confusion matrices explicitly: pred &lt;- h2o.predict(m3, test) #&gt; | | | 0% | |=================================================================| 100% pred #&gt; predict class_1 class_2 class_3 class_4 class_5 class_6 class_7 #&gt; 1 class_1 9.06e-01 9.18e-02 9.58e-04 2.81e-04 4.35e-05 2.24e-04 4.24e-04 #&gt; 2 class_1 1.00e+00 5.61e-06 1.62e-08 4.93e-09 3.84e-11 1.68e-09 8.43e-07 #&gt; 3 class_1 1.00e+00 1.17e-04 1.89e-06 9.03e-09 1.26e-11 5.87e-08 3.67e-04 #&gt; 4 class_1 9.94e-01 5.30e-03 1.95e-06 1.42e-08 2.24e-09 2.08e-06 7.86e-04 #&gt; 5 class_2 1.31e-02 9.85e-01 4.27e-04 3.05e-05 4.95e-04 4.04e-04 4.19e-05 #&gt; 6 class_5 1.86e-05 7.78e-02 3.73e-05 1.54e-06 9.22e-01 4.60e-05 4.02e-07 #&gt; #&gt; [115979 rows x 8 columns] test$Accuracy &lt;- pred$predict == test$Cover_Type 1-mean(test$Accuracy) #&gt; [1] 0.0818 40.5.6 Hyper-parameter Tuning with Grid Search Since there are a lot of parameters that can impact model accuracy, hyper-parameter tuning is especially important for Deep Learning: For speed, we will only train on the first 10,000 rows of the training dataset: sampled_train=train[1:10000,] The simplest hyperparameter search method is a brute-force scan of the full Cartesian product of all combinations specified by a grid search: hyper_params &lt;- list( hidden=list(c(32,32,32),c(64,64)), input_dropout_ratio=c(0,0.05), rate=c(0.01,0.02), rate_annealing=c(1e-8,1e-7,1e-6) ) hyper_params #&gt; $hidden #&gt; $hidden[[1]] #&gt; [1] 32 32 32 #&gt; #&gt; $hidden[[2]] #&gt; [1] 64 64 #&gt; #&gt; #&gt; $input_dropout_ratio #&gt; [1] 0.00 0.05 #&gt; #&gt; $rate #&gt; [1] 0.01 0.02 #&gt; #&gt; $rate_annealing #&gt; [1] 1e-08 1e-07 1e-06 grid &lt;- h2o.grid( algorithm=&quot;deeplearning&quot;, grid_id=&quot;dl_grid&quot;, training_frame=sampled_train, validation_frame=valid, x=predictors, y=response, epochs=10, stopping_metric=&quot;misclassification&quot;, stopping_tolerance=1e-2, ## stop when misclassification does not improve by &gt;=1% for 2 scoring events stopping_rounds=2, score_validation_samples=10000, ## downsample validation set for faster scoring score_duty_cycle=0.025, ## don&#39;t score more than 2.5% of the wall time adaptive_rate=F, ## manually tuned learning rate momentum_start=0.5, ## manually tuned momentum momentum_stable=0.9, momentum_ramp=1e7, l1=1e-5, l2=1e-5, activation=c(&quot;Rectifier&quot;), max_w2=10, ## can help improve stability for Rectifier hyper_params=hyper_params ) #&gt; | | | 0% | |=== | 5% | |===== | 8% | |======== | 13% | |=========== | 16% | |============== | 22% | |================= | 27% | |===================== | 32% | |======================== | 37% | |========================== | 40% | |============================ | 44% | |=============================== | 48% | |================================== | 52% | |===================================== | 57% | |========================================= | 63% | |=========================================== | 67% | |============================================== | 71% | |================================================ | 74% | |=================================================== | 79% | |===================================================== | 82% | |========================================================= | 88% | |============================================================ | 92% | |=============================================================== | 97% | |=================================================================| 100% grid #&gt; H2O Grid Details #&gt; ================ #&gt; #&gt; Grid ID: dl_grid #&gt; Used hyper parameters: #&gt; - hidden #&gt; - input_dropout_ratio #&gt; - rate #&gt; - rate_annealing #&gt; Number of models: 24 #&gt; Number of failed models: 0 #&gt; #&gt; Hyper-Parameter Search Summary: ordered by increasing logloss #&gt; hidden input_dropout_ratio rate rate_annealing model_ids #&gt; 1 [64, 64] 0.0 0.01 1.0E-7 dl_grid_model_10 #&gt; 2 [64, 64] 0.0 0.01 1.0E-6 dl_grid_model_18 #&gt; 3 [64, 64] 0.05 0.01 1.0E-6 dl_grid_model_20 #&gt; 4 [64, 64] 0.05 0.01 1.0E-8 dl_grid_model_4 #&gt; 5 [64, 64] 0.0 0.02 1.0E-7 dl_grid_model_14 #&gt; logloss #&gt; 1 0.57117515456626 #&gt; 2 0.5723985674510378 #&gt; 3 0.5750681881489403 #&gt; 4 0.583803641113243 #&gt; 5 0.5868203063102239 #&gt; #&gt; --- #&gt; hidden input_dropout_ratio rate rate_annealing model_ids #&gt; 19 [32, 32, 32] 0.0 0.02 1.0E-6 dl_grid_model_21 #&gt; 20 [32, 32, 32] 0.05 0.02 1.0E-7 dl_grid_model_15 #&gt; 21 [64, 64] 0.0 0.02 1.0E-8 dl_grid_model_6 #&gt; 22 [32, 32, 32] 0.0 0.02 1.0E-7 dl_grid_model_13 #&gt; 23 [32, 32, 32] 0.05 0.02 1.0E-8 dl_grid_model_7 #&gt; 24 [32, 32, 32] 0.05 0.02 1.0E-6 dl_grid_model_23 #&gt; logloss #&gt; 19 0.6279730192882965 #&gt; 20 0.6294542672298918 #&gt; 21 0.6334027751026112 #&gt; 22 0.6364128005928186 #&gt; 23 0.6388255596662025 #&gt; 24 0.6418252199985821 Let’s see which model had the lowest validation error: grid &lt;- h2o.getGrid(&quot;dl_grid&quot;,sort_by=&quot;err&quot;,decreasing=FALSE) grid #&gt; H2O Grid Details #&gt; ================ #&gt; #&gt; Grid ID: dl_grid #&gt; Used hyper parameters: #&gt; - hidden #&gt; - input_dropout_ratio #&gt; - rate #&gt; - rate_annealing #&gt; Number of models: 24 #&gt; Number of failed models: 0 #&gt; #&gt; Hyper-Parameter Search Summary: ordered by increasing err #&gt; hidden input_dropout_ratio rate rate_annealing model_ids #&gt; 1 [64, 64] 0.0 0.01 1.0E-6 dl_grid_model_18 #&gt; 2 [64, 64] 0.05 0.01 1.0E-6 dl_grid_model_20 #&gt; 3 [32, 32, 32] 0.0 0.01 1.0E-8 dl_grid_model_1 #&gt; 4 [64, 64] 0.0 0.01 1.0E-7 dl_grid_model_10 #&gt; 5 [64, 64] 0.05 0.01 1.0E-8 dl_grid_model_4 #&gt; err #&gt; 1 0.24485925334398084 #&gt; 2 0.24724441298412378 #&gt; 3 0.24761904761904763 #&gt; 4 0.24858025306366444 #&gt; 5 0.24866444914827135 #&gt; #&gt; --- #&gt; hidden input_dropout_ratio rate rate_annealing model_ids #&gt; 19 [64, 64] 0.0 0.02 1.0E-8 dl_grid_model_6 #&gt; 20 [64, 64] 0.05 0.02 1.0E-6 dl_grid_model_24 #&gt; 21 [32, 32, 32] 0.0 0.01 1.0E-6 dl_grid_model_17 #&gt; 22 [32, 32, 32] 0.0 0.02 1.0E-6 dl_grid_model_21 #&gt; 23 [32, 32, 32] 0.05 0.02 1.0E-8 dl_grid_model_7 #&gt; 24 [32, 32, 32] 0.05 0.02 1.0E-7 dl_grid_model_15 #&gt; err #&gt; 19 0.26576846307385227 #&gt; 20 0.26641274932344394 #&gt; 21 0.2679868643646134 #&gt; 22 0.26835847737036667 #&gt; 23 0.2718117459367833 #&gt; 24 0.28813053427519647 ## To see what other &quot;sort_by&quot; criteria are allowed #grid &lt;- h2o.getGrid(&quot;dl_grid&quot;,sort_by=&quot;wrong_thing&quot;,decreasing=FALSE) ## Sort by logloss h2o.getGrid(&quot;dl_grid&quot;,sort_by=&quot;logloss&quot;,decreasing=FALSE) #&gt; H2O Grid Details #&gt; ================ #&gt; #&gt; Grid ID: dl_grid #&gt; Used hyper parameters: #&gt; - hidden #&gt; - input_dropout_ratio #&gt; - rate #&gt; - rate_annealing #&gt; Number of models: 24 #&gt; Number of failed models: 0 #&gt; #&gt; Hyper-Parameter Search Summary: ordered by increasing logloss #&gt; hidden input_dropout_ratio rate rate_annealing model_ids #&gt; 1 [64, 64] 0.0 0.01 1.0E-7 dl_grid_model_10 #&gt; 2 [64, 64] 0.0 0.01 1.0E-6 dl_grid_model_18 #&gt; 3 [64, 64] 0.05 0.01 1.0E-6 dl_grid_model_20 #&gt; 4 [64, 64] 0.05 0.01 1.0E-8 dl_grid_model_4 #&gt; 5 [64, 64] 0.0 0.02 1.0E-7 dl_grid_model_14 #&gt; logloss #&gt; 1 0.57117515456626 #&gt; 2 0.5723985674510378 #&gt; 3 0.5750681881489403 #&gt; 4 0.583803641113243 #&gt; 5 0.5868203063102239 #&gt; #&gt; --- #&gt; hidden input_dropout_ratio rate rate_annealing model_ids #&gt; 19 [32, 32, 32] 0.0 0.02 1.0E-6 dl_grid_model_21 #&gt; 20 [32, 32, 32] 0.05 0.02 1.0E-7 dl_grid_model_15 #&gt; 21 [64, 64] 0.0 0.02 1.0E-8 dl_grid_model_6 #&gt; 22 [32, 32, 32] 0.0 0.02 1.0E-7 dl_grid_model_13 #&gt; 23 [32, 32, 32] 0.05 0.02 1.0E-8 dl_grid_model_7 #&gt; 24 [32, 32, 32] 0.05 0.02 1.0E-6 dl_grid_model_23 #&gt; logloss #&gt; 19 0.6279730192882965 #&gt; 20 0.6294542672298918 #&gt; 21 0.6334027751026112 #&gt; 22 0.6364128005928186 #&gt; 23 0.6388255596662025 #&gt; 24 0.6418252199985821 ## Find the best model and its full set of parameters grid@summary_table[1,] #&gt; Hyper-Parameter Search Summary: ordered by increasing err #&gt; hidden input_dropout_ratio rate rate_annealing model_ids #&gt; 1 [64, 64] 0.0 0.01 1.0E-6 dl_grid_model_18 #&gt; err #&gt; 1 0.24485925334398084 best_model &lt;- h2o.getModel(grid@model_ids[[1]]) best_model #&gt; Model Details: #&gt; ============== #&gt; #&gt; H2OMultinomialModel: deeplearning #&gt; Model ID: dl_grid_model_18 #&gt; Status of Neuron Layers: predicting Cover_Type, 7-class classification, multinomial distribution, CrossEntropy loss, 8,263 weights/biases, 72.5 KB, 100,000 training samples, mini-batch size 1 #&gt; layer units type dropout l1 l2 mean_rate rate_rms #&gt; 1 1 56 Input 0.00 % NA NA NA NA #&gt; 2 2 64 Rectifier 0.00 % 0.000010 0.000010 0.009091 0.000000 #&gt; 3 3 64 Rectifier 0.00 % 0.000010 0.000010 0.009091 0.000000 #&gt; 4 4 7 Softmax NA 0.000010 0.000010 0.009091 0.000000 #&gt; momentum mean_weight weight_rms mean_bias bias_rms #&gt; 1 NA NA NA NA NA #&gt; 2 0.504000 -0.004332 0.209345 0.154637 0.154465 #&gt; 3 0.504000 -0.055583 0.185252 0.856581 0.136058 #&gt; 4 0.504000 -0.013835 0.390873 0.006869 0.550936 #&gt; #&gt; #&gt; H2OMultinomialMetrics: deeplearning #&gt; ** Reported on training data. ** #&gt; ** Metrics reported on full training frame ** #&gt; #&gt; Training Set Metrics: #&gt; ===================== #&gt; #&gt; Extract training frame with `h2o.getFrame(&quot;RTMP_sid_b3e8_9&quot;)` #&gt; MSE: (Extract with `h2o.mse`) 0.167 #&gt; RMSE: (Extract with `h2o.rmse`) 0.408 #&gt; Logloss: (Extract with `h2o.logloss`) 0.517 #&gt; Mean Per-Class Error: 0.413 #&gt; Confusion Matrix: Extract with `h2o.confusionMatrix(&lt;model&gt;,train = TRUE)`) #&gt; ========================================================================= #&gt; Confusion Matrix: Row labels: Actual class; Column labels: Predicted class #&gt; class_1 class_2 class_3 class_4 class_5 class_6 class_7 Error #&gt; class_1 2779 897 0 0 1 1 10 0.2465 #&gt; class_2 669 4079 44 0 24 18 1 0.1564 #&gt; class_3 0 68 544 2 0 16 0 0.1365 #&gt; class_4 0 0 19 22 0 3 0 0.5000 #&gt; class_5 7 91 5 0 52 1 0 0.6667 #&gt; class_6 0 63 147 0 0 99 0 0.6796 #&gt; class_7 164 6 0 0 0 0 168 0.5030 #&gt; Totals 3619 5204 759 24 77 138 179 0.2257 #&gt; Rate #&gt; class_1 = 909 / 3,688 #&gt; class_2 = 756 / 4,835 #&gt; class_3 = 86 / 630 #&gt; class_4 = 22 / 44 #&gt; class_5 = 104 / 156 #&gt; class_6 = 210 / 309 #&gt; class_7 = 170 / 338 #&gt; Totals = 2,257 / 10,000 #&gt; #&gt; Hit Ratio Table: Extract with `h2o.hit_ratio_table(&lt;model&gt;,train = TRUE)` #&gt; ======================================================================= #&gt; Top-7 Hit Ratios: #&gt; k hit_ratio #&gt; 1 1 0.774300 #&gt; 2 2 0.979300 #&gt; 3 3 0.997100 #&gt; 4 4 0.999100 #&gt; 5 5 1.000000 #&gt; 6 6 1.000000 #&gt; 7 7 1.000000 #&gt; #&gt; #&gt; H2OMultinomialMetrics: deeplearning #&gt; ** Reported on validation data. ** #&gt; ** Metrics reported on temporary validation frame with 10018 samples ** #&gt; #&gt; Validation Set Metrics: #&gt; ===================== #&gt; #&gt; MSE: (Extract with `h2o.mse`) 0.182 #&gt; RMSE: (Extract with `h2o.rmse`) 0.426 #&gt; Logloss: (Extract with `h2o.logloss`) 0.572 #&gt; Mean Per-Class Error: 0.465 #&gt; Confusion Matrix: Extract with `h2o.confusionMatrix(&lt;model&gt;,valid = TRUE)`) #&gt; ========================================================================= #&gt; Confusion Matrix: Row labels: Actual class; Column labels: Predicted class #&gt; class_1 class_2 class_3 class_4 class_5 class_6 class_7 Error #&gt; class_1 2692 987 0 0 2 0 22 0.2730 #&gt; class_2 698 4094 55 0 19 18 1 0.1619 #&gt; class_3 0 80 484 4 0 23 0 0.1810 #&gt; class_4 0 1 24 17 0 4 0 0.6304 #&gt; class_5 8 96 6 0 42 3 0 0.7290 #&gt; class_6 0 75 138 1 0 78 0 0.7329 #&gt; class_7 186 2 0 0 0 0 158 0.5434 #&gt; Totals 3584 5335 707 22 63 126 181 0.2449 #&gt; Rate #&gt; class_1 = 1,011 / 3,703 #&gt; class_2 = 791 / 4,885 #&gt; class_3 = 107 / 591 #&gt; class_4 = 29 / 46 #&gt; class_5 = 113 / 155 #&gt; class_6 = 214 / 292 #&gt; class_7 = 188 / 346 #&gt; Totals = 2,453 / 10,018 #&gt; #&gt; Hit Ratio Table: Extract with `h2o.hit_ratio_table(&lt;model&gt;,valid = TRUE)` #&gt; ======================================================================= #&gt; Top-7 Hit Ratios: #&gt; k hit_ratio #&gt; 1 1 0.755141 #&gt; 2 2 0.972450 #&gt; 3 3 0.996606 #&gt; 4 4 0.998902 #&gt; 5 5 1.000000 #&gt; 6 6 1.000000 #&gt; 7 7 1.000000 print(best_model@allparameters) #&gt; $model_id #&gt; [1] &quot;dl_grid_model_18&quot; #&gt; #&gt; $training_frame #&gt; [1] &quot;RTMP_sid_b3e8_9&quot; #&gt; #&gt; $validation_frame #&gt; [1] &quot;valid.hex&quot; #&gt; #&gt; $nfolds #&gt; [1] 0 #&gt; #&gt; $keep_cross_validation_models #&gt; [1] TRUE #&gt; #&gt; $keep_cross_validation_predictions #&gt; [1] FALSE #&gt; #&gt; $keep_cross_validation_fold_assignment #&gt; [1] FALSE #&gt; #&gt; $fold_assignment #&gt; [1] &quot;AUTO&quot; #&gt; #&gt; $ignore_const_cols #&gt; [1] TRUE #&gt; #&gt; $score_each_iteration #&gt; [1] FALSE #&gt; #&gt; $balance_classes #&gt; [1] FALSE #&gt; #&gt; $max_after_balance_size #&gt; [1] 5 #&gt; #&gt; $max_confusion_matrix_size #&gt; [1] 20 #&gt; #&gt; $max_hit_ratio_k #&gt; [1] 0 #&gt; #&gt; $overwrite_with_best_model #&gt; [1] TRUE #&gt; #&gt; $use_all_factor_levels #&gt; [1] TRUE #&gt; #&gt; $standardize #&gt; [1] TRUE #&gt; #&gt; $activation #&gt; [1] &quot;Rectifier&quot; #&gt; #&gt; $hidden #&gt; [1] 64 64 #&gt; #&gt; $epochs #&gt; [1] 10 #&gt; #&gt; $train_samples_per_iteration #&gt; [1] -2 #&gt; #&gt; $target_ratio_comm_to_comp #&gt; [1] 0.05 #&gt; #&gt; $seed #&gt; [1] 1.67e+18 #&gt; #&gt; $adaptive_rate #&gt; [1] FALSE #&gt; #&gt; $rho #&gt; [1] 0.99 #&gt; #&gt; $epsilon #&gt; [1] 1e-08 #&gt; #&gt; $rate #&gt; [1] 0.01 #&gt; #&gt; $rate_annealing #&gt; [1] 1e-06 #&gt; #&gt; $rate_decay #&gt; [1] 1 #&gt; #&gt; $momentum_start #&gt; [1] 0.5 #&gt; #&gt; $momentum_ramp #&gt; [1] 1e+07 #&gt; #&gt; $momentum_stable #&gt; [1] 0.9 #&gt; #&gt; $nesterov_accelerated_gradient #&gt; [1] TRUE #&gt; #&gt; $input_dropout_ratio #&gt; [1] 0 #&gt; #&gt; $l1 #&gt; [1] 1e-05 #&gt; #&gt; $l2 #&gt; [1] 1e-05 #&gt; #&gt; $max_w2 #&gt; [1] 10 #&gt; #&gt; $initial_weight_distribution #&gt; [1] &quot;UniformAdaptive&quot; #&gt; #&gt; $initial_weight_scale #&gt; [1] 1 #&gt; #&gt; $loss #&gt; [1] &quot;Automatic&quot; #&gt; #&gt; $distribution #&gt; [1] &quot;AUTO&quot; #&gt; #&gt; $quantile_alpha #&gt; [1] 0.5 #&gt; #&gt; $tweedie_power #&gt; [1] 1.5 #&gt; #&gt; $huber_alpha #&gt; [1] 0.9 #&gt; #&gt; $score_interval #&gt; [1] 5 #&gt; #&gt; $score_training_samples #&gt; [1] 10000 #&gt; #&gt; $score_validation_samples #&gt; [1] 10000 #&gt; #&gt; $score_duty_cycle #&gt; [1] 0.025 #&gt; #&gt; $classification_stop #&gt; [1] 0 #&gt; #&gt; $regression_stop #&gt; [1] 1e-06 #&gt; #&gt; $stopping_rounds #&gt; [1] 2 #&gt; #&gt; $stopping_metric #&gt; [1] &quot;misclassification&quot; #&gt; #&gt; $stopping_tolerance #&gt; [1] 0.01 #&gt; #&gt; $max_runtime_secs #&gt; [1] 1.8e+308 #&gt; #&gt; $score_validation_sampling #&gt; [1] &quot;Uniform&quot; #&gt; #&gt; $diagnostics #&gt; [1] TRUE #&gt; #&gt; $fast_mode #&gt; [1] TRUE #&gt; #&gt; $force_load_balance #&gt; [1] TRUE #&gt; #&gt; $variable_importances #&gt; [1] TRUE #&gt; #&gt; $replicate_training_data #&gt; [1] TRUE #&gt; #&gt; $single_node_mode #&gt; [1] FALSE #&gt; #&gt; $shuffle_training_data #&gt; [1] FALSE #&gt; #&gt; $missing_values_handling #&gt; [1] &quot;MeanImputation&quot; #&gt; #&gt; $quiet_mode #&gt; [1] FALSE #&gt; #&gt; $autoencoder #&gt; [1] FALSE #&gt; #&gt; $sparse #&gt; [1] FALSE #&gt; #&gt; $col_major #&gt; [1] FALSE #&gt; #&gt; $average_activation #&gt; [1] 0 #&gt; #&gt; $sparsity_beta #&gt; [1] 0 #&gt; #&gt; $max_categorical_features #&gt; [1] 2147483647 #&gt; #&gt; $reproducible #&gt; [1] FALSE #&gt; #&gt; $export_weights_and_biases #&gt; [1] FALSE #&gt; #&gt; $mini_batch_size #&gt; [1] 1 #&gt; #&gt; $categorical_encoding #&gt; [1] &quot;AUTO&quot; #&gt; #&gt; $elastic_averaging #&gt; [1] FALSE #&gt; #&gt; $elastic_averaging_moving_rate #&gt; [1] 0.9 #&gt; #&gt; $elastic_averaging_regularization #&gt; [1] 0.001 #&gt; #&gt; $x #&gt; [1] &quot;Soil_Type&quot; #&gt; [2] &quot;Wilderness_Area&quot; #&gt; [3] &quot;Elevation&quot; #&gt; [4] &quot;Aspect&quot; #&gt; [5] &quot;Slope&quot; #&gt; [6] &quot;Horizontal_Distance_To_Hydrology&quot; #&gt; [7] &quot;Vertical_Distance_To_Hydrology&quot; #&gt; [8] &quot;Horizontal_Distance_To_Roadways&quot; #&gt; [9] &quot;Hillshade_9am&quot; #&gt; [10] &quot;Hillshade_Noon&quot; #&gt; [11] &quot;Hillshade_3pm&quot; #&gt; [12] &quot;Horizontal_Distance_To_Fire_Points&quot; #&gt; #&gt; $y #&gt; [1] &quot;Cover_Type&quot; print(h2o.performance(best_model, valid=T)) #&gt; H2OMultinomialMetrics: deeplearning #&gt; ** Reported on validation data. ** #&gt; ** Metrics reported on temporary validation frame with 10018 samples ** #&gt; #&gt; Validation Set Metrics: #&gt; ===================== #&gt; #&gt; MSE: (Extract with `h2o.mse`) 0.182 #&gt; RMSE: (Extract with `h2o.rmse`) 0.426 #&gt; Logloss: (Extract with `h2o.logloss`) 0.572 #&gt; Mean Per-Class Error: 0.465 #&gt; Confusion Matrix: Extract with `h2o.confusionMatrix(&lt;model&gt;,valid = TRUE)`) #&gt; ========================================================================= #&gt; Confusion Matrix: Row labels: Actual class; Column labels: Predicted class #&gt; class_1 class_2 class_3 class_4 class_5 class_6 class_7 Error #&gt; class_1 2692 987 0 0 2 0 22 0.2730 #&gt; class_2 698 4094 55 0 19 18 1 0.1619 #&gt; class_3 0 80 484 4 0 23 0 0.1810 #&gt; class_4 0 1 24 17 0 4 0 0.6304 #&gt; class_5 8 96 6 0 42 3 0 0.7290 #&gt; class_6 0 75 138 1 0 78 0 0.7329 #&gt; class_7 186 2 0 0 0 0 158 0.5434 #&gt; Totals 3584 5335 707 22 63 126 181 0.2449 #&gt; Rate #&gt; class_1 = 1,011 / 3,703 #&gt; class_2 = 791 / 4,885 #&gt; class_3 = 107 / 591 #&gt; class_4 = 29 / 46 #&gt; class_5 = 113 / 155 #&gt; class_6 = 214 / 292 #&gt; class_7 = 188 / 346 #&gt; Totals = 2,453 / 10,018 #&gt; #&gt; Hit Ratio Table: Extract with `h2o.hit_ratio_table(&lt;model&gt;,valid = TRUE)` #&gt; ======================================================================= #&gt; Top-7 Hit Ratios: #&gt; k hit_ratio #&gt; 1 1 0.755141 #&gt; 2 2 0.972450 #&gt; 3 3 0.996606 #&gt; 4 4 0.998902 #&gt; 5 5 1.000000 #&gt; 6 6 1.000000 #&gt; 7 7 1.000000 print(h2o.logloss(best_model, valid=T)) #&gt; [1] 0.572 40.5.7 Random Hyper-Parameter Search Often, hyper-parameter search for more than 4 parameters can be done more efficiently with random parameter search than with grid search. Basically, chances are good to find one of many good models in less time than performing an exhaustive grid search. We simply build up to max_models models with parameters drawn randomly from user-specified distributions (here, uniform). For this example, we use the adaptive learning rate and focus on tuning the network architecture and the regularization parameters. We also let the grid search stop automatically once the performance at the top of the leaderboard doesn’t change much anymore, i.e., once the search has converged. hyper_params &lt;- list( activation=c(&quot;Rectifier&quot;,&quot;Tanh&quot;,&quot;Maxout&quot;,&quot;RectifierWithDropout&quot;,&quot;TanhWithDropout&quot;,&quot;MaxoutWithDropout&quot;), hidden=list(c(20,20),c(50,50),c(30,30,30),c(25,25,25,25)), input_dropout_ratio=c(0,0.05), l1=seq(0,1e-4,1e-6), l2=seq(0,1e-4,1e-6) ) hyper_params ## Stop once the top 5 models are within 1% of each other (i.e., the windowed average varies less than 1%) search_criteria = list(strategy = &quot;RandomDiscrete&quot;, max_runtime_secs = 360, max_models = 100, seed=1234567, stopping_rounds=5, stopping_tolerance=1e-2) dl_random_grid &lt;- h2o.grid( algorithm=&quot;deeplearning&quot;, grid_id = &quot;dl_grid_random&quot;, training_frame=sampled_train, validation_frame=valid, x=predictors, y=response, epochs=1, stopping_metric=&quot;logloss&quot;, stopping_tolerance=1e-2, ## stop when logloss does not improve by &gt;=1% for 2 scoring events stopping_rounds=2, score_validation_samples=10000, ## downsample validation set for faster scoring score_duty_cycle=0.025, ## don&#39;t score more than 2.5% of the wall time max_w2=10, ## can help improve stability for Rectifier hyper_params = hyper_params, search_criteria = search_criteria ) grid &lt;- h2o.getGrid(&quot;dl_grid_random&quot;,sort_by=&quot;logloss&quot;,decreasing=FALSE) grid grid@summary_table[1,] best_model &lt;- h2o.getModel(grid@model_ids[[1]]) ## model with lowest logloss best_model Let’s look at the model with the lowest validation misclassification rate: grid &lt;- h2o.getGrid(&quot;dl_grid&quot;,sort_by=&quot;err&quot;,decreasing=FALSE) best_model &lt;- h2o.getModel(grid@model_ids[[1]]) ## model with lowest classification error (on validation, since it was available during training) h2o.confusionMatrix(best_model,valid=T) #&gt; Confusion Matrix: Row labels: Actual class; Column labels: Predicted class #&gt; class_1 class_2 class_3 class_4 class_5 class_6 class_7 Error #&gt; class_1 2692 987 0 0 2 0 22 0.2730 #&gt; class_2 698 4094 55 0 19 18 1 0.1619 #&gt; class_3 0 80 484 4 0 23 0 0.1810 #&gt; class_4 0 1 24 17 0 4 0 0.6304 #&gt; class_5 8 96 6 0 42 3 0 0.7290 #&gt; class_6 0 75 138 1 0 78 0 0.7329 #&gt; class_7 186 2 0 0 0 0 158 0.5434 #&gt; Totals 3584 5335 707 22 63 126 181 0.2449 #&gt; Rate #&gt; class_1 = 1,011 / 3,703 #&gt; class_2 = 791 / 4,885 #&gt; class_3 = 107 / 591 #&gt; class_4 = 29 / 46 #&gt; class_5 = 113 / 155 #&gt; class_6 = 214 / 292 #&gt; class_7 = 188 / 346 #&gt; Totals = 2,453 / 10,018 best_params &lt;- best_model@allparameters best_params$activation #&gt; [1] &quot;Rectifier&quot; best_params$hidden #&gt; [1] 64 64 best_params$input_dropout_ratio #&gt; [1] 0 best_params$l1 #&gt; [1] 1e-05 best_params$l2 #&gt; [1] 1e-05 40.5.8 Checkpointing Let’s continue training the manually tuned model from before, for 2 more epochs. Note that since many important parameters such as epochs, l1, l2, max_w2, score_interval, train_samples_per_iteration, input_dropout_ratio, hidden_dropout_ratios, score_duty_cycle, classification_stop, regression_stop, variable_importances, force_load_balance can be modified between checkpoint restarts, it is best to specify as many parameters as possible explicitly. max_epochs &lt;- 12 ## Add two more epochs m_cont &lt;- h2o.deeplearning( model_id=&quot;dl_model_tuned_continued&quot;, checkpoint=&quot;dl_model_tuned&quot;, training_frame=train, validation_frame=valid, x=predictors, y=response, hidden=c(128,128,128), ## more hidden layers -&gt; more complex interactions epochs=max_epochs, ## hopefully long enough to converge (otherwise restart again) stopping_metric=&quot;logloss&quot;, ## logloss is directly optimized by Deep Learning stopping_tolerance=1e-2, ## stop when validation logloss does not improve by &gt;=1% for 2 scoring events stopping_rounds=2, score_validation_samples=10000, ## downsample validation set for faster scoring score_duty_cycle=0.025, ## don&#39;t score more than 2.5% of the wall time adaptive_rate=F, ## manually tuned learning rate rate=0.01, rate_annealing=2e-6, momentum_start=0.2, ## manually tuned momentum momentum_stable=0.4, momentum_ramp=1e7, l1=1e-5, ## add some L1/L2 regularization l2=1e-5, max_w2=10 ## helps stability for Rectifier ) summary(m_cont) plot(m_cont) Once we are satisfied with the results, we can save the model to disk (on the cluster). In this example, we store the model in a directory called mybest_deeplearning_covtype_model, which will be created for us since force=TRUE. path &lt;- h2o.saveModel(m_cont, path = file.path(data_out_dir, &quot;mybest_deeplearning_covtype_model&quot;), force=TRUE) It can be loaded later with the following command: print(path) #&gt; [1] &quot;/home/datascience/repos/machine-learning-rsuite/export/mybest_deeplearning_covtype_model/dl_model_tuned_continued&quot; m_loaded &lt;- h2o.loadModel(path) summary(m_loaded) #&gt; Model Details: #&gt; ============== #&gt; #&gt; H2OMultinomialModel: deeplearning #&gt; Model Key: dl_model_tuned_continued #&gt; Status of Neuron Layers: predicting Cover_Type, 7-class classification, multinomial distribution, CrossEntropy loss, 41,223 weights/biases, 334.1 KB, 3,900,153 training samples, mini-batch size 1 #&gt; layer units type dropout l1 l2 mean_rate rate_rms #&gt; 1 1 56 Input 0.00 % NA NA NA NA #&gt; 2 2 128 Rectifier 0.00 % 0.000010 0.000010 0.001136 0.000000 #&gt; 3 3 128 Rectifier 0.00 % 0.000010 0.000010 0.001136 0.000000 #&gt; 4 4 128 Rectifier 0.00 % 0.000010 0.000010 0.001136 0.000000 #&gt; 5 5 7 Softmax NA 0.000010 0.000010 0.001136 0.000000 #&gt; momentum mean_weight weight_rms mean_bias bias_rms #&gt; 1 NA NA NA NA NA #&gt; 2 0.278003 -0.014110 0.315929 0.013443 0.294899 #&gt; 3 0.278003 -0.052447 0.221344 0.859128 0.355188 #&gt; 4 0.278003 -0.065243 0.216111 0.820031 0.199900 #&gt; 5 0.278003 -0.020254 0.270093 0.013140 0.966391 #&gt; #&gt; H2OMultinomialMetrics: deeplearning #&gt; ** Reported on training data. ** #&gt; ** Metrics reported on temporary training frame with 10066 samples ** #&gt; #&gt; Training Set Metrics: #&gt; ===================== #&gt; #&gt; MSE: (Extract with `h2o.mse`) 0.0566 #&gt; RMSE: (Extract with `h2o.rmse`) 0.238 #&gt; Logloss: (Extract with `h2o.logloss`) 0.188 #&gt; Mean Per-Class Error: 0.124 #&gt; Confusion Matrix: Extract with `h2o.confusionMatrix(&lt;model&gt;,train = TRUE)`) #&gt; ========================================================================= #&gt; Confusion Matrix: Row labels: Actual class; Column labels: Predicted class #&gt; class_1 class_2 class_3 class_4 class_5 class_6 class_7 Error #&gt; class_1 3409 230 0 0 0 1 20 0.0686 #&gt; class_2 266 4593 19 0 25 15 4 0.0668 #&gt; class_3 0 23 561 8 1 37 0 0.1095 #&gt; class_4 0 0 7 42 0 1 0 0.1600 #&gt; class_5 1 33 4 0 109 0 0 0.2585 #&gt; class_6 0 7 29 3 0 268 0 0.1270 #&gt; class_7 24 2 0 0 0 0 324 0.0743 #&gt; Totals 3700 4888 620 53 135 322 348 0.0755 #&gt; Rate #&gt; class_1 = 251 / 3,660 #&gt; class_2 = 329 / 4,922 #&gt; class_3 = 69 / 630 #&gt; class_4 = 8 / 50 #&gt; class_5 = 38 / 147 #&gt; class_6 = 39 / 307 #&gt; class_7 = 26 / 350 #&gt; Totals = 760 / 10,066 #&gt; #&gt; Hit Ratio Table: Extract with `h2o.hit_ratio_table(&lt;model&gt;,train = TRUE)` #&gt; ======================================================================= #&gt; Top-7 Hit Ratios: #&gt; k hit_ratio #&gt; 1 1 0.924498 #&gt; 2 2 0.998013 #&gt; 3 3 0.999801 #&gt; 4 4 1.000000 #&gt; 5 5 1.000000 #&gt; 6 6 1.000000 #&gt; 7 7 1.000000 #&gt; #&gt; #&gt; H2OMultinomialMetrics: deeplearning #&gt; ** Reported on validation data. ** #&gt; ** Metrics reported on temporary validation frame with 10047 samples ** #&gt; #&gt; Validation Set Metrics: #&gt; ===================== #&gt; #&gt; MSE: (Extract with `h2o.mse`) 0.0637 #&gt; RMSE: (Extract with `h2o.rmse`) 0.252 #&gt; Logloss: (Extract with `h2o.logloss`) 0.213 #&gt; Mean Per-Class Error: 0.158 #&gt; Confusion Matrix: Extract with `h2o.confusionMatrix(&lt;model&gt;,valid = TRUE)`) #&gt; ========================================================================= #&gt; Confusion Matrix: Row labels: Actual class; Column labels: Predicted class #&gt; class_1 class_2 class_3 class_4 class_5 class_6 class_7 Error #&gt; class_1 3411 295 0 0 0 0 16 0.0836 #&gt; class_2 257 4558 14 2 20 17 9 0.0654 #&gt; class_3 1 14 543 1 1 45 0 0.1025 #&gt; class_4 0 0 4 37 0 7 0 0.2292 #&gt; class_5 5 35 7 0 87 0 0 0.3507 #&gt; class_6 1 9 39 0 0 234 0 0.1731 #&gt; class_7 32 5 0 0 1 0 340 0.1005 #&gt; Totals 3707 4916 607 40 109 303 365 0.0833 #&gt; Rate #&gt; class_1 = 311 / 3,722 #&gt; class_2 = 319 / 4,877 #&gt; class_3 = 62 / 605 #&gt; class_4 = 11 / 48 #&gt; class_5 = 47 / 134 #&gt; class_6 = 49 / 283 #&gt; class_7 = 38 / 378 #&gt; Totals = 837 / 10,047 #&gt; #&gt; Hit Ratio Table: Extract with `h2o.hit_ratio_table(&lt;model&gt;,valid = TRUE)` #&gt; ======================================================================= #&gt; Top-7 Hit Ratios: #&gt; k hit_ratio #&gt; 1 1 0.916692 #&gt; 2 2 0.995222 #&gt; 3 3 0.999801 #&gt; 4 4 1.000000 #&gt; 5 5 1.000000 #&gt; 6 6 1.000000 #&gt; 7 7 1.000000 #&gt; #&gt; #&gt; #&gt; #&gt; Scoring History: #&gt; timestamp duration training_speed epochs #&gt; 1 2019-09-20 14:09:24 0.000 sec NA 0.00000 #&gt; 2 2019-09-20 14:09:30 5.922 sec 17850 obs/sec 0.28631 #&gt; 3 2019-09-20 14:09:42 17.752 sec 23214 obs/sec 1.14631 #&gt; 4 2019-09-20 14:09:52 28.089 sec 25547 obs/sec 2.00435 #&gt; 5 2019-09-20 14:10:02 37.975 sec 26946 obs/sec 2.86444 #&gt; 6 2019-09-20 14:10:11 47.573 sec 27959 obs/sec 3.72560 #&gt; 7 2019-09-20 14:10:21 57.520 sec 28432 obs/sec 4.58412 #&gt; 8 2019-09-20 14:10:31 1 min 7.206 sec 28884 obs/sec 5.44330 #&gt; 9 2019-09-20 14:10:39 1 min 15.521 sec 28414 obs/sec 6.01692 #&gt; 10 2019-09-20 14:10:49 1 min 25.534 sec 28661 obs/sec 6.87729 #&gt; 11 2019-09-20 14:10:59 1 min 35.345 sec 28924 obs/sec 7.73868 #&gt; 12 2019-09-20 14:11:09 1 min 45.086 sec 29150 obs/sec 8.59711 #&gt; 13 2019-09-20 14:11:19 1 min 55.010 sec 29290 obs/sec 9.45618 #&gt; 14 2019-09-20 14:11:26 2 min 1.615 sec 29388 obs/sec 10.02927 #&gt; 15 2019-09-20 14:12:35 2 min 4.966 sec 29453 obs/sec 10.31534 #&gt; 16 2019-09-20 14:12:45 2 min 15.228 sec 29473 obs/sec 11.17474 #&gt; 17 2019-09-20 14:12:45 2 min 15.421 sec 29472 obs/sec 11.17474 #&gt; iterations samples training_rmse training_logloss training_r2 #&gt; 1 0 0.000000 NA NA NA #&gt; 2 1 99927.000000 0.42109 0.55795 0.91218 #&gt; 3 4 400079.000000 0.35849 0.40459 0.93635 #&gt; 4 7 699548.000000 0.33478 0.35861 0.94449 #&gt; 5 10 999733.000000 0.30593 0.30189 0.95365 #&gt; 6 13 1300290.000000 0.28786 0.27027 0.95896 #&gt; 7 16 1599925.000000 0.27623 0.24983 0.96221 #&gt; 8 19 1899792.000000 0.27505 0.24491 0.96253 #&gt; 9 21 2099994.000000 0.26154 0.22497 0.96612 #&gt; 10 24 2400278.000000 0.26558 0.23185 0.96507 #&gt; 11 27 2700916.000000 0.24645 0.20130 0.96992 #&gt; 12 30 3000521.000000 0.23921 0.19258 0.97166 #&gt; 13 33 3300348.000000 0.23954 0.18881 0.97158 #&gt; 14 35 3500367.000000 0.23457 0.18181 0.97275 #&gt; 15 36 3600209.000000 0.23827 0.18806 0.97062 #&gt; 16 39 3900153.000000 0.23595 0.18688 0.97119 #&gt; 17 39 3900153.000000 0.23801 0.18836 0.97068 #&gt; training_classification_error validation_rmse validation_logloss #&gt; 1 NA NA NA #&gt; 2 0.24646 0.41788 0.54873 #&gt; 3 0.17741 0.35647 0.40274 #&gt; 4 0.14987 0.33856 0.36805 #&gt; 5 0.12522 0.30739 0.30475 #&gt; 6 0.11106 0.29033 0.27836 #&gt; 7 0.10158 0.28607 0.26733 #&gt; 8 0.10257 0.27794 0.25822 #&gt; 9 0.09160 0.26908 0.24045 #&gt; 10 0.09349 0.27016 0.24311 #&gt; 11 0.08152 0.25695 0.21998 #&gt; 12 0.07424 0.25264 0.21443 #&gt; 13 0.07713 0.25078 0.20800 #&gt; 14 0.07284 0.24597 0.20354 #&gt; 15 0.07560 0.25490 0.21380 #&gt; 16 0.07560 0.24760 0.20484 #&gt; 17 0.07550 0.25245 0.21260 #&gt; validation_r2 validation_classification_error #&gt; 1 NA NA #&gt; 2 0.90634 0.23924 #&gt; 3 0.93184 0.17008 #&gt; 4 0.93852 0.15697 #&gt; 5 0.94932 0.12441 #&gt; 6 0.95479 0.10989 #&gt; 7 0.95611 0.10797 #&gt; 8 0.95857 0.10172 #&gt; 9 0.96117 0.09578 #&gt; 10 0.96085 0.09658 #&gt; 11 0.96459 0.08882 #&gt; 12 0.96577 0.08338 #&gt; 13 0.96627 0.08368 #&gt; 14 0.96755 0.07954 #&gt; 15 0.96684 0.08829 #&gt; 16 0.96871 0.08082 #&gt; 17 0.96748 0.08331 #&gt; #&gt; Variable Importances: (Extract with `h2o.varimp`) #&gt; ================================================= #&gt; #&gt; Variable Importances: #&gt; variable relative_importance scaled_importance #&gt; 1 Elevation 1.000000 1.000000 #&gt; 2 Horizontal_Distance_To_Fire_Points 0.917530 0.917530 #&gt; 3 Horizontal_Distance_To_Roadways 0.871192 0.871192 #&gt; 4 Wilderness_Area.area_0 0.634473 0.634473 #&gt; 5 Horizontal_Distance_To_Hydrology 0.596334 0.596334 #&gt; percentage #&gt; 1 0.048702 #&gt; 2 0.044685 #&gt; 3 0.042429 #&gt; 4 0.030900 #&gt; 5 0.029043 #&gt; #&gt; --- #&gt; variable relative_importance scaled_importance #&gt; 51 Soil_Type.type_24 0.154181 0.154181 #&gt; 52 Soil_Type.type_35 0.153859 0.153859 #&gt; 53 Soil_Type.type_14 0.152899 0.152899 #&gt; 54 Soil_Type.type_13 0.149000 0.149000 #&gt; 55 Soil_Type.missing(NA) 0.000000 0.000000 #&gt; 56 Wilderness_Area.missing(NA) 0.000000 0.000000 #&gt; percentage #&gt; 51 0.007509 #&gt; 52 0.007493 #&gt; 53 0.007446 #&gt; 54 0.007257 #&gt; 55 0.000000 #&gt; 56 0.000000 This model is fully functional and can be inspected, restarted, or used to score a dataset, etc. Note that binary compatibility between H2O versions is currently not guaranteed. 40.5.9 Cross-Validation For N-fold cross-validation, specify nfolds&gt;1 instead of (or in addition to) a validation frame, and N+1 models will be built: 1 model on the full training data, and N models with each 1/N-th of the data held out (there are different holdout strategies). Those N models then score on the held out data, and their combined predictions on the full training data are scored to get the cross-validation metrics. dlmodel &lt;- h2o.deeplearning( x=predictors, y=response, training_frame=train, hidden=c(10,10), epochs=1, nfolds=5, fold_assignment=&quot;Modulo&quot; # can be &quot;AUTO&quot;, &quot;Modulo&quot;, &quot;Random&quot; or &quot;Stratified&quot; ) dlmodel N-fold cross-validation is especially useful with early stopping, as the main model will pick the ideal number of epochs from the convergence behavior of the cross-validation models. 40.6 Regression and Binary Classification Assume we want to turn the multi-class problem above into a binary classification problem. We create a binary response as follows: train$bin_response &lt;- ifelse(train[,response] == &quot;class_1&quot;, 0, 1) Let’s build a quick model and inspect the model: dlmodel &lt;- h2o.deeplearning( x=predictors, y=&quot;bin_response&quot;, training_frame=train, hidden=c(10,10), epochs=0.1 ) summary(dlmodel) Instead of a binary classification model, we find a regression model (H2ORegressionModel) that contains only 1 output neuron (instead of 2). The reason is that the response was a numerical feature (ordinal numbers 0 and 1), and H2O Deep Learning was run with distribution=AUTO, which defaulted to a Gaussian regression problem for a real-valued response. H2O Deep Learning supports regression for distributions other than Gaussian such as Poisson, Gamma, Tweedie, Laplace. It also supports Huber loss and per-row offsets specified via an offset_column. We refer to our H2O Deep Learning regression code examples for more information. To perform classification, the response must first be turned into a categorical (factor) feature: train$bin_response &lt;- as.factor(train$bin_response) ##make categorical dlmodel &lt;- h2o.deeplearning( x=predictors, y=&quot;bin_response&quot;, training_frame=train, hidden=c(10,10), epochs=0.1 #balance_classes=T ## enable this for high class imbalance ) summary(dlmodel) ## Now the model metrics contain AUC for binary classification plot(h2o.performance(dlmodel)) ## display ROC curve Now the model performs (binary) classification, and has multiple (2) output neurons. 40.7 Unsupervised Anomaly detection For instructions on how to build unsupervised models with H2O Deep Learning, we refer to our previous Tutorial on Anomaly Detection with H2O Deep Learning and our MNIST Anomaly detection code example, as well as our Stacked AutoEncoder R code example and another one for Unsupervised Pretraining with an AutoEncoder R code example. 40.8 H2O Deep Learning Tips &amp; Tricks 40.8.1 Performance Tuning The Definitive H2O Deep Learning Performance Tuning blog post covers many of the following points that affect the computational efficiency, so it’s highly recommended. 40.8.2 Activation Functions While sigmoids have been used historically for neural networks, H2O Deep Learning implements Tanh, a scaled and shifted variant of the sigmoid which is symmetric around 0. Since its output values are bounded by -1..1, the stability of the neural network is rarely endangered. However, the derivative of the tanh function is always non-zero and back-propagation (training) of the weights is more computationally expensive than for rectified linear units, or Rectifier, which is max(0,x) and has vanishing gradient for x&lt;=0, leading to much faster training speed for large networks and is often the fastest path to accuracy on larger problems. In case you encounter instabilities with the Rectifier (in which case model building is automatically aborted), try a limited value to re-scale the weights: max_w2=10. The Maxout activation function is computationally more expensive, but can lead to higher accuracy. It is a generalized version of the Rectifier with two non-zero channels. In practice, the Rectifier (and RectifierWithDropout, see below) is the most versatile and performant option for most problems. 40.8.3 Generalization Techniques L1 and L2 penalties can be applied by specifying the l1 and l2 parameters. Intuition: L1 lets only strong weights survive (constant pulling force towards zero), while L2 prevents any single weight from getting too big. Dropout has recently been introduced as a powerful generalization technique, and is available as a parameter per layer, including the input layer. input_dropout_ratio controls the amount of input layer neurons that are randomly dropped (set to zero), while hidden_dropout_ratios are specified for each hidden layer. The former controls overfitting with respect to the input data (useful for high-dimensional noisy data), while the latter controls overfitting of the learned features. Note that hidden_dropout_ratios require the activation function to end with …WithDropout. 40.8.4 Early stopping and optimizing for lowest validation error By default, Deep Learning training stops when the stopping_metric does not improve by at least stopping_tolerance (0.01 means 1% improvement) for stopping_rounds consecutive scoring events on the training (or validation) data. By default, overwrite_with_best_model is enabled and the model returned after training for the specified number of epochs (or after stopping early due to convergence) is the model that has the best training set error (according to the metric specified by stopping_metric), or, if a validation set is provided, the lowest validation set error. Note that the training or validation set errors can be based on a subset of the training or validation data, depending on the values for score_validation_samples or score_training_samples, see below. For early stopping on a predefined error rate on the training data (accuracy for classification or MSE for regression), specify classification_stop or regression_stop. 40.8.5 Training Samples per MapReduce Iteration The parameter train_samples_per_iteration matters especially in multi-node operation. It controls the number of rows trained on for each MapReduce iteration. Depending on the value selected, one MapReduce pass can sample observations, and multiple such passes are needed to train for one epoch. All H2O compute nodes then communicate to agree on the best model coefficients (weights/biases) so far, and the model may then be scored (controlled by other parameters below). The default value of -2 indicates auto-tuning, which attemps to keep the communication overhead at 5% of the total runtime. The parameter target_ratio_comm_to_comp controls this ratio. This parameter is explained in more detail in the H2O Deep Learning booklet, 40.8.6 Categorical Data For categorical data, a feature with K factor levels is automatically one-hot encoded (horizontalized) into K-1 input neurons. Hence, the input neuron layer can grow substantially for datasets with high factor counts. In these cases, it might make sense to reduce the number of hidden neurons in the first hidden layer, such that large numbers of factor levels can be handled. In the limit of 1 neuron in the first hidden layer, the resulting model is similar to logistic regression with stochastic gradient descent, except that for classification problems, there’s still a softmax output layer, and that the activation function is not necessarily a sigmoid (Tanh). If variable importances are computed, it is recommended to turn on use_all_factor_levels (K input neurons for K levels). The experimental option max_categorical_features uses feature hashing to reduce the number of input neurons via the hash trick at the expense of hash collisions and reduced accuracy. Another way to reduce the dimensionality of the (categorical) features is to use h2o.glrm(), we refer to the GLRM tutorial for more details. 40.8.7 Sparse Data If the input data is sparse (many zeros), then it might make sense to enable the sparse option. This will result in the input not being standardized (0 mean, 1 variance), but only de-scaled (1 variance) and 0 values remain 0, leading to more efficient back-propagation. Sparsity is also a reason why CPU implementations can be faster than GPU implementations, because they can take advantage of if/else statements more effectively. 40.8.8 Missing Values H2O Deep Learning automatically does mean imputation for missing values during training (leaving the input layer activation at 0 after standardizing the values). For testing, missing test set values are also treated the same way by default. See the h2o.impute function to do your own mean imputation. 40.8.9 Loss functions, Distributions, Offsets, Observation Weights H2O Deep Learning supports advanced statistical features such as multiple loss functions, non-Gaussian distributions, per-row offsets and observation weights. In addition to Gaussian distributions and Squared loss, H2O Deep Learning supports Poisson, Gamma, Tweedie and Laplace distributions. It also supports Absolute and Huber loss and per-row offsets specified via an offset_column. Observation weights are supported via a user-specified weights_column. We refer to our H2O Deep Learning R test code examples for more information. 40.8.10 Exporting Weights and Biases The model parameters (weights connecting two adjacent layers and per-neuron bias terms) can be stored as H2O Frames (like a dataset) by enabling export_weights_and_biases, and they can be accessed as follows: iris_dl &lt;- h2o.deeplearning(1:4,5,as.h2o(iris), export_weights_and_biases=T) #&gt; | | | 0% | |=================================================================| 100% #&gt; | | | 0% | |====== | 10% | |=================================================================| 100% h2o.weights(iris_dl, matrix_id=1) #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width #&gt; 1 -0.0870 0.00154 0.15280 0.00931 #&gt; 2 0.1192 0.04756 -0.12703 -0.10815 #&gt; 3 0.0715 -0.08125 0.08345 -0.16043 #&gt; 4 -0.0117 0.07703 0.16130 0.05491 #&gt; 5 -0.0285 -0.05633 0.00425 -0.10684 #&gt; 6 0.1622 -0.00684 0.11072 0.17957 #&gt; #&gt; [200 rows x 4 columns] h2o.weights(iris_dl, matrix_id=2) #&gt; C1 C2 C3 C4 C5 C6 C7 C8 #&gt; 1 0.107829 0.1277 0.0869 0.06621 0.123956 0.0618 0.0751 0.0577 #&gt; 2 -0.090833 -0.1182 0.0852 -0.00329 0.080453 -0.0658 -0.0484 -0.0207 #&gt; 3 -0.000646 0.0917 -0.1087 0.06621 0.000315 -0.0339 0.0828 0.0438 #&gt; 4 -0.025255 0.0836 0.0544 0.10748 0.083134 0.1197 0.0212 -0.1035 #&gt; 5 -0.034917 -0.1277 0.0431 0.06003 -0.003279 0.0803 0.0994 0.0387 #&gt; 6 0.086986 0.1036 -0.0496 0.06662 0.086459 -0.1356 0.0598 0.0242 #&gt; C9 C10 C11 C12 C13 C14 C15 C16 #&gt; 1 -0.08565 -0.1036 -0.0912 0.039972 0.099843 -0.00404 -0.0846 -0.0194 #&gt; 2 -0.06138 0.0833 -0.0954 0.092244 -0.053918 0.10785 0.1026 -0.0678 #&gt; 3 0.10816 0.0949 -0.0827 0.094026 0.000669 0.01487 0.0696 -0.0352 #&gt; 4 0.08121 0.0215 -0.0314 0.000954 -0.059629 0.11984 0.0448 -0.0129 #&gt; 5 -0.01523 0.0657 0.1014 0.088703 -0.117456 -0.10496 0.0722 0.0518 #&gt; 6 0.00718 -0.0325 -0.0477 -0.056411 0.043812 -0.08859 -0.0306 -0.0902 #&gt; C17 C18 C19 C20 C21 C22 C23 C24 #&gt; 1 0.028723 -0.07205 0.0342 -0.06156 0.0731 -0.0546 -4.56e-02 -0.00406 #&gt; 2 -0.042586 0.09101 -0.0592 -0.08596 0.0444 0.0501 7.85e-03 -0.12030 #&gt; 3 0.083259 -0.00341 0.0155 0.08543 -0.0917 0.0154 -9.61e-05 -0.01162 #&gt; 4 0.023042 0.02415 -0.1259 -0.06654 -0.0373 0.0332 1.78e-02 0.02686 #&gt; 5 0.001115 -0.02766 0.0866 0.12255 -0.1068 0.0509 6.76e-04 0.03836 #&gt; 6 0.000205 0.08822 0.0584 0.00745 0.0546 0.1063 2.06e-02 -0.02708 #&gt; C25 C26 C27 C28 C29 C30 C31 C32 C33 #&gt; 1 0.0268 -0.0115 -0.0735 -0.00268 -0.0888 -0.0279 -0.0796 0.0415 0.0889 #&gt; 2 -0.0275 0.1188 -0.1235 -0.12237 0.0121 0.0648 -0.1060 -0.0935 -0.1082 #&gt; 3 -0.1057 0.0705 -0.0157 0.01391 -0.0298 -0.1080 -0.0511 0.0803 -0.0458 #&gt; 4 -0.0653 -0.1036 0.0394 -0.05123 0.0863 0.0868 -0.0921 -0.1175 -0.0113 #&gt; 5 0.0685 0.0351 -0.1044 -0.04850 0.0794 -0.0654 0.0366 0.0989 -0.0660 #&gt; 6 -0.0977 0.1081 -0.1303 -0.06890 0.1355 -0.0282 0.0672 -0.0979 -0.0910 #&gt; C34 C35 C36 C37 C38 C39 C40 C41 #&gt; 1 0.00141 -0.0758 -0.0697 -0.1145 -0.00897 -0.00763 0.0824 0.0471 #&gt; 2 -0.07059 -0.0819 0.0611 -0.0380 -0.06713 -0.08721 -0.0290 -0.1303 #&gt; 3 0.04641 0.0568 -0.1099 -0.0839 0.02800 -0.08550 0.0131 -0.0608 #&gt; 4 0.10683 0.1144 -0.0968 0.0941 0.09475 0.05617 0.0425 0.0474 #&gt; 5 -0.04266 0.1184 0.1002 0.0458 -0.04558 0.10865 0.0418 0.0293 #&gt; 6 0.09481 -0.0348 0.0373 0.0547 -0.00728 0.08601 0.0164 0.0471 #&gt; C42 C43 C44 C45 C46 C47 C48 C49 #&gt; 1 0.04842 -0.0798 0.0487 0.00578 -0.0491 0.1116 -0.0981 -0.0403 #&gt; 2 0.01555 0.0431 -0.0067 0.04798 -0.1046 0.0461 -0.0245 0.1003 #&gt; 3 0.11542 -0.0612 -0.0820 -0.06198 0.0305 -0.0133 0.0629 -0.0589 #&gt; 4 0.00687 0.0470 0.1031 -0.05628 0.1171 0.1010 -0.0663 -0.0782 #&gt; 5 -0.11490 0.1111 -0.0724 -0.05252 0.1051 -0.0157 -0.0325 -0.0118 #&gt; 6 -0.11367 0.0355 -0.1012 -0.06519 0.0717 0.0476 -0.0874 0.0900 #&gt; C50 C51 C52 C53 C54 C55 C56 C57 #&gt; 1 -0.1068 0.1226 0.000291 0.10955 0.0174 -0.0323 0.0601 -0.0976 #&gt; 2 0.0273 -0.0906 -0.090692 0.06990 0.0391 -0.0448 0.1009 0.0858 #&gt; 3 0.0361 0.0641 0.046363 0.06922 0.0132 -0.0743 -0.0168 -0.0119 #&gt; 4 -0.0803 -0.0322 0.034457 -0.00695 0.1215 0.0281 0.1102 0.0526 #&gt; 5 -0.0422 -0.1031 0.069352 -0.02824 0.1202 -0.1184 -0.0369 -0.0854 #&gt; 6 -0.0904 -0.0682 0.027310 -0.02310 0.0305 0.0136 0.0723 -0.0947 #&gt; C58 C59 C60 C61 C62 C63 C64 C65 #&gt; 1 -0.0579 -0.00642 0.07213 0.1025 0.0924 -0.0822 -0.11090 0.01038 #&gt; 2 0.0595 0.07741 -0.06229 -0.0208 -0.1030 0.0378 0.10366 -0.00161 #&gt; 3 0.0897 -0.03195 -0.04590 0.0544 0.0579 -0.1076 0.11410 -0.02253 #&gt; 4 0.0967 0.09415 -0.08499 -0.0195 -0.0334 -0.0612 0.08532 -0.10281 #&gt; 5 -0.0528 -0.06734 0.03670 0.0738 -0.1266 0.0505 0.00781 0.08857 #&gt; 6 -0.0312 -0.01491 0.00239 0.0820 0.0739 0.0909 -0.03319 0.04479 #&gt; C66 C67 C68 C69 C70 C71 C72 C73 #&gt; 1 0.0483 -0.05477 -0.09579 0.0679 0.0342 -0.0829 -0.0131 -0.0650 #&gt; 2 0.0872 0.03166 -0.05111 -0.0940 0.0935 0.1158 -0.0948 -0.0448 #&gt; 3 0.0434 0.06582 -0.04545 -0.1095 -0.1090 -0.0127 -0.0888 0.0589 #&gt; 4 -0.0261 0.08272 0.00254 -0.0204 0.0981 -0.0451 0.0720 0.0782 #&gt; 5 -0.0840 -0.00917 -0.10823 0.0983 0.0336 0.0859 0.0561 -0.0652 #&gt; 6 0.1059 0.11613 -0.10438 -0.1094 -0.0164 0.1119 0.0861 0.0536 #&gt; C74 C75 C76 C77 C78 C79 C80 C81 #&gt; 1 0.0044 0.09579 -0.0166 -0.0327 -0.0655 -0.0229 0.0359 0.12105 #&gt; 2 -0.0668 -0.00355 -0.0965 0.0531 -0.0656 -0.0501 -0.0485 -0.09847 #&gt; 3 -0.1024 -0.04479 -0.0999 -0.0900 0.0376 0.0066 -0.0289 0.05917 #&gt; 4 0.0130 0.05593 -0.0296 0.0934 0.0567 -0.0314 -0.0303 -0.00131 #&gt; 5 0.0814 -0.04239 0.0783 0.0356 0.1304 0.0490 0.0446 -0.05152 #&gt; 6 -0.1097 0.02310 -0.0258 -0.0877 0.0115 0.0626 -0.0206 0.03163 #&gt; C82 C83 C84 C85 C86 C87 C88 C89 #&gt; 1 -0.0285 0.0854 0.0509 0.0170 -0.11259 0.0915 -0.05281 -0.1171 #&gt; 2 0.0782 0.0270 -0.0591 -0.0451 0.00734 -0.0686 0.00672 -0.0363 #&gt; 3 0.1168 -0.0301 0.0572 -0.0414 0.05590 0.1037 -0.00434 -0.0541 #&gt; 4 0.1063 -0.1280 -0.0219 0.0466 0.11370 0.0763 0.07360 0.0125 #&gt; 5 0.0346 -0.0975 -0.0906 0.0827 0.02945 0.0442 -0.04874 0.0591 #&gt; 6 -0.0995 -0.0922 0.0363 0.0753 0.13563 -0.1140 0.00582 0.1141 #&gt; C90 C91 C92 C93 C94 C95 C96 C97 C98 #&gt; 1 -0.0367 0.1065 0.0788 -0.0121 -0.0450 -0.0685 -0.0532 0.0377 0.05378 #&gt; 2 0.0696 -0.0193 0.0669 0.0827 -0.0642 -0.0419 -0.0739 0.1012 0.06008 #&gt; 3 0.0781 -0.0672 -0.0850 0.0858 -0.1198 0.0515 -0.0015 -0.0732 -0.09775 #&gt; 4 0.0472 -0.0454 -0.0189 -0.0888 -0.0127 -0.1079 0.0130 0.0037 0.00838 #&gt; 5 -0.0410 -0.0625 0.0736 -0.0905 0.0213 0.1195 -0.0854 0.0287 -0.02501 #&gt; 6 -0.1327 -0.0771 -0.1128 -0.0505 -0.0811 0.0308 -0.0511 -0.0742 -0.07074 #&gt; C99 C100 C101 C102 C103 C104 C105 C106 #&gt; 1 0.0961 0.0502 -0.0284 -0.020862 0.0665 0.0684 -0.1142 0.0376 #&gt; 2 0.0931 0.0212 -0.1228 0.066691 -0.0425 0.0742 -0.0591 -0.1120 #&gt; 3 -0.0806 0.0775 0.0343 0.041735 0.0905 -0.0981 -0.1193 -0.0841 #&gt; 4 -0.0477 -0.0448 -0.0104 -0.000969 0.0761 -0.0185 -0.0202 -0.0259 #&gt; 5 -0.0320 -0.0342 0.1088 -0.098690 0.0829 -0.0143 -0.0637 0.0425 #&gt; 6 0.0912 0.0327 -0.0595 -0.002881 0.1348 0.0220 0.0980 -0.1167 #&gt; C107 C108 C109 C110 C111 C112 C113 C114 #&gt; 1 0.0540 -0.0447 0.0720 0.000836 -0.0212 0.0671 0.02255 -0.08904 #&gt; 2 -0.0230 -0.0951 0.0453 -0.115631 -0.0904 0.0587 0.01048 -0.11148 #&gt; 3 -0.0937 -0.0091 0.0378 -0.008058 0.0839 0.0295 -0.05559 -0.05204 #&gt; 4 -0.0955 0.0986 -0.1159 0.036538 0.0112 -0.1135 -0.04100 -0.00771 #&gt; 5 0.0228 0.0851 -0.0992 0.011814 -0.0924 0.0312 0.00764 0.01835 #&gt; 6 -0.0414 0.1226 -0.0919 -0.080460 0.1228 -0.0614 0.01029 0.01123 #&gt; C115 C116 C117 C118 C119 C120 C121 C122 #&gt; 1 0.1017 0.00391 0.0432 -0.0256 -0.0245 0.05984 -0.0457 0.1150 #&gt; 2 -0.0400 -0.03847 0.0917 -0.1099 0.0554 0.04787 -0.1125 0.1149 #&gt; 3 -0.0434 -0.03250 0.0778 -0.0467 -0.0102 0.00829 0.1052 -0.0826 #&gt; 4 -0.0535 -0.02972 0.0498 0.0856 -0.0813 0.08945 0.0674 -0.1055 #&gt; 5 0.0257 0.04784 0.0322 -0.0912 -0.0054 -0.03921 -0.1069 -0.1092 #&gt; 6 -0.0903 0.03133 -0.0160 0.0364 -0.1091 -0.06062 -0.0714 0.0165 #&gt; C123 C124 C125 C126 C127 C128 C129 C130 #&gt; 1 0.01594 0.08335 -0.0556 -0.07181 -0.0028 0.09968 -0.0817 -0.00807 #&gt; 2 -0.11523 -0.00136 -0.0735 0.05121 0.0433 -0.01121 0.1028 0.10214 #&gt; 3 0.00177 0.03724 -0.0079 0.04867 0.1069 0.02507 0.0191 -0.06188 #&gt; 4 0.02580 0.03899 -0.0279 -0.00281 0.0254 0.07368 -0.1205 -0.06448 #&gt; 5 0.04419 -0.11854 0.0236 -0.01710 -0.0787 -0.10690 -0.0548 0.02725 #&gt; 6 0.07412 -0.08303 0.1141 -0.03502 -0.0711 -0.00255 0.1312 -0.06113 #&gt; C131 C132 C133 C134 C135 C136 C137 C138 #&gt; 1 0.04782 -0.0644 0.0634 0.0711 -0.1164 -0.00711 -0.0677 0.03936 #&gt; 2 0.00688 -0.0555 0.0602 -0.0483 0.0178 -0.11610 0.1048 0.10214 #&gt; 3 -0.07686 0.0300 0.0664 -0.0368 -0.0966 -0.04576 -0.0627 0.00662 #&gt; 4 -0.04566 -0.1121 -0.0159 -0.0359 -0.0373 -0.07872 0.1048 0.05456 #&gt; 5 0.09015 -0.0548 -0.0266 0.0742 -0.0797 -0.09195 -0.0230 -0.08031 #&gt; 6 -0.09411 0.1054 -0.0274 -0.1260 0.0149 -0.01007 -0.1174 0.05941 #&gt; C139 C140 C141 C142 C143 C144 C145 C146 #&gt; 1 0.12028 -0.0788 0.1119 0.03871 0.11042 -0.1081 0.0830 0.06451 #&gt; 2 0.00676 0.0618 0.0932 0.10408 -0.07563 0.0742 -0.0232 0.08748 #&gt; 3 -0.05368 0.0573 0.1046 0.00696 -0.11931 -0.0275 0.0323 0.00589 #&gt; 4 -0.01408 -0.0107 0.0775 -0.05172 -0.04524 0.0826 0.0179 0.03513 #&gt; 5 0.05412 0.0852 -0.0685 -0.11284 0.00926 -0.0647 -0.0866 -0.11570 #&gt; 6 -0.00623 0.1040 0.0476 -0.08199 -0.05326 0.0199 0.1414 0.10640 #&gt; C147 C148 C149 C150 C151 C152 C153 C154 C155 #&gt; 1 -0.0497 0.1062 0.1165 0.0545 -0.0166 0.0666 -0.0759 0.0328 0.0803 #&gt; 2 -0.0697 0.1216 0.0226 0.0304 0.1222 -0.0300 0.1104 -0.0534 0.0469 #&gt; 3 0.0860 0.0140 -0.0901 0.0179 0.1053 0.1054 0.0125 -0.0108 0.1221 #&gt; 4 -0.0498 -0.0587 0.0250 -0.0703 0.0130 -0.0958 -0.0400 -0.0589 0.0537 #&gt; 5 0.0420 -0.0211 0.0430 -0.1124 0.0438 -0.0676 -0.0267 0.0385 0.1181 #&gt; 6 -0.0846 -0.1095 0.0811 0.0298 -0.0466 0.0544 -0.0311 0.0856 0.0609 #&gt; C156 C157 C158 C159 C160 C161 C162 C163 #&gt; 1 -0.0149 -0.06879 0.0246 -0.096376 0.063127 -0.01219 -0.0413 -0.08747 #&gt; 2 -0.0855 -0.11697 0.1057 -0.034987 -0.010620 0.09080 0.1059 -0.00345 #&gt; 3 -0.1140 -0.03644 -0.0813 -0.122811 -0.048008 -0.00762 -0.1090 -0.03875 #&gt; 4 0.1085 0.09062 -0.0150 0.000775 -0.011457 0.00688 -0.0654 0.02909 #&gt; 5 0.0243 -0.04277 -0.0817 0.012996 0.062901 -0.06023 0.0642 0.08507 #&gt; 6 -0.0436 0.00853 -0.0200 -0.016101 0.000446 0.05556 -0.0174 -0.03031 #&gt; C164 C165 C166 C167 C168 C169 C170 C171 #&gt; 1 -0.01476 -0.00545 -0.00961 -0.1051 -0.05468 0.0681 -0.0409 -0.02444 #&gt; 2 0.08519 0.07288 0.09580 -0.0476 0.00874 -0.0840 0.0273 0.00202 #&gt; 3 -0.00365 -0.07968 -0.02970 0.0553 -0.11462 -0.0486 0.0821 0.04749 #&gt; 4 0.08940 0.05294 -0.07548 -0.0231 0.04800 -0.0775 0.0860 -0.07312 #&gt; 5 -0.06233 -0.07841 0.09729 -0.0908 -0.05680 -0.0510 0.1029 0.09928 #&gt; 6 0.08345 0.10334 -0.08513 0.0456 -0.10723 -0.0384 -0.0340 0.06515 #&gt; C172 C173 C174 C175 C176 C177 C178 C179 C180 #&gt; 1 -0.1218 -0.1047 -0.0302 0.0582 -0.09306 0.0582 0.0191 -0.0121 0.1216 #&gt; 2 0.0871 -0.0942 0.0565 -0.0169 -0.08493 -0.1261 -0.0370 0.0347 0.1191 #&gt; 3 0.0378 0.0927 -0.0984 0.1088 -0.04524 0.0452 -0.1203 0.0754 -0.0516 #&gt; 4 -0.0324 0.0243 -0.0478 0.0272 0.04949 0.0821 0.0880 -0.0269 -0.1139 #&gt; 5 0.1057 -0.1112 -0.0390 0.0817 -0.00142 0.0978 0.0934 -0.0243 0.0640 #&gt; 6 -0.0502 0.1325 0.1165 -0.0672 0.07527 -0.0179 0.1294 -0.0406 0.0469 #&gt; C181 C182 C183 C184 C185 C186 C187 C188 #&gt; 1 -0.09508 0.1096 -0.1106 -0.1093 -0.0248 0.0992 -0.06118 -0.05164 #&gt; 2 -0.01950 -0.0906 0.0367 0.0426 0.0711 -0.0671 -0.11344 -0.00923 #&gt; 3 -0.08736 -0.0991 0.0774 -0.1119 -0.0431 -0.0493 0.11103 0.08811 #&gt; 4 0.00511 0.0861 -0.0864 0.0998 0.1074 0.0211 0.01215 0.06394 #&gt; 5 -0.01527 0.0524 0.0536 -0.0287 0.0154 -0.0426 0.00722 -0.11610 #&gt; 6 0.07885 0.1037 -0.0546 0.0649 0.0387 0.1208 -0.06239 -0.08920 #&gt; C189 C190 C191 C192 C193 C194 C195 C196 #&gt; 1 -0.06246 -0.0733 0.1057 -0.0979 0.0418 0.05362 -0.03370 0.0617 #&gt; 2 -0.00775 -0.0548 -0.0683 -0.0159 0.0913 0.00633 0.02050 0.0895 #&gt; 3 0.08593 0.0783 -0.0717 -0.0192 0.0089 0.04720 0.10542 0.0074 #&gt; 4 -0.03573 -0.0223 -0.0453 -0.0696 -0.1082 -0.04173 -0.08734 -0.0512 #&gt; 5 0.08863 0.0785 -0.0978 -0.1133 0.0344 0.09452 -0.00688 -0.0678 #&gt; 6 -0.09987 0.0999 -0.0405 -0.0204 -0.0989 -0.05206 -0.09278 -0.0360 #&gt; C197 C198 C199 C200 #&gt; 1 0.01196 -0.0772 -0.1044 0.0449 #&gt; 2 -0.03282 -0.1042 0.1205 -0.0730 #&gt; 3 -0.00212 -0.0727 0.1072 0.0582 #&gt; 4 -0.04101 -0.1177 -0.1107 -0.0733 #&gt; 5 0.05320 -0.0514 0.0829 -0.1199 #&gt; 6 0.00339 0.1118 0.0617 -0.0323 #&gt; #&gt; [200 rows x 200 columns] h2o.weights(iris_dl, matrix_id=3) #&gt; C1 C2 C3 C4 C5 C6 C7 C8 C9 C10 #&gt; 1 0.536 0.4489 0.6319 0.0779 0.401 0.332 -0.140 -0.374 -0.178 0.112 #&gt; 2 0.631 -0.2337 -0.0555 0.2791 0.174 -0.297 0.252 0.672 0.631 0.467 #&gt; 3 0.535 0.0588 0.5188 0.2634 0.624 -0.516 -0.451 -0.290 -0.574 -0.416 #&gt; C11 C12 C13 C14 C15 C16 C17 C18 C19 C20 #&gt; 1 -0.126 -0.171 0.499 -0.070 -0.620 -0.255 0.639 -0.507 0.230 0.277 #&gt; 2 -0.336 -0.516 0.123 -0.149 -0.534 -0.413 -0.175 0.505 -0.062 -0.219 #&gt; 3 -0.440 -0.319 -0.314 -0.547 0.457 -0.624 0.642 0.497 0.165 -0.517 #&gt; C21 C22 C23 C24 C25 C26 C27 C28 C29 #&gt; 1 -0.253 -0.43847 -0.643 -0.389 -0.687 -0.4904 -0.563 0.00317 -0.610 #&gt; 2 0.547 -0.07458 -0.358 -0.635 -0.203 0.5571 -0.517 -0.66071 0.519 #&gt; 3 -0.286 -0.00611 -0.353 0.622 0.277 0.0373 0.257 0.27002 -0.163 #&gt; C30 C31 C32 C33 C34 C35 C36 C37 C38 #&gt; 1 0.2569 -0.288 -0.670 0.391 -0.687 -0.216155 0.54285 0.684 -0.16245 #&gt; 2 0.0043 0.133 0.222 -0.491 -0.201 -0.451180 -0.50092 0.200 0.12162 #&gt; 3 -0.4248 0.405 -0.380 -0.528 0.438 -0.000804 0.00396 0.409 0.00525 #&gt; C39 C40 C41 C42 C43 C44 C45 C46 C47 #&gt; 1 0.482 -0.0138 0.48122 0.333 -0.148 -0.00687 0.225 -0.1431 0.0318 #&gt; 2 0.347 0.3138 -0.00735 -0.659 -0.251 -0.40531 0.440 0.1340 -0.6429 #&gt; 3 0.228 -0.4450 0.03243 -0.441 0.185 0.49618 0.646 0.0944 -0.1420 #&gt; C48 C49 C50 C51 C52 C53 C54 C55 C56 C57 #&gt; 1 0.164 0.553 -0.116 -0.5410 0.435 -0.3372 -0.429 0.431 0.594 0.6325 #&gt; 2 -0.188 -0.616 -0.589 0.3827 -0.281 0.4506 -0.187 0.143 0.575 0.0793 #&gt; 3 0.123 0.227 -0.374 -0.0243 -0.626 -0.0265 0.404 -0.299 -0.104 -0.6376 #&gt; C58 C59 C60 C61 C62 C63 C64 C65 C66 #&gt; 1 -0.4071 -0.5426 0.152 -0.680 -0.593 -0.540 -0.4624 -0.54546 -0.343 #&gt; 2 0.5457 -0.0982 -0.246 -0.312 -0.231 -0.349 0.0112 0.00337 0.106 #&gt; 3 -0.0307 0.3382 -0.572 -0.436 -0.392 -0.325 -0.1679 0.38471 -0.484 #&gt; C67 C68 C69 C70 C71 C72 C73 C74 C75 #&gt; 1 0.683 0.220 0.50723 0.40244 0.6350 -0.3331 0.29470 -0.0634 0.606 #&gt; 2 0.157 -0.123 -0.00534 -0.63194 -0.0102 0.4559 0.57693 -0.0888 0.320 #&gt; 3 -0.150 -0.089 0.43024 0.00335 0.5083 -0.0239 0.00264 0.0302 0.401 #&gt; C76 C77 C78 C79 C80 C81 C82 C83 C84 #&gt; 1 0.422 -0.2889 -0.4063 -0.650 0.661 -0.590 -0.3170 -0.0505 -0.0668 #&gt; 2 -0.229 -0.6000 -0.6691 0.111 -0.475 -0.176 0.0531 -0.3611 -0.3856 #&gt; 3 0.199 -0.0551 0.0643 -0.185 0.498 -0.207 0.4886 -0.2303 -0.1833 #&gt; C85 C86 C87 C88 C89 C90 C91 C92 C93 C94 #&gt; 1 -0.485 -0.354 0.358 0.6501 -0.362 -0.5495 0.416 0.589 0.469 -0.1038 #&gt; 2 0.124 0.348 0.442 -0.3635 0.576 -0.0676 -0.574 -0.318 0.341 0.4133 #&gt; 3 -0.244 0.123 0.381 0.0929 0.528 -0.2310 -0.572 0.395 0.173 0.0483 #&gt; C95 C96 C97 C98 C99 C100 C101 C102 C103 C104 #&gt; 1 0.683 0.329 0.317 -0.1418 -0.445 0.576 0.3882 0.342 0.6583 0.359 #&gt; 2 0.304 0.123 -0.146 0.2931 -0.688 0.235 0.0423 -0.470 0.0153 0.466 #&gt; 3 0.376 0.108 -0.432 0.0731 -0.420 -0.445 -0.0667 0.464 0.5205 -0.423 #&gt; C105 C106 C107 C108 C109 C110 C111 C112 C113 C114 #&gt; 1 0.119 -0.2347 0.6333 -0.0263 -0.137 0.670 0.668 -0.394 -0.195 -0.181 #&gt; 2 0.542 -0.3187 0.4516 0.5882 -0.529 -0.406 -0.236 -0.122 0.250 0.630 #&gt; 3 0.254 0.0173 -0.0789 0.5461 0.575 0.542 0.197 -0.377 0.636 0.155 #&gt; C115 C116 C117 C118 C119 C120 C121 C122 C123 #&gt; 1 -0.530 0.0631 -0.664 0.00366 0.337 -0.0819 -0.595 0.4400 0.1621 #&gt; 2 -0.537 -0.3988 -0.491 0.61438 -0.024 0.0445 0.297 -0.5196 0.0105 #&gt; 3 0.113 -0.6476 0.645 -0.58626 0.533 0.0917 -0.224 0.0217 0.1184 #&gt; C124 C125 C126 C127 C128 C129 C130 C131 C132 #&gt; 1 0.0526 0.348 0.6504 0.377 0.0345 -0.0358 0.3684 -0.222 -0.217 #&gt; 2 0.3996 0.642 -0.0473 0.201 -0.1346 -0.3085 -0.4481 -0.434 -0.449 #&gt; 3 -0.4985 -0.540 -0.2644 -0.112 -0.3522 0.2086 0.0752 0.540 -0.241 #&gt; C133 C134 C135 C136 C137 C138 C139 C140 C141 C142 #&gt; 1 -0.363 -0.0348 0.229 0.463 0.553 -0.417 0.286 -0.353 -0.455 -0.0075 #&gt; 2 -0.222 -0.1191 0.285 -0.298 0.408 -0.288 0.528 -0.444 0.410 0.5588 #&gt; 3 0.372 -0.4502 -0.278 0.116 0.289 0.216 -0.377 0.116 -0.454 -0.6373 #&gt; C143 C144 C145 C146 C147 C148 C149 C150 C151 C152 #&gt; 1 -0.676 0.338 0.209 0.493 0.5413 -0.253 0.2416 -0.239 0.399 0.0620 #&gt; 2 0.169 0.497 0.568 -0.603 0.0581 -0.414 0.0367 0.318 0.467 -0.0551 #&gt; 3 0.336 0.291 0.153 0.353 -0.0101 0.344 -0.1645 -0.340 -0.288 0.4117 #&gt; C153 C154 C155 C156 C157 C158 C159 C160 C161 C162 #&gt; 1 0.351 -0.557 0.138 -0.129 0.214 0.312 0.0786 -0.436 0.0907 -0.466 #&gt; 2 0.294 0.492 0.309 -0.324 -0.234 -0.439 0.3967 -0.361 -0.3859 0.237 #&gt; 3 0.418 -0.201 0.420 0.589 0.112 0.240 -0.4683 -0.147 -0.1469 -0.351 #&gt; C163 C164 C165 C166 C167 C168 C169 C170 C171 #&gt; 1 0.0339 -0.499 -0.5693 0.202 0.0996 0.0321 0.212 -0.6664 -0.273 #&gt; 2 -0.5120 0.369 0.0593 -0.319 -0.4940 -0.3511 0.203 -0.0536 0.237 #&gt; 3 -0.2697 -0.447 -0.1927 0.200 0.2398 -0.1912 -0.406 -0.6257 -0.219 #&gt; C172 C173 C174 C175 C176 C177 C178 C179 C180 C181 #&gt; 1 0.168 0.558 -0.117 0.4605 0.275 -0.0576 0.253 -0.64254 -0.34414 0.463 #&gt; 2 0.611 -0.449 0.214 -0.4756 0.395 -0.5746 0.586 -0.00198 -0.00853 0.352 #&gt; 3 0.549 -0.665 0.425 -0.0835 0.272 -0.2208 0.541 0.41546 -0.15029 -0.104 #&gt; C182 C183 C184 C185 C186 C187 C188 C189 C190 C191 #&gt; 1 -0.142 -0.2379 0.578 0.622 -0.33145 0.207 -0.308 0.0573 0.346 0.217 #&gt; 2 0.494 0.0128 -0.577 -0.461 -0.00271 0.120 -0.153 -0.0479 0.463 -0.621 #&gt; 3 0.381 -0.2305 0.585 0.477 0.10786 -0.312 -0.192 0.2083 0.133 0.211 #&gt; C192 C193 C194 C195 C196 C197 C198 C199 C200 #&gt; 1 -0.152 -0.0547 0.1984 -0.2038 -0.122 0.498 0.427 -0.0830 -0.00905 #&gt; 2 -0.606 0.2069 0.0409 -0.5968 -0.478 -0.265 -0.383 -0.4941 0.53689 #&gt; 3 0.293 -0.4322 -0.3370 0.0344 -0.510 0.269 0.163 -0.0946 -0.46863 #&gt; #&gt; [3 rows x 200 columns] h2o.biases(iris_dl, vector_id=1) #&gt; C1 #&gt; 1 0.479 #&gt; 2 0.479 #&gt; 3 0.483 #&gt; 4 0.475 #&gt; 5 0.444 #&gt; 6 0.467 #&gt; #&gt; [200 rows x 1 column] h2o.biases(iris_dl, vector_id=2) #&gt; C1 #&gt; 1 1.002 #&gt; 2 0.995 #&gt; 3 0.994 #&gt; 4 1.001 #&gt; 5 0.998 #&gt; 6 0.997 #&gt; #&gt; [200 rows x 1 column] h2o.biases(iris_dl, vector_id=3) #&gt; C1 #&gt; 1 -0.001143 #&gt; 2 0.001000 #&gt; 3 -0.000161 #&gt; #&gt; [3 rows x 1 column] #plot weights connecting `Sepal.Length` to first hidden neurons plot(as.data.frame(h2o.weights(iris_dl, matrix_id=1))[,1]) 40.8.11 Reproducibility Every run of DeepLearning results in different results since multithreading is done via Hogwild! that benefits from intentional lock-free race conditions between threads. To get reproducible results for small datasets and testing purposes, set reproducible=T and set seed=1337 (pick any integer). This will not work for big data for technical reasons, and is probably also not desired because of the significant slowdown (runs on 1 core only). 40.8.12 Scoring on Training/Validation Sets During Training The training and/or validation set errors can be based on a subset of the training or validation data, depending on the values for score_validation_samples (defaults to 0: all) or score_training_samples (defaults to 10,000 rows, since the training error is only used for early stopping and monitoring). For large datasets, Deep Learning can automatically sample the validation set to avoid spending too much time in scoring during training, especially since scoring results are not currently displayed in the model returned to R. Note that the default value of score_duty_cycle=0.1 limits the amount of time spent in scoring to 10%, so a large number of scoring samples won’t slow down overall training progress too much, but it will always score once after the first MapReduce iteration, and once at the end of training. Stratified sampling of the validation dataset can help with scoring on datasets with class imbalance. Note that this option also requires balance_classes to be enabled (used to over/under-sample the training dataset, based on the max. relative size of the resulting training dataset, max_after_balance_size): More information can be found in the H2O Deep Learning booklet, in our H2O SlideShare Presentations, our H2O YouTube channel, as well as on our H2O Github Repository, especially in our H2O Deep Learning R tests, and H2O Deep Learning Python tests. 40.9 All done, shutdown H2O h2o.shutdown(prompt=FALSE) #&gt; [1] TRUE "],
["regression-with-ann-yacht-hydrodynamics.html", "Chapter 41 Regression with ANN - Yacht Hydrodynamics 41.1 Introduction 41.2 Replication Requirements 41.3 Data Preparation 41.4 1st Regression ANN 41.5 Regression Hyperparameters 41.6 Wrapping Up", " Chapter 41 Regression with ANN - Yacht Hydrodynamics 41.1 Introduction Regression ANNs predict an output variable as a function of the inputs. The input features (independent variables) can be categorical or numeric types, however, for regression ANNs, we require a numeric dependent variable. If the output variable is a categorical variable (or binary) the ANN will function as a classifier (see next tutorial). Source: http://uc-r.github.io/ann_regression In this tutorial we introduce a neural network used for numeric predictions and cover: Replication requirements: What you’ll need to reproduce the analysis in this tutorial. Data Preparation: Preparing our data. 1st Regression ANN: Constructing a 1-hidden layer ANN with 1 neuron. Regression Hyperparameters: Tuning the model. Wrapping Up: Final comments and some exercises to test your skills. 41.2 Replication Requirements We require the following packages for the analysis. library(tidyverse) #&gt; Registered S3 methods overwritten by &#39;ggplot2&#39;: #&gt; method from #&gt; [.quosures rlang #&gt; c.quosures rlang #&gt; print.quosures rlang #&gt; Registered S3 method overwritten by &#39;rvest&#39;: #&gt; method from #&gt; read_xml.response xml2 #&gt; ── Attaching packages ───────────────────────────────── tidyverse 1.2.1 ── #&gt; ✔ ggplot2 3.1.1 ✔ purrr 0.3.2 #&gt; ✔ tibble 2.1.1 ✔ dplyr 0.8.0.1 #&gt; ✔ tidyr 0.8.3 ✔ stringr 1.4.0 #&gt; ✔ readr 1.3.1 ✔ forcats 0.4.0 #&gt; ── Conflicts ──────────────────────────────────── tidyverse_conflicts() ── #&gt; ✖ dplyr::filter() masks stats::filter() #&gt; ✖ dplyr::lag() masks stats::lag() library(neuralnet) #&gt; #&gt; Attaching package: &#39;neuralnet&#39; #&gt; The following object is masked from &#39;package:dplyr&#39;: #&gt; #&gt; compute library(GGally) #&gt; Registered S3 method overwritten by &#39;GGally&#39;: #&gt; method from #&gt; +.gg ggplot2 #&gt; #&gt; Attaching package: &#39;GGally&#39; #&gt; The following object is masked from &#39;package:dplyr&#39;: #&gt; #&gt; nasa 41.3 Data Preparation Our regression ANN will use the Yacht Hydrodynamics data set from UCI’s Machine Learning Repository. The yacht data was provided by Dr. Roberto Lopez email. This data set contains data contains results from 308 full-scale experiments performed at the Delft Ship Hydromechanics Laboratory where they test 22 different hull forms. Their experiment tested the effect of variations in the hull geometry and the ship’s Froude number on the craft’s residuary resistance per unit weight of displacement. To begin we download the data from UCI. url &lt;- &#39;http://archive.ics.uci.edu/ml/machine-learning-databases/00243/yacht_hydrodynamics.data&#39; Yacht_Data &lt;- read_table(file = url, col_names = c(&#39;LongPos_COB&#39;, &#39;Prismatic_Coeff&#39;, &#39;Len_Disp_Ratio&#39;, &#39;Beam_Draut_Ratio&#39;, &#39;Length_Beam_Ratio&#39;,&#39;Froude_Num&#39;, &#39;Residuary_Resist&#39;)) %&gt;% na.omit() #&gt; Parsed with column specification: #&gt; cols( #&gt; LongPos_COB = col_double(), #&gt; Prismatic_Coeff = col_double(), #&gt; Len_Disp_Ratio = col_double(), #&gt; Beam_Draut_Ratio = col_double(), #&gt; Length_Beam_Ratio = col_double(), #&gt; Froude_Num = col_double(), #&gt; Residuary_Resist = col_double() #&gt; ) dplyr::glimpse(Yacht_Data) #&gt; Observations: 308 #&gt; Variables: 7 #&gt; $ LongPos_COB &lt;dbl&gt; -2.3, -2.3, -2.3, -2.3, -2.3, -2.3, -2.3, -2.3… #&gt; $ Prismatic_Coeff &lt;dbl&gt; 0.568, 0.568, 0.568, 0.568, 0.568, 0.568, 0.56… #&gt; $ Len_Disp_Ratio &lt;dbl&gt; 4.78, 4.78, 4.78, 4.78, 4.78, 4.78, 4.78, 4.78… #&gt; $ Beam_Draut_Ratio &lt;dbl&gt; 3.99, 3.99, 3.99, 3.99, 3.99, 3.99, 3.99, 3.99… #&gt; $ Length_Beam_Ratio &lt;dbl&gt; 3.17, 3.17, 3.17, 3.17, 3.17, 3.17, 3.17, 3.17… #&gt; $ Froude_Num &lt;dbl&gt; 0.125, 0.150, 0.175, 0.200, 0.225, 0.250, 0.27… #&gt; $ Residuary_Resist &lt;dbl&gt; 0.11, 0.27, 0.47, 0.78, 1.18, 1.82, 2.61, 3.76… # save the dataset locally write.csv(Yacht_Data, file = file.path(data_raw_dir, &quot;yach_data.csv&quot;)) Prior to any data analysis lets take a look at the data set. ggpairs(Yacht_Data, title = &quot;Scatterplot Matrix of the Features of the Yacht Data Set&quot;) Here we see an excellent summary of the variation of each feature in our data set. Draw your attention to the bottom-most strip of scatter-plots. This shows the residuary resistance as a function of the other data set features (independent experimental values). The greatest variation appears with the Froude Number feature. It will be interesting to see how this pattern appears in the subsequent regression ANNs. Prior to regression ANN construction we first must split the Yacht data set into test and training data sets. Before we split, first scale each feature to fall in the [0,1] interval. # Scale the Data scale01 &lt;- function(x){ (x - min(x)) / (max(x) - min(x)) } Yacht_Data &lt;- Yacht_Data %&gt;% mutate_all(scale01) # Split into test and train sets set.seed(12345) Yacht_Data_Train &lt;- sample_frac(tbl = Yacht_Data, replace = FALSE, size = 0.80) Yacht_Data_Test &lt;- anti_join(Yacht_Data, Yacht_Data_Train) #&gt; Joining, by = c(&quot;LongPos_COB&quot;, &quot;Prismatic_Coeff&quot;, &quot;Len_Disp_Ratio&quot;, &quot;Beam_Draut_Ratio&quot;, &quot;Length_Beam_Ratio&quot;, &quot;Froude_Num&quot;, &quot;Residuary_Resist&quot;) The scale01() function maps each data observation onto the [0,1] interval as called in the dplyr mutate_all() function. We then provided a seed for reproducible results and randomly extracted (without replacement) 80% of the observations to build the Yacht_Data_Train data set. Using dplyr’s anti_join() function we extracted all the observations not within the Yacht_Data_Train data set as our test data set in Yacht_Data_Test. 41.4 1st Regression ANN To begin we construct a 1-hidden layer ANN with 1 neuron, the simplest of all neural networks. set.seed(12321) Yacht_NN1 &lt;- neuralnet(Residuary_Resist ~ LongPos_COB + Prismatic_Coeff + Len_Disp_Ratio + Beam_Draut_Ratio + Length_Beam_Ratio + Froude_Num, data = Yacht_Data_Train) The Yacht_NN1 is a list containing all parameters of the regression ANN as well as the results of the neural network on the test data set. To view a diagram of the Yacht_NN1 use the plot() function. plot(Yacht_NN1, rep = &#39;best&#39;) This plot shows the weights learned by the Yacht_NN1 neural network, and displays the number of iterations before convergence, as well as the SSE of the training data set. To manually compute the SSE you can use the following: NN1_Train_SSE &lt;- sum((Yacht_NN1$net.result - Yacht_Data_Train[, 7])^2)/2 paste(&quot;SSE: &quot;, round(NN1_Train_SSE, 4)) #&gt; [1] &quot;SSE: 0.0365&quot; ## [1] &quot;SSE: 0.0361&quot; This SSE is the error associated with the training data set. A superior metric for estimating the generalization capability of the ANN would be the SSE of the test data set. Recall, the test data set contains observations not used to train the Yacht_NN1 ANN. To calculate the test error, we first must run our test observations through the Yacht_NN1 ANN. This is accomplished with the neuralnet package compute() function, which takes as its first input the desired neural network object created by the neuralnet() function, and the second argument the test data set feature (independent variable(s)) values. Test_NN1_Output &lt;- compute(Yacht_NN1, Yacht_Data_Test[, 1:6])$net.result NN1_Test_SSE &lt;- sum((Test_NN1_Output - Yacht_Data_Test[, 7])^2)/2 NN1_Test_SSE #&gt; [1] 0.0139 ## [1] 0.008417631461 The compute() function outputs the response variable, in our case the Residuary_Resist, as estimated by the neural network. Once we have the ANN estimated response we can compute the test SSE. Comparing the test error of 0.0084 to the training error of 0.0361 we see that in our case our test error is smaller than our training error. 41.5 Regression Hyperparameters We have constructed the most basic of regression ANNs without modifying any of the default hyperparameters associated with the neuralnet() function. We should try and improve the network by modifying its basic structure and hyperparameter modification. To begin we will add depth to the hidden layer of the network, then we will change the activation function from the logistic to the tangent hyperbolicus (tanh) to determine if these modifications can improve the test data set SSE. When using the tanh activation function, we first must rescale the data from \\([0,1]\\) to \\([-1,1]\\) using the rescale package. For the purposes of this exercise we will use the same random seed for reproducible results, generally this is not a best practice. # 2-Hidden Layers, Layer-1 4-neurons, Layer-2, 1-neuron, logistic activation # function set.seed(12321) Yacht_NN2 &lt;- neuralnet(Residuary_Resist ~ LongPos_COB + Prismatic_Coeff + Len_Disp_Ratio + Beam_Draut_Ratio + Length_Beam_Ratio + Froude_Num, data = Yacht_Data_Train, hidden = c(4, 1), act.fct = &quot;logistic&quot;) ## Training Error NN2_Train_SSE &lt;- sum((Yacht_NN2$net.result - Yacht_Data_Train[, 7])^2)/2 ## Test Error Test_NN2_Output &lt;- compute(Yacht_NN2, Yacht_Data_Test[, 1:6])$net.result NN2_Test_SSE &lt;- sum((Test_NN2_Output - Yacht_Data_Test[, 7])^2)/2 # Rescale for tanh activation function scale11 &lt;- function(x) { (2 * ((x - min(x))/(max(x) - min(x)))) - 1 } Yacht_Data_Train &lt;- Yacht_Data_Train %&gt;% mutate_all(scale11) Yacht_Data_Test &lt;- Yacht_Data_Test %&gt;% mutate_all(scale11) # 2-Hidden Layers, Layer-1 4-neurons, Layer-2, 1-neuron, tanh activation # function set.seed(12321) Yacht_NN3 &lt;- neuralnet(Residuary_Resist ~ LongPos_COB + Prismatic_Coeff + Len_Disp_Ratio + Beam_Draut_Ratio + Length_Beam_Ratio + Froude_Num, data = Yacht_Data_Train, hidden = c(4, 1), act.fct = &quot;tanh&quot;) ## Training Error NN3_Train_SSE &lt;- sum((Yacht_NN3$net.result - Yacht_Data_Train[, 7])^2)/2 ## Test Error Test_NN3_Output &lt;- compute(Yacht_NN3, Yacht_Data_Test[, 1:6])$net.result NN3_Test_SSE &lt;- sum((Test_NN3_Output - Yacht_Data_Test[, 7])^2)/2 # 1-Hidden Layer, 1-neuron, tanh activation function set.seed(12321) Yacht_NN4 &lt;- neuralnet(Residuary_Resist ~ LongPos_COB + Prismatic_Coeff + Len_Disp_Ratio + Beam_Draut_Ratio + Length_Beam_Ratio + Froude_Num, data = Yacht_Data_Train, act.fct = &quot;tanh&quot;) ## Training Error NN4_Train_SSE &lt;- sum((Yacht_NN4$net.result - Yacht_Data_Train[, 7])^2)/2 ## Test Error Test_NN4_Output &lt;- compute(Yacht_NN4, Yacht_Data_Test[, 1:6])$net.result NN4_Test_SSE &lt;- sum((Test_NN4_Output - Yacht_Data_Test[, 7])^2)/2 # Bar plot of results Regression_NN_Errors &lt;- tibble(Network = rep(c(&quot;NN1&quot;, &quot;NN2&quot;, &quot;NN3&quot;, &quot;NN4&quot;), each = 2), DataSet = rep(c(&quot;Train&quot;, &quot;Test&quot;), time = 4), SSE = c(NN1_Train_SSE, NN1_Test_SSE, NN2_Train_SSE, NN2_Test_SSE, NN3_Train_SSE, NN3_Test_SSE, NN4_Train_SSE, NN4_Test_SSE)) Regression_NN_Errors %&gt;% ggplot(aes(Network, SSE, fill = DataSet)) + geom_col(position = &quot;dodge&quot;) + ggtitle(&quot;Regression ANN&#39;s SSE&quot;) As evident from the plot, we see that the best regression ANN we found was Yacht_NN2 with a training and test SSE of 0.0188 and 0.0057. We make this determination by the value of the training and test SSEs only. Yacht_NN2’s structure is presented here: plot(Yacht_NN2, rep = &quot;best&quot;) set.seed(12321) Yacht_NN2 &lt;- neuralnet(Residuary_Resist ~ LongPos_COB + Prismatic_Coeff + Len_Disp_Ratio + Beam_Draut_Ratio + Length_Beam_Ratio + Froude_Num, data = Yacht_Data_Train, hidden = c(4, 1), act.fct = &quot;logistic&quot;, rep = 10) plot(Yacht_NN2, rep = &quot;best&quot;) By setting the same seed, prior to running the 10 repetitions of ANNs, we force the software to reproduce the exact same Yacht_NN2 ANN for the first replication. The subsequent 9 generated ANNs, use a different random set of starting weights. Comparing the ‘best’ of the 10 repetitions, to the Yacht_NN2, we observe a decrease in training set error indicating we have a superior set of weights. 41.6 Wrapping Up We have briefly covered regression ANNs in this tutorial. In the next tutorial we will cover classification ANNs. The neuralnet package used in this tutorial is one of many tools available for ANN implementation in R. Others include: nnet autoencoder caret RSNNS h2o Before you move on to the next tutorial, test your new knowledge on the exercises that follow. Why do we split the yacht data into a training and test data sets? Re-load the Yacht Data from the UCI Machine learning repository yacht data without scaling. Run any regression ANN. What happens? Why do you think this happens? After completing exercise question 1, re-scale the yacht data. Perform a simple linear regression fitting Residuary_Resist as a function of all other features. Now run a regression neural network (see 1st Regression ANN section). Plot the regression ANN and compare the weights on the features in the ANN to the p-values for the regressors. Build your own regression ANN using the scaled yacht data modifying one hyperparameter. Use ?neuralnet to see the function options. Plot your ANN. "],
["regression-cereals-dataset.html", "Chapter 42 Regression - cereals dataset 42.1 Introduction 42.2 The Basics of Neural Networks 42.3 Fitting a Neural Network in R 42.4 End Notes", " Chapter 42 Regression - cereals dataset 42.1 Introduction Source: https://www.analyticsvidhya.com/blog/2017/09/creating-visualizing-neural-network-in-r/ Neural network is an information-processing machine and can be viewed as analogous to human nervous system. Just like human nervous system, which is made up of interconnected neurons, a neural network is made up of interconnected information processing units. The information processing units do not work in a linear manner. In fact, neural network draws its strength from parallel processing of information, which allows it to deal with non-linearity. Neural network becomes handy to infer meaning and detect patterns from complex data sets. Neural network is considered as one of the most useful technique in the world of data analytics. However, it is complex and is often regarded as a black box, i.e. users view the input and output of a neural network but remain clueless about the knowledge generating process. We hope that the article will help readers learn about the internal mechanism of a neural network and get hands-on experience to implement it in R. 42.2 The Basics of Neural Networks A neural network is a model characterized by an activation function, which is used by interconnected information processing units to transform input into output. A neural network has always been compared to human nervous system. Information in passed through interconnected units analogous to information passage through neurons in humans. The first layer of the neural network receives the raw input, processes it and passes the processed information to the hidden layers. The hidden layer passes the information to the last layer, which produces the output. The advantage of neural network is that it is adaptive in nature. It learns from the information provided, i.e. trains itself from the data, which has a known outcome and optimizes its weights for a better prediction in situations with unknown outcome. A perceptron, viz. single layer neural network, is the most basic form of a neural network. A perceptron receives multidimensional input and processes it using a weighted summation and an activation function. It is trained using a labeled data and learning algorithm that optimize the weights in the summation processor. A major limitation of perceptron model is its inability to deal with non-linearity. A multilayered neural network overcomes this limitation and helps solve non-linear problems. The input layer connects with hidden layer, which in turn connects to the output layer. The connections are weighted and weights are optimized using a learning rule. There are many learning rules that are used with neural network: least mean square; gradient descent; newton’s rule; conjugate gradient etc. The learning rules can be used in conjunction with backpropgation error method. The learning rule is used to calculate the error at the output unit. This error is backpropagated to all the units such that the error at each unit is proportional to the contribution of that unit towards total error at the output unit. The errors at each unit are then used to optimize the weight at each connection. Figure 1 displays the structure of a simple neural network model for better understanding. 42.3 Fitting a Neural Network in R Now we will fit a neural network model in R. In this article, we use a subset of cereal dataset shared by Carnegie Mellon University (CMU). The details of the dataset are on the following link: http://lib.stat.cmu.edu/DASL/Datafiles/Cereals.html. The objective is to predict rating of the cereals variables such as calories, proteins, fat etc. The R script is provided side by side and is commented for better understanding of the user. . The data is in .csv format and can be downloaded by clicking: cereals. Please set working directory in R using setwd( ) function, and keep cereal.csv in the working directory. We use rating as the dependent variable and calories, proteins, fat, sodium and fiber as the independent variables. We divide the data into training and test set. Training set is used to find the relationship between dependent and independent variables while the test set assesses the performance of the model. We use 60% of the dataset as training set. The assignment of the data to training and test set is done using random sampling. We perform random sampling on R using sample ( ) function. We have used set.seed( ) to generate same random sample everytime and maintain consistency. We will use the index variable while fitting neural network to create training and test data sets. The R script is as follows: ## Creating index variable # Read the Data data = read.csv(file.path(data_raw_dir, &quot;cereals.csv&quot;), header=T) # Random sampling samplesize = 0.60 * nrow(data) set.seed(80) index = sample( seq_len ( nrow ( data ) ), size = samplesize ) # Create training and test set datatrain = data[ index, ] datatest = data[ -index, ] dplyr::glimpse(data) #&gt; Observations: 75 #&gt; Variables: 6 #&gt; $ calories &lt;int&gt; 70, 120, 70, 50, 110, 110, 130, 90, 90, 120, 110, 120, … #&gt; $ protein &lt;int&gt; 4, 3, 4, 4, 2, 2, 3, 2, 3, 1, 6, 1, 3, 1, 2, 2, 1, 1, 3… #&gt; $ fat &lt;int&gt; 1, 5, 1, 0, 2, 0, 2, 1, 0, 2, 2, 3, 2, 1, 0, 0, 0, 1, 3… #&gt; $ sodium &lt;int&gt; 130, 15, 260, 140, 180, 125, 210, 200, 210, 220, 290, 2… #&gt; $ fiber &lt;dbl&gt; 10.0, 2.0, 9.0, 14.0, 1.5, 1.0, 2.0, 4.0, 5.0, 0.0, 2.0… #&gt; $ rating &lt;dbl&gt; 68.4, 34.0, 59.4, 93.7, 29.5, 33.2, 37.0, 49.1, 53.3, 1… Now we fit a neural network on our data. We use neuralnet library for the analysis. The first step is to scale the cereal dataset. The scaling of data is essential because otherwise a variable may have large impact on the prediction variable only because of its scale. Using unscaled may lead to meaningless results. The common techniques to scale data are: min-max normalization, Z-score normalization, median and MAD, and tan-h estimators. The min-max normalization transforms the data into a common range, thus removing the scaling effect from all the variables. Unlike Z-score normalization and median and MAD method, the min-max method retains the original distribution of the variables. We use min-max normalization to scale the data. The R script for scaling the data is as follows. ## Scale data for neural network max = apply(data , 2 , max) min = apply(data, 2 , min) scaled = as.data.frame(scale(data, center = min, scale = max - min)) ## Fit neural network # install library # install.packages(&quot;neuralnet &quot;) # load library library(neuralnet) # creating training and test set trainNN = scaled[index , ] testNN = scaled[-index , ] # fit neural network set.seed(2) NN = neuralnet(rating ~ calories + protein + fat + sodium + fiber, trainNN, hidden = 3 , linear.output = T ) # plot neural network plot(NN) ## Prediction using neural network predict_testNN = compute(NN, testNN[,c(1:5)]) predict_testNN = (predict_testNN$net.result * (max(data$rating) - min(data$rating))) + min(data$rating) plot(datatest$rating, predict_testNN, col=&#39;blue&#39;, pch=16, ylab = &quot;predicted rating NN&quot;, xlab = &quot;real rating&quot;) abline(0,1) # Calculate Root Mean Square Error (RMSE) RMSE.NN = (sum((datatest$rating - predict_testNN)^2) / nrow(datatest)) ^ 0.5 ## Cross validation of neural network model # install relevant libraries # install.packages(&quot;boot&quot;) # install.packages(&quot;plyr&quot;) # Load libraries library(boot) library(plyr) # Initialize variables set.seed(50) k = 100 RMSE.NN = NULL List = list( ) # Fit neural network model within nested for loop for(j in 10:65){ for (i in 1:k) { index = sample(1:nrow(data),j ) trainNN = scaled[index,] testNN = scaled[-index,] datatest = data[-index,] NN = neuralnet(rating ~ calories + protein + fat + sodium + fiber, trainNN, hidden = 3, linear.output= T) predict_testNN = compute(NN,testNN[,c(1:5)]) predict_testNN = (predict_testNN$net.result*(max(data$rating)-min(data$rating)))+min(data$rating) RMSE.NN [i]&lt;- (sum((datatest$rating - predict_testNN)^2)/nrow(datatest))^0.5 } List[[j]] = RMSE.NN } Matrix.RMSE = do.call(cbind, List) ## Prepare boxplot boxplot(Matrix.RMSE[,56], ylab = &quot;RMSE&quot;, main = &quot;RMSE BoxPlot (length of traning set = 65)&quot;) ## Variation of median RMSE # install.packages(&quot;matrixStats&quot;) library(matrixStats) #&gt; #&gt; Attaching package: &#39;matrixStats&#39; #&gt; The following object is masked from &#39;package:plyr&#39;: #&gt; #&gt; count med = colMedians(Matrix.RMSE) X = seq(10,65) plot (med~X, type = &quot;l&quot;, xlab = &quot;length of training set&quot;, ylab = &quot;median RMSE&quot;, main = &quot;Variation of RMSE with length of training set&quot;) Figure 42.1: Variation of RMSE Figure 42.1) shows that the median RMSE of our model decreases as the length of the training the set. This is an important result. The reader must remember that the model accuracy is dependent on the length of training set. The performance of neural network model is sensitive to training-test split. 42.4 End Notes The article discusses the theoretical aspects of a neural network, its implementation in R and post training evaluation. Neural network is inspired from biological nervous system. Similar to nervous system the information is passed through layers of processors. The significance of variables is represented by weights of each connection. The article provides basic understanding of back propagation algorithm, which is used to assign these weights. In this article we also implement neural network on R. We use a publically available dataset shared by CMU. The aim is to predict the rating of cereals using information such as calories, fat, protein etc. After constructing the neural network we evaluate the model for accuracy and robustness. We compute RMSE and perform cross-validation analysis. In cross validation, we check the variation in model accuracy as the length of training set is changed. We consider training sets with length 10 to 65. For each length a 100 samples are random picked and median RMSE is calculated. We show that model accuracy increases when training set is large. Before using the model for prediction, it is important to check the robustness of performance through cross validation. The article provides a quick review neural network and is a useful reference for data enthusiasts. We have provided commented R code throughout the article to help readers with hands on experience of using neural networks. "],
["fitting-a-neural-network.html", "Chapter 43 Fitting a neural network 43.1 Introduction 43.2 The dataset 43.3 Preparing to fit the neural network 43.4 Parameters 43.5 Predicting medv using the neural network 43.6 A (fast) cross validation 43.7 A final note on model interpretability", " Chapter 43 Fitting a neural network 43.1 Introduction https://www.r-bloggers.com/fitting-a-neural-network-in-r-neuralnet-package/ https://datascienceplus.com/fitting-neural-network-in-r/ Neural networks have always been one of the fascinating machine learning models in my opinion, not only because of the fancy backpropagation algorithm but also because of their complexity (think of deep learning with many hidden layers) and structure inspired by the brain. Neural networks have not always been popular, partly because they were, and still are in some cases, computationally expensive and partly because they did not seem to yield better results when compared with simpler methods such as support vector machines (SVMs). Nevertheless, Neural Networks have, once again, raised attention and become popular. Update: We published another post about Network analysis at DataScience+ Network analysis of Game of Thrones In this post, we are going to fit a simple neural network using the neuralnet package and fit a linear model as a comparison. 43.2 The dataset We are going to use the Boston dataset in the MASS package. The Boston dataset is a collection of data about housing values in the suburbs of Boston. Our goal is to predict the median value of owner-occupied homes (medv) using all the other continuous variables available. set.seed(500) library(MASS) data &lt;- Boston dplyr::glimpse(data) #&gt; Observations: 506 #&gt; Variables: 14 #&gt; $ crim &lt;dbl&gt; 0.00632, 0.02731, 0.02729, 0.03237, 0.06905, 0.02985, 0.… #&gt; $ zn &lt;dbl&gt; 18.0, 0.0, 0.0, 0.0, 0.0, 0.0, 12.5, 12.5, 12.5, 12.5, 1… #&gt; $ indus &lt;dbl&gt; 2.31, 7.07, 7.07, 2.18, 2.18, 2.18, 7.87, 7.87, 7.87, 7.… #&gt; $ chas &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… #&gt; $ nox &lt;dbl&gt; 0.538, 0.469, 0.469, 0.458, 0.458, 0.458, 0.524, 0.524, … #&gt; $ rm &lt;dbl&gt; 6.58, 6.42, 7.18, 7.00, 7.15, 6.43, 6.01, 6.17, 5.63, 6.… #&gt; $ age &lt;dbl&gt; 65.2, 78.9, 61.1, 45.8, 54.2, 58.7, 66.6, 96.1, 100.0, 8… #&gt; $ dis &lt;dbl&gt; 4.09, 4.97, 4.97, 6.06, 6.06, 6.06, 5.56, 5.95, 6.08, 6.… #&gt; $ rad &lt;int&gt; 1, 2, 2, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 4, 4, 4, 4, 4, 4,… #&gt; $ tax &lt;dbl&gt; 296, 242, 242, 222, 222, 222, 311, 311, 311, 311, 311, 3… #&gt; $ ptratio &lt;dbl&gt; 15.3, 17.8, 17.8, 18.7, 18.7, 18.7, 15.2, 15.2, 15.2, 15… #&gt; $ black &lt;dbl&gt; 397, 397, 393, 395, 397, 394, 396, 397, 387, 387, 393, 3… #&gt; $ lstat &lt;dbl&gt; 4.98, 9.14, 4.03, 2.94, 5.33, 5.21, 12.43, 19.15, 29.93,… #&gt; $ medv &lt;dbl&gt; 24.0, 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18… First we need to check that no datapoint is missing, otherwise we need to fix the dataset. apply(data,2,function(x) sum(is.na(x))) #&gt; crim zn indus chas nox rm age dis rad #&gt; 0 0 0 0 0 0 0 0 0 #&gt; tax ptratio black lstat medv #&gt; 0 0 0 0 0 There is no missing data, good. We proceed by randomly splitting the data into a train and a test set, then we fit a linear regression model and test it on the test set. Note that I am using the gml() function instead of the lm() this will become useful later when cross validating the linear model. index &lt;- sample(1:nrow(data),round(0.75*nrow(data))) train &lt;- data[index,] test &lt;- data[-index,] lm.fit &lt;- glm(medv~., data=train) summary(lm.fit) #&gt; #&gt; Call: #&gt; glm(formula = medv ~ ., data = train) #&gt; #&gt; Deviance Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -15.211 -2.559 -0.655 1.828 29.711 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 31.11170 5.45981 5.70 2.5e-08 *** #&gt; crim -0.11137 0.03326 -3.35 0.00090 *** #&gt; zn 0.04263 0.01431 2.98 0.00308 ** #&gt; indus 0.00148 0.06745 0.02 0.98247 #&gt; chas 1.75684 0.98109 1.79 0.07417 . #&gt; nox -18.18485 4.47157 -4.07 5.8e-05 *** #&gt; rm 4.76034 0.48047 9.91 &lt; 2e-16 *** #&gt; age -0.01344 0.01410 -0.95 0.34119 #&gt; dis -1.55375 0.21893 -7.10 6.7e-12 *** #&gt; rad 0.28818 0.07202 4.00 7.6e-05 *** #&gt; tax -0.01374 0.00406 -3.38 0.00079 *** #&gt; ptratio -0.94755 0.14012 -6.76 5.4e-11 *** #&gt; black 0.00950 0.00290 3.28 0.00115 ** #&gt; lstat -0.38890 0.05973 -6.51 2.5e-10 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for gaussian family taken to be 20.2) #&gt; #&gt; Null deviance: 32463.5 on 379 degrees of freedom #&gt; Residual deviance: 7407.1 on 366 degrees of freedom #&gt; AIC: 2237 #&gt; #&gt; Number of Fisher Scoring iterations: 2 pr.lm &lt;- predict(lm.fit,test) MSE.lm &lt;- sum((pr.lm - test$medv)^2)/nrow(test) The sample(x,size) function simply outputs a vector of the specified size of randomly selected samples from the vector x. By default the sampling is without replacement: index is essentially a random vector of indeces. Since we are dealing with a regression problem, we are going to use the mean squared error (MSE) as a measure of how much our predictions are far away from the real data. 43.3 Preparing to fit the neural network Before fitting a neural network, some preparation need to be done. Neural networks are not that easy to train and tune. As a first step, we are going to address data preprocessing. It is good practice to normalize your data before training a neural network. I cannot emphasize enough how important this step is: depending on your dataset, avoiding normalization may lead to useless results or to a very difficult training process (most of the times the algorithm will not converge before the number of maximum iterations allowed). You can choose different methods to scale the data (z-normalization, min-max scale, etc…). I chose to use the min-max method and scale the data in the interval [0,1]. Usually scaling in the intervals [0,1] or [-1,1] tends to give better results. We therefore scale and split the data before moving on: maxs &lt;- apply(data, 2, max) mins &lt;- apply(data, 2, min) scaled &lt;- as.data.frame(scale(data, center = mins, scale = maxs - mins)) train_ &lt;- scaled[index,] test_ &lt;- scaled[-index,] Note that scale returns a matrix that needs to be coerced into a data.frame. 43.4 Parameters As far as I know there is no fixed rule as to how many layers and neurons to use although there are several more or less accepted rules of thumb. Usually, if at all necessary, one hidden layer is enough for a vast numbers of applications. As far as the number of neurons is concerned, it should be between the input layer size and the output layer size, usually 2/3 of the input size. At least in my brief experience testing again and again is the best solution since there is no guarantee that any of these rules will fit your model best. Since this is a toy example, we are going to use 2 hidden layers with this configuration: 13:5:3:1. The input layer has 13 inputs, the two hidden layers have 5 and 3 neurons and the output layer has, of course, a single output since we are doing regression. Let’s fit the net: library(neuralnet) n &lt;- names(train_) f &lt;- as.formula(paste(&quot;medv ~&quot;, paste(n[!n %in% &quot;medv&quot;], collapse = &quot; + &quot;))) nn &lt;- neuralnet(f,data=train_,hidden=c(5,3),linear.output=T) A couple of notes: For some reason the formula y~. is not accepted in the neuralnet() function. You need to first write the formula and then pass it as an argument in the fitting function. The hidden argument accepts a vector with the number of neurons for each hidden layer, while the argument linear.output is used to specify whether we want to do regression linear.output=TRUE or classification linear.output=FALSE The neuralnet package provides a nice tool to plot the model: This is the graphical representation of the model with the weights on each connection: plot(nn) The black lines show the connections between each layer and the weights on each connection while the blue lines show the bias term added in each step. The bias can be thought as the intercept of a linear model. The net is essentially a black box so we cannot say that much about the fitting, the weights and the model. Suffice to say that the training algorithm has converged and therefore the model is ready to be used. 43.5 Predicting medv using the neural network Now we can try to predict the values for the test set and calculate the MSE. Remember that the net will output a normalized prediction, so we need to scale it back in order to make a meaningful comparison (or just a simple prediction). pr.nn &lt;- compute(nn,test_[,1:13]) pr.nn_ &lt;- pr.nn$net.result*(max(data$medv)-min(data$medv))+min(data$medv) test.r &lt;- (test_$medv)*(max(data$medv)-min(data$medv))+min(data$medv) MSE.nn &lt;- sum((test.r - pr.nn_)^2)/nrow(test_) we then compare the two MSEs print(paste(MSE.lm,MSE.nn)) #&gt; [1] &quot;31.2630222372615 16.4595537665717&quot; Apparently, the net is doing a better work than the linear model at predicting medv. Once again, be careful because this result depends on the train-test split performed above. Below, after the visual plot, we are going to perform a fast cross validation in order to be more confident about the results. A first visual approach to the performance of the network and the linear model on the test set is plotted below par(mfrow=c(1,2)) plot(test$medv,pr.nn_,col=&#39;red&#39;,main=&#39;Real vs predicted NN&#39;,pch=18,cex=0.7) abline(0,1,lwd=2) legend(&#39;bottomright&#39;,legend=&#39;NN&#39;,pch=18,col=&#39;red&#39;, bty=&#39;n&#39;) plot(test$medv,pr.lm,col=&#39;blue&#39;,main=&#39;Real vs predicted lm&#39;,pch=18, cex=0.7) abline(0,1,lwd=2) legend(&#39;bottomright&#39;,legend=&#39;LM&#39;,pch=18,col=&#39;blue&#39;, bty=&#39;n&#39;, cex=.95) By visually inspecting the plot we can see that the predictions made by the neural network are (in general) more concetrated around the line (a perfect alignment with the line would indicate a MSE of 0 and thus an ideal perfect prediction) than those made by the linear model. plot(test$medv,pr.nn_,col=&#39;red&#39;,main=&#39;Real vs predicted NN&#39;,pch=18,cex=0.7) points(test$medv,pr.lm,col=&#39;blue&#39;,pch=18,cex=0.7) abline(0,1,lwd=2) legend(&#39;bottomright&#39;,legend=c(&#39;NN&#39;,&#39;LM&#39;),pch=18,col=c(&#39;red&#39;,&#39;blue&#39;)) 43.6 A (fast) cross validation Cross validation is another very important step of building predictive models. While there are different kind of cross validation methods, the basic idea is repeating the following process a number of time: train-test split Do the train-test split Fit the model to the train set Test the model on the test set Calculate the prediction error Repeat the process K times Then by calculating the average error we can get a grasp of how the model is doing. We are going to implement a fast cross validation using a for loop for the neural network and the cv.glm() function in the boot package for the linear model. As far as I know, there is no built-in function in R to perform cross-validation on this kind of neural network, if you do know such a function, please let me know in the comments. Here is the 10 fold cross-validated MSE for the linear model: library(boot) set.seed(200) lm.fit &lt;- glm(medv~.,data=data) cv.glm(data,lm.fit,K=10)$delta[1] #&gt; [1] 23.2 Now the net. Note that I am splitting the data in this way: 90% train set and 10% test set in a random way for 10 times. I am also initializing a progress bar using the plyr library because I want to keep an eye on the status of the process since the fitting of the neural network may take a while. set.seed(450) cv.error &lt;- NULL k &lt;- 10 library(plyr) pbar &lt;- create_progress_bar(&#39;text&#39;) pbar$init(k) #&gt; | | | 0% for(i in 1:k){ index &lt;- sample(1:nrow(data),round(0.9*nrow(data))) train.cv &lt;- scaled[index,] test.cv &lt;- scaled[-index,] nn &lt;- neuralnet(f,data=train.cv,hidden=c(5,2),linear.output=T) pr.nn &lt;- compute(nn,test.cv[,1:13]) pr.nn &lt;- pr.nn$net.result*(max(data$medv)-min(data$medv))+min(data$medv) test.cv.r &lt;- (test.cv$medv)*(max(data$medv)-min(data$medv))+min(data$medv) cv.error[i] &lt;- sum((test.cv.r - pr.nn)^2)/nrow(test.cv) pbar$step() } #&gt; | |====== | 10% | |============= | 20% | |==================== | 30% | |========================== | 40% | |================================ | 50% | |======================================= | 60% | |============================================== | 70% | |==================================================== | 80% | |========================================================== | 90% | |=================================================================| 100% After a while, the process is done, we calculate the average MSE and plot the results as a boxplot mean(cv.error) #&gt; [1] 7.64 cv.error #&gt; [1] 13.33 7.10 6.58 5.70 6.84 5.77 10.75 5.38 9.45 5.50 The code for the box plot: The code above outputs the following boxplot: boxplot(cv.error,xlab=&#39;MSE CV&#39;,col=&#39;cyan&#39;, border=&#39;blue&#39;,names=&#39;CV error (MSE)&#39;, main=&#39;CV error (MSE) for NN&#39;,horizontal=TRUE) As you can see, the average MSE for the neural network (10.33) is lower than the one of the linear model although there seems to be a certain degree of variation in the MSEs of the cross validation. This may depend on the splitting of the data or the random initialization of the weights in the net. By running the simulation different times with different seeds you can get a more precise point estimate for the average MSE. 43.7 A final note on model interpretability Neural networks resemble black boxes a lot: explaining their outcome is much more difficult than explaining the outcome of simpler model such as a linear model. Therefore, depending on the kind of application you need, you might want to take into account this factor too. Furthermore, as you have seen above, extra care is needed to fit a neural network and small changes can lead to different results. A gist with the full code for this post can be found here. Thank you for reading this post, leave a comment below if you have any question. "],
["visualization-of-neural-networks.html", "Chapter 44 Visualization of neural networks 44.1 caret and plot NN 44.2 Multiple hidden layers 44.3 Binary predictors 44.4 color coding the input layer", " Chapter 44 Visualization of neural networks https://beckmw.wordpress.com/tag/neuralnet/ In my last post I said I wasn’t going to write anymore about neural networks (i.e., multilayer feedforward perceptron, supervised ANN, etc.). That was a lie. I’ve received several requests to update the neural network plotting function described in the original post. As previously explained, R does not provide a lot of options for visualizing neural networks. The only option I know of is a plotting method for objects from the neuralnet package. This may be my opinion, but I think this plot leaves much to be desired (see below). Also, no plotting methods exist for neural networks created in other packages, i.e., nnet and RSNNS. These packages are the only ones listed on the CRAN task view, so I’ve updated my original plotting function to work with all three. Additionally, I’ve added a new option for plotting a raw weight vector to allow use with neural networks created elsewhere. This blog describes these changes, as well as some new arguments added to the original function. As usual, I’ll simulate some data to use for creating the neural networks. The dataset contains eight input variables and two output variables. The final dataset is a data frame with all variables, as well as separate data frames for the input and output variables. I’ve retained separate datasets based on the syntax for each package. library(clusterGeneration) #&gt; Loading required package: MASS library(tictoc) seed.val&lt;- 12345 set.seed(seed.val) num.vars&lt;-8 num.obs&lt;-1000 # input variables cov.mat &lt;-genPositiveDefMat(num.vars,covMethod=c(&quot;unifcorrmat&quot;))$Sigma rand.vars &lt;-mvrnorm(num.obs,rep(0,num.vars),Sigma=cov.mat) # output variables parms &lt;-runif(num.vars,-10,10) y1 &lt;- rand.vars %*% matrix(parms) + rnorm(num.obs,sd=20) parms2 &lt;- runif(num.vars,-10,10) y2 &lt;- rand.vars %*% matrix(parms2) + rnorm(num.obs,sd=20) # final datasets rand.vars &lt;- data.frame(rand.vars) resp &lt;- data.frame(y1,y2) names(resp) &lt;- c(&#39;Y1&#39;,&#39;Y2&#39;) dat.in &lt;- data.frame(resp, rand.vars) dplyr::glimpse(dat.in) #&gt; Observations: 1,000 #&gt; Variables: 10 #&gt; $ Y1 &lt;dbl&gt; 25.442, -14.578, -36.214, 15.216, -6.393, -20.849, -28.665, -… #&gt; $ Y2 &lt;dbl&gt; 16.9, 38.8, 31.2, -31.2, 93.3, 11.7, 59.7, -103.5, -49.8, 50.… #&gt; $ X1 &lt;dbl&gt; 3.138, -0.705, -4.373, 0.837, 0.787, 1.923, -1.419, 1.121, -0… #&gt; $ X2 &lt;dbl&gt; 0.195, -0.302, 0.773, 1.311, 3.506, 1.245, 3.800, -0.165, 0.3… #&gt; $ X3 &lt;dbl&gt; -1.795, -2.596, 2.308, 4.081, -3.921, 1.473, -0.926, 7.101, 2… #&gt; $ X4 &lt;dbl&gt; -2.7216, 3.0589, 1.2455, 3.4607, 2.3775, -2.9833, 2.6669, -0.… #&gt; $ X5 &lt;dbl&gt; 0.0407, 0.7602, -3.0217, -4.2799, 2.0859, 1.4765, 0.0561, 2.8… #&gt; $ X6 &lt;dbl&gt; -1.4820, -0.5014, 0.0603, -1.8551, 2.2817, 1.7386, 1.7450, -2… #&gt; $ X7 &lt;dbl&gt; -0.7169, -0.3618, -1.5283, 4.2026, -6.1548, -0.3545, -6.0284,… #&gt; $ X8 &lt;dbl&gt; 1.152, 1.810, -1.357, 0.598, -1.425, -1.210, -1.004, 2.494, -… The various neural network packages are used to create separate models for plotting. # first model with nnet #nnet function from nnet package library(nnet) set.seed(seed.val) tic() mod1 &lt;- nnet(rand.vars, resp, data = dat.in, size = 10, linout = T) #&gt; # weights: 112 #&gt; initial value 4784162.893260 #&gt; iter 10 value 1794537.980652 #&gt; iter 20 value 1577753.498759 #&gt; iter 30 value 1485254.945755 #&gt; iter 40 value 1449238.248788 #&gt; iter 50 value 1427720.291804 #&gt; iter 60 value 1416977.236373 #&gt; iter 70 value 1405167.753521 #&gt; iter 80 value 1395046.792257 #&gt; iter 90 value 1370522.267277 #&gt; iter 100 value 1363709.540981 #&gt; final value 1363709.540981 #&gt; stopped after 100 iterations toc() #&gt; 0.194 sec elapsed # nn &lt;- neuralnet(form.in, # data = dat.sc, # # hidden = c(13, 10, 3), # hidden = c(5), # act.fct = &quot;tanh&quot;, # linear.output = FALSE, # lifesign = &quot;minimal&quot;) # 2nd model with neuralnet # neuralnet function from neuralnet package, notice use of only one response library(neuralnet) softplus &lt;- function(x) log(1 + exp(x)) sigmoid &lt;- function(x) log(1 + exp(-x)) dat.sc &lt;- scale(dat.in) form.in &lt;- as.formula(&#39;Y1 ~ X1+X2+X3+X4+X5+X6+X7+X8&#39;) set.seed(seed.val) tic() mod2 &lt;- neuralnet(form.in, data = dat.sc, hidden = 10, lifesign = &quot;minimal&quot;, linear.output = FALSE, act.fct = &quot;tanh&quot;) #&gt; hidden: 10 thresh: 0.01 rep: 1/1 steps: 26361 error: 160.06372 time: 42.63 secs toc() #&gt; 42.636 sec elapsed # third model with RSNNS # mlp function from RSNNS package library(RSNNS) #&gt; Loading required package: Rcpp set.seed(seed.val) tic() mod3 &lt;- mlp(rand.vars, resp, size = 10, linOut = T) toc() #&gt; 0.355 sec elapsed I’ve noticed some differences between the functions that could lead to some confusion. For simplicity, the above code represents my interpretation of the most direct way to create a neural network in each package. Be very aware that direct comparison of results is not advised given that the default arguments differ between the packages. A few key differences are as follows, although many others should be noted. First, the functions differ in the methods for passing the primary input variables. The nnet function can take separate (or combined) x and y inputs as data frames or as a formula, the neuralnet function can only use a formula as input, and the mlp function can only take a data frame as combined or separate variables as input. As far as I know, the neuralnet function is not capable of modelling multiple response variables, unless the response is a categorical variable that uses one node for each outcome. Additionally, the default output for the neuralnet function is linear, whereas the opposite is true for the other two functions. Specifics aside, here’s how to use the updated plot function. Note that the same syntax is used to plot each model # import the function from Github library(devtools) source_url(&#39;https://gist.githubusercontent.com/fawda123/7471137/raw/466c1474d0a505ff044412703516c34f1a4684a5/nnet_plot_update.r&#39;) #&gt; SHA-1 hash of file is 74c80bd5ddbc17ab3ae5ece9c0ed9beb612e87ef # plot each model plot.nnet(mod1) #&gt; Loading required package: scales #&gt; Loading required package: reshape plot.nnet(mod2) plot.nnet(mod3) #&gt; Warning in plot.nnet(mod3): Bias layer not applicable for rsnns object The plotting function can also now be used with an arbitrary weight vector, rather than a specific model object. The struct argument must also be included if this option is used. I thought the easiest way to use the plotting function with your own weights was to have the input weights as a numeric vector, including bias layers. I’ve shown how this can be done using the weights directly from mod1 for simplicity. wts.in &lt;- mod1$wts struct &lt;- mod1$n plot.nnet(wts.in,struct=struct) Note that wts.in is a numeric vector with length equal to the expected given the architecture (i.e., for 8 10 2 network, 100 connection weights plus 12 bias weights). The plot should look the same as the plot for the neural network from nnet. The weights in the input vector need to be in a specific order for correct plotting. I realize this is not clear by looking directly at wt.in but this was the simplest approach I could think of. The weight vector shows the weights for each hidden node in sequence, starting with the bias input for each node, then the weights for each output node in sequence, starting with the bias input for each output node. Note that the bias layer has to be included even if the network was not created with biases. If this is the case, simply input a random number where the bias values should go and use the argument bias=F. I’ll show the correct order of the weights using an example with plot.nn from the neuralnet package since the weights are included directly on the plot. If we pretend that the above figure wasn’t created in R, we would input the mod.in argument for the updated plotting function as follows. Also note that struct must be included if using this approach. mod.in&lt;-c(13.12,1.49,0.16,-0.11,-0.19,-0.16,0.56,-0.52,0.81) struct&lt;-c(2,2,1) #two inputs, two hidden, one output plot.nnet(mod.in, struct=struct) Note the comparability with the figure created using the neuralnet package. That is, larger weights have thicker lines and color indicates sign (+ black, – grey). One of these days I’ll actually put these functions in a package. In the meantime, please let me know if any bugs are encountered. 44.1 caret and plot NN I’ve changed the function to work with neural networks created using the train function from the caret package. The link above is updated but you can also grab it here. library(caret) #&gt; Loading required package: lattice #&gt; Loading required package: ggplot2 #&gt; Registered S3 methods overwritten by &#39;ggplot2&#39;: #&gt; method from #&gt; [.quosures rlang #&gt; c.quosures rlang #&gt; print.quosures rlang #&gt; #&gt; Attaching package: &#39;caret&#39; #&gt; The following objects are masked from &#39;package:RSNNS&#39;: #&gt; #&gt; confusionMatrix, train mod4 &lt;- train(Y1 ~., method=&#39;nnet&#39;, data=dat.in, linout=T) plot.nnet(mod4,nid=T) #&gt; Warning in plot.nnet(mod4, nid = T): Using best nnet model from train #&gt; output fact&lt;-factor(sample(c(&#39;a&#39;,&#39;b&#39;,&#39;c&#39;),size=num.obs,replace=T)) form.in&lt;-formula(&#39;cbind(Y2,Y1)~X1+X2+X3+fact&#39;) mod5&lt;-nnet(form.in,data=cbind(dat.in,fact),size=10,linout=T) #&gt; # weights: 82 #&gt; initial value 4799569.423556 #&gt; iter 10 value 2864553.218126 #&gt; iter 20 value 2595828.194160 #&gt; iter 30 value 2517965.483941 #&gt; iter 40 value 2464882.178217 #&gt; iter 50 value 2444238.700834 #&gt; iter 60 value 2424302.290643 #&gt; iter 70 value 2395226.949866 #&gt; iter 80 value 2375558.751266 #&gt; iter 90 value 2343011.050867 #&gt; iter 100 value 2298860.593948 #&gt; final value 2298860.593948 #&gt; stopped after 100 iterations plot.nnet(mod5,nid=T) 44.2 Multiple hidden layers More updates… I’ve now modified the function to plot multiple hidden layers for networks created using the mlp function in the RSNNS package and neuralnet in the neuralnet package. As far as I know, these are the only neural network functions in R that can create multiple hidden layers. All others use a single hidden layer. I have not tested the plotting function using manual input for the weight vectors with multiple hidden layers. My guess is it won’t work but I can’t be bothered to change the function unless it’s specifically requested. The updated function can be grabbed here (all above links to the function have also been changed). library(RSNNS) # neural net with three hidden layers, 9, 11, and 8 nodes in each tic() mod &lt;-mlp(rand.vars, resp, size = c(9,11,8), linOut = T) toc() #&gt; 0.428 sec elapsed par(mar=numeric(4),family=&#39;serif&#39;) plot.nnet(mod) #&gt; Warning in plot.nnet(mod): Bias layer not applicable for rsnns object 44.3 Binary predictors Here’s an example using the neuralnet function with binary predictors and categorical outputs (credit to Tao Ma for the model code). library(neuralnet) #response AND&lt;-c(rep(0,7),1) OR&lt;-c(0,rep(1,7)) # response with predictors binary.data &lt;- data.frame(expand.grid(c(0,1), c(0,1), c(0,1)), AND, OR) #model tic() net &lt;- neuralnet(AND+OR ~ Var1+Var2+Var3, binary.data, hidden =c(6,12,8), rep = 10, err.fct=&quot;ce&quot;, linear.output=FALSE) toc() #&gt; 0.13 sec elapsed #plot ouput par(mar=numeric(4),family=&#39;serif&#39;) plot.nnet(net) 44.4 color coding the input layer The color vector argument (circle.col) for the nodes was changed to allow a separate color vector for the input layer. The following example shows how this can be done using relative importance of the input variables to color-code the first layer. # example showing use of separate colors for input layer # color based on relative importance using &#39;gar.fun&#39; ## #create input data seed.val&lt;-3 set.seed(seed.val) num.vars&lt;-8 num.obs&lt;-1000 #input variables library(clusterGeneration) cov.mat&lt;-genPositiveDefMat(num.vars,covMethod=c(&quot;unifcorrmat&quot;))$Sigma rand.vars&lt;-mvrnorm(num.obs,rep(0,num.vars),Sigma=cov.mat) # output variables parms&lt;-runif(num.vars,-10,10) y1&lt;-rand.vars %*% matrix(parms) + rnorm(num.obs,sd=20) # final datasets rand.vars&lt;-data.frame(rand.vars) resp&lt;-data.frame(y1) names(resp)&lt;-&#39;Y1&#39; dat.in &lt;- data.frame(resp,rand.vars) ## # create model library(nnet) mod1 &lt;- nnet(rand.vars,resp,data=dat.in,size=10,linout=T) #&gt; # weights: 101 #&gt; initial value 844959.580478 #&gt; iter 10 value 543616.101824 #&gt; iter 20 value 479986.887846 #&gt; iter 30 value 465607.784054 #&gt; iter 40 value 454237.073298 #&gt; iter 50 value 445032.412421 #&gt; iter 60 value 433191.158624 #&gt; iter 70 value 426321.161292 #&gt; iter 80 value 424900.966883 #&gt; iter 90 value 423816.437605 #&gt; iter 100 value 422064.114812 #&gt; final value 422064.114812 #&gt; stopped after 100 iterations ## # relative importance function library(devtools) source_url(&#39;https://gist.github.com/fawda123/6206737/raw/2e1bc9cbc48d1a56d2a79dd1d33f414213f5f1b1/gar_fun.r&#39;) #&gt; SHA-1 hash of file is 9faa58824c46956c3ff78081696290d9b32d845f # relative importance of input variables for Y1 rel.imp &lt;- gar.fun(&#39;Y1&#39;,mod1,bar.plot=F)$rel.imp #color vector based on relative importance of input values cols&lt;-colorRampPalette(c(&#39;green&#39;,&#39;red&#39;))(num.vars)[rank(rel.imp)] ## #plotting function source_url(&#39;https://gist.githubusercontent.com/fawda123/7471137/raw/466c1474d0a505ff044412703516c34f1a4684a5/nnet_plot_update.r&#39;) #&gt; SHA-1 hash of file is 74c80bd5ddbc17ab3ae5ece9c0ed9beb612e87ef #plot model with new color vector #separate colors for input vectors using a list for &#39;circle.col&#39; plot(mod1,circle.col=list(cols,&#39;lightblue&#39;)) "],
["temperature-modeling-using-nested-dataframes.html", "Chapter 45 Temperature modeling using nested dataframes 45.1 Prepare the data 45.2 Define the models 45.3 Test modeling on one dataset 45.4 Making a nested dataframe 45.5 Apply multiple models on a nested structure 45.6 Using broom package to look at model-statistics", " Chapter 45 Temperature modeling using nested dataframes 45.1 Prepare the data http://ijlyttle.github.io/isugg_purrr/presentation.html#(1) 45.1.1 Packages to run this presentation library(&quot;readr&quot;) library(&quot;tibble&quot;) library(&quot;dplyr&quot;) library(&quot;tidyr&quot;) library(&quot;stringr&quot;) library(&quot;ggplot2&quot;) library(&quot;purrr&quot;) library(&quot;broom&quot;) 45.1.2 Motivation As you know, purrr is a recent package from Hadley Wickham, focused on lists and functional programming, like dplyr is focused on data-frames. I figure a good way to learn a new package is to try to solve a problem, so we have a dataset: you can view or download you can download the source of this presentation these are three temperatures recorded simultaneously in a piece of electronics it will be very valuable to be able to characterize the transient temperature for each sensor we want to apply the same set of models across all three sensors it will be easier to show using pictures 45.1.3 Let’s get the data into shape Using the readr package temperature_wide &lt;- read_csv(file.path(data_raw_dir, &quot;temperature.csv&quot;)) %&gt;% print() #&gt; Parsed with column specification: #&gt; cols( #&gt; instant = col_datetime(format = &quot;&quot;), #&gt; temperature_a = col_double(), #&gt; temperature_b = col_double(), #&gt; temperature_c = col_double() #&gt; ) #&gt; # A tibble: 327 x 4 #&gt; instant temperature_a temperature_b temperature_c #&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 2015-11-13 06:10:19 116. 91.7 84.2 #&gt; 2 2015-11-13 06:10:23 116. 91.7 84.2 #&gt; 3 2015-11-13 06:10:27 116. 91.6 84.2 #&gt; 4 2015-11-13 06:10:31 116. 91.7 84.2 #&gt; 5 2015-11-13 06:10:36 116. 91.7 84.2 #&gt; 6 2015-11-13 06:10:41 116. 91.6 84.2 #&gt; # … with 321 more rows 45.1.4 Is temperature_wide “tidy”? #&gt; # A tibble: 327 x 4 #&gt; instant temperature_a temperature_b temperature_c #&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 2015-11-13 06:10:19 116. 91.7 84.2 #&gt; 2 2015-11-13 06:10:23 116. 91.7 84.2 #&gt; 3 2015-11-13 06:10:27 116. 91.6 84.2 #&gt; 4 2015-11-13 06:10:31 116. 91.7 84.2 #&gt; 5 2015-11-13 06:10:36 116. 91.7 84.2 #&gt; 6 2015-11-13 06:10:41 116. 91.6 84.2 #&gt; # … with 321 more rows Why or why not? 45.1.5 Tidy data Each column is a variable Each row is an observation Each cell is a value (http://www.jstatsoft.org/v59/i10/paper) My personal observation is that “tidy” can depend on the context, on what you want to do with the data. 45.1.6 Let’s get this into a tidy form temperature_tall &lt;- temperature_wide %&gt;% gather(key = &quot;id_sensor&quot;, value = &quot;temperature&quot;, starts_with(&quot;temp&quot;)) %&gt;% mutate(id_sensor = str_replace(id_sensor, &quot;temperature_&quot;, &quot;&quot;)) %&gt;% print() #&gt; # A tibble: 981 x 3 #&gt; instant id_sensor temperature #&gt; &lt;dttm&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 2015-11-13 06:10:19 a 116. #&gt; 2 2015-11-13 06:10:23 a 116. #&gt; 3 2015-11-13 06:10:27 a 116. #&gt; 4 2015-11-13 06:10:31 a 116. #&gt; 5 2015-11-13 06:10:36 a 116. #&gt; 6 2015-11-13 06:10:41 a 116. #&gt; # … with 975 more rows 45.1.7 Now, it’s easier to visualize temperature_tall %&gt;% ggplot(aes(x = instant, y = temperature, color = id_sensor)) + geom_line() 45.1.8 Calculate delta time (\\(\\Delta t\\)) and delta temperature (\\(\\Delta T\\)) delta_time \\(\\Delta t\\) change in time since event started, s delta_temperature: \\(\\Delta T\\) change in temperature since event started, °C delta &lt;- temperature_tall %&gt;% arrange(id_sensor, instant) %&gt;% group_by(id_sensor) %&gt;% mutate( delta_time = as.numeric(instant) - as.numeric(instant[[1]]), delta_temperature = temperature - temperature[[1]] ) %&gt;% select(id_sensor, delta_time, delta_temperature) 45.1.9 Let’s have a look # plot delta time vs delta temperature, by sensor delta %&gt;% ggplot(aes(x = delta_time, y = delta_temperature, color = id_sensor)) + geom_line() 45.2 Define the models We want to see how three different curve-fits might perform on these three data-sets: 45.2.0.1 Newtonian cooling \\[\\Delta T = \\Delta {T_0} * (1 - e^{-\\frac{\\delta t}{\\tau_0}})\\] 45.2.1 Semi-infinite solid \\[\\Delta T = \\Delta T_0 * erfc(\\sqrt{\\frac{\\tau_0}{\\delta t}}))\\] 45.2.2 Semi-infinite solid with convection \\[\\Delta T = \\Delta T_0 * \\big [ \\operatorname erfc(\\sqrt{\\frac{\\tau_0}{\\delta t}}) - e^ {Bi_0 + (\\frac {Bi_0}{2})^2 \\frac {\\delta t}{\\tau_0}} * \\operatorname erfc (\\sqrt \\frac{\\tau_0}{\\delta t} + \\frac {Bi_0}{2} * \\sqrt \\frac{\\delta t }{\\tau_0} \\big]\\] 45.2.3 erf and erfc functions # reference: http://stackoverflow.com/questions/29067916/r-error-function-erfz # (see Abramowitz and Stegun 29.2.29) erf &lt;- function(x) 2 * pnorm(x * sqrt(2)) - 1 erfc &lt;- function(x) 2 * pnorm(x * sqrt(2), lower = FALSE) 45.2.4 Newton cooling equation newton_cooling &lt;- function(x) { nls( delta_temperature ~ delta_temperature_0 * (1 - exp(-delta_time/tau_0)), start = list(delta_temperature_0 = -10, tau_0 = 50), data = x ) } 45.2.5 Temperature models: simple and convection semi_infinite_simple &lt;- function(x) { nls( delta_temperature ~ delta_temperature_0 * erfc(sqrt(tau_0 / delta_time)), start = list(delta_temperature_0 = -10, tau_0 = 50), data = x ) } semi_infinite_convection &lt;- function(x){ nls( delta_temperature ~ delta_temperature_0 * ( erfc(sqrt(tau_0 / delta_time)) - exp(Bi_0 + (Bi_0/2)^2 * delta_time / tau_0) * erfc(sqrt(tau_0 / delta_time) + (Bi_0/2) * sqrt(delta_time / tau_0)) ), start = list(delta_temperature_0 = -5, tau_0 = 50, Bi_0 = 1.e6), data = x ) } 45.3 Test modeling on one dataset 45.3.1 Before going into purrr Before doing anything, we want to show that we can do something with one dataset and one model-function: # only one sensor; it is a test tmp_data &lt;- delta %&gt;% filter(id_sensor == &quot;a&quot;) tmp_model &lt;- newton_cooling(tmp_data) summary(tmp_model) #&gt; #&gt; Formula: delta_temperature ~ delta_temperature_0 * (1 - exp(-delta_time/tau_0)) #&gt; #&gt; Parameters: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; delta_temperature_0 -15.0608 0.0526 -286 &lt;2e-16 *** #&gt; tau_0 500.0138 4.8367 103 &lt;2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 0.327 on 325 degrees of freedom #&gt; #&gt; Number of iterations to convergence: 7 #&gt; Achieved convergence tolerance: 4.14e-06 45.3.2 Look at predictions # apply prediction and make it tidy tmp_pred &lt;- tmp_data %&gt;% mutate(modeled = predict(tmp_model, data = .)) %&gt;% select(id_sensor, delta_time, measured = delta_temperature, modeled) %&gt;% gather(&quot;type&quot;, &quot;delta_temperature&quot;, measured:modeled) %&gt;% print() #&gt; # A tibble: 654 x 4 #&gt; # Groups: id_sensor [1] #&gt; id_sensor delta_time type delta_temperature #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 a 0 measured 0 #&gt; 2 a 4 measured 0 #&gt; 3 a 8 measured -0.06 #&gt; 4 a 12 measured -0.06 #&gt; 5 a 17 measured -0.211 #&gt; 6 a 22 measured -0.423 #&gt; # … with 648 more rows 45.3.3 Plot Newton model tmp_pred %&gt;% ggplot(aes(x = delta_time, y = delta_temperature, linetype = type)) + geom_line() + labs(title = &quot;Newton temperature model&quot;, subtitle = &quot;One sensor: a&quot;) 45.3.4 “Regular” data-frame (deltas) print(delta) #&gt; # A tibble: 981 x 3 #&gt; # Groups: id_sensor [3] #&gt; id_sensor delta_time delta_temperature #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 a 0 0 #&gt; 2 a 4 0 #&gt; 3 a 8 -0.06 #&gt; 4 a 12 -0.06 #&gt; 5 a 17 -0.211 #&gt; 6 a 22 -0.423 #&gt; # … with 975 more rows Each column of the dataframe is a vector - in this case, a character vector and two doubles 45.4 Making a nested dataframe 45.4.1 How to make a weird data-frame Here’s where the fun starts - a column of a data-frame can be a list. use tidyr::nest() to makes a column data, which is a list of data-frames this seems like a stronger expression of the dplyr::group_by() idea # nest delta_time and delta_temperature variables delta_nested &lt;- delta %&gt;% nest(-id_sensor) %&gt;% print() #&gt; # A tibble: 3 x 2 #&gt; id_sensor data #&gt; &lt;chr&gt; &lt;list&gt; #&gt; 1 a &lt;tibble [327 × 2]&gt; #&gt; 2 b &lt;tibble [327 × 2]&gt; #&gt; 3 c &lt;tibble [327 × 2]&gt; 45.4.2 Map dataframes to a modeling function (Newton) map() is like lapply() map() returns a list-column (it keeps the weirdness) model_nested &lt;- delta_nested %&gt;% mutate(model = map(data, newton_cooling)) %&gt;% print() #&gt; # A tibble: 3 x 3 #&gt; id_sensor data model #&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; #&gt; 1 a &lt;tibble [327 × 2]&gt; &lt;nls&gt; #&gt; 2 b &lt;tibble [327 × 2]&gt; &lt;nls&gt; #&gt; 3 c &lt;tibble [327 × 2]&gt; &lt;nls&gt; We get an additional list-column model. 45.4.3 We can use map2() to make the predictions map2() is like mapply() designed to map two colunms (model, data) to a function predict() predict_nested &lt;- model_nested %&gt;% mutate(pred = map2(model, data, predict)) %&gt;% print() #&gt; # A tibble: 3 x 4 #&gt; id_sensor data model pred #&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; #&gt; 1 a &lt;tibble [327 × 2]&gt; &lt;nls&gt; &lt;dbl [327]&gt; #&gt; 2 b &lt;tibble [327 × 2]&gt; &lt;nls&gt; &lt;dbl [327]&gt; #&gt; 3 c &lt;tibble [327 × 2]&gt; &lt;nls&gt; &lt;dbl [327]&gt; Another list-column pred for the prediction results. 45.4.4 We need to get out of the weirdness use unnest() to get back to a regular data-frame predict_unnested &lt;- predict_nested %&gt;% unnest(data, pred) %&gt;% print() #&gt; # A tibble: 981 x 4 #&gt; id_sensor pred delta_time delta_temperature #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 a 0 0 0 #&gt; 2 a -0.120 4 0 #&gt; 3 a -0.239 8 -0.06 #&gt; 4 a -0.357 12 -0.06 #&gt; 5 a -0.503 17 -0.211 #&gt; 6 a -0.648 22 -0.423 #&gt; # … with 975 more rows 45.4.5 We can wrangle the predictions get into a form that makes it easier to plot predict_tall &lt;- predict_unnested %&gt;% rename(modeled = pred, measured = delta_temperature) %&gt;% gather(&quot;type&quot;, &quot;delta_temperature&quot;, modeled, measured) %&gt;% print() #&gt; # A tibble: 1,962 x 4 #&gt; id_sensor delta_time type delta_temperature #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 a 0 modeled 0 #&gt; 2 a 4 modeled -0.120 #&gt; 3 a 8 modeled -0.239 #&gt; 4 a 12 modeled -0.357 #&gt; 5 a 17 modeled -0.503 #&gt; 6 a 22 modeled -0.648 #&gt; # … with 1,956 more rows 45.4.6 We can visualize the predictions predict_tall %&gt;% ggplot(aes(x = delta_time, y = delta_temperature)) + geom_line(aes(color = id_sensor, linetype = type)) + labs(title = &quot;Newton temperature modeling&quot;, subtitle = &quot;Three sensors: a, b, c&quot;) 45.5 Apply multiple models on a nested structure 45.5.1 Step 1: Selection of models Make a list of functions to model: list_model &lt;- list( newton_cooling = newton_cooling, semi_infinite_simple = semi_infinite_simple, semi_infinite_convection = semi_infinite_convection ) 45.5.2 Step 2: write a function to define the “inner” loop # add additional variable with the model name fn_model &lt;- function(.model, df) { # one parameter for the model in the list, the second for the data # safer to avoid non-standard evaluation # df %&gt;% mutate(model = map(data, .model)) df$model &lt;- map(df$data, possibly(.model, NULL)) df } for a given model-function and a given (weird) data-frame, return a modified version of that data-frame with a column model, which is the model-function applied to each element of the data-frame’s data column (which is itself a list of data-frames) the purrr functions safely() and possibly() are very interesting. I think they could be useful outside of purrr as a friendlier way to do error-handling. 45.5.3 Step 3: Use map_df() to define the “outer” loop # this dataframe will be the second input of fn_model delta_nested %&gt;% print() #&gt; # A tibble: 3 x 2 #&gt; id_sensor data #&gt; &lt;chr&gt; &lt;list&gt; #&gt; 1 a &lt;tibble [327 × 2]&gt; #&gt; 2 b &lt;tibble [327 × 2]&gt; #&gt; 3 c &lt;tibble [327 × 2]&gt; # fn_model is receiving two inputs: one from list_model and from delta_nested model_nested_new &lt;- list_model %&gt;% map_df(fn_model, delta_nested, .id = &quot;id_model&quot;) %&gt;% print() #&gt; # A tibble: 9 x 4 #&gt; id_model id_sensor data model #&gt; &lt;chr&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; #&gt; 1 newton_cooling a &lt;tibble [327 × 2]&gt; &lt;nls&gt; #&gt; 2 newton_cooling b &lt;tibble [327 × 2]&gt; &lt;nls&gt; #&gt; 3 newton_cooling c &lt;tibble [327 × 2]&gt; &lt;nls&gt; #&gt; 4 semi_infinite_simple a &lt;tibble [327 × 2]&gt; &lt;nls&gt; #&gt; 5 semi_infinite_simple b &lt;tibble [327 × 2]&gt; &lt;nls&gt; #&gt; 6 semi_infinite_simple c &lt;tibble [327 × 2]&gt; &lt;nls&gt; #&gt; # … with 3 more rows for each element of a list of model-functions, run the inner-loop function, and row-bind the results into a data-frame we want to discard the rows where the model failed we also want to investigate why they failed, but that’s a different talk 45.5.4 Step 4: Use map() to identify the null models model_nested_new &lt;- list_model %&gt;% map_df(fn_model, delta_nested, .id = &quot;id_model&quot;) %&gt;% mutate(is_null = map(model, is.null)) %&gt;% print() #&gt; # A tibble: 9 x 5 #&gt; id_model id_sensor data model is_null #&gt; &lt;chr&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; #&gt; 1 newton_cooling a &lt;tibble [327 × 2]&gt; &lt;nls&gt; &lt;lgl [1]&gt; #&gt; 2 newton_cooling b &lt;tibble [327 × 2]&gt; &lt;nls&gt; &lt;lgl [1]&gt; #&gt; 3 newton_cooling c &lt;tibble [327 × 2]&gt; &lt;nls&gt; &lt;lgl [1]&gt; #&gt; 4 semi_infinite_simple a &lt;tibble [327 × 2]&gt; &lt;nls&gt; &lt;lgl [1]&gt; #&gt; 5 semi_infinite_simple b &lt;tibble [327 × 2]&gt; &lt;nls&gt; &lt;lgl [1]&gt; #&gt; 6 semi_infinite_simple c &lt;tibble [327 × 2]&gt; &lt;nls&gt; &lt;lgl [1]&gt; #&gt; # … with 3 more rows using map(model, is.null) returns a list column to use filter(), we have to escape the weirdness 45.5.5 Step 5: map_lgl() to identify nulls and get out of the weirdness model_nested_new &lt;- list_model %&gt;% map_df(fn_model, delta_nested, .id = &quot;id_model&quot;) %&gt;% mutate(is_null = map_lgl(model, is.null)) %&gt;% print() #&gt; # A tibble: 9 x 5 #&gt; id_model id_sensor data model is_null #&gt; &lt;chr&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; &lt;lgl&gt; #&gt; 1 newton_cooling a &lt;tibble [327 × 2]&gt; &lt;nls&gt; FALSE #&gt; 2 newton_cooling b &lt;tibble [327 × 2]&gt; &lt;nls&gt; FALSE #&gt; 3 newton_cooling c &lt;tibble [327 × 2]&gt; &lt;nls&gt; FALSE #&gt; 4 semi_infinite_simple a &lt;tibble [327 × 2]&gt; &lt;nls&gt; FALSE #&gt; 5 semi_infinite_simple b &lt;tibble [327 × 2]&gt; &lt;nls&gt; FALSE #&gt; 6 semi_infinite_simple c &lt;tibble [327 × 2]&gt; &lt;nls&gt; FALSE #&gt; # … with 3 more rows using map_lgl(model, is.null) returns a vector column 45.5.6 Step 6: filter() nulls and select() variables to clean up model_nested_new &lt;- list_model %&gt;% map_df(fn_model, delta_nested, .id = &quot;id_model&quot;) %&gt;% mutate(is_null = map_lgl(model, is.null)) %&gt;% filter(!is_null) %&gt;% select(-is_null) %&gt;% print() #&gt; # A tibble: 6 x 4 #&gt; id_model id_sensor data model #&gt; &lt;chr&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; #&gt; 1 newton_cooling a &lt;tibble [327 × 2]&gt; &lt;nls&gt; #&gt; 2 newton_cooling b &lt;tibble [327 × 2]&gt; &lt;nls&gt; #&gt; 3 newton_cooling c &lt;tibble [327 × 2]&gt; &lt;nls&gt; #&gt; 4 semi_infinite_simple a &lt;tibble [327 × 2]&gt; &lt;nls&gt; #&gt; 5 semi_infinite_simple b &lt;tibble [327 × 2]&gt; &lt;nls&gt; #&gt; 6 semi_infinite_simple c &lt;tibble [327 × 2]&gt; &lt;nls&gt; 45.5.7 Step 7: Calculate predictions on nested dataframe predict_nested &lt;- model_nested_new %&gt;% mutate(pred = map2(model, data, predict)) %&gt;% print() #&gt; # A tibble: 6 x 5 #&gt; id_model id_sensor data model pred #&gt; &lt;chr&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; #&gt; 1 newton_cooling a &lt;tibble [327 × 2]&gt; &lt;nls&gt; &lt;dbl [327]&gt; #&gt; 2 newton_cooling b &lt;tibble [327 × 2]&gt; &lt;nls&gt; &lt;dbl [327]&gt; #&gt; 3 newton_cooling c &lt;tibble [327 × 2]&gt; &lt;nls&gt; &lt;dbl [327]&gt; #&gt; 4 semi_infinite_simple a &lt;tibble [327 × 2]&gt; &lt;nls&gt; &lt;dbl [327]&gt; #&gt; 5 semi_infinite_simple b &lt;tibble [327 × 2]&gt; &lt;nls&gt; &lt;dbl [327]&gt; #&gt; 6 semi_infinite_simple c &lt;tibble [327 × 2]&gt; &lt;nls&gt; &lt;dbl [327]&gt; 45.5.8 unnest(), make it tall and tidy predict_tall &lt;- predict_nested %&gt;% unnest(data, pred) %&gt;% rename(modeled = pred, measured = delta_temperature) %&gt;% gather(&quot;type&quot;, &quot;delta_temperature&quot;, modeled, measured) %&gt;% print() #&gt; # A tibble: 3,924 x 5 #&gt; id_model id_sensor delta_time type delta_temperature #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 newton_cooling a 0 modeled 0 #&gt; 2 newton_cooling a 4 modeled -0.120 #&gt; 3 newton_cooling a 8 modeled -0.239 #&gt; 4 newton_cooling a 12 modeled -0.357 #&gt; 5 newton_cooling a 17 modeled -0.503 #&gt; 6 newton_cooling a 22 modeled -0.648 #&gt; # … with 3,918 more rows 45.5.9 Visualize the predictions predict_tall %&gt;% ggplot(aes(x = delta_time, y = delta_temperature)) + geom_line(aes(color = id_sensor, linetype = type)) + facet_grid(id_model ~ .) + labs(title = &quot;Newton and Semi-infinite temperature modeling&quot;, subtitle = &quot;Three sensors: a, b, c&quot;) 45.5.10 Let’s get the residuals resid &lt;- model_nested_new %&gt;% mutate(resid = map(model, resid)) %&gt;% unnest(data, resid) %&gt;% print() #&gt; # A tibble: 1,962 x 5 #&gt; id_model id_sensor resid delta_time delta_temperature #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 newton_cooling a 0 0 0 #&gt; 2 newton_cooling a 0.120 4 0 #&gt; 3 newton_cooling a 0.179 8 -0.06 #&gt; 4 newton_cooling a 0.297 12 -0.06 #&gt; 5 newton_cooling a 0.292 17 -0.211 #&gt; 6 newton_cooling a 0.225 22 -0.423 #&gt; # … with 1,956 more rows 45.5.11 And visualize them resid %&gt;% ggplot(aes(x = delta_time, y = resid)) + geom_line(aes(color = id_sensor)) + facet_grid(id_model ~ .) + labs(title = &quot;Residuals for Newton and Semi-infinite models&quot;) 45.6 Using broom package to look at model-statistics We will use a previous defined dataframe with the model and data: model_nested_new %&gt;% print() #&gt; # A tibble: 6 x 4 #&gt; id_model id_sensor data model #&gt; &lt;chr&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; #&gt; 1 newton_cooling a &lt;tibble [327 × 2]&gt; &lt;nls&gt; #&gt; 2 newton_cooling b &lt;tibble [327 × 2]&gt; &lt;nls&gt; #&gt; 3 newton_cooling c &lt;tibble [327 × 2]&gt; &lt;nls&gt; #&gt; 4 semi_infinite_simple a &lt;tibble [327 × 2]&gt; &lt;nls&gt; #&gt; 5 semi_infinite_simple b &lt;tibble [327 × 2]&gt; &lt;nls&gt; #&gt; 6 semi_infinite_simple c &lt;tibble [327 × 2]&gt; &lt;nls&gt; The tidy() function extracts statistics from a model. # apply over model_nested_new but only three variables model_parameters &lt;- model_nested_new %&gt;% select(id_model, id_sensor, model) %&gt;% mutate(tidy = map(model, tidy)) %&gt;% select(-model) %&gt;% unnest() %&gt;% print() #&gt; # A tibble: 12 x 7 #&gt; id_model id_sensor term estimate std.error statistic p.value #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 newton_coo… a delta_tempe… -15.1 0.0526 -286. 0. #&gt; 2 newton_coo… a tau_0 500. 4.84 103. 1.07e-250 #&gt; 3 newton_coo… b delta_tempe… -7.59 0.0676 -112. 6.38e-262 #&gt; 4 newton_coo… b tau_0 1041. 16.2 64.2 9.05e-187 #&gt; 5 newton_coo… c delta_tempe… -9.87 0.704 -14.0 3.16e- 35 #&gt; 6 newton_coo… c tau_0 3525. 299. 11.8 5.61e- 27 #&gt; # … with 6 more rows 45.6.1 Get a sense of the coefficients model_summary &lt;- model_parameters %&gt;% select(id_model, id_sensor, term, estimate) %&gt;% spread(key = &quot;term&quot;, value = &quot;estimate&quot;) %&gt;% print() #&gt; # A tibble: 6 x 4 #&gt; id_model id_sensor delta_temperature_0 tau_0 #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 newton_cooling a -15.1 500. #&gt; 2 newton_cooling b -7.59 1041. #&gt; 3 newton_cooling c -9.87 3525. #&gt; 4 semi_infinite_simple a -21.5 139. #&gt; 5 semi_infinite_simple b -10.6 287. #&gt; 6 semi_infinite_simple c -8.04 500. 45.6.2 Summary this is just a smalll part of purrr there seem to be parallels between tidyr::nest()/purrr::map() and dplyr::group_by()/dplyr::do() to my mind, the purrr framework is more understandable update tweet from Hadley References from Hadley: purrr 0.1.0 announcement purrr 0.2.0 announcement chapter from Garrett Grolemund and Hadley’s forthcoming book "],
["linear-regression-world-happiness.html", "Chapter 46 Linear Regression. World Happiness 46.1 Introduction 46.2 A quick exploration of the data 46.3 Linear regression with R 46.4 Regression summary 46.5 Regression analysis 46.6 Analysis of colinearity 46.7 What drives happiness", " Chapter 46 Linear Regression. World Happiness 46.1 Introduction Source: http://enhancedatascience.com/2017/04/25/r-basics-linear-regression-with-r/ Data: https://www.kaggle.com/unsdsn/world-happiness Linear regression is one of the basics of statistics and machine learning. Hence, it is a must-have to know how to perform a linear regression with R and how to interpret the results. Linear regression algorithm will fit the best straight line that fits the data? To do so, it will minimise the squared distance between the points of the dataset and the fitted line. For this tutorial, we will use the World Happiness report dataset from Kaggle. This report analyses the Happiness of each country according to several factors such as wealth, health, family life, … Our goal will be to find the most important factors of happiness. What a noble goal! 46.2 A quick exploration of the data Before fitting any model, we need to know our data better. First, let’s import the data into R. Please download the dataset from Kaggle and put it in your working directory. The code below imports the data as data.table and clean the column names (a lot of . were appearing in the original ones) require(data.table) #&gt; Loading required package: data.table data_happiness_dir &lt;- file.path(data_raw_dir, &quot;happiness&quot;) Happiness_Data = data.table(read.csv(file.path(data_happiness_dir, &#39;2016.csv&#39;))) colnames(Happiness_Data) &lt;- gsub(&#39;.&#39;,&#39;&#39;,colnames(Happiness_Data), fixed=T) Now, let’s plot a Scatter Plot Matrix to get a grasp of how our variables are related one to another. To do so, the GGally package is great. require(ggplot2) #&gt; Loading required package: ggplot2 #&gt; Registered S3 methods overwritten by &#39;ggplot2&#39;: #&gt; method from #&gt; [.quosures rlang #&gt; c.quosures rlang #&gt; print.quosures rlang require(GGally) #&gt; Loading required package: GGally #&gt; Registered S3 method overwritten by &#39;GGally&#39;: #&gt; method from #&gt; +.gg ggplot2 ggpairs(Happiness_Data[,c(4,7:13), with=F], lower = list( continuous = &quot;smooth&quot;)) All the variables are positively correlated with the Happiness score. We can expect that most of the coefficients in the linear regression will be positive. However, the correlation between the variable is often more than 0.5, so we can expect that multicollinearity will appear in the regression. In the data, we also have access to the Country where the score was computed. Even if it’s not useful for the regression, let’s plot the data on a map! require(&#39;rworldmap&#39;) #&gt; Loading required package: rworldmap #&gt; Loading required package: sp #&gt; ### Welcome to rworldmap ### #&gt; For a short introduction type : vignette(&#39;rworldmap&#39;) library(reshape2) #&gt; #&gt; Attaching package: &#39;reshape2&#39; #&gt; The following objects are masked from &#39;package:data.table&#39;: #&gt; #&gt; dcast, melt map.world &lt;- map_data(map=&quot;world&quot;) dataPlot&lt;- melt(Happiness_Data, id.vars =&#39;Country&#39;, measure.vars = colnames(Happiness_Data)[c(4,7:13)]) #Correcting names that are different dataPlot[Country == &#39;United States&#39;, Country:=&#39;USA&#39;] dataPlot[Country == &#39;United Kingdoms&#39;, Country:=&#39;UK&#39;] ##Rescaling each variable to have nice gradient dataPlot[,value:=value/max(value), by=variable] dataMap = data.table(merge(map.world, dataPlot, by.x=&#39;region&#39;, by.y=&#39;Country&#39;, all.x=T)) dataMap = dataMap[order(order)] dataMap = dataMap[order(order)][!is.na(variable)] gg &lt;- ggplot() gg &lt;- gg + geom_map(data=dataMap, map=dataMap, aes(map_id = region, x=long, y=lat, fill=value)) + # facet_wrap(~variable, scale=&#39;free&#39;) facet_wrap(~variable) #&gt; Warning: Ignoring unknown aesthetics: x, y gg &lt;- gg + scale_fill_gradient(low = &quot;navy&quot;, high = &quot;lightblue&quot;) gg &lt;- gg + coord_equal() The code above is a classic code for a map. A few important points: We reordered the point before plotting to avoid some artefacts. The merge is a right outer join, all the points of the map need to be kept. Otherwise, points will be missing which will mess up the map. Each variable is rescaled so that a facet_wrap can be used. Here, the absolute level of a variable is not of primary interest. This is the relative level of a variable between countries that we want to visualise. gg The distinction between North and South is quite visible. In addition to this, countries that have suffered from the crisis are also really visible. 46.3 Linear regression with R Now that we have taken a look at our data, a first model can be fitted. The explanatory variables are the DGP per capita, the life expectancy, the level of freedom and the trust in the government. ##First model model1 &lt;- lm(HappinessScore ~ EconomyGDPperCapita + Family + HealthLifeExpectancy + Freedom + TrustGovernmentCorruption, data=Happiness_Data) 46.4 Regression summary The summary function provides a very easy way to assess a linear regression in R. require(stargazer) #&gt; Loading required package: stargazer #&gt; #&gt; Please cite as: #&gt; Hlavac, Marek (2018). stargazer: Well-Formatted Regression and Summary Statistics Tables. #&gt; R package version 5.2.2. https://CRAN.R-project.org/package=stargazer ##Quick summary sum1=summary(model1) sum1 #&gt; #&gt; Call: #&gt; lm(formula = HappinessScore ~ EconomyGDPperCapita + Family + #&gt; HealthLifeExpectancy + Freedom + TrustGovernmentCorruption, #&gt; data = Happiness_Data) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -1.4833 -0.2817 -0.0277 0.3280 1.4615 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 2.212 0.150 14.73 &lt; 2e-16 *** #&gt; EconomyGDPperCapita 0.697 0.209 3.33 0.0011 ** #&gt; Family 1.234 0.229 5.39 2.6e-07 *** #&gt; HealthLifeExpectancy 1.462 0.343 4.26 3.5e-05 *** #&gt; Freedom 1.559 0.373 4.18 5.0e-05 *** #&gt; TrustGovernmentCorruption 0.959 0.455 2.11 0.0365 * #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 0.535 on 151 degrees of freedom #&gt; Multiple R-squared: 0.787, Adjusted R-squared: 0.78 #&gt; F-statistic: 112 on 5 and 151 DF, p-value: &lt;2e-16 stargazer(model1,type=&#39;text&#39;) #&gt; #&gt; ===================================================== #&gt; Dependent variable: #&gt; --------------------------- #&gt; HappinessScore #&gt; ----------------------------------------------------- #&gt; EconomyGDPperCapita 0.697*** #&gt; (0.209) #&gt; #&gt; Family 1.230*** #&gt; (0.229) #&gt; #&gt; HealthLifeExpectancy 1.460*** #&gt; (0.343) #&gt; #&gt; Freedom 1.560*** #&gt; (0.373) #&gt; #&gt; TrustGovernmentCorruption 0.959** #&gt; (0.455) #&gt; #&gt; Constant 2.210*** #&gt; (0.150) #&gt; #&gt; ----------------------------------------------------- #&gt; Observations 157 #&gt; R2 0.787 #&gt; Adjusted R2 0.780 #&gt; Residual Std. Error 0.535 (df = 151) #&gt; F Statistic 112.000*** (df = 5; 151) #&gt; ===================================================== #&gt; Note: *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01 A quick interpretation: All the coefficient are significative at a .05 threshold The overall model is also significative It explains 78.7% of Happiness in the dataset As expected all the relationship between the explanatory variables and the output variable are positives. The model is doing well! You can also easily get a given indicator of the model performance, such as R², the different coefficients or the p-value of the overall model. ##R² sum1$r.squared*100 #&gt; [1] 78.7 ##Coefficients sum1$coefficients #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 2.212 0.150 14.73 5.20e-31 #&gt; EconomyGDPperCapita 0.697 0.209 3.33 1.10e-03 #&gt; Family 1.234 0.229 5.39 2.62e-07 #&gt; HealthLifeExpectancy 1.462 0.343 4.26 3.53e-05 #&gt; Freedom 1.559 0.373 4.18 5.01e-05 #&gt; TrustGovernmentCorruption 0.959 0.455 2.11 3.65e-02 ##p-value df(sum1$fstatistic[1],sum1$fstatistic[2],sum1$fstatistic[3]) #&gt; value #&gt; 3.39e-49 ##Confidence interval of the coefficient confint(model1,level = 0.95) #&gt; 2.5 % 97.5 % #&gt; (Intercept) 1.9152 2.51 #&gt; EconomyGDPperCapita 0.2833 1.11 #&gt; Family 0.7821 1.69 #&gt; HealthLifeExpectancy 0.7846 2.14 #&gt; Freedom 0.8212 2.30 #&gt; TrustGovernmentCorruption 0.0609 1.86 confint(model1,level = 0.99) #&gt; 0.5 % 99.5 % #&gt; (Intercept) 1.820 2.60 #&gt; EconomyGDPperCapita 0.151 1.24 #&gt; Family 0.637 1.83 #&gt; HealthLifeExpectancy 0.568 2.36 #&gt; Freedom 0.585 2.53 #&gt; TrustGovernmentCorruption -0.227 2.14 confint(model1,level = 0.90) #&gt; 5 % 95 % #&gt; (Intercept) 1.963 2.46 #&gt; EconomyGDPperCapita 0.350 1.04 #&gt; Family 0.856 1.61 #&gt; HealthLifeExpectancy 0.895 2.03 #&gt; Freedom 0.941 2.18 #&gt; TrustGovernmentCorruption 0.207 1.71 46.5 Regression analysis 46.5.1 Residual analysis Now that the regression has been done, the analysis and validity of the result can be analysed. Let’s begin with residuals and the assumption of normality and homoscedasticity. # Visualisation of residuals ggplot(model1, aes(model1$residuals)) + geom_histogram(bins=20, aes(y = ..density..)) + geom_density(color=&#39;blue&#39;, fill = &#39;blue&#39;, alpha = 0.2) + geom_vline(xintercept = mean(model1$residuals), color=&#39;red&#39;) + stat_function(fun=dnorm, color=&quot;red&quot;, size=1, args = list(mean = mean(model1$residuals), sd = sd(model1$residuals))) + xlab(&#39;residuals values&#39;) The residual versus fitted plot is used to see if the residuals behave the same for the different value of the output (i.e, they have the same variance and mean). The plot shows no strong evidence of heteroscedasticity. ggplot(model1, aes(model1$fitted.values, model1$residuals)) + geom_point() + geom_hline(yintercept = c(1.96 * sd(model1$residuals), - 1.96 * sd(model1$residuals)), color=&#39;red&#39;) + xlab(&#39;fitted value&#39;) + ylab(&#39;residuals values&#39;) 46.6 Analysis of colinearity The colinearity can be assessed using VIF, the car package provides a function to compute it directly. require(&#39;car&#39;) #&gt; Loading required package: car #&gt; Loading required package: carData vif(model1) #&gt; EconomyGDPperCapita Family #&gt; 4.07 2.03 #&gt; HealthLifeExpectancy Freedom #&gt; 3.37 1.61 #&gt; TrustGovernmentCorruption #&gt; 1.39 All the VIF are less than 5, and hence there is no sign of colinearity. 46.7 What drives happiness Now let’s compute standardised betas to see what really drives happiness. ##Standardized betas std_betas = sum1$coefficients[-1,1] * data.table(model1$model)[, lapply(.SD, sd), .SDcols=2:6] / sd(model1$model$HappinessScore) std_betas #&gt; EconomyGDPperCapita Family HealthLifeExpectancy Freedom #&gt; 1: 0.252 0.288 0.294 0.199 #&gt; TrustGovernmentCorruption #&gt; 1: 0.0933 Though the code above may seem complicated, it is just computing the standardised betas for all variables std_beta=beta*sd(x)/sd(y). The top three coefficients are Health and Life expectancy, Family and GDP per Capita. Though money does not make happiness it is among the top three factors of Happiness! Now you know how to perform a linear regression with R! "],
["linear-regression-on-advertising.html", "Chapter 47 Linear Regression on Advertising", " Chapter 47 Linear Regression on Advertising Videos, slides: https://www.r-bloggers.com/in-depth-introduction-to-machine-learning-in-15-hours-of-expert-videos/ Data: http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv code: http://subasish.github.io/pages/ISLwithR/ http://math480-s15-zarringhalam.wikispaces.umb.edu/R+Code https://github.com/yahwes/ISLR https://www.tau.ac.il/~saharon/IntroStatLearn.html https://www.waxworksmath.com/Authors/G_M/James/WWW/chapter_3.html https://github.com/asadoughi/stat-learning plots: https://onlinecourses.science.psu.edu/stat857/node/28/ library(readr) advertising &lt;- read_csv(file.path(data_raw_dir, &quot;Advertising.csv&quot;)) #&gt; Warning: Missing column names filled in: &#39;X1&#39; [1] #&gt; Parsed with column specification: #&gt; cols( #&gt; X1 = col_double(), #&gt; TV = col_double(), #&gt; radio = col_double(), #&gt; newspaper = col_double(), #&gt; sales = col_double() #&gt; ) advertising #&gt; # A tibble: 200 x 5 #&gt; X1 TV radio newspaper sales #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 230. 37.8 69.2 22.1 #&gt; 2 2 44.5 39.3 45.1 10.4 #&gt; 3 3 17.2 45.9 69.3 9.3 #&gt; 4 4 152. 41.3 58.5 18.5 #&gt; 5 5 181. 10.8 58.4 12.9 #&gt; 6 6 8.7 48.9 75 7.2 #&gt; # … with 194 more rows The Advertising data set. The plot displays sales, in thousands of units, as a function of TV, radio, and newspaper budgets, in thousands of dollars, for 200 diﬀerent markets. par(mfrow=c(1,3)) plot(advertising$TV, advertising$sales, xlab = &quot;TV&quot;, ylab = &quot;Sales&quot;, col = &quot;red&quot;) plot(advertising$radio, advertising$sales, xlab=&quot;Radio&quot;, ylab=&quot;Sales&quot;, col=&quot;red&quot;) plot(advertising$radio, advertising$newspaper, xlab=&quot;Newspaper&quot;, ylab=&quot;Sales&quot;, col=&quot;red&quot;) In each plot we show the simple least squares ﬁt of sales to that variable, as described in Chapter 3. In other words, each blue line represents a simple model that can be used to predict sales using TV, radio, and newspaper, respectively. par(mfrow=c(1,3)) tv_model &lt;- lm(sales ~ TV, data = advertising) radio_model &lt;- lm(sales ~ radio, data = advertising) newspaper_model &lt;- lm(sales ~ newspaper, data = advertising) plot(advertising$TV, advertising$sales, xlab = &quot;TV&quot;, ylab = &quot;Sales&quot;, col = &quot;red&quot;) abline(tv_model, col = &quot;blue&quot;) plot(advertising$radio, advertising$sales, xlab=&quot;Radio&quot;, ylab=&quot;Sales&quot;, col=&quot;red&quot;) abline(radio_model) plot(advertising$newspaper, advertising$sales, xlab=&quot;Newspaper&quot;, ylab=&quot;Sales&quot;, col=&quot;red&quot;) abline(newspaper_model) Recall the Advertising data from Chapter 2. Figure 2.1 displays sales (in thousands of units) for a particular product as a function of advertis- ing budgets (in thousands of dollars) for TV, radio, and newspaper media. Suppose that in our role as statistical consultants we are asked to suggest, on the basis of this data, a marketing plan for next year that will result in high product sales. What information would be useful in order to provide such a recommendation? Here are a few important questions that we might seek to address: Is there a relationship between advertising budget and sales? How strong is the relationship between advertising budget and sales? Which media contribute to sales? How accurately can we estimate the eﬀect of each medium on sales? For the Advertising data, the least squares fit for the regression of sales onto TV is shown. The fit is found by minimizing the sum of squared errors. Each grey line segment represents an error, and the fit makes a compro- mise by averaging their squares. In this case a linear fit captures the essence of the relationship, although it is somewhat deficient in the left of the plot. tv_model &lt;- lm(sales ~ TV, data = advertising) plot(advertising$TV, advertising$sales, xlab = &quot;TV&quot;, ylab = &quot;Sales&quot;, col = &quot;red&quot;, pch=16) abline(tv_model, col = &quot;blue&quot;, lwd=2) segments(advertising$TV, advertising$sales, advertising$TV, predict(tv_model), col = &quot;gray&quot;) smry &lt;- summary(tv_model) smry #&gt; #&gt; Call: #&gt; lm(formula = sales ~ TV, data = advertising) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -8.386 -1.955 -0.191 2.067 7.212 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 7.03259 0.45784 15.4 &lt;2e-16 *** #&gt; TV 0.04754 0.00269 17.7 &lt;2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 3.26 on 198 degrees of freedom #&gt; Multiple R-squared: 0.612, Adjusted R-squared: 0.61 #&gt; F-statistic: 312 on 1 and 198 DF, p-value: &lt;2e-16 library(lattice) minRss &lt;- sqrt(abs(min(smry$residuals))) * sign(min(smry$residuals)) maxRss &lt;- sqrt(max(smry$residuals)) twovar &lt;- function(x, y) { x^2 + y^2 } mat &lt;- outer( seq(minRss, maxRss, length = 100), seq(minRss, maxRss, length = 100), Vectorize( function(x,y) twovar(x, y) ) ) contourplot(mat, at = c(1,2,3)) tv_model #&gt; #&gt; Call: #&gt; lm(formula = sales ~ TV, data = advertising) #&gt; #&gt; Coefficients: #&gt; (Intercept) TV #&gt; 7.0326 0.0475 tv.lm &lt;- lm(sales ~ poly(sales, TV, degree=2), data = advertising) # contour(tv.lm, sales ~ TV) library(rsm) mpg.lm &lt;- lm(mpg ~ poly(hp, disp, degree = 3), data = mtcars) contour(mpg.lm, hp ~ disp) x &lt;- -6:16 op &lt;- par(mfrow = c(2, 2)) contour(outer(x, x), method = &quot;flattest&quot;, vfont = c(&quot;sans serif&quot;, &quot;plain&quot;)) "],
["lab-3a-regression-iris-dataset.html", "Chapter 48 Lab 3A: Regression. iris dataset 48.1 Introduction 48.2 Explore the Data 48.3 Create Training and Test Sets 48.4 Predict with Simple Linear Regression 48.5 Predict with Multiple Regression 48.6 5. Predict with Neural Network Regression 48.7 6. Evaluate all the regression Models", " Chapter 48 Lab 3A: Regression. iris dataset 48.1 Introduction https://www.matthewrenze.com/workshops/practical-machine-learning-with-r/lab-3a-regression.html 48.2 Explore the Data Load Iris data Plot scatterplot Plot correlogram data(iris) write.csv(iris, file.path(data_raw_dir, &quot;iris.csv&quot;)) Create scatterplot matrix plot(iris[1:4]) library(corrgram) #&gt; Registered S3 methods overwritten by &#39;ggplot2&#39;: #&gt; method from #&gt; [.quosures rlang #&gt; c.quosures rlang #&gt; print.quosures rlang #&gt; Registered S3 method overwritten by &#39;seriation&#39;: #&gt; method from #&gt; reorder.hclust gclus corrgram(iris[1:4]) cor(iris[1:4]) #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width #&gt; Sepal.Length 1.000 -0.118 0.872 0.818 #&gt; Sepal.Width -0.118 1.000 -0.428 -0.366 #&gt; Petal.Length 0.872 -0.428 1.000 0.963 #&gt; Petal.Width 0.818 -0.366 0.963 1.000 cor( x = iris$Petal.Length, y = iris$Petal.Width) #&gt; [1] 0.963 plot( x = iris$Petal.Length, y = iris$Petal.Width, xlim = c(0.25, 7), ylim = c(0.25, 2.5)) 48.3 Create Training and Test Sets set.seed(42) indexes &lt;- sample( x = 1:150, size = 100) train &lt;- iris[indexes, ] test &lt;- iris[-indexes, ] 48.4 Predict with Simple Linear Regression simpleModel &lt;- lm( formula = Petal.Width ~ Petal.Length, data = train) plot( x = iris$Petal.Length, y = iris$Petal.Width, xlim = c(0.25, 7), ylim = c(0.25, 2.5)) lines( x = train$Petal.Length, y = simpleModel$fitted, col = &quot;red&quot;, lwd = 3) summary(simpleModel) #&gt; #&gt; Call: #&gt; lm(formula = Petal.Width ~ Petal.Length, data = train) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -0.5684 -0.1279 -0.0307 0.1280 0.6385 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) -0.3486 0.0476 -7.33 6.7e-11 *** #&gt; Petal.Length 0.4137 0.0119 34.80 &lt; 2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 0.209 on 98 degrees of freedom #&gt; Multiple R-squared: 0.925, Adjusted R-squared: 0.924 #&gt; F-statistic: 1.21e+03 on 1 and 98 DF, p-value: &lt;2e-16 simplePredictions &lt;- predict( object = simpleModel, newdata = test) plot( x = iris$Petal.Length, y = iris$Petal.Width, xlim = c(0.25, 7), ylim = c(0.25, 2.5)) points( x = test$Petal.Length, y = simplePredictions, col = &quot;blue&quot;, pch = 4, lwd = 2) points( x = test$Petal.Length, y = test$Petal.Width, col = &quot;red&quot;, pch = 16) simpleRMSE &lt;- sqrt(mean((test$Petal.Width - simplePredictions)^2)) print(simpleRMSE) #&gt; [1] 0.201 48.5 Predict with Multiple Regression multipleModel &lt;- lm( formula = Petal.Width ~ ., data = train) summary(multipleModel) #&gt; #&gt; Call: #&gt; lm(formula = Petal.Width ~ ., data = train) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -0.5769 -0.0843 -0.0066 0.0978 0.4731 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) -0.5088 0.2277 -2.23 0.02779 * #&gt; Sepal.Length -0.0486 0.0593 -0.82 0.41435 #&gt; Sepal.Width 0.2032 0.0594 3.42 0.00092 *** #&gt; Petal.Length 0.2103 0.0641 3.28 0.00146 ** #&gt; Speciesversicolor 0.6769 0.1583 4.28 4.5e-05 *** #&gt; Speciesvirginica 1.0762 0.2126 5.06 2.1e-06 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 0.176 on 94 degrees of freedom #&gt; Multiple R-squared: 0.949, Adjusted R-squared: 0.947 #&gt; F-statistic: 352 on 5 and 94 DF, p-value: &lt;2e-16 multiplePredictions &lt;- predict( object = multipleModel, newdata = test) plot( x = iris$Petal.Length, y = iris$Petal.Width, xlim = c(0.25, 7), ylim = c(0.25, 2.5)) points( x = test$Petal.Length, y = multiplePredictions, col = &quot;blue&quot;, pch = 4, lwd = 2) points( x = test$Petal.Length, y = test$Petal.Width, col = &quot;red&quot;, pch = 16) multipleRMSE &lt;- sqrt(mean((test$Petal.Width - multiplePredictions)^2)) print(multipleRMSE) #&gt; [1] 0.15 48.6 5. Predict with Neural Network Regression normalize &lt;- function(x) { (x - min(x)) / (max(x) - min(x)) - 0.5 } denormalize &lt;- function(x, y) { ((x + 0.5) * (max(y) - min(y))) + min(y) } scaledIris &lt;- data.frame( Sepal.Length = normalize(iris$Sepal.Length), Sepal.Width = normalize(iris$Sepal.Width), Petal.Length = normalize(iris$Petal.Length), Petal.Width = normalize(iris$Petal.Width), Species = iris$Species) scaledTrain &lt;- scaledIris[indexes, ] scaledTest &lt;- scaledIris[-indexes, ] library(nnet) neuralRegressor &lt;- nnet( formula = Petal.Width ~ ., data = scaledTrain, linout = TRUE, skip = TRUE, size = 4, decay = 0.0001, maxit = 500) #&gt; # weights: 34 #&gt; initial value 64.175158 #&gt; iter 10 value 0.498340 #&gt; iter 20 value 0.439307 #&gt; iter 30 value 0.419373 #&gt; iter 40 value 0.415119 #&gt; iter 50 value 0.412305 #&gt; iter 60 value 0.410862 #&gt; iter 70 value 0.404854 #&gt; iter 80 value 0.402606 #&gt; iter 90 value 0.397903 #&gt; iter 100 value 0.396295 #&gt; iter 110 value 0.394291 #&gt; iter 120 value 0.392652 #&gt; iter 130 value 0.390227 #&gt; iter 140 value 0.389581 #&gt; iter 150 value 0.388891 #&gt; iter 160 value 0.387501 #&gt; iter 170 value 0.382381 #&gt; iter 180 value 0.377034 #&gt; iter 190 value 0.371871 #&gt; iter 200 value 0.364243 #&gt; iter 210 value 0.357845 #&gt; iter 220 value 0.353726 #&gt; iter 230 value 0.348595 #&gt; iter 240 value 0.345766 #&gt; iter 250 value 0.341638 #&gt; iter 260 value 0.340492 #&gt; iter 270 value 0.339963 #&gt; iter 280 value 0.338600 #&gt; iter 290 value 0.338192 #&gt; iter 300 value 0.336018 #&gt; iter 310 value 0.332364 #&gt; iter 320 value 0.331113 #&gt; iter 330 value 0.330340 #&gt; iter 340 value 0.329913 #&gt; iter 350 value 0.329630 #&gt; iter 360 value 0.329433 #&gt; iter 370 value 0.328969 #&gt; iter 380 value 0.328461 #&gt; iter 390 value 0.327849 #&gt; iter 400 value 0.326887 #&gt; iter 410 value 0.326022 #&gt; iter 420 value 0.325114 #&gt; iter 430 value 0.323672 #&gt; iter 440 value 0.321995 #&gt; iter 450 value 0.320491 #&gt; iter 460 value 0.318875 #&gt; iter 470 value 0.317241 #&gt; iter 480 value 0.316544 #&gt; iter 490 value 0.316008 #&gt; iter 500 value 0.315713 #&gt; final value 0.315713 #&gt; stopped after 500 iterations library(NeuralNetTools) plotnet(neuralRegressor) scaledPredictions &lt;- predict( object = neuralRegressor, newdata = scaledTest) neuralPredictions &lt;- denormalize( x = scaledPredictions, y = iris$Petal.Width) plot( x = iris$Petal.Length, y = iris$Petal.Width, xlim = c(0.25, 7), ylim = c(0.25, 2.5)) points( x = test$Petal.Length, y = neuralPredictions, col = &quot;blue&quot;, pch = 4, lwd = 2) points( x = test$Petal.Length, y = test$Petal.Width, col = &quot;red&quot;, pch = 16) neuralRMSE &lt;- sqrt(mean((test$Petal.Width - neuralPredictions)^2)) print(neuralRMSE) #&gt; [1] 0.183 48.7 6. Evaluate all the regression Models print(simpleRMSE) #&gt; [1] 0.201 print(multipleRMSE) #&gt; [1] 0.15 print(neuralRMSE) #&gt; [1] 0.183 "],
["regression-3b-rates-dataset-slr-mlr-nn.html", "Chapter 49 Regression 3b. Rates dataset. (SLR, MLR, NN) 49.1 Introduction 49.2 Split the Data into Test and Training Sets 49.3 Predict with Simple Linear Regression 49.4 Predict with Multiple Linear Regression 49.5 Predict with Neural Network Regression 49.6 Evaluate the Regression Models", " Chapter 49 Regression 3b. Rates dataset. (SLR, MLR, NN) 49.1 Introduction line 29 does not plot Source: https://www.matthewrenze.com/workshops/practical-machine-learning-with-r/lab-3b-regression.html library(readr) policies &lt;- read_csv(file.path(data_raw_dir, &quot;Rates.csv&quot;)) #&gt; Parsed with column specification: #&gt; cols( #&gt; Gender = col_character(), #&gt; State = col_character(), #&gt; State.Rate = col_double(), #&gt; Height = col_double(), #&gt; Weight = col_double(), #&gt; BMI = col_double(), #&gt; Age = col_double(), #&gt; Rate = col_double() #&gt; ) policies #&gt; # A tibble: 1,942 x 8 #&gt; Gender State State.Rate Height Weight BMI Age Rate #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Male MA 0.100 184 67.8 20.0 77 0.332 #&gt; 2 Male VA 0.142 163 89.4 33.6 82 0.869 #&gt; 3 Male NY 0.0908 170 81.2 28.1 31 0.01 #&gt; 4 Male TN 0.120 175 99.7 32.6 39 0.0215 #&gt; 5 Male FL 0.110 184 72.1 21.3 68 0.150 #&gt; 6 Male WA 0.163 166 98.4 35.7 64 0.211 #&gt; # … with 1,936 more rows summary(policies) #&gt; Gender State State.Rate Height #&gt; Length:1942 Length:1942 Min. :0.001 Min. :150 #&gt; Class :character Class :character 1st Qu.:0.110 1st Qu.:162 #&gt; Mode :character Mode :character Median :0.128 Median :170 #&gt; Mean :0.138 Mean :170 #&gt; 3rd Qu.:0.144 3rd Qu.:176 #&gt; Max. :0.318 Max. :190 #&gt; Weight BMI Age Rate #&gt; Min. : 44.1 Min. :16.0 Min. :18.0 Min. :0.001 #&gt; 1st Qu.: 68.6 1st Qu.:23.7 1st Qu.:34.0 1st Qu.:0.015 #&gt; Median : 81.3 Median :28.1 Median :51.0 Median :0.046 #&gt; Mean : 81.2 Mean :28.3 Mean :50.8 Mean :0.138 #&gt; 3rd Qu.: 93.8 3rd Qu.:32.5 3rd Qu.:68.0 3rd Qu.:0.173 #&gt; Max. :116.5 Max. :46.8 Max. :84.0 Max. :0.999 library(RColorBrewer) palette &lt;- brewer.pal(9, &quot;Reds&quot;) # plot( # x = policies, # col = palette[cut(x = policies$Rate, breaks = 9)] # ) library(corrgram) #&gt; Registered S3 methods overwritten by &#39;ggplot2&#39;: #&gt; method from #&gt; [.quosures rlang #&gt; c.quosures rlang #&gt; print.quosures rlang #&gt; Registered S3 method overwritten by &#39;seriation&#39;: #&gt; method from #&gt; reorder.hclust gclus corrgram(policies) cor(policies[3:8]) #&gt; State.Rate Height Weight BMI Age Rate #&gt; State.Rate 1.00000 -0.0165 0.00923 0.0192 0.1123 0.2269 #&gt; Height -0.01652 1.0000 0.23809 -0.3170 -0.1648 -0.1286 #&gt; Weight 0.00923 0.2381 1.00000 0.8396 0.0117 0.0609 #&gt; BMI 0.01924 -0.3170 0.83963 1.0000 0.1023 0.1405 #&gt; Age 0.11235 -0.1648 0.01168 0.1023 1.0000 0.7801 #&gt; Rate 0.22685 -0.1286 0.06094 0.1405 0.7801 1.0000 cor( x = policies$Age, y = policies$Rate) #&gt; [1] 0.78 plot( x = policies$Age, y = policies$Rate) 49.2 Split the Data into Test and Training Sets set.seed(42) library(caret) #&gt; Loading required package: lattice #&gt; #&gt; Attaching package: &#39;lattice&#39; #&gt; The following object is masked from &#39;package:corrgram&#39;: #&gt; #&gt; panel.fill #&gt; Loading required package: ggplot2 indexes &lt;- createDataPartition( y = policies$Rate, p = 0.80, list = FALSE) train &lt;- policies[indexes, ] test &lt;- policies[-indexes, ] print(nrow(train)) #&gt; [1] 1555 print(nrow(test)) #&gt; [1] 387 49.3 Predict with Simple Linear Regression simpleModel &lt;- lm( formula = Rate ~ Age, data = train) plot( x = policies$Age, y = policies$Rate) lines( x = train$Age, y = simpleModel$fitted, col = &quot;red&quot;, lwd = 3) summary(simpleModel) #&gt; #&gt; Call: #&gt; lm(formula = Rate ~ Age, data = train) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -0.1799 -0.0881 -0.0208 0.0617 0.6300 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) -0.265244 0.008780 -30.2 &lt;2e-16 *** #&gt; Age 0.007928 0.000161 49.3 &lt;2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 0.123 on 1553 degrees of freedom #&gt; Multiple R-squared: 0.61, Adjusted R-squared: 0.609 #&gt; F-statistic: 2.43e+03 on 1 and 1553 DF, p-value: &lt;2e-16 simplePredictions &lt;- predict( object = simpleModel, newdata = test) plot( x = policies$Age, y = policies$Rate) points( x = test$Age, y = simplePredictions, col = &quot;blue&quot;, pch = 4, lwd = 2) simpleRMSE &lt;- sqrt(mean((test$Rate - simplePredictions)^2)) print(simpleRMSE) #&gt; [1] 0.119 49.4 Predict with Multiple Linear Regression multipleModel &lt;- lm( formula = Rate ~ Age + Gender + State.Rate + BMI, data = train) summary(multipleModel) #&gt; #&gt; Call: #&gt; lm(formula = Rate ~ Age + Gender + State.Rate + BMI, data = train) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -0.2255 -0.0865 -0.0292 0.0590 0.6053 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) -0.428141 0.018742 -22.84 &lt; 2e-16 *** #&gt; Age 0.007703 0.000156 49.28 &lt; 2e-16 *** #&gt; GenderMale 0.030350 0.006001 5.06 4.8e-07 *** #&gt; State.Rate 0.613139 0.068330 8.97 &lt; 2e-16 *** #&gt; BMI 0.002634 0.000518 5.09 4.1e-07 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 0.118 on 1550 degrees of freedom #&gt; Multiple R-squared: 0.64, Adjusted R-squared: 0.639 #&gt; F-statistic: 688 on 4 and 1550 DF, p-value: &lt;2e-16 multiplePredictions &lt;- predict( object = multipleModel, newdata = test) plot( x = policies$Age, y = policies$Rate) points( x = test$Age, y = multiplePredictions, col = &quot;blue&quot;, pch = 4, lwd = 2) multipleRMSE &lt;- sqrt(mean((test$Rate - multiplePredictions)^2)) print(multipleRMSE) #&gt; [1] 0.114 49.5 Predict with Neural Network Regression normalize &lt;- function(x) { (x - min(x)) / (max(x) - min(x)) - 0.5 } denormalize &lt;- function(x, y) { ((x + 0.5) * (max(y) - min(y))) + min(y) } scaledPolicies &lt;- data.frame( Gender = policies$Gender, State.Rate = normalize(policies$State.Rate), BMI = normalize(policies$BMI), Age = normalize(policies$Age), Rate = normalize(policies$Rate)) scaledTrain &lt;- scaledPolicies[indexes, ] scaledTest &lt;- scaledPolicies[-indexes, ] library(nnet) neuralRegressor &lt;- nnet( formula = Rate ~ ., data = scaledTrain, linout = TRUE, size = 5, decay = 0.0001, maxit = 1000) #&gt; # weights: 31 #&gt; initial value 548.090539 #&gt; iter 10 value 10.610284 #&gt; iter 20 value 3.927378 #&gt; iter 30 value 3.735266 #&gt; iter 40 value 3.513899 #&gt; iter 50 value 3.073390 #&gt; iter 60 value 2.547202 #&gt; iter 70 value 2.296126 #&gt; iter 80 value 2.166120 #&gt; iter 90 value 2.106996 #&gt; iter 100 value 2.092654 #&gt; iter 110 value 2.058596 #&gt; iter 120 value 2.039404 #&gt; iter 130 value 2.023721 #&gt; iter 140 value 2.018781 #&gt; iter 150 value 2.006931 #&gt; iter 160 value 1.999122 #&gt; iter 170 value 1.993920 #&gt; iter 180 value 1.990678 #&gt; iter 190 value 1.989269 #&gt; iter 200 value 1.988846 #&gt; iter 210 value 1.988042 #&gt; iter 220 value 1.987739 #&gt; iter 230 value 1.987678 #&gt; iter 240 value 1.987598 #&gt; iter 250 value 1.987574 #&gt; iter 260 value 1.987549 #&gt; iter 270 value 1.987536 #&gt; iter 280 value 1.987529 #&gt; final value 1.987526 #&gt; converged scaledPredictions &lt;- predict( object = neuralRegressor, newdata = scaledTest) neuralPredictions &lt;- denormalize( x = scaledPredictions, y = policies$Rate) plot( x = train$Age, y = train$Rate) points( x = test$Age, y = neuralPredictions, col = &quot;blue&quot;, pch = 4, lwd = 2) library(NeuralNetTools) plotnet(neuralRegressor) neuralRMSE &lt;- sqrt(mean((test$Rate - neuralPredictions)^2)) print(neuralRMSE) #&gt; [1] 0.0368 49.6 Evaluate the Regression Models print(simpleRMSE) #&gt; [1] 0.119 print(multipleRMSE) #&gt; [1] 0.114 print(neuralRMSE) #&gt; [1] 0.0368 "],
["regression-boston-nnet.html", "Chapter 50 Regression Boston nnet 50.1 Neural Network 50.2 Linear Regression", " Chapter 50 Regression Boston nnet ### ### prepare data ### library(mlbench) data(BostonHousing) # inspect the range which is 1-50 summary(BostonHousing$medv) #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; 5.0 17.0 21.2 22.5 25.0 50.0 ## ## model linear regression ## lm.fit &lt;- lm(medv ~ ., data=BostonHousing) lm.predict &lt;- predict(lm.fit) # mean squared error: 21.89483 mean((lm.predict - BostonHousing$medv)^2) #&gt; [1] 21.9 plot(BostonHousing$medv, lm.predict, main=&quot;Linear regression predictions vs actual&quot;, xlab=&quot;Actual&quot;) ## ## model neural network ## require(nnet) #&gt; Loading required package: nnet # scale inputs: divide by 50 to get 0-1 range nnet.fit &lt;- nnet(medv/50 ~ ., data=BostonHousing, size=2) #&gt; # weights: 31 #&gt; initial value 17.039194 #&gt; iter 10 value 13.754559 #&gt; iter 20 value 13.537235 #&gt; iter 30 value 13.537183 #&gt; iter 40 value 13.530522 #&gt; final value 13.529736 #&gt; converged # multiply 50 to restore original scale nnet.predict &lt;- predict(nnet.fit)*50 # mean squared error: 16.40581 mean((nnet.predict - BostonHousing$medv)^2) #&gt; [1] 66.8 plot(BostonHousing$medv, nnet.predict, main=&quot;Neural network predictions vs actual&quot;, xlab=&quot;Actual&quot;) 50.1 Neural Network Now, let’s use the function train() from the package caret to optimize the neural network hyperparameters decay and size, Also, caret performs resampling to give a better estimate of the error. In this case we scale linear regression by the same value, so the error statistics are directly comparable. library(mlbench) data(BostonHousing) require(caret) #&gt; Loading required package: caret #&gt; Loading required package: lattice #&gt; Loading required package: ggplot2 #&gt; Registered S3 methods overwritten by &#39;ggplot2&#39;: #&gt; method from #&gt; [.quosures rlang #&gt; c.quosures rlang #&gt; print.quosures rlang mygrid &lt;- expand.grid(.decay=c(0.5, 0.1), .size=c(4,5,6)) nnetfit &lt;- train(medv/50 ~ ., data=BostonHousing, method=&quot;nnet&quot;, maxit=1000, tuneGrid=mygrid, trace=F) print(nnetfit) #&gt; Neural Network #&gt; #&gt; 506 samples #&gt; 13 predictor #&gt; #&gt; No pre-processing #&gt; Resampling: Bootstrapped (25 reps) #&gt; Summary of sample sizes: 506, 506, 506, 506, 506, 506, ... #&gt; Resampling results across tuning parameters: #&gt; #&gt; decay size RMSE Rsquared MAE #&gt; 0.1 4 0.0835 0.787 0.0571 #&gt; 0.1 5 0.0822 0.794 0.0565 #&gt; 0.1 6 0.0799 0.806 0.0544 #&gt; 0.5 4 0.0908 0.757 0.0626 #&gt; 0.5 5 0.0900 0.761 0.0624 #&gt; 0.5 6 0.0895 0.763 0.0622 #&gt; #&gt; RMSE was used to select the optimal model using the smallest value. #&gt; The final values used for the model were size = 6 and decay = 0.1. 506 samples 13 predictors No pre-processing Resampling: Bootstrap (25 reps) Summary of sample sizes: 506, 506, 506, 506, 506, 506, ... Resampling results across tuning parameters: size decay RMSE Rsquared RMSE SD Rsquared SD 4 0.1 0.0852 0.785 0.00863 0.0406 4 0.5 0.0923 0.753 0.00891 0.0436 5 0.1 0.0836 0.792 0.00829 0.0396 5 0.5 0.0899 0.765 0.00858 0.0399 6 0.1 0.0835 0.793 0.00804 0.0318 6 0.5 0.0895 0.768 0.00789 0.0344 50.2 Linear Regression lmfit &lt;- train(medv/50 ~ ., data=BostonHousing, method=&quot;lm&quot;) print(lmfit) #&gt; Linear Regression #&gt; #&gt; 506 samples #&gt; 13 predictor #&gt; #&gt; No pre-processing #&gt; Resampling: Bootstrapped (25 reps) #&gt; Summary of sample sizes: 506, 506, 506, 506, 506, 506, ... #&gt; Resampling results: #&gt; #&gt; RMSE Rsquared MAE #&gt; 0.0988 0.726 0.0692 #&gt; #&gt; Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE 506 samples 13 predictors No pre-processing Resampling: Bootstrap (25 reps) Summary of sample sizes: 506, 506, 506, 506, 506, 506, ... Resampling results RMSE Rsquared RMSE SD Rsquared SD 0.0994 0.703 0.00741 0.0389 A tuned neural network has a RMSE of 0.0835 compared to linear regression’s RMSE of 0.0994. "],
["comparing-multiple-vs-neural-network-regression.html", "Chapter 51 Comparing Multiple vs. Neural Network Regression 51.1 Introduction 51.2 Multiple Regression 51.3 Neural Network", " Chapter 51 Comparing Multiple vs. Neural Network Regression 51.1 Introduction Source: http://beyondvalence.blogspot.com/2014/04/r-comparing-multiple-and-neural-network.html Here we will compare and evaluate the results from multiple regression and a neural network on the diamonds data set from the ggplot2 package in R. Consisting of 53,940 observations with 10 variables, diamonds contains data on the carat, cut, color, clarity, price, and diamond dimensions. These variables have a particular effect on price, and we would like to see if they can predict the price of various diamonds. library(ggplot2) #&gt; Registered S3 methods overwritten by &#39;ggplot2&#39;: #&gt; method from #&gt; [.quosures rlang #&gt; c.quosures rlang #&gt; print.quosures rlang library(RSNNS) #&gt; Loading required package: Rcpp library(MASS) library(caret) #&gt; Loading required package: lattice #&gt; #&gt; Attaching package: &#39;caret&#39; #&gt; The following objects are masked from &#39;package:RSNNS&#39;: #&gt; #&gt; confusionMatrix, train # library(diamonds) head(diamonds) #&gt; # A tibble: 6 x 10 #&gt; carat cut color clarity depth table price x y z #&gt; &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 #&gt; 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 #&gt; 3 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 #&gt; 4 0.290 Premium I VS2 62.4 58 334 4.2 4.23 2.63 #&gt; 5 0.31 Good J SI2 63.3 58 335 4.34 4.35 2.75 #&gt; 6 0.24 Very Good J VVS2 62.8 57 336 3.94 3.96 2.48 dplyr::glimpse(diamonds) #&gt; Observations: 53,940 #&gt; Variables: 10 #&gt; $ carat &lt;dbl&gt; 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.26, 0.22, 0.… #&gt; $ cut &lt;ord&gt; Ideal, Premium, Good, Premium, Good, Very Good, Very Goo… #&gt; $ color &lt;ord&gt; E, E, E, I, J, J, I, H, E, H, J, J, F, J, E, E, I, J, J,… #&gt; $ clarity &lt;ord&gt; SI2, SI1, VS1, VS2, SI2, VVS2, VVS1, SI1, VS2, VS1, SI1,… #&gt; $ depth &lt;dbl&gt; 61.5, 59.8, 56.9, 62.4, 63.3, 62.8, 62.3, 61.9, 65.1, 59… #&gt; $ table &lt;dbl&gt; 55, 61, 65, 58, 58, 57, 57, 55, 61, 61, 55, 56, 61, 54, … #&gt; $ price &lt;int&gt; 326, 326, 327, 334, 335, 336, 336, 337, 337, 338, 339, 3… #&gt; $ x &lt;dbl&gt; 3.95, 3.89, 4.05, 4.20, 4.34, 3.94, 3.95, 4.07, 3.87, 4.… #&gt; $ y &lt;dbl&gt; 3.98, 3.84, 4.07, 4.23, 4.35, 3.96, 3.98, 4.11, 3.78, 4.… #&gt; $ z &lt;dbl&gt; 2.43, 2.31, 2.31, 2.63, 2.75, 2.48, 2.47, 2.53, 2.49, 2.… The cut, color, and clarity variables are factors, and must be treated as dummy variables in multiple and neural network regressions. Let us start with multiple regression. 51.2 Multiple Regression First we ready a Multiple Regression by sampling the rows to randomize the observations, and then create a sample index of 0’s and 1’s to separate the training and test sets. Note that the depth and table columns (5, 6) are removed because they are linear combinations of the dimensions, x, y, and z. See that the observations in the training and test sets approximate 70% and 30% of the total observations, from which we sampled and set the probabilities. set.seed(1234567) diamonds &lt;- diamonds[sample(1:nrow(diamonds), nrow(diamonds)),] d.index = sample(0:1, nrow(diamonds), prob=c(0.3, 0.7), rep = TRUE) d.train &lt;- diamonds[d.index==1, c(-5,-6)] d.test &lt;- diamonds[d.index==0, c(-5,-6)] dim(d.train) #&gt; [1] 37502 8 dim(d.test) #&gt; [1] 16438 8 Now we move into the next stage with multiple regression via the train() function from the caret library, instead of the regular lm() function. We specify the predictors, the response variable (price), the “lm” method, and the cross validation resampling method. x &lt;- d.train[,-5] y &lt;- as.numeric(d.train[,5]$price) ds.lm &lt;- caret::train(x, y, method = &quot;lm&quot;, trainControl = trainControl(method = &quot;cv&quot;)) #&gt; Warning: Setting row names on a tibble is deprecated. #&gt; Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) : #&gt; extra argument &#39;trainControl&#39; will be disregarded #&gt; Warning: Setting row names on a tibble is deprecated. #&gt; Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) : #&gt; extra argument &#39;trainControl&#39; will be disregarded #&gt; Warning: Setting row names on a tibble is deprecated. #&gt; Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) : #&gt; extra argument &#39;trainControl&#39; will be disregarded #&gt; Warning: Setting row names on a tibble is deprecated. #&gt; Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) : #&gt; extra argument &#39;trainControl&#39; will be disregarded #&gt; Warning: Setting row names on a tibble is deprecated. #&gt; Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) : #&gt; extra argument &#39;trainControl&#39; will be disregarded #&gt; Warning: Setting row names on a tibble is deprecated. #&gt; Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) : #&gt; extra argument &#39;trainControl&#39; will be disregarded #&gt; Warning: Setting row names on a tibble is deprecated. #&gt; Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) : #&gt; extra argument &#39;trainControl&#39; will be disregarded #&gt; Warning: Setting row names on a tibble is deprecated. #&gt; Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) : #&gt; extra argument &#39;trainControl&#39; will be disregarded #&gt; Warning: Setting row names on a tibble is deprecated. #&gt; Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) : #&gt; extra argument &#39;trainControl&#39; will be disregarded #&gt; Warning: Setting row names on a tibble is deprecated. #&gt; Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) : #&gt; extra argument &#39;trainControl&#39; will be disregarded #&gt; Warning: Setting row names on a tibble is deprecated. #&gt; Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) : #&gt; extra argument &#39;trainControl&#39; will be disregarded #&gt; Warning: Setting row names on a tibble is deprecated. #&gt; Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) : #&gt; extra argument &#39;trainControl&#39; will be disregarded #&gt; Warning: Setting row names on a tibble is deprecated. #&gt; Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) : #&gt; extra argument &#39;trainControl&#39; will be disregarded #&gt; Warning: Setting row names on a tibble is deprecated. #&gt; Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) : #&gt; extra argument &#39;trainControl&#39; will be disregarded #&gt; Warning: Setting row names on a tibble is deprecated. #&gt; Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) : #&gt; extra argument &#39;trainControl&#39; will be disregarded #&gt; Warning: Setting row names on a tibble is deprecated. #&gt; Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) : #&gt; extra argument &#39;trainControl&#39; will be disregarded #&gt; Warning: Setting row names on a tibble is deprecated. #&gt; Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) : #&gt; extra argument &#39;trainControl&#39; will be disregarded #&gt; Warning: Setting row names on a tibble is deprecated. #&gt; Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) : #&gt; extra argument &#39;trainControl&#39; will be disregarded #&gt; Warning: Setting row names on a tibble is deprecated. #&gt; Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) : #&gt; extra argument &#39;trainControl&#39; will be disregarded #&gt; Warning: Setting row names on a tibble is deprecated. #&gt; Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) : #&gt; extra argument &#39;trainControl&#39; will be disregarded #&gt; Warning: Setting row names on a tibble is deprecated. #&gt; Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) : #&gt; extra argument &#39;trainControl&#39; will be disregarded #&gt; Warning: Setting row names on a tibble is deprecated. #&gt; Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) : #&gt; extra argument &#39;trainControl&#39; will be disregarded #&gt; Warning: Setting row names on a tibble is deprecated. #&gt; Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) : #&gt; extra argument &#39;trainControl&#39; will be disregarded #&gt; Warning: Setting row names on a tibble is deprecated. #&gt; Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) : #&gt; extra argument &#39;trainControl&#39; will be disregarded #&gt; Warning: Setting row names on a tibble is deprecated. #&gt; Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) : #&gt; extra argument &#39;trainControl&#39; will be disregarded #&gt; Warning: Setting row names on a tibble is deprecated. #&gt; Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) : #&gt; extra argument &#39;trainControl&#39; will be disregarded ds.lm #&gt; Linear Regression #&gt; #&gt; 37502 samples #&gt; 7 predictor #&gt; #&gt; No pre-processing #&gt; Resampling: Bootstrapped (25 reps) #&gt; Summary of sample sizes: 37502, 37502, 37502, 37502, 37502, 37502, ... #&gt; Resampling results: #&gt; #&gt; RMSE Rsquared MAE #&gt; 1140 0.919 745 #&gt; #&gt; Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE When we call the train(ed) object, we can see the attributes of the training set, resampling, sample sizes, and the results. Note the root mean square error value of 1150. Will that be low enough to take down heavy weight TEAM: Neural Network? Below we visualize the training diamond prices and the predicted prices with ggplot(). library(dplyr) #&gt; #&gt; Attaching package: &#39;dplyr&#39; #&gt; The following object is masked from &#39;package:MASS&#39;: #&gt; #&gt; select #&gt; The following objects are masked from &#39;package:stats&#39;: #&gt; #&gt; filter, lag #&gt; The following objects are masked from &#39;package:base&#39;: #&gt; #&gt; intersect, setdiff, setequal, union data.frame(obs = y, pred = ds.lm$finalModel$fitted.values) %&gt;% ggplot(aes(x = obs, y = pred)) + geom_point(alpha=0.1) + geom_abline(color=&quot;blue&quot;) + labs(title=&quot;Diamond train price&quot;, x=&quot;observed&quot;, y=&quot;predicted&quot;) We see from the axis, the predicted prices have some high values compared to the actual prices. Also, there are predicted prices below 0, which cannot be possible in the observed, which will set TEAM: Multiple Regression back a few points. Next we use ggplot() again to visualize the predicted and observed diamond prices from the test data, which did not train the linear regression model. # predict on test set ds.lm.p &lt;- predict(ds.lm, d.test[,-5], type=&quot;raw&quot;) # compare observed vs predicted prices in the test set data.frame(obs = d.test[,5]$price, pred = ds.lm.p) %&gt;% ggplot(aes(x = obs, y = pred)) + geom_point(alpha=0.1) + geom_abline(color=&quot;blue&quot;)+ labs(&quot;Diamond Test Price&quot;, x=&quot;observed&quot;, y=&quot;predicted&quot;) Similar to the training prices plot, we see here in the test prices that the model over predicts larger values and also predicted negative price values. In order for the Multiple Regression to win, the Neural Network has to have more wild prediction values. Lastly, we calculate the root mean square error, by taking the mean of the squared difference between the predicted and observed diamond prices. The resulting RMSE is 1110.843, similar to the RMSE of the training set. ds.lm.mse &lt;- (1 / nrow(d.test)) * sum((ds.lm.p - d.test[,5])^2) lm.rmse &lt;- sqrt(ds.lm.mse) lm.rmse #&gt; [1] 1168 Below is a detailed output of the model summary, with the coefficients and residuals. Observe how carat is the best predictor, with the highest t value at 191.7, with every increase in 1 carat holding all other variables equal, results in a 10,873 dollar increase in value. As we look at the factor variables, we do not see a reliable increase in coefficients with increases in level value. summary(ds.lm) #&gt; #&gt; Call: #&gt; lm(formula = .outcome ~ ., data = dat, trainControl = ..1) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -21090 -598 -183 378 10778 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 3.68 94.63 0.04 0.9690 #&gt; carat 11142.68 57.43 194.02 &lt; 2e-16 *** #&gt; cut.L 767.70 24.31 31.58 &lt; 2e-16 *** #&gt; cut.Q -336.63 21.41 -15.72 &lt; 2e-16 *** #&gt; cut.C 157.31 18.81 8.36 &lt; 2e-16 *** #&gt; cut^4 -22.81 14.78 -1.54 0.1228 #&gt; color.L -1950.28 20.66 -94.42 &lt; 2e-16 *** #&gt; color.Q -665.60 18.82 -35.37 &lt; 2e-16 *** #&gt; color.C -147.16 17.61 -8.36 &lt; 2e-16 *** #&gt; color^4 44.64 16.20 2.76 0.0059 ** #&gt; color^5 -91.21 15.32 -5.95 2.7e-09 *** #&gt; color^6 -54.74 13.92 -3.93 8.5e-05 *** #&gt; clarity.L 4115.45 36.68 112.19 &lt; 2e-16 *** #&gt; clarity.Q -1959.71 34.33 -57.09 &lt; 2e-16 *** #&gt; clarity.C 990.60 29.29 33.83 &lt; 2e-16 *** #&gt; clarity^4 -370.82 23.30 -15.92 &lt; 2e-16 *** #&gt; clarity^5 240.60 18.91 12.72 &lt; 2e-16 *** #&gt; clarity^6 -7.99 16.37 -0.49 0.6253 #&gt; clarity^7 80.62 14.48 5.57 2.6e-08 *** #&gt; x -1400.26 95.70 -14.63 &lt; 2e-16 *** #&gt; y 545.42 94.57 5.77 8.1e-09 *** #&gt; z -190.86 31.20 -6.12 9.6e-10 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 1130 on 37480 degrees of freedom #&gt; Multiple R-squared: 0.92, Adjusted R-squared: 0.92 #&gt; F-statistic: 2.05e+04 on 21 and 37480 DF, p-value: &lt;2e-16 Now we move on to the neural network regression. 51.3 Neural Network Because neural networks operate in terms of 0 to 1, or -1 to 1, we must first normalize the price variable to 0 to 1, making the lowest value 0 and the highest value 1. We accomplished this using the normalizeData() function. Save the price output in order to revert the normalization after training the data. Also, we take the factor variables and turn them into numeric labels using toNumericClassLabels(). Below we see the normalized prices before they are split into a training and test set with splitForTrainingAndTest() function. diamonds[,3] &lt;- toNumericClassLabels(diamonds[,3]$color) diamonds[,4] &lt;- toNumericClassLabels(diamonds[,4]$clarity) prices &lt;- normalizeData(diamonds[,7], type=&quot;0_1&quot;) head(prices) #&gt; [,1] #&gt; [1,] 0.0841 #&gt; [2,] 0.1491 #&gt; [3,] 0.0237 #&gt; [4,] 0.3247 #&gt; [5,] 0.0280 #&gt; [6,] 0.0252 dsplit &lt;- splitForTrainingAndTest(diamonds[, c(-2,-5,-6,-7,-9,-10)], prices, ratio=0.3) Now the Neural Network are ready for the multi-layer perceptron (MLP) regression. We define the training inputs (predictor variables) and targets (prices), the size of the layer (5), the incremented learning parameter (0.1), the max iterations (100 epochs), and also the test input/targets. # mlp model d.nn &lt;- mlp(dsplit$inputsTrain, dsplit$targetsTrain, size = c(5), learnFuncParams = c(0.1), maxit=100, inputsTest = dsplit$inputsTest, targetsTest = dsplit$targetsTest, metric = &quot;RMSE&quot;, linout = FALSE) If you spectators have dealt with mlp() before, you know the summary output can be quite lenghty, so it is omitted (we dislike commercials too). We move to the visual description of the MLP model with the iterative sum of square error for the training and test sets. Additionally, we plot the regression error (predicted vs observed) for the training and test prices. Time for the Neural Network so show off its statistical muscles! First up, we have the iterative sum of square error for each epoch, noting that we specified a maximum of 100 in the MLP model. We see an immediate drop in the SSE with the first few iterations, with the SSE leveling out around 50. The test SSE, in red, fluctuations just above 50 as well. Since the SSE began to plateau, the model fit well but not too well, since we want to avoid over fitting the model. So 100 iterations was a good choice. # SSE error plotIterativeError(d.nn, main = &quot;Diamonds RSNNS-SSE&quot;) Second, we observe the regression plot with the fitted (predicted) and target (observed) prices from the training set. The prices fit reasonably well, and we see the red model regression line close to the black (y=x) optimal line. Note that some middle prices were over predicted by the model, and there were no negative prices, unlike the linear regression model. # regression errors plotRegressionError(dsplit$targetsTrain, d.nn$fitted.values, main = &quot;Diamonds Training Fit&quot;) Third, we look at the predicted and observed prices from the test set. Again the red regression line approximates the optimal black line, and more price values were over predicted by the model. Again, there are no negative predicted prices, a good sign. plotRegressionError(dsplit$targetsTest, d.nn$fittedTestValues, main = &quot;Diamonds Test Fit&quot;) Now we calculate the RMSE for the training set, which we get 692.5155. This looks promising for the Neural Network! # train set train.pred &lt;- denormalizeData(d.nn$fitted.values, getNormParameters(prices)) train.obs &lt;- denormalizeData(dsplit$targetsTrain, getNormParameters(prices)) train.mse &lt;- (1 / nrow(dsplit$inputsTrain)) * sum((train.pred - train.obs)^2) rsnns.train.rmse &lt;- sqrt(train.mse) rsnns.train.rmse #&gt; [1] 739 Naturally we want to calculate the RMSE for the test set, but note that in the real world, we would not have the luxury of knowing the real test values. We arrive at 679.5265. # test set test.pred &lt;- denormalizeData(d.nn$fittedTestValues, getNormParameters(prices)) test.obs &lt;- denormalizeData(dsplit$targetsTest, getNormParameters(prices)) test.mse &lt;- (1 / nrow(dsplit$inputsTest)) * sum((test.pred - test.obs)^2) rsnns.test.rmse &lt;- sqrt(test.mse) rsnns.test.rmse #&gt; [1] 751 Which model was better in predicting the diamond price? The linear regression model with 10 fold cross validation, or the multi-layer perceptron model with 5 nodes run to 100 iterations? Who won the rumble? RUMBLE RESULTS From calculating the two RMSE’s from the training and test sets for the two TEAMS, we wrap them in a list. We named the TEAM: Multiple Regression as linear, and the TEAM: Neural Network regression as neural. # aggregate all rmse d.rmse &lt;- list(linear.train = ds.lm$results$RMSE, linear.test = lm.rmse, neural.train = rsnns.train.rmse, neural.test = rsnns.test.rmse) Below we can evaluate the models from their RMSE values. d.rmse #&gt; $linear.train #&gt; [1] 1140 #&gt; #&gt; $linear.test #&gt; [1] 1168 #&gt; #&gt; $neural.train #&gt; [1] 739 #&gt; #&gt; $neural.test #&gt; [1] 751 Looking at the training RMSE first, we see a clear difference as the linear RMSE was 66% larger than the neural RMSE, at 1,152.393 versus 692.5155. Peeking into the test sets, we have a similar 63% larger linear RMSE than the neural RMSE, with 1,110.843 and 679.5265 respectively. TEAM: Neural Network begins to gain the upper hand in the evaluation round. One important difference between the two models was the range of the predictions. Recall from both training and test plots that the linear regression model predicted negative price values, whereas the MLP model predicted only positive prices. This is a devastating blow to the Multiple Regression. Also, the over-prediction of prices existed in both models, however the linear regression model over predicted those middle values higher the anticipated maximum price values. Sometimes the simple models are optimal, and other times more complicated models are better. This time, the neural network model prevailed in predicting diamond prices. "]
]
