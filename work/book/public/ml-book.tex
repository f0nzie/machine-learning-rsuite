\documentclass[]{book}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={A Machine Learning Book},
            pdfauthor={Alfonso R. Reyes},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{natbib}
\bibliographystyle{apalike}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\providecommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}

  \title{A Machine Learning Book}
    \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
    \author{Alfonso R. Reyes}
    \preauthor{\centering\large\emph}
  \postauthor{\par}
      \predate{\centering\large\emph}
  \postdate{\par}
    \date{2019-09-20}

\usepackage{booktabs}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{prerequisites}{%
\chapter*{Prerequisites}\label{prerequisites}}
\addcontentsline{toc}{chapter}{Prerequisites}

This is a \emph{sample} book written in \textbf{Markdown}. You can use anything that Pandoc's Markdown supports, e.g., a math equation \(a^2 + b^2 = c^2\).

The \textbf{bookdown} package can be installed from CRAN or Github:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"bookdown"}\NormalTok{)}
\CommentTok{# or the development version}
\CommentTok{# devtools::install_github("rstudio/bookdown")}
\end{Highlighting}
\end{Shaded}

\hypertarget{part-algorithms-comparison}{%
\part{Algorithms Comparison}\label{part-algorithms-comparison}}

\hypertarget{introduction-to-h2o.}{%
\chapter{Introduction to h2o.}\label{introduction-to-h2o.}}

\hypertarget{bad-loans-dataset.-glm-rf-gbm-dl-nb}{%
\section{\texorpdfstring{Bad Loans dataset. \emph{(GLM, RF, GBM, DL, NB)}}{Bad Loans dataset. (GLM, RF, GBM, DL, NB)}}\label{bad-loans-dataset.-glm-rf-gbm-dl-nb}}

Source: \url{https://github.com/h2oai/h2o-tutorials/blob/master/h2o-open-tour-2016/chicago/intro-to-h2o.R}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

Introductory H2O Machine Learning Tutorial
Prepared for H2O Open Chicago 2016: \url{http://open.h2o.ai/chicago.html}

\hypertarget{install-and-download-h2o}{%
\subsection{\texorpdfstring{Install and download \texttt{h2o}}{Install and download h2o}}\label{install-and-download-h2o}}

First step is to download \& install the h2o R library
The latest version is available by clicking on the R tab here: \url{http://h2o-release.s3.amazonaws.com/h2o/latest_stable.html}

Load the H2O library and start up the H2O cluster locally on your machine:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Load the H2O library and start up the H2O cluster locally on your machine}
\KeywordTok{library}\NormalTok{(h2o)}
\KeywordTok{h2o.init}\NormalTok{(}\DataTypeTok{nthreads =} \DecValTok{-1}\NormalTok{, }\CommentTok{#Number of threads -1 means use all cores on your machine}
         \DataTypeTok{max_mem_size =} \StringTok{"8G"}\NormalTok{)  }\CommentTok{#max mem size is the maximum memory to allocate to H2O}
\CommentTok{#> }
\CommentTok{#> H2O is not running yet, starting it now...}
\CommentTok{#> }
\CommentTok{#> Note:  In case of errors look at the following log files:}
\CommentTok{#>     /tmp/RtmpBLLJK4/h2o_datascience_started_from_r.out}
\CommentTok{#>     /tmp/RtmpBLLJK4/h2o_datascience_started_from_r.err}
\CommentTok{#> }
\CommentTok{#> }
\CommentTok{#> Starting H2O JVM and connecting: . Connection successful!}
\CommentTok{#> }
\CommentTok{#> R is connected to the H2O cluster: }
\CommentTok{#>     H2O cluster uptime:         1 seconds 326 milliseconds }
\CommentTok{#>     H2O cluster timezone:       America/Chicago }
\CommentTok{#>     H2O data parsing timezone:  UTC }
\CommentTok{#>     H2O cluster version:        3.22.1.1 }
\CommentTok{#>     H2O cluster version age:    8 months and 23 days !!! }
\CommentTok{#>     H2O cluster name:           H2O_started_from_R_datascience_mwl453 }
\CommentTok{#>     H2O cluster total nodes:    1 }
\CommentTok{#>     H2O cluster total memory:   7.11 GB }
\CommentTok{#>     H2O cluster total cores:    8 }
\CommentTok{#>     H2O cluster allowed cores:  8 }
\CommentTok{#>     H2O cluster healthy:        TRUE }
\CommentTok{#>     H2O Connection ip:          localhost }
\CommentTok{#>     H2O Connection port:        54321 }
\CommentTok{#>     H2O Connection proxy:       NA }
\CommentTok{#>     H2O Internal Security:      FALSE }
\CommentTok{#>     H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4 }
\CommentTok{#>     R Version:                  R version 3.6.0 (2019-04-26)}
\end{Highlighting}
\end{Shaded}

\hypertarget{load-the-dataset}{%
\subsection{Load the dataset}\label{load-the-dataset}}

Next we will import a cleaned up version of the Lending Club ``Bad Loans'' dataset The purpose here is to predict whether a loan will be bad (not repaid to the lender). The response column, bad\_loan, is 1 if the loan was bad, and 0 otherwise

Import the data
\texttt{loan\_csv\ \textless{}-\ "/Volumes/H2OTOUR/loan.csv"}\\
Alternatively, you can import the data directly from a URL

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# modify this for your machine}
\NormalTok{loan_csv <-}\StringTok{ "https://raw.githubusercontent.com/h2oai/app-consumer-loan/master/data/loan.csv"}
\NormalTok{data <-}\StringTok{ }\KeywordTok{h2o.importFile}\NormalTok{(loan_csv)  }\CommentTok{# 163,987 rows x 15 columns}
\CommentTok{#> }
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|}\StringTok{                                                                 }\ErrorTok{|}\StringTok{   }\DecValTok{0}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==============}\StringTok{                                                   }\ErrorTok{|}\StringTok{  }\DecValTok{22}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==============================}\StringTok{                                   }\ErrorTok{|}\StringTok{  }\DecValTok{47}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=======================================}\StringTok{                          }\ErrorTok{|}\StringTok{  }\DecValTok{59}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=======================================================}\StringTok{          }\ErrorTok{|}\StringTok{  }\DecValTok{84}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|===============================================================}\StringTok{  }\ErrorTok{|}\StringTok{  }\DecValTok{97}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=================================================================|}\StringTok{ }\DecValTok{100}\NormalTok{%}
\KeywordTok{dim}\NormalTok{(data)}
\CommentTok{#> [1] 163987     15}
\CommentTok{# [1] 163987     15}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{url <-}\StringTok{ "https://raw.githubusercontent.com/h2oai/app-consumer-loan/master/data/loan.csv"}
\NormalTok{loans <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(url)}
\KeywordTok{write.csv}\NormalTok{(loans, }\DataTypeTok{file =} \KeywordTok{file.path}\NormalTok{(data_raw_dir, }\StringTok{"loan.csv"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\hypertarget{feature-engineering}{%
\subsection{Feature Engineering}\label{feature-engineering}}

Since we want to train a binary classification model,
we must ensure that the response is coded as a factor
If the response is 0/1, H2O will assume it's numeric,
which means that H2O will train a regression model instead

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data}\OperatorTok{$}\NormalTok{bad_loan <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(data}\OperatorTok{$}\NormalTok{bad_loan)  }\CommentTok{#encode the binary repsonse as a factor}
\KeywordTok{h2o.levels}\NormalTok{(data}\OperatorTok{$}\NormalTok{bad_loan)  }\CommentTok{#optional: after encoding, this shows the two factor levels, '0' and '1'}
\CommentTok{#> [1] "0" "1"}
\CommentTok{# [1] "0" "1"}
\end{Highlighting}
\end{Shaded}

\hypertarget{partition-data}{%
\subsection{Partition data}\label{partition-data}}

Partition the data into training, validation and test sets

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Partition the data into training, validation and test sets}
\NormalTok{splits <-}\StringTok{ }\KeywordTok{h2o.splitFrame}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ data, }
                         \DataTypeTok{ratios =} \KeywordTok{c}\NormalTok{(}\FloatTok{0.7}\NormalTok{, }\FloatTok{0.15}\NormalTok{),  }\CommentTok{#partition data into 70%, 15%, 15% chunks}
                         \DataTypeTok{seed =} \DecValTok{1}\NormalTok{)  }\CommentTok{#setting a seed will guarantee reproducibility}
\NormalTok{train <-}\StringTok{ }\NormalTok{splits[[}\DecValTok{1}\NormalTok{]]}
\NormalTok{valid <-}\StringTok{ }\NormalTok{splits[[}\DecValTok{2}\NormalTok{]]}
\NormalTok{test <-}\StringTok{ }\NormalTok{splits[[}\DecValTok{3}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

Take a look at the size of each partition
Notice that h2o.splitFrame uses approximate splitting not exact splitting (for efficiency) so these are not exactly 70\%, 15\% and 15\% of the total rows

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{nrow}\NormalTok{(train)  }\CommentTok{# 114908}
\CommentTok{#> [1] 114908}
\KeywordTok{nrow}\NormalTok{(valid) }\CommentTok{# 24498}
\CommentTok{#> [1] 24498}
\KeywordTok{nrow}\NormalTok{(test)  }\CommentTok{# 24581}
\CommentTok{#> [1] 24581}
\end{Highlighting}
\end{Shaded}

\hypertarget{identify-response-and-predictor-variables}{%
\subsection{Identify response and predictor variables}\label{identify-response-and-predictor-variables}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Identify response and predictor variables}
\NormalTok{y <-}\StringTok{ "bad_loan"}
\NormalTok{x <-}\StringTok{ }\KeywordTok{setdiff}\NormalTok{(}\KeywordTok{names}\NormalTok{(data), }\KeywordTok{c}\NormalTok{(y, }\StringTok{"int_rate"}\NormalTok{))  }\CommentTok{# remove the interest rate column because it's correlated with the outcome}
\KeywordTok{print}\NormalTok{(x)}
\CommentTok{#>  [1] "loan_amnt"             "term"                 }
\CommentTok{#>  [3] "emp_length"            "home_ownership"       }
\CommentTok{#>  [5] "annual_inc"            "purpose"              }
\CommentTok{#>  [7] "addr_state"            "dti"                  }
\CommentTok{#>  [9] "delinq_2yrs"           "revol_util"           }
\CommentTok{#> [11] "total_acc"             "longest_credit_length"}
\CommentTok{#> [13] "verification_status"}
\CommentTok{# [1] "loan_amnt"             "term"                 }
\CommentTok{# [3] "emp_length"            "home_ownership"       }
\CommentTok{# [5] "annual_inc"            "verification_status"  }
\CommentTok{# [7] "purpose"               "addr_state"           }
\CommentTok{# [9] "dti"                   "delinq_2yrs"          }
\CommentTok{# [11] "revol_util"            "total_acc"            }
\CommentTok{# [13] "longest_credit_length"}
\end{Highlighting}
\end{Shaded}

\hypertarget{algorithms}{%
\section{Algorithms}\label{algorithms}}

Now that we have prepared the data, we can train some models
We will start by training a single model from each of the H2O supervised algos:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Generalized Linear Model (GLM)
\item
  Random Forest (RF)
\item
  Gradient Boosting Machine (GBM)
\item
  Deep Learning (DL)
\item
  Naive Bayes (NB)
\end{enumerate}

\hypertarget{glm}{%
\section{GLM}\label{glm}}

Let's start with a basic binomial Generalized Linear Model
By default, h2o.glm uses a regularized, elastic net model

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{glm_fit1 <-}\StringTok{ }\KeywordTok{h2o.glm}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ x, }
                    \DataTypeTok{y =}\NormalTok{ y, }
                    \DataTypeTok{training_frame =}\NormalTok{ train,}
                    \DataTypeTok{model_id =} \StringTok{"glm_fit1"}\NormalTok{,}
                    \DataTypeTok{family =} \StringTok{"binomial"}\NormalTok{)  }\CommentTok{#similar to R's glm, h2o.glm has the family argument}
\CommentTok{#> }
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|}\StringTok{                                                                 }\ErrorTok{|}\StringTok{   }\DecValTok{0}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=================================================================|}\StringTok{ }\DecValTok{100}\NormalTok{%}
\end{Highlighting}
\end{Shaded}

Next we will do some automatic tuning by passing in a validation frame and setting
\texttt{lambda\_search\ =\ True}. Since we are training a GLM with regularization, we should
try to find the right amount of regularization (to avoid overfitting). The model
parameter, \texttt{lambda}, controls the amount of regularization in a GLM model and we can
find the optimal value for \texttt{lambda} automatically by setting \texttt{lambda\_search\ =\ TRUE}
and passing in a validation frame (which is used to evaluate model performance using a
particular value of lambda).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{glm_fit2 <-}\StringTok{ }\KeywordTok{h2o.glm}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ x, }
                    \DataTypeTok{y =}\NormalTok{ y, }
                    \DataTypeTok{training_frame =}\NormalTok{ train,}
                    \DataTypeTok{model_id =} \StringTok{"glm_fit2"}\NormalTok{,}
                    \DataTypeTok{validation_frame =}\NormalTok{ valid,}
                    \DataTypeTok{family =} \StringTok{"binomial"}\NormalTok{,}
                    \DataTypeTok{lambda_search =} \OtherTok{TRUE}\NormalTok{)}
\CommentTok{#> }
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|}\StringTok{                                                                 }\ErrorTok{|}\StringTok{   }\DecValTok{0}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|===================}\StringTok{                                              }\ErrorTok{|}\StringTok{  }\DecValTok{29}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=====================================}\StringTok{                            }\ErrorTok{|}\StringTok{  }\DecValTok{57}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=================================================================|}\StringTok{ }\DecValTok{100}\NormalTok{%}
\end{Highlighting}
\end{Shaded}

Let's compare the performance of the two GLMs

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Let's compare the performance of the two GLMs}
\NormalTok{glm_perf1 <-}\StringTok{ }\KeywordTok{h2o.performance}\NormalTok{(}\DataTypeTok{model =}\NormalTok{ glm_fit1,}
                             \DataTypeTok{newdata =}\NormalTok{ test)}
\NormalTok{glm_perf2 <-}\StringTok{ }\KeywordTok{h2o.performance}\NormalTok{(}\DataTypeTok{model =}\NormalTok{ glm_fit2,}
                             \DataTypeTok{newdata =}\NormalTok{ test)}

\CommentTok{# Print model performance}
\NormalTok{glm_perf1}
\CommentTok{#> H2OBinomialMetrics: glm}
\CommentTok{#> }
\CommentTok{#> MSE:  0.142}
\CommentTok{#> RMSE:  0.377}
\CommentTok{#> LogLoss:  0.451}
\CommentTok{#> Mean Per-Class Error:  0.37}
\CommentTok{#> AUC:  0.677}
\CommentTok{#> pr_auc:  0.327}
\CommentTok{#> Gini:  0.355}
\CommentTok{#> R^2:  0.0639}
\CommentTok{#> Residual Deviance:  22176}
\CommentTok{#> AIC:  22280}
\CommentTok{#> }
\CommentTok{#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:}
\CommentTok{#>            0    1    Error         Rate}
\CommentTok{#> 0      13647 6344 0.317343  =6344/19991}
\CommentTok{#> 1       1939 2651 0.422440   =1939/4590}
\CommentTok{#> Totals 15586 8995 0.336968  =8283/24581}
\CommentTok{#> }
\CommentTok{#> Maximum Metrics: Maximum metrics at their respective thresholds}
\CommentTok{#>                         metric threshold    value idx}
\CommentTok{#> 1                       max f1  0.193323 0.390283 222}
\CommentTok{#> 2                       max f2  0.118600 0.556655 307}
\CommentTok{#> 3                 max f0point5  0.276272 0.354086 146}
\CommentTok{#> 4                 max accuracy  0.494244 0.814410  29}
\CommentTok{#> 5                max precision  0.744500 1.000000   0}
\CommentTok{#> 6                   max recall  0.001225 1.000000 399}
\CommentTok{#> 7              max specificity  0.744500 1.000000   0}
\CommentTok{#> 8             max absolute_mcc  0.198334 0.210606 216}
\CommentTok{#> 9   max min_per_class_accuracy  0.180070 0.627783 236}
\CommentTok{#> 10 max mean_per_class_accuracy  0.193323 0.630109 222}
\CommentTok{#> }
\CommentTok{#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`}
\NormalTok{glm_perf2}
\CommentTok{#> H2OBinomialMetrics: glm}
\CommentTok{#> }
\CommentTok{#> MSE:  0.142}
\CommentTok{#> RMSE:  0.377}
\CommentTok{#> LogLoss:  0.451}
\CommentTok{#> Mean Per-Class Error:  0.372}
\CommentTok{#> AUC:  0.677}
\CommentTok{#> pr_auc:  0.326}
\CommentTok{#> Gini:  0.354}
\CommentTok{#> R^2:  0.0635}
\CommentTok{#> Residual Deviance:  22186}
\CommentTok{#> AIC:  22282}
\CommentTok{#> }
\CommentTok{#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:}
\CommentTok{#>            0    1    Error         Rate}
\CommentTok{#> 0      13699 6292 0.314742  =6292/19991}
\CommentTok{#> 1       1968 2622 0.428758   =1968/4590}
\CommentTok{#> Totals 15667 8914 0.336032  =8260/24581}
\CommentTok{#> }
\CommentTok{#> Maximum Metrics: Maximum metrics at their respective thresholds}
\CommentTok{#>                         metric threshold    value idx}
\CommentTok{#> 1                       max f1  0.194171 0.388329 216}
\CommentTok{#> 2                       max f2  0.119200 0.555998 306}
\CommentTok{#> 3                 max f0point5  0.256488 0.351893 153}
\CommentTok{#> 4                 max accuracy  0.474001 0.814654  32}
\CommentTok{#> 5                max precision  0.736186 1.000000   0}
\CommentTok{#> 6                   max recall  0.001255 1.000000 399}
\CommentTok{#> 7              max specificity  0.736186 1.000000   0}
\CommentTok{#> 8             max absolute_mcc  0.198114 0.208337 212}
\CommentTok{#> 9   max min_per_class_accuracy  0.180131 0.625181 231}
\CommentTok{#> 10 max mean_per_class_accuracy  0.194171 0.628250 216}
\CommentTok{#> }
\CommentTok{#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`}
\end{Highlighting}
\end{Shaded}

Instead of printing the entire model performance metrics object,
it is probably easier to print just the metric that you are interested in comparing.
Retreive test set AUC

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{h2o.auc}\NormalTok{(glm_perf1)  }\CommentTok{#0.677449084114}
\CommentTok{#> [1] 0.677}
\KeywordTok{h2o.auc}\NormalTok{(glm_perf2)  }\CommentTok{#0.677675858276}
\CommentTok{#> [1] 0.677}
\end{Highlighting}
\end{Shaded}

Compare test AUC to the training AUC and validation AUC

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Compare test AUC to the training AUC and validation AUC}
\KeywordTok{h2o.auc}\NormalTok{(glm_fit2, }\DataTypeTok{train =} \OtherTok{TRUE}\NormalTok{)  }\CommentTok{#0.674306164325 }
\CommentTok{#> [1] 0.673}
\KeywordTok{h2o.auc}\NormalTok{(glm_fit2, }\DataTypeTok{valid =} \OtherTok{TRUE}\NormalTok{)  }\CommentTok{#0.675512216705}
\CommentTok{#> [1] 0.675}
\NormalTok{glm_fit2}\OperatorTok{@}\NormalTok{model}\OperatorTok{$}\NormalTok{validation_metrics  }\CommentTok{#0.675512216705}
\CommentTok{#> H2OBinomialMetrics: glm}
\CommentTok{#> ** Reported on validation data. **}
\CommentTok{#> }
\CommentTok{#> MSE:  0.142}
\CommentTok{#> RMSE:  0.377}
\CommentTok{#> LogLoss:  0.451}
\CommentTok{#> Mean Per-Class Error:  0.37}
\CommentTok{#> AUC:  0.675}
\CommentTok{#> pr_auc:  0.316}
\CommentTok{#> Gini:  0.351}
\CommentTok{#> R^2:  0.0597}
\CommentTok{#> Residual Deviance:  22101}
\CommentTok{#> AIC:  22197}
\CommentTok{#> }
\CommentTok{#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:}
\CommentTok{#>            0    1    Error         Rate}
\CommentTok{#> 0      13591 6365 0.318952  =6365/19956}
\CommentTok{#> 1       1916 2626 0.421841   =1916/4542}
\CommentTok{#> Totals 15507 8991 0.338028  =8281/24498}
\CommentTok{#> }
\CommentTok{#> Maximum Metrics: Maximum metrics at their respective thresholds}
\CommentTok{#>                         metric threshold    value idx}
\CommentTok{#> 1                       max f1  0.193519 0.388088 217}
\CommentTok{#> 2                       max f2  0.116436 0.555055 308}
\CommentTok{#> 3                 max f0point5  0.288405 0.343386 132}
\CommentTok{#> 4                 max accuracy  0.487882 0.815250  29}
\CommentTok{#> 5                max precision  0.576333 0.681818   9}
\CommentTok{#> 6                   max recall  0.004789 1.000000 398}
\CommentTok{#> 7              max specificity  0.715719 0.999950   0}
\CommentTok{#> 8             max absolute_mcc  0.195417 0.209494 215}
\CommentTok{#> 9   max min_per_class_accuracy  0.180760 0.627731 230}
\CommentTok{#> 10 max mean_per_class_accuracy  0.192639 0.629672 218}
\CommentTok{#> }
\CommentTok{#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`}
\end{Highlighting}
\end{Shaded}

\hypertarget{random-forest}{%
\section{Random Forest}\label{random-forest}}

H2O's Random Forest (RF) implements a distributed version of the standard
Random Forest algorithm and variable importance measures.
First we will train a basic Random Forest model with default parameters.
The Random Forest model will infer the response distribution from the response encoding.
A seed is required for reproducibility.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rf_fit1 <-}\StringTok{ }\KeywordTok{h2o.randomForest}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ x,}
                            \DataTypeTok{y =}\NormalTok{ y,}
                            \DataTypeTok{training_frame =}\NormalTok{ train,}
                            \DataTypeTok{model_id =} \StringTok{"rf_fit1"}\NormalTok{,}
                            \DataTypeTok{seed =} \DecValTok{1}\NormalTok{)}
\CommentTok{#> }
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|}\StringTok{                                                                 }\ErrorTok{|}\StringTok{   }\DecValTok{0}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|===}\StringTok{                                                              }\ErrorTok{|}\StringTok{   }\DecValTok{4}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|========}\StringTok{                                                         }\ErrorTok{|}\StringTok{  }\DecValTok{12}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==============}\StringTok{                                                   }\ErrorTok{|}\StringTok{  }\DecValTok{22}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=====================}\StringTok{                                            }\ErrorTok{|}\StringTok{  }\DecValTok{32}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==============================}\StringTok{                                   }\ErrorTok{|}\StringTok{  }\DecValTok{46}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=======================================}\StringTok{                          }\ErrorTok{|}\StringTok{  }\DecValTok{60}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=================================================}\StringTok{                }\ErrorTok{|}\StringTok{  }\DecValTok{76}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=========================================================}\StringTok{        }\ErrorTok{|}\StringTok{  }\DecValTok{88}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=================================================================|}\StringTok{ }\DecValTok{100}\NormalTok{%}
\end{Highlighting}
\end{Shaded}

Next we will increase the number of trees used in the forest by setting \texttt{ntrees\ =\ 100}.
The default number of trees in an H2O Random Forest is 50, so this RF will be twice as
big as the default. Usually increasing the number of trees in a RF will increase
performance as well. Unlike Gradient Boosting Machines (GBMs), Random Forests are fairly
resistant (although not free from) overfitting.
See the GBM example below for additional guidance on preventing overfitting using H2O's
early stopping functionality.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rf_fit2 <-}\StringTok{ }\KeywordTok{h2o.randomForest}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ x,}
                            \DataTypeTok{y =}\NormalTok{ y,}
                            \DataTypeTok{training_frame =}\NormalTok{ train,}
                            \DataTypeTok{model_id =} \StringTok{"rf_fit2"}\NormalTok{,}
                            \CommentTok{#validation_frame = valid,  #only used if stopping_rounds > 0}
                            \DataTypeTok{ntrees =} \DecValTok{100}\NormalTok{,}
                            \DataTypeTok{seed =} \DecValTok{1}\NormalTok{)}
\CommentTok{#> }
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|}\StringTok{                                                                 }\ErrorTok{|}\StringTok{   }\DecValTok{0}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|====}\StringTok{                                                             }\ErrorTok{|}\StringTok{   }\DecValTok{6}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=======}\StringTok{                                                          }\ErrorTok{|}\StringTok{  }\DecValTok{11}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==========}\StringTok{                                                       }\ErrorTok{|}\StringTok{  }\DecValTok{16}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==============}\StringTok{                                                   }\ErrorTok{|}\StringTok{  }\DecValTok{22}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|===================}\StringTok{                                              }\ErrorTok{|}\StringTok{  }\DecValTok{29}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|========================}\StringTok{                                         }\ErrorTok{|}\StringTok{  }\DecValTok{37}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=============================}\StringTok{                                    }\ErrorTok{|}\StringTok{  }\DecValTok{44}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=================================}\StringTok{                                }\ErrorTok{|}\StringTok{  }\DecValTok{51}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|======================================}\StringTok{                           }\ErrorTok{|}\StringTok{  }\DecValTok{58}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|===========================================}\StringTok{                      }\ErrorTok{|}\StringTok{  }\DecValTok{66}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|===============================================}\StringTok{                  }\ErrorTok{|}\StringTok{  }\DecValTok{73}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|====================================================}\StringTok{             }\ErrorTok{|}\StringTok{  }\DecValTok{80}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=========================================================}\StringTok{        }\ErrorTok{|}\StringTok{  }\DecValTok{88}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==============================================================}\StringTok{   }\ErrorTok{|}\StringTok{  }\DecValTok{95}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=================================================================|}\StringTok{ }\DecValTok{100}\NormalTok{%}
\end{Highlighting}
\end{Shaded}

Let's compare the performance of the two RFs

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Let's compare the performance of the two RFs}
\NormalTok{rf_perf1 <-}\StringTok{ }\KeywordTok{h2o.performance}\NormalTok{(}\DataTypeTok{model =}\NormalTok{ rf_fit1,}
                            \DataTypeTok{newdata =}\NormalTok{ test)}
\NormalTok{rf_perf2 <-}\StringTok{ }\KeywordTok{h2o.performance}\NormalTok{(}\DataTypeTok{model =}\NormalTok{ rf_fit2,}
                            \DataTypeTok{newdata =}\NormalTok{ test)}

\CommentTok{# Print model performance}
\NormalTok{rf_perf1}
\CommentTok{#> H2OBinomialMetrics: drf}
\CommentTok{#> }
\CommentTok{#> MSE:  0.144}
\CommentTok{#> RMSE:  0.379}
\CommentTok{#> LogLoss:  0.459}
\CommentTok{#> Mean Per-Class Error:  0.379}
\CommentTok{#> AUC:  0.663}
\CommentTok{#> pr_auc:  0.311}
\CommentTok{#> Gini:  0.327}
\CommentTok{#> }
\CommentTok{#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:}
\CommentTok{#>            0    1    Error         Rate}
\CommentTok{#> 0      12842 7149 0.357611  =7149/19991}
\CommentTok{#> 1       1839 2751 0.400654   =1839/4590}
\CommentTok{#> Totals 14681 9900 0.365648  =8988/24581}
\CommentTok{#> }
\CommentTok{#> Maximum Metrics: Maximum metrics at their respective thresholds}
\CommentTok{#>                         metric threshold    value idx}
\CommentTok{#> 1                       max f1  0.193397 0.379710 229}
\CommentTok{#> 2                       max f2  0.077797 0.547982 344}
\CommentTok{#> 3                 max f0point5  0.277524 0.344251 158}
\CommentTok{#> 4                 max accuracy  0.543639 0.813799  29}
\CommentTok{#> 5                max precision  0.817454 1.000000   0}
\CommentTok{#> 6                   max recall  0.001181 1.000000 399}
\CommentTok{#> 7              max specificity  0.817454 1.000000   0}
\CommentTok{#> 8             max absolute_mcc  0.252480 0.195090 178}
\CommentTok{#> 9   max min_per_class_accuracy  0.186549 0.619826 235}
\CommentTok{#> 10 max mean_per_class_accuracy  0.192603 0.620890 230}
\CommentTok{#> }
\CommentTok{#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`}
\NormalTok{rf_perf2}
\CommentTok{#> H2OBinomialMetrics: drf}
\CommentTok{#> }
\CommentTok{#> MSE:  0.143}
\CommentTok{#> RMSE:  0.378}
\CommentTok{#> LogLoss:  0.454}
\CommentTok{#> Mean Per-Class Error:  0.377}
\CommentTok{#> AUC:  0.669}
\CommentTok{#> pr_auc:  0.32}
\CommentTok{#> Gini:  0.339}
\CommentTok{#> }
\CommentTok{#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:}
\CommentTok{#>            0    1    Error         Rate}
\CommentTok{#> 0      13172 6819 0.341103  =6819/19991}
\CommentTok{#> 1       1891 2699 0.411983   =1891/4590}
\CommentTok{#> Totals 15063 9518 0.354339  =8710/24581}
\CommentTok{#> }
\CommentTok{#> Maximum Metrics: Maximum metrics at their respective thresholds}
\CommentTok{#>                         metric threshold    value idx}
\CommentTok{#> 1                       max f1  0.196407 0.382620 225}
\CommentTok{#> 2                       max f2  0.092270 0.549691 331}
\CommentTok{#> 3                 max f0point5  0.291396 0.349342 144}
\CommentTok{#> 4                 max accuracy  0.555908 0.813840  20}
\CommentTok{#> 5                max precision  0.651522 0.785714   5}
\CommentTok{#> 6                   max recall  0.004212 1.000000 398}
\CommentTok{#> 7              max specificity  0.711667 0.999950   0}
\CommentTok{#> 8             max absolute_mcc  0.229251 0.204236 194}
\CommentTok{#> 9   max min_per_class_accuracy  0.184594 0.619829 235}
\CommentTok{#> 10 max mean_per_class_accuracy  0.196407 0.623457 225}
\CommentTok{#> }
\CommentTok{#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`}

\CommentTok{# Retreive test set AUC}
\KeywordTok{h2o.auc}\NormalTok{(rf_perf1)  }\CommentTok{# 0.662266990734}
\CommentTok{#> [1] 0.663}
\KeywordTok{h2o.auc}\NormalTok{(rf_perf2)  }\CommentTok{# 0.66525468051}
\CommentTok{#> [1] 0.669}
\end{Highlighting}
\end{Shaded}

\hypertarget{cross-validate-performance}{%
\subsection{Cross-validate performance}\label{cross-validate-performance}}

Rather than using held-out test set to evaluate model performance, a user may wish
to estimate model performance using cross-validation. Using the RF algorithm
(with default model parameters) as an example, we demonstrate how to perform k-fold
cross-validation using H2O. No custom code or loops are required, you simply specify
the number of desired folds in the nfolds argument.
Since we are not going to use a test set here, we can use the original (full) dataset,
which we called data rather than the subsampled \texttt{train} dataset. Note that this will
take approximately k (nfolds) times longer than training a single RF model, since it
will train k models in the cross-validation process (trained on n(k-1)/k rows), in
addition to the final model trained on the full training\_frame dataset with n rows.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rf_fit3 <-}\StringTok{ }\KeywordTok{h2o.randomForest}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ x,}
                            \DataTypeTok{y =}\NormalTok{ y,}
                            \DataTypeTok{training_frame =}\NormalTok{ train,}
                            \DataTypeTok{model_id =} \StringTok{"rf_fit3"}\NormalTok{,}
                            \DataTypeTok{seed =} \DecValTok{1}\NormalTok{,}
                            \DataTypeTok{nfolds =} \DecValTok{5}\NormalTok{)}
\CommentTok{#> }
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|}\StringTok{                                                                 }\ErrorTok{|}\StringTok{   }\DecValTok{0}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=}\StringTok{                                                                }\ErrorTok{|}\StringTok{   }\DecValTok{1}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==}\StringTok{                                                               }\ErrorTok{|}\StringTok{   }\DecValTok{2}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==}\StringTok{                                                               }\ErrorTok{|}\StringTok{   }\DecValTok{4}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|===}\StringTok{                                                              }\ErrorTok{|}\StringTok{   }\DecValTok{5}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=====}\StringTok{                                                            }\ErrorTok{|}\StringTok{   }\DecValTok{7}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|======}\StringTok{                                                           }\ErrorTok{|}\StringTok{  }\DecValTok{10}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|========}\StringTok{                                                         }\ErrorTok{|}\StringTok{  }\DecValTok{12}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=========}\StringTok{                                                        }\ErrorTok{|}\StringTok{  }\DecValTok{14}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|===========}\StringTok{                                                      }\ErrorTok{|}\StringTok{  }\DecValTok{16}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|===========}\StringTok{                                                      }\ErrorTok{|}\StringTok{  }\DecValTok{18}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|============}\StringTok{                                                     }\ErrorTok{|}\StringTok{  }\DecValTok{19}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=============}\StringTok{                                                    }\ErrorTok{|}\StringTok{  }\DecValTok{20}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==============}\StringTok{                                                   }\ErrorTok{|}\StringTok{  }\DecValTok{22}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|===============}\StringTok{                                                  }\ErrorTok{|}\StringTok{  }\DecValTok{24}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=================}\StringTok{                                                }\ErrorTok{|}\StringTok{  }\DecValTok{26}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==================}\StringTok{                                               }\ErrorTok{|}\StringTok{  }\DecValTok{28}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|====================}\StringTok{                                             }\ErrorTok{|}\StringTok{  }\DecValTok{31}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=====================}\StringTok{                                            }\ErrorTok{|}\StringTok{  }\DecValTok{32}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|======================}\StringTok{                                           }\ErrorTok{|}\StringTok{  }\DecValTok{34}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=======================}\StringTok{                                          }\ErrorTok{|}\StringTok{  }\DecValTok{35}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|========================}\StringTok{                                         }\ErrorTok{|}\StringTok{  }\DecValTok{37}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=========================}\StringTok{                                        }\ErrorTok{|}\StringTok{  }\DecValTok{38}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==========================}\StringTok{                                       }\ErrorTok{|}\StringTok{  }\DecValTok{39}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|===========================}\StringTok{                                      }\ErrorTok{|}\StringTok{  }\DecValTok{42}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=============================}\StringTok{                                    }\ErrorTok{|}\StringTok{  }\DecValTok{44}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==============================}\StringTok{                                   }\ErrorTok{|}\StringTok{  }\DecValTok{46}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|===============================}\StringTok{                                  }\ErrorTok{|}\StringTok{  }\DecValTok{48}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|================================}\StringTok{                                 }\ErrorTok{|}\StringTok{  }\DecValTok{50}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=================================}\StringTok{                                }\ErrorTok{|}\StringTok{  }\DecValTok{51}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==================================}\StringTok{                               }\ErrorTok{|}\StringTok{  }\DecValTok{53}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|===================================}\StringTok{                              }\ErrorTok{|}\StringTok{  }\DecValTok{54}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|====================================}\StringTok{                             }\ErrorTok{|}\StringTok{  }\DecValTok{55}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=====================================}\StringTok{                            }\ErrorTok{|}\StringTok{  }\DecValTok{57}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=======================================}\StringTok{                          }\ErrorTok{|}\StringTok{  }\DecValTok{60}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|========================================}\StringTok{                         }\ErrorTok{|}\StringTok{  }\DecValTok{62}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==========================================}\StringTok{                       }\ErrorTok{|}\StringTok{  }\DecValTok{64}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|===========================================}\StringTok{                      }\ErrorTok{|}\StringTok{  }\DecValTok{66}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|============================================}\StringTok{                     }\ErrorTok{|}\StringTok{  }\DecValTok{67}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=============================================}\StringTok{                    }\ErrorTok{|}\StringTok{  }\DecValTok{69}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==============================================}\StringTok{                   }\ErrorTok{|}\StringTok{  }\DecValTok{70}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==============================================}\StringTok{                   }\ErrorTok{|}\StringTok{  }\DecValTok{71}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|===============================================}\StringTok{                  }\ErrorTok{|}\StringTok{  }\DecValTok{73}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=================================================}\StringTok{                }\ErrorTok{|}\StringTok{  }\DecValTok{75}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==================================================}\StringTok{               }\ErrorTok{|}\StringTok{  }\DecValTok{78}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|====================================================}\StringTok{             }\ErrorTok{|}\StringTok{  }\DecValTok{80}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=====================================================}\StringTok{            }\ErrorTok{|}\StringTok{  }\DecValTok{82}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|======================================================}\StringTok{           }\ErrorTok{|}\StringTok{  }\DecValTok{83}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=======================================================}\StringTok{          }\ErrorTok{|}\StringTok{  }\DecValTok{84}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|========================================================}\StringTok{         }\ErrorTok{|}\StringTok{  }\DecValTok{86}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=========================================================}\StringTok{        }\ErrorTok{|}\StringTok{  }\DecValTok{88}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==========================================================}\StringTok{       }\ErrorTok{|}\StringTok{  }\DecValTok{90}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|============================================================}\StringTok{     }\ErrorTok{|}\StringTok{  }\DecValTok{92}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==============================================================}\StringTok{   }\ErrorTok{|}\StringTok{  }\DecValTok{95}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|===============================================================}\StringTok{  }\ErrorTok{|}\StringTok{  }\DecValTok{97}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=================================================================|}\StringTok{  }\DecValTok{99}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=================================================================|}\StringTok{ }\DecValTok{100}\NormalTok{%}
\end{Highlighting}
\end{Shaded}

To evaluate the cross-validated AUC, do the following:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# To evaluate the cross-validated AUC, do the following:}
\KeywordTok{h2o.auc}\NormalTok{(rf_fit3, }\DataTypeTok{xval =} \OtherTok{TRUE}\NormalTok{)  }\CommentTok{# 0.661201482614}
\CommentTok{#> [1] 0.659}
\end{Highlighting}
\end{Shaded}

\hypertarget{gradient-boosting-machine-gbm}{%
\section{Gradient Boosting Machine (GBM)}\label{gradient-boosting-machine-gbm}}

H2O's Gradient Boosting Machine (GBM) offers a Stochastic GBM, which can
increase performance quite a bit compared to the original GBM implementation.

Now we will train a basic GBM model
The GBM model will infer the response distribution from the response encoding if not specified explicitly through the \texttt{distribution} argument. A seed is required for reproducibility.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gbm_fit1 <-}\StringTok{ }\KeywordTok{h2o.gbm}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ x,}
                    \DataTypeTok{y =}\NormalTok{ y,}
                    \DataTypeTok{training_frame =}\NormalTok{ train,}
                    \DataTypeTok{model_id =} \StringTok{"gbm_fit1"}\NormalTok{,}
                    \DataTypeTok{seed =} \DecValTok{1}\NormalTok{)}
\CommentTok{#> }
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|}\StringTok{                                                                 }\ErrorTok{|}\StringTok{   }\DecValTok{0}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=============}\StringTok{                                                    }\ErrorTok{|}\StringTok{  }\DecValTok{20}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=============================}\StringTok{                                    }\ErrorTok{|}\StringTok{  }\DecValTok{44}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==============================================}\StringTok{                   }\ErrorTok{|}\StringTok{  }\DecValTok{70}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==============================================================}\StringTok{   }\ErrorTok{|}\StringTok{  }\DecValTok{96}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=================================================================|}\StringTok{ }\DecValTok{100}\NormalTok{%}
\end{Highlighting}
\end{Shaded}

Next we will increase the number of trees used in the GBM by setting \texttt{ntrees=500}.
The default number of trees in an H2O GBM is 50, so this GBM will trained using ten times
the default. Increasing the number of trees in a GBM is one way to increase performance
of the model, however, you have to be careful not to overfit your model to the training data
by using too many trees. To automatically find the optimal number of trees, you must use
H2O's early stopping functionality. This example will not do that, however, the following \# example will.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gbm_fit2 <-}\StringTok{ }\KeywordTok{h2o.gbm}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ x,}
                    \DataTypeTok{y =}\NormalTok{ y,}
                    \DataTypeTok{training_frame =}\NormalTok{ train,}
                    \DataTypeTok{model_id =} \StringTok{"gbm_fit2"}\NormalTok{,}
                    \CommentTok{#validation_frame = valid,  #only used if stopping_rounds > 0}
                    \DataTypeTok{ntrees =} \DecValTok{500}\NormalTok{,}
                    \DataTypeTok{seed =} \DecValTok{1}\NormalTok{)}
\CommentTok{#> }
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|}\StringTok{                                                                 }\ErrorTok{|}\StringTok{   }\DecValTok{0}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==}\StringTok{                                                               }\ErrorTok{|}\StringTok{   }\DecValTok{3}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|====}\StringTok{                                                             }\ErrorTok{|}\StringTok{   }\DecValTok{5}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=====}\StringTok{                                                            }\ErrorTok{|}\StringTok{   }\DecValTok{8}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=======}\StringTok{                                                          }\ErrorTok{|}\StringTok{  }\DecValTok{11}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==========}\StringTok{                                                       }\ErrorTok{|}\StringTok{  }\DecValTok{16}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==============}\StringTok{                                                   }\ErrorTok{|}\StringTok{  }\DecValTok{21}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|===================}\StringTok{                                              }\ErrorTok{|}\StringTok{  }\DecValTok{29}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|===============================}\StringTok{                                  }\ErrorTok{|}\StringTok{  }\DecValTok{48}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|============================================}\StringTok{                     }\ErrorTok{|}\StringTok{  }\DecValTok{68}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=========================================================}\StringTok{        }\ErrorTok{|}\StringTok{  }\DecValTok{88}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=================================================================|}\StringTok{ }\DecValTok{100}\NormalTok{%}
\end{Highlighting}
\end{Shaded}

We will again set \texttt{ntrees\ =\ 500}, however, this time we will use early stopping in order to
prevent overfitting (from too many trees). All of H2O's algorithms have early stopping available, however early stopping is not enabled by default (with the exception of Deep Learning).
There are several parameters that should be used to control early stopping. The three that are common to all the algorithms are: \texttt{stopping\_rounds}, \texttt{stopping\_metric} and \texttt{stopping\_tolerance}.
The stopping metric is the metric by which you'd like to measure performance, and so we will choose AUC here. The \texttt{score\_tree\_interval} is a parameter specific to the Random Forest model and the GBM.
Setting \texttt{score\_tree\_interval\ =\ 5} will score the model after every five trees. The parameters we have set below specify that the model will stop training after there have been three scoring intervals where the AUC has not increased more than 0.0005. Since we have specified a validation frame, the stopping tolerance will be computed on validation AUC rather than training AUC.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gbm_fit3 <-}\StringTok{ }\KeywordTok{h2o.gbm}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ x,}
                    \DataTypeTok{y =}\NormalTok{ y,}
                    \DataTypeTok{training_frame =}\NormalTok{ train,}
                    \DataTypeTok{model_id =} \StringTok{"gbm_fit3"}\NormalTok{,}
                    \DataTypeTok{validation_frame =}\NormalTok{ valid,  }\CommentTok{#only used if stopping_rounds > 0}
                    \DataTypeTok{ntrees =} \DecValTok{500}\NormalTok{,}
                    \DataTypeTok{score_tree_interval =} \DecValTok{5}\NormalTok{,      }\CommentTok{#used for early stopping}
                    \DataTypeTok{stopping_rounds =} \DecValTok{3}\NormalTok{,          }\CommentTok{#used for early stopping}
                    \DataTypeTok{stopping_metric =} \StringTok{"AUC"}\NormalTok{,      }\CommentTok{#used for early stopping}
                    \DataTypeTok{stopping_tolerance =} \FloatTok{0.0005}\NormalTok{,  }\CommentTok{#used for early stopping}
                    \DataTypeTok{seed =} \DecValTok{1}\NormalTok{)}
\CommentTok{#> }
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|}\StringTok{                                                                 }\ErrorTok{|}\StringTok{   }\DecValTok{0}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|===}\StringTok{                                                              }\ErrorTok{|}\StringTok{   }\DecValTok{4}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=====}\StringTok{                                                            }\ErrorTok{|}\StringTok{   }\DecValTok{8}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=======}\StringTok{                                                          }\ErrorTok{|}\StringTok{  }\DecValTok{11}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==========}\StringTok{                                                       }\ErrorTok{|}\StringTok{  }\DecValTok{15}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|============}\StringTok{                                                     }\ErrorTok{|}\StringTok{  }\DecValTok{19}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=================================================================|}\StringTok{ }\DecValTok{100}\NormalTok{%}
\end{Highlighting}
\end{Shaded}

Let's compare the performance of the two GBMs

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Let's compare the performance of the two GBMs}
\NormalTok{gbm_perf1 <-}\StringTok{ }\KeywordTok{h2o.performance}\NormalTok{(}\DataTypeTok{model =}\NormalTok{ gbm_fit1,}
                             \DataTypeTok{newdata =}\NormalTok{ test)}
\NormalTok{gbm_perf2 <-}\StringTok{ }\KeywordTok{h2o.performance}\NormalTok{(}\DataTypeTok{model =}\NormalTok{ gbm_fit2,}
                             \DataTypeTok{newdata =}\NormalTok{ test)}
\NormalTok{gbm_perf3 <-}\StringTok{ }\KeywordTok{h2o.performance}\NormalTok{(}\DataTypeTok{model =}\NormalTok{ gbm_fit3,}
                             \DataTypeTok{newdata =}\NormalTok{ test)}

\CommentTok{# Print model performance}
\NormalTok{gbm_perf1}
\CommentTok{#> H2OBinomialMetrics: gbm}
\CommentTok{#> }
\CommentTok{#> MSE:  0.141}
\CommentTok{#> RMSE:  0.376}
\CommentTok{#> LogLoss:  0.448}
\CommentTok{#> Mean Per-Class Error:  0.367}
\CommentTok{#> AUC:  0.684}
\CommentTok{#> pr_auc:  0.332}
\CommentTok{#> Gini:  0.368}
\CommentTok{#> }
\CommentTok{#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:}
\CommentTok{#>            0     1    Error         Rate}
\CommentTok{#> 0      12143  7848 0.392577  =7848/19991}
\CommentTok{#> 1       1571  3019 0.342266   =1571/4590}
\CommentTok{#> Totals 13714 10867 0.383182  =9419/24581}
\CommentTok{#> }
\CommentTok{#> Maximum Metrics: Maximum metrics at their respective thresholds}
\CommentTok{#>                         metric threshold    value idx}
\CommentTok{#> 1                       max f1  0.171139 0.390632 251}
\CommentTok{#> 2                       max f2  0.108885 0.560103 328}
\CommentTok{#> 3                 max f0point5  0.285149 0.356430 145}
\CommentTok{#> 4                 max accuracy  0.510077 0.814410  27}
\CommentTok{#> 5                max precision  0.601699 0.636364   8}
\CommentTok{#> 6                   max recall  0.037543 1.000000 398}
\CommentTok{#> 7              max specificity  0.719189 0.999950   0}
\CommentTok{#> 8             max absolute_mcc  0.220471 0.215401 199}
\CommentTok{#> 9   max min_per_class_accuracy  0.176689 0.628540 244}
\CommentTok{#> 10 max mean_per_class_accuracy  0.170225 0.632624 252}
\CommentTok{#> }
\CommentTok{#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`}
\NormalTok{gbm_perf2}
\CommentTok{#> H2OBinomialMetrics: gbm}
\CommentTok{#> }
\CommentTok{#> MSE:  0.142}
\CommentTok{#> RMSE:  0.376}
\CommentTok{#> LogLoss:  0.449}
\CommentTok{#> Mean Per-Class Error:  0.367}
\CommentTok{#> AUC:  0.684}
\CommentTok{#> pr_auc:  0.329}
\CommentTok{#> Gini:  0.368}
\CommentTok{#> }
\CommentTok{#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:}
\CommentTok{#>            0    1    Error         Rate}
\CommentTok{#> 0      13661 6330 0.316642  =6330/19991}
\CommentTok{#> 1       1918 2672 0.417865   =1918/4590}
\CommentTok{#> Totals 15579 9002 0.335544  =8248/24581}
\CommentTok{#> }
\CommentTok{#> Maximum Metrics: Maximum metrics at their respective thresholds}
\CommentTok{#>                         metric threshold    value idx}
\CommentTok{#> 1                       max f1  0.189615 0.393172 234}
\CommentTok{#> 2                       max f2  0.096969 0.558918 333}
\CommentTok{#> 3                 max f0point5  0.278776 0.359560 160}
\CommentTok{#> 4                 max accuracy  0.521287 0.814287  37}
\CommentTok{#> 5                max precision  0.901295 1.000000   0}
\CommentTok{#> 6                   max recall  0.018504 1.000000 398}
\CommentTok{#> 7              max specificity  0.901295 1.000000   0}
\CommentTok{#> 8             max absolute_mcc  0.231089 0.217255 196}
\CommentTok{#> 9   max min_per_class_accuracy  0.174914 0.630834 249}
\CommentTok{#> 10 max mean_per_class_accuracy  0.156944 0.633792 267}
\CommentTok{#> }
\CommentTok{#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`}
\NormalTok{gbm_perf3}
\CommentTok{#> H2OBinomialMetrics: gbm}
\CommentTok{#> }
\CommentTok{#> MSE:  0.141}
\CommentTok{#> RMSE:  0.376}
\CommentTok{#> LogLoss:  0.448}
\CommentTok{#> Mean Per-Class Error:  0.367}
\CommentTok{#> AUC:  0.684}
\CommentTok{#> pr_auc:  0.331}
\CommentTok{#> Gini:  0.369}
\CommentTok{#> }
\CommentTok{#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:}
\CommentTok{#>            0    1    Error         Rate}
\CommentTok{#> 0      13652 6339 0.317093  =6339/19991}
\CommentTok{#> 1       1917 2673 0.417647   =1917/4590}
\CommentTok{#> Totals 15569 9012 0.335869  =8256/24581}
\CommentTok{#> }
\CommentTok{#> Maximum Metrics: Maximum metrics at their respective thresholds}
\CommentTok{#>                         metric threshold    value idx}
\CommentTok{#> 1                       max f1  0.189870 0.393030 234}
\CommentTok{#> 2                       max f2  0.109837 0.559580 321}
\CommentTok{#> 3                 max f0point5  0.294571 0.356508 148}
\CommentTok{#> 4                 max accuracy  0.510869 0.814572  40}
\CommentTok{#> 5                max precision  0.797620 1.000000   0}
\CommentTok{#> 6                   max recall  0.026373 1.000000 397}
\CommentTok{#> 7              max specificity  0.797620 1.000000   0}
\CommentTok{#> 8             max absolute_mcc  0.231530 0.218255 197}
\CommentTok{#> 9   max min_per_class_accuracy  0.176566 0.631808 248}
\CommentTok{#> 10 max mean_per_class_accuracy  0.175468 0.633400 249}
\CommentTok{#> }
\CommentTok{#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`}

\CommentTok{# Retreive test set AUC}
\KeywordTok{h2o.auc}\NormalTok{(gbm_perf1)  }\CommentTok{# 0.682765594191}
\CommentTok{#> [1] 0.684}
\KeywordTok{h2o.auc}\NormalTok{(gbm_perf2)  }\CommentTok{# 0.671854616713}
\CommentTok{#> [1] 0.684}
\KeywordTok{h2o.auc}\NormalTok{(gbm_perf3)  }\CommentTok{# 0.68309902855}
\CommentTok{#> [1] 0.684}
\end{Highlighting}
\end{Shaded}

To examine the scoring history, use the \texttt{scoring\_history} method on a trained model.
If \texttt{score\_tree\_interval} is not specified, it will score at various intervals, as we can
see for \texttt{h2o.scoreHistory()} below. However, regular 5-tree intervals are used
for \texttt{h2o.scoreHistory()}.
The \texttt{gbm\_fit2} was trained only using a training set (no validation set), so the scoring
history is calculated for training set performance metrics only.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{h2o.scoreHistory}\NormalTok{(gbm_fit2)}
\CommentTok{#> Scoring History: }
\CommentTok{#>             timestamp   duration number_of_trees training_rmse}
\CommentTok{#> 1 2019-09-20 14:19:45  0.002 sec               0       0.38563}
\CommentTok{#> 2 2019-09-20 14:19:45  0.146 sec               1       0.38370}
\CommentTok{#> 3 2019-09-20 14:19:45  0.211 sec               2       0.38206}
\CommentTok{#> 4 2019-09-20 14:19:45  0.258 sec               3       0.38069}
\CommentTok{#> 5 2019-09-20 14:19:45  0.299 sec               4       0.37954}
\CommentTok{#>   training_logloss training_auc training_pr_auc training_lift}
\CommentTok{#> 1          0.47403      0.50000         0.00000       1.00000}
\CommentTok{#> 2          0.46913      0.65779         0.30174       2.68330}
\CommentTok{#> 3          0.46512      0.66583         0.31164       2.79399}
\CommentTok{#> 4          0.46184      0.66851         0.31500       2.97100}
\CommentTok{#> 5          0.45912      0.67011         0.31822       2.97544}
\CommentTok{#>   training_classification_error}
\CommentTok{#> 1                       0.81825}
\CommentTok{#> 2                       0.40069}
\CommentTok{#> 3                       0.33325}
\CommentTok{#> 4                       0.34475}
\CommentTok{#> 5                       0.33180}
\CommentTok{#> }
\CommentTok{#> ---}
\CommentTok{#>              timestamp   duration number_of_trees training_rmse}
\CommentTok{#> 48 2019-09-20 14:19:49  3.719 sec              47       0.36727}
\CommentTok{#> 49 2019-09-20 14:19:49  3.805 sec              48       0.36717}
\CommentTok{#> 50 2019-09-20 14:19:49  3.886 sec              49       0.36709}
\CommentTok{#> 51 2019-09-20 14:19:49  3.967 sec              50       0.36699}
\CommentTok{#> 52 2019-09-20 14:19:53  7.983 sec             223       0.36129}
\CommentTok{#> 53 2019-09-20 14:19:56 10.802 sec             500       0.36129}
\CommentTok{#>    training_logloss training_auc training_pr_auc training_lift}
\CommentTok{#> 48          0.43065      0.71457         0.37309       3.66973}
\CommentTok{#> 49          0.43044      0.71493         0.37382       3.65538}
\CommentTok{#> 50          0.43025      0.71524         0.37443       3.68408}
\CommentTok{#> 51          0.43002      0.71568         0.37493       3.66016}
\CommentTok{#> 52          0.41770      0.74024         0.41866       4.25823}
\CommentTok{#> 53          0.41770      0.74024         0.41866       4.25823}
\CommentTok{#>    training_classification_error}
\CommentTok{#> 48                       0.28870}
\CommentTok{#> 49                       0.30103}
\CommentTok{#> 50                       0.30056}
\CommentTok{#> 51                       0.30179}
\CommentTok{#> 52                       0.25719}
\CommentTok{#> 53                       0.25719}
\end{Highlighting}
\end{Shaded}

When early stopping is used, we see that training stopped at 105 trees instead of the full 500. Since we used a validation set in \texttt{gbm\_fit3}, both training and validation performance metrics are stored in the scoring history object. Take a look at the validation AUC to observe that the correct stopping tolerance was enforced.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{h2o.scoreHistory}\NormalTok{(gbm_fit3)}
\CommentTok{#> Scoring History: }
\CommentTok{#>              timestamp   duration number_of_trees training_rmse}
\CommentTok{#> 1  2019-09-20 14:19:56  0.004 sec               0       0.38563}
\CommentTok{#> 2  2019-09-20 14:19:57  0.288 sec               5       0.37853}
\CommentTok{#> 3  2019-09-20 14:19:57  0.541 sec              10       0.37508}
\CommentTok{#> 4  2019-09-20 14:19:57  0.811 sec              15       0.37307}
\CommentTok{#> 5  2019-09-20 14:19:57  1.076 sec              20       0.37152}
\CommentTok{#> 6  2019-09-20 14:19:58  1.336 sec              25       0.37041}
\CommentTok{#> 7  2019-09-20 14:19:58  1.597 sec              30       0.36947}
\CommentTok{#> 8  2019-09-20 14:19:58  1.839 sec              35       0.36877}
\CommentTok{#> 9  2019-09-20 14:19:59  2.102 sec              40       0.36808}
\CommentTok{#> 10 2019-09-20 14:19:59  2.351 sec              45       0.36748}
\CommentTok{#> 11 2019-09-20 14:19:59  2.748 sec              50       0.36699}
\CommentTok{#> 12 2019-09-20 14:19:59  3.007 sec              55       0.36651}
\CommentTok{#> 13 2019-09-20 14:20:00  3.266 sec              60       0.36607}
\CommentTok{#> 14 2019-09-20 14:20:00  3.519 sec              65       0.36574}
\CommentTok{#> 15 2019-09-20 14:20:00  3.785 sec              70       0.36531}
\CommentTok{#> 16 2019-09-20 14:20:00  4.038 sec              75       0.36503}
\CommentTok{#> 17 2019-09-20 14:20:01  4.302 sec              80       0.36460}
\CommentTok{#> 18 2019-09-20 14:20:01  4.571 sec              85       0.36426}
\CommentTok{#> 19 2019-09-20 14:20:01  4.828 sec              90       0.36394}
\CommentTok{#> 20 2019-09-20 14:20:02  5.116 sec              95       0.36362}
\CommentTok{#>    training_logloss training_auc training_pr_auc training_lift}
\CommentTok{#> 1           0.47403      0.50000         0.00000       1.00000}
\CommentTok{#> 2           0.45676      0.67362         0.32194       3.04518}
\CommentTok{#> 3           0.44884      0.68117         0.33330       3.19128}
\CommentTok{#> 4           0.44424      0.68708         0.34105       3.30132}
\CommentTok{#> 5           0.44060      0.69498         0.34930       3.46878}
\CommentTok{#> 6           0.43800      0.69983         0.35479       3.47356}
\CommentTok{#> 7           0.43578      0.70425         0.36014       3.48792}
\CommentTok{#> 8           0.43410      0.70746         0.36373       3.53576}
\CommentTok{#> 9           0.43252      0.71082         0.36837       3.61710}
\CommentTok{#> 10          0.43116      0.71346         0.37174       3.66495}
\CommentTok{#> 11          0.43002      0.71568         0.37493       3.66016}
\CommentTok{#> 12          0.42899      0.71765         0.37852       3.70801}
\CommentTok{#> 13          0.42801      0.71953         0.38157       3.73193}
\CommentTok{#> 14          0.42726      0.72093         0.38411       3.78934}
\CommentTok{#> 15          0.42632      0.72277         0.38733       3.82762}
\CommentTok{#> 16          0.42572      0.72396         0.38955       3.85154}
\CommentTok{#> 17          0.42477      0.72596         0.39282       3.95202}
\CommentTok{#> 18          0.42407      0.72713         0.39550       3.99508}
\CommentTok{#> 19          0.42338      0.72852         0.39819       4.02857}
\CommentTok{#> 20          0.42272      0.72984         0.40069       4.02378}
\CommentTok{#>    training_classification_error validation_rmse validation_logloss}
\CommentTok{#> 1                        0.81825         0.38864            0.47953}
\CommentTok{#> 2                        0.32117         0.38233            0.46398}
\CommentTok{#> 3                        0.32202         0.37958            0.45742}
\CommentTok{#> 4                        0.32027         0.37828            0.45428}
\CommentTok{#> 5                        0.33371         0.37739            0.45210}
\CommentTok{#> 6                        0.32537         0.37676            0.45053}
\CommentTok{#> 7                        0.29722         0.37636            0.44949}
\CommentTok{#> 8                        0.29544         0.37604            0.44866}
\CommentTok{#> 9                        0.28871         0.37587            0.44818}
\CommentTok{#> 10                       0.30181         0.37574            0.44781}
\CommentTok{#> 11                       0.30179         0.37560            0.44744}
\CommentTok{#> 12                       0.29464         0.37552            0.44718}
\CommentTok{#> 13                       0.30343         0.37547            0.44703}
\CommentTok{#> 14                       0.28692         0.37543            0.44694}
\CommentTok{#> 15                       0.28579         0.37536            0.44676}
\CommentTok{#> 16                       0.26903         0.37536            0.44673}
\CommentTok{#> 17                       0.28476         0.37534            0.44664}
\CommentTok{#> 18                       0.26950         0.37537            0.44671}
\CommentTok{#> 19                       0.27036         0.37538            0.44671}
\CommentTok{#> 20                       0.26573         0.37540            0.44673}
\CommentTok{#>    validation_auc validation_pr_auc validation_lift}
\CommentTok{#> 1         0.50000           0.00000         1.00000}
\CommentTok{#> 2         0.66168           0.30421         2.75098}
\CommentTok{#> 3         0.66767           0.30959         2.68582}
\CommentTok{#> 4         0.67061           0.31287         2.70784}
\CommentTok{#> 5         0.67426           0.31757         2.79590}
\CommentTok{#> 6         0.67685           0.32121         2.99403}
\CommentTok{#> 7         0.67866           0.32312         2.97202}
\CommentTok{#> 8         0.68006           0.32477         3.03806}
\CommentTok{#> 9         0.68113           0.32493         2.90597}
\CommentTok{#> 10        0.68183           0.32549         2.88396}
\CommentTok{#> 11        0.68251           0.32659         2.86194}
\CommentTok{#> 12        0.68326           0.32626         2.86194}
\CommentTok{#> 13        0.68354           0.32686         2.88396}
\CommentTok{#> 14        0.68364           0.32705         2.83993}
\CommentTok{#> 15        0.68405           0.32754         2.97202}
\CommentTok{#> 16        0.68423           0.32747         2.88396}
\CommentTok{#> 17        0.68454           0.32735         2.86194}
\CommentTok{#> 18        0.68431           0.32727         2.83993}
\CommentTok{#> 19        0.68434           0.32718         2.95000}
\CommentTok{#> 20        0.68436           0.32678         2.92799}
\CommentTok{#>    validation_classification_error}
\CommentTok{#> 1                          0.81460}
\CommentTok{#> 2                          0.35387}
\CommentTok{#> 3                          0.35285}
\CommentTok{#> 4                          0.39028}
\CommentTok{#> 5                          0.36770}
\CommentTok{#> 6                          0.35240}
\CommentTok{#> 7                          0.34848}
\CommentTok{#> 8                          0.34386}
\CommentTok{#> 9                          0.34807}
\CommentTok{#> 10                         0.38681}
\CommentTok{#> 11                         0.33774}
\CommentTok{#> 12                         0.34215}
\CommentTok{#> 13                         0.34431}
\CommentTok{#> 14                         0.34146}
\CommentTok{#> 15                         0.34329}
\CommentTok{#> 16                         0.33872}
\CommentTok{#> 17                         0.34276}
\CommentTok{#> 18                         0.34256}
\CommentTok{#> 19                         0.34289}
\CommentTok{#> 20                         0.35097}
\end{Highlighting}
\end{Shaded}

Look at scoring history for third GBM model

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Look at scoring history for third GBM model}
\KeywordTok{plot}\NormalTok{(gbm_fit3, }
     \DataTypeTok{timestep =} \StringTok{"number_of_trees"}\NormalTok{, }
     \DataTypeTok{metric =} \StringTok{"AUC"}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(gbm_fit3, }
     \DataTypeTok{timestep =} \StringTok{"number_of_trees"}\NormalTok{, }
     \DataTypeTok{metric =} \StringTok{"logloss"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{algo-classification_907-bad_loans-h2o_files/figure-latex/plot-gbm-fit3-1} \includegraphics[width=0.7\linewidth]{algo-classification_907-bad_loans-h2o_files/figure-latex/plot-gbm-fit3-2} \end{center}

\hypertarget{deep-learning}{%
\section{Deep Learning}\label{deep-learning}}

H2O's Deep Learning algorithm is a multilayer feed-forward artificial neural network.
It can also be used to train an autoencoder. In this example we will train
a standard supervised prediction model.

\hypertarget{train-a-default-dl}{%
\subsection{Train a default DL}\label{train-a-default-dl}}

First we will train a basic DL model with default parameters. The DL model will infer the response distribution from the response encoding if it is not specified explicitly through the \texttt{distribution} argument. H2O's DL will not be reproducible if it is run on more than a single core, so in this example, the performance metrics below may vary slightly from what you see on your machine.
In H2O's DL, early stopping is enabled by default, so below, it will use the training set and default stopping parameters to perform early stopping.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dl_fit1 <-}\StringTok{ }\KeywordTok{h2o.deeplearning}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ x,}
                            \DataTypeTok{y =}\NormalTok{ y,}
                            \DataTypeTok{training_frame =}\NormalTok{ train,}
                            \DataTypeTok{model_id =} \StringTok{"dl_fit1"}\NormalTok{,}
                            \DataTypeTok{seed =} \DecValTok{1}\NormalTok{)}
\CommentTok{#> }
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|}\StringTok{                                                                 }\ErrorTok{|}\StringTok{   }\DecValTok{0}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|======}\StringTok{                                                           }\ErrorTok{|}\StringTok{   }\DecValTok{9}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|===========}\StringTok{                                                      }\ErrorTok{|}\StringTok{  }\DecValTok{17}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=================}\StringTok{                                                }\ErrorTok{|}\StringTok{  }\DecValTok{26}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=======================}\StringTok{                                          }\ErrorTok{|}\StringTok{  }\DecValTok{35}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|============================}\StringTok{                                     }\ErrorTok{|}\StringTok{  }\DecValTok{44}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==================================}\StringTok{                               }\ErrorTok{|}\StringTok{  }\DecValTok{52}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|========================================}\StringTok{                         }\ErrorTok{|}\StringTok{  }\DecValTok{61}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=============================================}\StringTok{                    }\ErrorTok{|}\StringTok{  }\DecValTok{70}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|===================================================}\StringTok{              }\ErrorTok{|}\StringTok{  }\DecValTok{78}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=========================================================}\StringTok{        }\ErrorTok{|}\StringTok{  }\DecValTok{87}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==============================================================}\StringTok{   }\ErrorTok{|}\StringTok{  }\DecValTok{96}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=================================================================|}\StringTok{ }\DecValTok{100}\NormalTok{%}
\end{Highlighting}
\end{Shaded}

\hypertarget{train-a-dl-with-new-architecture-and-more-epochs.}{%
\subsection{Train a DL with new architecture and more epochs.}\label{train-a-dl-with-new-architecture-and-more-epochs.}}

Next we will increase the number of epochs used in the GBM by setting \texttt{epochs=20} (the default is 10).
Increasing the number of epochs in a deep neural net may increase performance of the model, however, you have to be careful not to overfit your model to your training data. To automatically find the optimal number of epochs, you must use H2O's early stopping functionality. Unlike the rest of the H2O algorithms, H2O's DL will use early stopping by default, so for comparison we will first turn off early stopping. We do this in the next example by setting \texttt{stopping\_rounds=0}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dl_fit2 <-}\StringTok{ }\KeywordTok{h2o.deeplearning}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ x,}
                            \DataTypeTok{y =}\NormalTok{ y,}
                            \DataTypeTok{training_frame =}\NormalTok{ train,}
                            \DataTypeTok{model_id =} \StringTok{"dl_fit2"}\NormalTok{,}
                            \CommentTok{#validation_frame = valid,  #only used if stopping_rounds > 0}
                            \DataTypeTok{epochs =} \DecValTok{20}\NormalTok{,}
                            \DataTypeTok{hidden=} \KeywordTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{,}\DecValTok{10}\NormalTok{),}
                            \DataTypeTok{stopping_rounds =} \DecValTok{0}\NormalTok{,  }\CommentTok{# disable early stopping}
                            \DataTypeTok{seed =} \DecValTok{1}\NormalTok{)}
\CommentTok{#> }
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|}\StringTok{                                                                 }\ErrorTok{|}\StringTok{   }\DecValTok{0}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|===========}\StringTok{                                                      }\ErrorTok{|}\StringTok{  }\DecValTok{17}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|============================}\StringTok{                                     }\ErrorTok{|}\StringTok{  }\DecValTok{44}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=============================================}\StringTok{                    }\ErrorTok{|}\StringTok{  }\DecValTok{70}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==============================================================}\StringTok{   }\ErrorTok{|}\StringTok{  }\DecValTok{96}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=================================================================|}\StringTok{ }\DecValTok{100}\NormalTok{%}
\end{Highlighting}
\end{Shaded}

\hypertarget{train-a-dl-with-early-stopping}{%
\subsection{Train a DL with early stopping}\label{train-a-dl-with-early-stopping}}

This example will use the same model parameters as \texttt{dl\_fit2}. This time, we will turn on
early stopping and specify the stopping criterion. We will also pass a validation set, as is recommended for early stopping.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dl_fit3 <-}\StringTok{ }\KeywordTok{h2o.deeplearning}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ x,}
                            \DataTypeTok{y =}\NormalTok{ y,}
                            \DataTypeTok{training_frame =}\NormalTok{ train,}
                            \DataTypeTok{model_id =} \StringTok{"dl_fit3"}\NormalTok{,}
                            \DataTypeTok{validation_frame =}\NormalTok{ valid,  }\CommentTok{#in DL, early stopping is on by default}
                            \DataTypeTok{epochs =} \DecValTok{20}\NormalTok{,}
                            \DataTypeTok{hidden =} \KeywordTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{,}\DecValTok{10}\NormalTok{),}
                            \DataTypeTok{score_interval =} \DecValTok{1}\NormalTok{,           }\CommentTok{#used for early stopping}
                            \DataTypeTok{stopping_rounds =} \DecValTok{3}\NormalTok{,          }\CommentTok{#used for early stopping}
                            \DataTypeTok{stopping_metric =} \StringTok{"AUC"}\NormalTok{,      }\CommentTok{#used for early stopping}
                            \DataTypeTok{stopping_tolerance =} \FloatTok{0.0005}\NormalTok{,  }\CommentTok{#used for early stopping}
                            \DataTypeTok{seed =} \DecValTok{1}\NormalTok{)}
\CommentTok{#> }
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|}\StringTok{                                                                 }\ErrorTok{|}\StringTok{   }\DecValTok{0}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==============}\StringTok{                                                   }\ErrorTok{|}\StringTok{  }\DecValTok{22}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|============================}\StringTok{                                     }\ErrorTok{|}\StringTok{  }\DecValTok{44}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=============================================}\StringTok{                    }\ErrorTok{|}\StringTok{  }\DecValTok{70}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==============================================================}\StringTok{   }\ErrorTok{|}\StringTok{  }\DecValTok{96}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=================================================================|}\StringTok{ }\DecValTok{100}\NormalTok{%}
\end{Highlighting}
\end{Shaded}

Let's compare the performance of the three DL models

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Let's compare the performance of the three DL models}
\NormalTok{dl_perf1 <-}\StringTok{ }\KeywordTok{h2o.performance}\NormalTok{(}\DataTypeTok{model =}\NormalTok{ dl_fit1,}
                            \DataTypeTok{newdata =}\NormalTok{ test)}
\NormalTok{dl_perf2 <-}\StringTok{ }\KeywordTok{h2o.performance}\NormalTok{(}\DataTypeTok{model =}\NormalTok{ dl_fit2,}
                            \DataTypeTok{newdata =}\NormalTok{ test)}
\NormalTok{dl_perf3 <-}\StringTok{ }\KeywordTok{h2o.performance}\NormalTok{(}\DataTypeTok{model =}\NormalTok{ dl_fit3,}
                            \DataTypeTok{newdata =}\NormalTok{ test)}

\CommentTok{# Print model performance}
\NormalTok{dl_perf1}
\CommentTok{#> H2OBinomialMetrics: deeplearning}
\CommentTok{#> }
\CommentTok{#> MSE:  0.143}
\CommentTok{#> RMSE:  0.378}
\CommentTok{#> LogLoss:  0.455}
\CommentTok{#> Mean Per-Class Error:  0.373}
\CommentTok{#> AUC:  0.677}
\CommentTok{#> pr_auc:  0.325}
\CommentTok{#> Gini:  0.354}
\CommentTok{#> }
\CommentTok{#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:}
\CommentTok{#>            0    1    Error         Rate}
\CommentTok{#> 0      12835 7156 0.357961  =7156/19991}
\CommentTok{#> 1       1783 2807 0.388453   =1783/4590}
\CommentTok{#> Totals 14618 9963 0.363655  =8939/24581}
\CommentTok{#> }
\CommentTok{#> Maximum Metrics: Maximum metrics at their respective thresholds}
\CommentTok{#>                         metric threshold    value idx}
\CommentTok{#> 1                       max f1  0.158250 0.385762 248}
\CommentTok{#> 2                       max f2  0.072876 0.560782 337}
\CommentTok{#> 3                 max f0point5  0.330120 0.349636 125}
\CommentTok{#> 4                 max accuracy  0.500168 0.814816  38}
\CommentTok{#> 5                max precision  0.869680 1.000000   0}
\CommentTok{#> 6                   max recall  0.007413 1.000000 397}
\CommentTok{#> 7              max specificity  0.869680 1.000000   0}
\CommentTok{#> 8             max absolute_mcc  0.174876 0.204549 234}
\CommentTok{#> 9   max min_per_class_accuracy  0.153375 0.625490 252}
\CommentTok{#> 10 max mean_per_class_accuracy  0.158250 0.626793 248}
\CommentTok{#> }
\CommentTok{#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`}
\NormalTok{dl_perf2}
\CommentTok{#> H2OBinomialMetrics: deeplearning}
\CommentTok{#> }
\CommentTok{#> MSE:  0.142}
\CommentTok{#> RMSE:  0.376}
\CommentTok{#> LogLoss:  0.449}
\CommentTok{#> Mean Per-Class Error:  0.367}
\CommentTok{#> AUC:  0.681}
\CommentTok{#> pr_auc:  0.33}
\CommentTok{#> Gini:  0.362}
\CommentTok{#> }
\CommentTok{#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:}
\CommentTok{#>            0    1    Error         Rate}
\CommentTok{#> 0      13202 6789 0.339603  =6789/19991}
\CommentTok{#> 1       1808 2782 0.393900   =1808/4590}
\CommentTok{#> Totals 15010 9571 0.349742  =8597/24581}
\CommentTok{#> }
\CommentTok{#> Maximum Metrics: Maximum metrics at their respective thresholds}
\CommentTok{#>                         metric threshold    value idx}
\CommentTok{#> 1                       max f1  0.196576 0.392910 227}
\CommentTok{#> 2                       max f2  0.127511 0.554906 297}
\CommentTok{#> 3                 max f0point5  0.294748 0.355638 146}
\CommentTok{#> 4                 max accuracy  0.515466 0.814694  35}
\CommentTok{#> 5                max precision  0.783239 1.000000   0}
\CommentTok{#> 6                   max recall  0.012191 1.000000 396}
\CommentTok{#> 7              max specificity  0.783239 1.000000   0}
\CommentTok{#> 8             max absolute_mcc  0.196576 0.212984 227}
\CommentTok{#> 9   max min_per_class_accuracy  0.189148 0.630834 235}
\CommentTok{#> 10 max mean_per_class_accuracy  0.191007 0.633403 233}
\CommentTok{#> }
\CommentTok{#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`}
\NormalTok{dl_perf3}
\CommentTok{#> H2OBinomialMetrics: deeplearning}
\CommentTok{#> }
\CommentTok{#> MSE:  0.143}
\CommentTok{#> RMSE:  0.378}
\CommentTok{#> LogLoss:  0.452}
\CommentTok{#> Mean Per-Class Error:  0.365}
\CommentTok{#> AUC:  0.683}
\CommentTok{#> pr_auc:  0.333}
\CommentTok{#> Gini:  0.366}
\CommentTok{#> }
\CommentTok{#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:}
\CommentTok{#>            0    1    Error         Rate}
\CommentTok{#> 0      13005 6986 0.349457  =6986/19991}
\CommentTok{#> 1       1743 2847 0.379739   =1743/4590}
\CommentTok{#> Totals 14748 9833 0.355112  =8729/24581}
\CommentTok{#> }
\CommentTok{#> Maximum Metrics: Maximum metrics at their respective thresholds}
\CommentTok{#>                         metric threshold    value idx}
\CommentTok{#> 1                       max f1  0.228083 0.394786 222}
\CommentTok{#> 2                       max f2  0.146546 0.556546 301}
\CommentTok{#> 3                 max f0point5  0.338461 0.361426 133}
\CommentTok{#> 4                 max accuracy  0.562225 0.814857  27}
\CommentTok{#> 5                max precision  0.744195 1.000000   0}
\CommentTok{#> 6                   max recall  0.009547 1.000000 398}
\CommentTok{#> 7              max specificity  0.744195 1.000000   0}
\CommentTok{#> 8             max absolute_mcc  0.243434 0.216859 209}
\CommentTok{#> 9   max min_per_class_accuracy  0.223531 0.633435 226}
\CommentTok{#> 10 max mean_per_class_accuracy  0.228083 0.635402 222}
\CommentTok{#> }
\CommentTok{#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`}

\CommentTok{# Retreive test set AUC}
\KeywordTok{h2o.auc}\NormalTok{(dl_perf1)  }\CommentTok{# 0.6774335}
\CommentTok{#> [1] 0.677}
\KeywordTok{h2o.auc}\NormalTok{(dl_perf2)  }\CommentTok{# 0.678446}
\CommentTok{#> [1] 0.681}
\KeywordTok{h2o.auc}\NormalTok{(dl_perf3)  }\CommentTok{# 0.6770498}
\CommentTok{#> [1] 0.683}
\end{Highlighting}
\end{Shaded}

Look at scoring history for third DL model

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Look at scoring history for third DL model}
\KeywordTok{plot}\NormalTok{(dl_fit3, }
     \DataTypeTok{timestep =} \StringTok{"epochs"}\NormalTok{, }
     \DataTypeTok{metric =} \StringTok{"AUC"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{algo-classification_907-bad_loans-h2o_files/figure-latex/unnamed-chunk-14-1} \end{center}

\hypertarget{scoring-history}{%
\subsection{Scoring history}\label{scoring-history}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Scoring history}
\KeywordTok{h2o.scoreHistory}\NormalTok{(dl_fit3)}
\CommentTok{#> Scoring History: }
\CommentTok{#>             timestamp   duration training_speed   epochs iterations}
\CommentTok{#> 1 2019-09-20 14:20:42  0.000 sec             NA  0.00000          0}
\CommentTok{#> 2 2019-09-20 14:20:43  0.380 sec 377328 obs/sec  0.87019          1}
\CommentTok{#> 3 2019-09-20 14:20:44  1.481 sec 544275 obs/sec  6.09129          7}
\CommentTok{#> 4 2019-09-20 14:20:45  2.609 sec 556052 obs/sec 11.31383         13}
\CommentTok{#> 5 2019-09-20 14:20:46  3.670 sec 571338 obs/sec 16.53733         19}
\CommentTok{#> 6 2019-09-20 14:20:47  4.361 sec 583091 obs/sec 20.01859         23}
\CommentTok{#> 7 2019-09-20 14:20:47  4.429 sec 582796 obs/sec 20.01859         23}
\CommentTok{#>          samples training_rmse training_logloss training_r2 training_auc}
\CommentTok{#> 1       0.000000            NA               NA          NA           NA}
\CommentTok{#> 2   99992.000000       0.38222          0.46751     0.03339      0.66379}
\CommentTok{#> 3  699938.000000       0.37765          0.45229     0.05635      0.67113}
\CommentTok{#> 4 1300050.000000       0.37930          0.45919     0.04809      0.67552}
\CommentTok{#> 5 1900272.000000       0.37809          0.45273     0.05418      0.67844}
\CommentTok{#> 6 2300296.000000       0.37881          0.45656     0.05056      0.67416}
\CommentTok{#> 7 2300296.000000       0.37809          0.45273     0.05418      0.67844}
\CommentTok{#>   training_pr_auc training_lift training_classification_error}
\CommentTok{#> 1              NA            NA                            NA}
\CommentTok{#> 2         0.31001       2.93921                       0.36812}
\CommentTok{#> 3         0.31705       2.66706                       0.35750}
\CommentTok{#> 4         0.31536       2.61263                       0.33445}
\CommentTok{#> 5         0.32043       2.72149                       0.37894}
\CommentTok{#> 6         0.31587       2.61263                       0.37561}
\CommentTok{#> 7         0.32043       2.72149                       0.37894}
\CommentTok{#>   validation_rmse validation_logloss validation_r2 validation_auc}
\CommentTok{#> 1              NA                 NA            NA             NA}
\CommentTok{#> 2         0.38175            0.46603       0.03505        0.66833}
\CommentTok{#> 3         0.37699            0.45116       0.05900        0.67574}
\CommentTok{#> 4         0.37885            0.45929       0.04968        0.67617}
\CommentTok{#> 5         0.37804            0.45311       0.05374        0.67868}
\CommentTok{#> 6         0.37899            0.45761       0.04899        0.67177}
\CommentTok{#> 7         0.37804            0.45311       0.05374        0.67868}
\CommentTok{#>   validation_pr_auc validation_lift validation_classification_error}
\CommentTok{#> 1                NA              NA                              NA}
\CommentTok{#> 2           0.31118         2.64179                         0.36362}
\CommentTok{#> 3           0.31512         2.46567                         0.36350}
\CommentTok{#> 4           0.32010         2.61978                         0.37415}
\CommentTok{#> 5           0.32047         2.64179                         0.32048}
\CommentTok{#> 6           0.31141         2.50970                         0.38321}
\CommentTok{#> 7           0.32047         2.64179                         0.32048}
\CommentTok{# Scoring History: }
\CommentTok{#   timestamp   duration  training_speed   epochs}
\CommentTok{# 1 2016-05-03 10:33:29  0.000 sec                  0.00000}
\CommentTok{# 2 2016-05-03 10:33:29  0.347 sec 424697 rows/sec  0.86851}
\CommentTok{# 3 2016-05-03 10:33:30  1.356 sec 601925 rows/sec  6.09185}
\CommentTok{# 4 2016-05-03 10:33:31  2.348 sec 717617 rows/sec 13.05168}
\CommentTok{# 5 2016-05-03 10:33:32  3.281 sec 777538 rows/sec 20.00783}
\CommentTok{# 6 2016-05-03 10:33:32  3.345 sec 777275 rows/sec 20.00783}
\CommentTok{# iterations        samples training_MSE training_r2}
\CommentTok{# 1          0       0.000000                         }
\CommentTok{# 2          1   99804.000000      0.14402     0.03691}
\CommentTok{# 3          7  700039.000000      0.14157     0.05333}
\CommentTok{# 4         15 1499821.000000      0.14033     0.06159}
\CommentTok{# 5         23 2299180.000000      0.14079     0.05853}
\CommentTok{# 6         23 2299180.000000      0.14157     0.05333}
\CommentTok{# training_logloss training_AUC training_lift}
\CommentTok{# 1                                            }
\CommentTok{# 2          0.45930      0.66685       2.20727}
\CommentTok{# 3          0.45220      0.68133       2.59354}
\CommentTok{# 4          0.44710      0.67993       2.70390}
\CommentTok{# 5          0.45100      0.68192       2.81426}
\CommentTok{# 6          0.45220      0.68133       2.59354}
\CommentTok{# training_classification_error validation_MSE validation_r2}
\CommentTok{# 1                                                           }
\CommentTok{# 2                       0.36145        0.14682       0.03426}
\CommentTok{# 3                       0.33647        0.14500       0.04619}
\CommentTok{# 4                       0.37126        0.14411       0.05204}
\CommentTok{# 5                       0.32868        0.14474       0.04793}
\CommentTok{# 6                       0.33647        0.14500       0.04619}
\CommentTok{# validation_logloss validation_AUC validation_lift}
\CommentTok{# 1                                                  }
\CommentTok{# 2            0.46692        0.66582         2.53209}
\CommentTok{# 3            0.46256        0.67354         2.64124}
\CommentTok{# 4            0.45789        0.66986         2.44478}
\CommentTok{# 5            0.46292        0.67117         2.70672}
\CommentTok{# 6            0.46256        0.67354         2.64124}
\CommentTok{# validation_classification_error}
\CommentTok{# 1                                }
\CommentTok{# 2                         0.37197}
\CommentTok{# 3                         0.34716}
\CommentTok{# 4                         0.34385}
\CommentTok{# 5                         0.36544}
\CommentTok{# 6                         0.34716}
\end{Highlighting}
\end{Shaded}

\hypertarget{naive-bayes-model}{%
\section{Naive Bayes model}\label{naive-bayes-model}}

The Naive Bayes (NB) algorithm does not usually beat an algorithm like a Random Forest
or GBM, however it is still a popular algorithm, especially in the text domain (when your
input is text encoded as ``Bag of Words'', for example). The Naive Bayes algorithm is for
binary or multiclass classification problems only, not regression. Therefore, your response
must be a factor instead of a numeric.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# First we will train a basic NB model with default parameters. }
\NormalTok{nb_fit1 <-}\StringTok{ }\KeywordTok{h2o.naiveBayes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ x,}
                          \DataTypeTok{y =}\NormalTok{ y,}
                          \DataTypeTok{training_frame =}\NormalTok{ train,}
                          \DataTypeTok{model_id =} \StringTok{"nb_fit1"}\NormalTok{)}
\CommentTok{#> }
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|}\StringTok{                                                                 }\ErrorTok{|}\StringTok{   }\DecValTok{0}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|===========}\StringTok{                                                      }\ErrorTok{|}\StringTok{  }\DecValTok{17}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=================================================================|}\StringTok{ }\DecValTok{100}\NormalTok{%}
\end{Highlighting}
\end{Shaded}

\hypertarget{train-a-nb-model-with-laplace-smoothing}{%
\subsection{Train a NB model with Laplace Smoothing}\label{train-a-nb-model-with-laplace-smoothing}}

One of the few tunable model parameters for the Naive Bayes algorithm is the amount of Laplace
smoothing. The H2O Naive Bayes model will not use any Laplace smoothing by default.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nb_fit2 <-}\StringTok{ }\KeywordTok{h2o.naiveBayes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ x,}
                          \DataTypeTok{y =}\NormalTok{ y,}
                          \DataTypeTok{training_frame =}\NormalTok{ train,}
                          \DataTypeTok{model_id =} \StringTok{"nb_fit2"}\NormalTok{,}
                          \DataTypeTok{laplace =} \DecValTok{6}\NormalTok{)}
\CommentTok{#> }
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|}\StringTok{                                                                 }\ErrorTok{|}\StringTok{   }\DecValTok{0}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|======================================================}\StringTok{           }\ErrorTok{|}\StringTok{  }\DecValTok{83}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=================================================================|}\StringTok{ }\DecValTok{100}\NormalTok{%}

\CommentTok{# Let's compare the performance of the two NB models}
\NormalTok{nb_perf1 <-}\StringTok{ }\KeywordTok{h2o.performance}\NormalTok{(}\DataTypeTok{model =}\NormalTok{ nb_fit1,}
                            \DataTypeTok{newdata =}\NormalTok{ test)}
\NormalTok{nb_perf2 <-}\StringTok{ }\KeywordTok{h2o.performance}\NormalTok{(}\DataTypeTok{model =}\NormalTok{ nb_fit2,}
                            \DataTypeTok{newdata =}\NormalTok{ test)}

\CommentTok{# Print model performance}
\NormalTok{nb_perf1}
\CommentTok{#> H2OBinomialMetrics: naivebayes}
\CommentTok{#> }
\CommentTok{#> MSE:  0.15}
\CommentTok{#> RMSE:  0.387}
\CommentTok{#> LogLoss:  0.489}
\CommentTok{#> Mean Per-Class Error:  0.39}
\CommentTok{#> AUC:  0.651}
\CommentTok{#> pr_auc:  0.297}
\CommentTok{#> Gini:  0.303}
\CommentTok{#> }
\CommentTok{#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:}
\CommentTok{#>            0    1    Error         Rate}
\CommentTok{#> 0      13184 6807 0.340503  =6807/19991}
\CommentTok{#> 1       2021 2569 0.440305   =2021/4590}
\CommentTok{#> Totals 15205 9376 0.359139  =8828/24581}
\CommentTok{#> }
\CommentTok{#> Maximum Metrics: Maximum metrics at their respective thresholds}
\CommentTok{#>                         metric threshold    value idx}
\CommentTok{#> 1                       max f1  0.225948 0.367893 236}
\CommentTok{#> 2                       max f2  0.090634 0.545538 346}
\CommentTok{#> 3                 max f0point5  0.340471 0.335807 166}
\CommentTok{#> 4                 max accuracy  0.999554 0.812945   0}
\CommentTok{#> 5                max precision  0.559607 0.428747  70}
\CommentTok{#> 6                   max recall  0.000196 1.000000 399}
\CommentTok{#> 7              max specificity  0.999554 0.999550   0}
\CommentTok{#> 8             max absolute_mcc  0.287231 0.188641 196}
\CommentTok{#> 9   max min_per_class_accuracy  0.208724 0.602832 250}
\CommentTok{#> 10 max mean_per_class_accuracy  0.225948 0.609596 236}
\CommentTok{#> }
\CommentTok{#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`}
\NormalTok{nb_perf2}
\CommentTok{#> H2OBinomialMetrics: naivebayes}
\CommentTok{#> }
\CommentTok{#> MSE:  0.15}
\CommentTok{#> RMSE:  0.387}
\CommentTok{#> LogLoss:  0.489}
\CommentTok{#> Mean Per-Class Error:  0.39}
\CommentTok{#> AUC:  0.651}
\CommentTok{#> pr_auc:  0.297}
\CommentTok{#> Gini:  0.303}
\CommentTok{#> }
\CommentTok{#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:}
\CommentTok{#>            0    1    Error         Rate}
\CommentTok{#> 0      14002 5989 0.299585  =5989/19991}
\CommentTok{#> 1       2207 2383 0.480828   =2207/4590}
\CommentTok{#> Totals 16209 8372 0.333428  =8196/24581}
\CommentTok{#> }
\CommentTok{#> Maximum Metrics: Maximum metrics at their respective thresholds}
\CommentTok{#>                         metric threshold    value idx}
\CommentTok{#> 1                       max f1  0.242206 0.367690 222}
\CommentTok{#> 2                       max f2  0.088660 0.545677 347}
\CommentTok{#> 3                 max f0point5  0.362995 0.336012 152}
\CommentTok{#> 4                 max accuracy  0.999564 0.812945   0}
\CommentTok{#> 5                max precision  0.574610 0.428775  63}
\CommentTok{#> 6                   max recall  0.000207 1.000000 399}
\CommentTok{#> 7              max specificity  0.999564 0.999550   0}
\CommentTok{#> 8             max absolute_mcc  0.286635 0.189479 192}
\CommentTok{#> 9   max min_per_class_accuracy  0.207609 0.604357 248}
\CommentTok{#> 10 max mean_per_class_accuracy  0.247906 0.609878 218}
\CommentTok{#> }
\CommentTok{#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`}

\CommentTok{# Retreive test set AUC}
\KeywordTok{h2o.auc}\NormalTok{(nb_perf1)  }\CommentTok{# 0.6488014}
\CommentTok{#> [1] 0.651}
\KeywordTok{h2o.auc}\NormalTok{(nb_perf2)  }\CommentTok{# 0.6490678}
\CommentTok{#> [1] 0.651}
\end{Highlighting}
\end{Shaded}

\hypertarget{classification-algorithms-comparison.-diabetes-datset.-cart-lda-svm-knn-rf}{%
\chapter{\texorpdfstring{Classification algorithms comparison. Diabetes datset. (\emph{CART, LDA, SVM, KNN, RF})}{Classification algorithms comparison. Diabetes datset. (CART, LDA, SVM, KNN, RF)}}\label{classification-algorithms-comparison.-diabetes-datset.-cart-lda-svm-knn-rf}}

\hypertarget{pimaindiansdiabetes-dataset}{%
\section{\texorpdfstring{\texttt{PimaIndiansDiabetes} dataset}{PimaIndiansDiabetes dataset}}\label{pimaindiansdiabetes-dataset}}

\hypertarget{introduction-1}{%
\section{Introduction}\label{introduction-1}}

We compare the following clasification algorithms:

\begin{itemize}
\tightlist
\item
  CART
\item
  LDA
\item
  SVM
\item
  KNN
\item
  RF
\end{itemize}

\hypertarget{workflow}{%
\section{Workflow}\label{workflow}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Load dataset
\item
  Create the train dataset
\item
  Train the models
\item
  Collect resamples
\item
  Plot comparison
\item
  Summarize p-values
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load packages}
\KeywordTok{library}\NormalTok{(mlbench)}
\KeywordTok{library}\NormalTok{(caret)}
\CommentTok{# load the dataset}
\KeywordTok{data}\NormalTok{(PimaIndiansDiabetes)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{glimpse}\NormalTok{(PimaIndiansDiabetes)}
\CommentTok{#> Observations: 768}
\CommentTok{#> Variables: 9}
\CommentTok{#> $ pregnant <dbl> 6, 1, 8, 1, 0, 5, 3, 10, 2, 8, 4, 10, 10, 1, 5, 7, 0,...}
\CommentTok{#> $ glucose  <dbl> 148, 85, 183, 89, 137, 116, 78, 115, 197, 125, 110, 1...}
\CommentTok{#> $ pressure <dbl> 72, 66, 64, 66, 40, 74, 50, 0, 70, 96, 92, 74, 80, 60...}
\CommentTok{#> $ triceps  <dbl> 35, 29, 0, 23, 35, 0, 32, 0, 45, 0, 0, 0, 0, 23, 19, ...}
\CommentTok{#> $ insulin  <dbl> 0, 0, 0, 94, 168, 0, 88, 0, 543, 0, 0, 0, 0, 846, 175...}
\CommentTok{#> $ mass     <dbl> 33.6, 26.6, 23.3, 28.1, 43.1, 25.6, 31.0, 35.3, 30.5,...}
\CommentTok{#> $ pedigree <dbl> 0.627, 0.351, 0.672, 0.167, 2.288, 0.201, 0.248, 0.13...}
\CommentTok{#> $ age      <dbl> 50, 31, 32, 21, 33, 30, 26, 29, 53, 54, 30, 34, 57, 5...}
\CommentTok{#> $ diabetes <fct> pos, neg, pos, neg, pos, neg, pos, neg, pos, pos, neg...}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tibble}\OperatorTok{::}\KeywordTok{as_tibble}\NormalTok{(PimaIndiansDiabetes)}
\CommentTok{#> # A tibble: 768 x 9}
\CommentTok{#>   pregnant glucose pressure triceps insulin  mass pedigree   age diabetes}
\CommentTok{#>      <dbl>   <dbl>    <dbl>   <dbl>   <dbl> <dbl>    <dbl> <dbl> <fct>   }
\CommentTok{#> 1        6     148       72      35       0  33.6    0.627    50 pos     }
\CommentTok{#> 2        1      85       66      29       0  26.6    0.351    31 neg     }
\CommentTok{#> 3        8     183       64       0       0  23.3    0.672    32 pos     }
\CommentTok{#> 4        1      89       66      23      94  28.1    0.167    21 neg     }
\CommentTok{#> 5        0     137       40      35     168  43.1    2.29     33 pos     }
\CommentTok{#> 6        5     116       74       0       0  25.6    0.201    30 neg     }
\CommentTok{#> # ... with 762 more rows}
\end{Highlighting}
\end{Shaded}

\hypertarget{train-the-models-using-cross-validation}{%
\section{Train the models using cross-validation}\label{train-the-models-using-cross-validation}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# prepare training scheme}
\NormalTok{trainControl <-}\StringTok{ }\KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"repeatedcv"}\NormalTok{, }
                             \DataTypeTok{number=}\DecValTok{10}\NormalTok{, }
                             \DataTypeTok{repeats=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# CART}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{7}\NormalTok{)}
\NormalTok{fit.cart <-}\StringTok{ }\KeywordTok{train}\NormalTok{(diabetes}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{PimaIndiansDiabetes, }
                  \DataTypeTok{method =} \StringTok{"rpart"}\NormalTok{, }\DataTypeTok{trControl=}\NormalTok{trainControl)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# LDA: Linear Discriminant Analysis}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{7}\NormalTok{)}
\NormalTok{fit.lda <-}\StringTok{ }\KeywordTok{train}\NormalTok{(diabetes}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{PimaIndiansDiabetes, }
                 \DataTypeTok{method=}\StringTok{"lda"}\NormalTok{, }\DataTypeTok{trControl=}\NormalTok{trainControl)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# SVM}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{7}\NormalTok{)}
\NormalTok{fit.svm <-}\StringTok{ }\KeywordTok{train}\NormalTok{(diabetes}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{PimaIndiansDiabetes, }
                 \DataTypeTok{method=}\StringTok{"svmRadial"}\NormalTok{, }\DataTypeTok{trControl=}\NormalTok{trainControl)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# KNN}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{7}\NormalTok{)}
\NormalTok{fit.knn <-}\StringTok{ }\KeywordTok{train}\NormalTok{(diabetes}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{PimaIndiansDiabetes, }
                 \DataTypeTok{method=}\StringTok{"knn"}\NormalTok{, }\DataTypeTok{trControl=}\NormalTok{trainControl)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Random Forest}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{7}\NormalTok{)}
\NormalTok{fit.rf <-}\StringTok{ }\KeywordTok{train}\NormalTok{(diabetes}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{PimaIndiansDiabetes, }
                \DataTypeTok{method=}\StringTok{"rf"}\NormalTok{, }\DataTypeTok{trControl=}\NormalTok{trainControl)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# collect resamples}
\NormalTok{results <-}\StringTok{ }\KeywordTok{resamples}\NormalTok{(}\KeywordTok{list}\NormalTok{(}\DataTypeTok{CART=}\NormalTok{fit.cart, }
                          \DataTypeTok{LDA=}\NormalTok{fit.lda, }
                          \DataTypeTok{SVM=}\NormalTok{fit.svm, }
                          \DataTypeTok{KNN=}\NormalTok{fit.knn, }
                          \DataTypeTok{RF=}\NormalTok{fit.rf))}
\end{Highlighting}
\end{Shaded}

\hypertarget{compare-models}{%
\section{Compare models}\label{compare-models}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# summarize differences between models}
\KeywordTok{summary}\NormalTok{(results)}
\CommentTok{#> }
\CommentTok{#> Call:}
\CommentTok{#> summary.resamples(object = results)}
\CommentTok{#> }
\CommentTok{#> Models: CART, LDA, SVM, KNN, RF }
\CommentTok{#> Number of resamples: 30 }
\CommentTok{#> }
\CommentTok{#> Accuracy }
\CommentTok{#>       Min. 1st Qu. Median  Mean 3rd Qu.  Max. NA's}
\CommentTok{#> CART 0.675   0.727  0.753 0.747   0.766 0.792    0}
\CommentTok{#> LDA  0.714   0.751  0.766 0.779   0.800 0.908    0}
\CommentTok{#> SVM  0.724   0.751  0.763 0.771   0.792 0.895    0}
\CommentTok{#> KNN  0.675   0.704  0.727 0.737   0.766 0.831    0}
\CommentTok{#> RF   0.684   0.731  0.760 0.764   0.802 0.842    0}
\CommentTok{#> }
\CommentTok{#> Kappa }
\CommentTok{#>       Min. 1st Qu. Median  Mean 3rd Qu.  Max. NA's}
\CommentTok{#> CART 0.276   0.362  0.424 0.415   0.486 0.525    0}
\CommentTok{#> LDA  0.301   0.419  0.466 0.486   0.531 0.781    0}
\CommentTok{#> SVM  0.339   0.400  0.446 0.462   0.523 0.748    0}
\CommentTok{#> KNN  0.255   0.341  0.384 0.398   0.454 0.620    0}
\CommentTok{#> RF   0.295   0.378  0.464 0.463   0.545 0.643    0}
\end{Highlighting}
\end{Shaded}

\hypertarget{plot-comparison}{%
\section{Plot comparison}\label{plot-comparison}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# box and whisker plots to compare models}
\NormalTok{scales <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}\DataTypeTok{x=}\KeywordTok{list}\NormalTok{(}\DataTypeTok{relation=}\StringTok{"free"}\NormalTok{), }\DataTypeTok{y=}\KeywordTok{list}\NormalTok{(}\DataTypeTok{relation=}\StringTok{"free"}\NormalTok{))}
\KeywordTok{bwplot}\NormalTok{(results, }\DataTypeTok{scales=}\NormalTok{scales)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{algo-comparison_312-classification-diabetes-multi_files/figure-latex/unnamed-chunk-5-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# density plots of accuracy}
\NormalTok{scales <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}\DataTypeTok{x=}\KeywordTok{list}\NormalTok{(}\DataTypeTok{relation=}\StringTok{"free"}\NormalTok{), }\DataTypeTok{y=}\KeywordTok{list}\NormalTok{(}\DataTypeTok{relation=}\StringTok{"free"}\NormalTok{))}
\KeywordTok{densityplot}\NormalTok{(results, }\DataTypeTok{scales=}\NormalTok{scales, }\DataTypeTok{pch =} \StringTok{"|"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{algo-comparison_312-classification-diabetes-multi_files/figure-latex/density_plot-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# dot plots of accuracy}
\NormalTok{scales <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}\DataTypeTok{x=}\KeywordTok{list}\NormalTok{(}\DataTypeTok{relation=}\StringTok{"free"}\NormalTok{), }\DataTypeTok{y=}\KeywordTok{list}\NormalTok{(}\DataTypeTok{relation=}\StringTok{"free"}\NormalTok{))}
\KeywordTok{dotplot}\NormalTok{(results, }\DataTypeTok{scales=}\NormalTok{scales)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{algo-comparison_312-classification-diabetes-multi_files/figure-latex/dot_plot-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# parallel plots to compare models}
\KeywordTok{parallelplot}\NormalTok{(results)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{algo-comparison_312-classification-diabetes-multi_files/figure-latex/parallel_plot-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# pairwise scatter plots of predictions to compare models}
\KeywordTok{splom}\NormalTok{(results)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{algo-comparison_312-classification-diabetes-multi_files/figure-latex/pairwise_plot-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# xyplot plots to compare models}
\KeywordTok{xyplot}\NormalTok{(results, }\DataTypeTok{models=}\KeywordTok{c}\NormalTok{(}\StringTok{"LDA"}\NormalTok{, }\StringTok{"SVM"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{algo-comparison_312-classification-diabetes-multi_files/figure-latex/xy_plot-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# difference in model predictions}
\NormalTok{diffs <-}\StringTok{ }\KeywordTok{diff}\NormalTok{(results)}
\CommentTok{# summarize p-values for pairwise comparisons}
\KeywordTok{summary}\NormalTok{(diffs)}
\CommentTok{#> }
\CommentTok{#> Call:}
\CommentTok{#> summary.diff.resamples(object = diffs)}
\CommentTok{#> }
\CommentTok{#> p-value adjustment: bonferroni }
\CommentTok{#> Upper diagonal: estimates of the difference}
\CommentTok{#> Lower diagonal: p-value for H0: difference = 0}
\CommentTok{#> }
\CommentTok{#> Accuracy }
\CommentTok{#>      CART     LDA      SVM      KNN      RF      }
\CommentTok{#> CART          -0.03214 -0.02432  0.01002 -0.01688}
\CommentTok{#> LDA  0.001186           0.00781  0.04216  0.01525}
\CommentTok{#> SVM  0.011640 0.915689           0.03434  0.00744}
\CommentTok{#> KNN  1.000000 6.68e-05 0.000294          -0.02690}
\CommentTok{#> RF   0.272754 0.449062 1.000000 0.018379         }
\CommentTok{#> }
\CommentTok{#> Kappa }
\CommentTok{#>      CART     LDA       SVM       KNN       RF       }
\CommentTok{#> CART          -0.071016 -0.046972  0.016687 -0.047894}
\CommentTok{#> LDA  0.000809            0.024044  0.087703  0.023122}
\CommentTok{#> SVM  0.025808 0.356273             0.063659 -0.000922}
\CommentTok{#> KNN  1.000000 0.000386  0.004082            -0.064581}
\CommentTok{#> RF   0.021176 1.000000  1.000000  0.015897}
\end{Highlighting}
\end{Shaded}

\hypertarget{multiclass-classification-comparison.-diabetes-dataset.-lda-cart-knn-svm-rf}{%
\chapter{\texorpdfstring{Multiclass classification comparison. Diabetes dataset. (\emph{LDA, CART, KNN, SVM, RF})}{Multiclass classification comparison. Diabetes dataset. (LDA, CART, KNN, SVM, RF)}}\label{multiclass-classification-comparison.-diabetes-dataset.-lda-cart-knn-svm-rf}}

\hypertarget{iris-dataset}{%
\section{\texorpdfstring{\texttt{iris} dataset}{iris dataset}}\label{iris-dataset}}

\hypertarget{introduction-2}{%
\section{Introduction}\label{introduction-2}}

These are the algorithms used:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  LDA
\item
  CART
\item
  KNN
\item
  SVM
\item
  RF
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load the caret package}
\KeywordTok{library}\NormalTok{(caret)}
\CommentTok{#> Loading required package: lattice}
\CommentTok{#> Loading required package: ggplot2}
\CommentTok{#> Registered S3 methods overwritten by 'ggplot2':}
\CommentTok{#>   method         from }
\CommentTok{#>   [.quosures     rlang}
\CommentTok{#>   c.quosures     rlang}
\CommentTok{#>   print.quosures rlang}
\CommentTok{# attach the iris dataset to the environment}
\KeywordTok{data}\NormalTok{(iris)}
\CommentTok{# rename the dataset}
\NormalTok{dataset <-}\StringTok{ }\NormalTok{iris}
\end{Highlighting}
\end{Shaded}

\hypertarget{workflow-1}{%
\section{Workflow}\label{workflow-1}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Load dataset
\item
  Create train and test datasets, 80/20
\item
  Inspect dataset
\item
  Visualize features
\item
  Set the train control to
\end{enumerate}

\begin{itemize}
\tightlist
\item
  10 cross-validations
\item
  Metric: accuracy
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  Train the models
\item
  Compare accuracy of models
\item
  Visual comparison
\item
  Make predictions on \texttt{validation} set
\end{enumerate}

We will split the loaded dataset into two, 80\% of which we will use to train our models and 20\% that we will hold back as a validation dataset.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# create a list of 80% of the rows in the original dataset we can use for training}
\NormalTok{validationIndex <-}\StringTok{ }\KeywordTok{createDataPartition}\NormalTok{(dataset}\OperatorTok{$}\NormalTok{Species, }\DataTypeTok{p=}\FloatTok{0.80}\NormalTok{, }\DataTypeTok{list=}\OtherTok{FALSE}\NormalTok{)}
\CommentTok{# select 20% of the data for validation}
\NormalTok{validation <-}\StringTok{ }\NormalTok{dataset[}\OperatorTok{-}\NormalTok{validationIndex,]}

\CommentTok{# use the remaining 80% of data to training and testing the models}
\NormalTok{dataset <-}\StringTok{ }\NormalTok{dataset[validationIndex,]}

\CommentTok{# dimensions of dataset}
\KeywordTok{dim}\NormalTok{(dataset)}
\CommentTok{#> [1] 120   5}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# list types for each attribute}
\KeywordTok{sapply}\NormalTok{(dataset, class)}
\CommentTok{#> Sepal.Length  Sepal.Width Petal.Length  Petal.Width      Species }
\CommentTok{#>    "numeric"    "numeric"    "numeric"    "numeric"     "factor"}
\end{Highlighting}
\end{Shaded}

\hypertarget{peek-at-the-dataset}{%
\section{Peek at the dataset}\label{peek-at-the-dataset}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# take a peek at the first 5 rows of the data}
\KeywordTok{head}\NormalTok{(dataset)}
\CommentTok{#>   Sepal.Length Sepal.Width Petal.Length Petal.Width Species}
\CommentTok{#> 1          5.1         3.5          1.4         0.2  setosa}
\CommentTok{#> 2          4.9         3.0          1.4         0.2  setosa}
\CommentTok{#> 3          4.7         3.2          1.3         0.2  setosa}
\CommentTok{#> 4          4.6         3.1          1.5         0.2  setosa}
\CommentTok{#> 5          5.0         3.6          1.4         0.2  setosa}
\CommentTok{#> 6          5.4         3.9          1.7         0.4  setosa}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(dplyr)}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'dplyr'}
\CommentTok{#> The following objects are masked from 'package:stats':}
\CommentTok{#> }
\CommentTok{#>     filter, lag}
\CommentTok{#> The following objects are masked from 'package:base':}
\CommentTok{#> }
\CommentTok{#>     intersect, setdiff, setequal, union}

\KeywordTok{glimpse}\NormalTok{(dataset)}
\CommentTok{#> Observations: 120}
\CommentTok{#> Variables: 5}
\CommentTok{#> $ Sepal.Length <dbl> 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9,...}
\CommentTok{#> $ Sepal.Width  <dbl> 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1,...}
\CommentTok{#> $ Petal.Length <dbl> 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5,...}
\CommentTok{#> $ Petal.Width  <dbl> 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1,...}
\CommentTok{#> $ Species      <fct> setosa, setosa, setosa, setosa, setosa, setosa, s...}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(skimr)}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'skimr'}
\CommentTok{#> The following object is masked from 'package:stats':}
\CommentTok{#> }
\CommentTok{#>     filter}

\KeywordTok{skim}\NormalTok{(dataset)}
\CommentTok{#> Skim summary statistics}
\CommentTok{#>  n obs: 120 }
\CommentTok{#>  n variables: 5 }
\CommentTok{#> }
\CommentTok{#> -- Variable type:factor --------------------------------------------------}
\CommentTok{#>  variable missing complete   n n_unique                       top_counts}
\CommentTok{#>   Species       0      120 120        3 set: 40, ver: 40, vir: 40, NA: 0}
\CommentTok{#>  ordered}
\CommentTok{#>    FALSE}
\CommentTok{#> }
\CommentTok{#> -- Variable type:numeric -------------------------------------------------}
\CommentTok{#>      variable missing complete   n mean   sd  p0  p25  p50  p75 p100}
\CommentTok{#>  Petal.Length       0      120 120 3.76 1.78 1   1.58 4.35 5.1   6.9}
\CommentTok{#>   Petal.Width       0      120 120 1.2  0.76 0.1 0.3  1.3  1.8   2.5}
\CommentTok{#>  Sepal.Length       0      120 120 5.86 0.84 4.3 5.1  5.8  6.4   7.9}
\CommentTok{#>   Sepal.Width       0      120 120 3.06 0.44 2   2.8  3    3.32  4.4}
\CommentTok{#>      hist}
\CommentTok{#>  }
\CommentTok{#>  }
\CommentTok{#>  }
\CommentTok{#>  }
\end{Highlighting}
\end{Shaded}

\hypertarget{levels-of-the-class}{%
\section{Levels of the class}\label{levels-of-the-class}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# list the levels for the class}
\KeywordTok{levels}\NormalTok{(dataset}\OperatorTok{$}\NormalTok{Species)}
\CommentTok{#> [1] "setosa"     "versicolor" "virginica"}
\end{Highlighting}
\end{Shaded}

\hypertarget{class-distribution}{%
\section{class distribution}\label{class-distribution}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# summarize the class distribution}
\NormalTok{percentage <-}\StringTok{ }\KeywordTok{prop.table}\NormalTok{(}\KeywordTok{table}\NormalTok{(dataset}\OperatorTok{$}\NormalTok{Species)) }\OperatorTok{*}\StringTok{ }\DecValTok{100}
\KeywordTok{cbind}\NormalTok{(}\DataTypeTok{freq=}\KeywordTok{table}\NormalTok{(dataset}\OperatorTok{$}\NormalTok{Species), }\DataTypeTok{percentage=}\NormalTok{percentage)}
\CommentTok{#>            freq percentage}
\CommentTok{#> setosa       40       33.3}
\CommentTok{#> versicolor   40       33.3}
\CommentTok{#> virginica    40       33.3}
\end{Highlighting}
\end{Shaded}

\hypertarget{visualize-the-dataset}{%
\section{Visualize the dataset}\label{visualize-the-dataset}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# split input and output}
\NormalTok{x <-}\StringTok{ }\NormalTok{dataset[,}\DecValTok{1}\OperatorTok{:}\DecValTok{4}\NormalTok{]}
\NormalTok{y <-}\StringTok{ }\NormalTok{dataset[,}\DecValTok{5}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# boxplot for each attribute on one image}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{4}\NormalTok{))}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{4}\NormalTok{) \{}
    \KeywordTok{boxplot}\NormalTok{(x[,i], }\DataTypeTok{main=}\KeywordTok{names}\NormalTok{(dataset)[i])}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{algo-comparison_320-classification_iris-multi_files/figure-latex/unnamed-chunk-11-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# barplot for class breakdown}
\KeywordTok{plot}\NormalTok{(y)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{algo-comparison_320-classification_iris-multi_files/figure-latex/unnamed-chunk-12-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# scatter plot matrix}
\KeywordTok{featurePlot}\NormalTok{(}\DataTypeTok{x=}\NormalTok{x, }\DataTypeTok{y=}\NormalTok{y, }\DataTypeTok{plot=}\StringTok{"ellipse"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{algo-comparison_320-classification_iris-multi_files/figure-latex/unnamed-chunk-13-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# box and whisker plots for each attribute}
\KeywordTok{featurePlot}\NormalTok{(}\DataTypeTok{x=}\NormalTok{x, }\DataTypeTok{y=}\NormalTok{y, }\DataTypeTok{plot=}\StringTok{"box"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{algo-comparison_320-classification_iris-multi_files/figure-latex/unnamed-chunk-14-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# density plots for each attribute by class value}
\NormalTok{scales <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}\DataTypeTok{x=}\KeywordTok{list}\NormalTok{(}\DataTypeTok{relation=}\StringTok{"free"}\NormalTok{), }\DataTypeTok{y=}\KeywordTok{list}\NormalTok{(}\DataTypeTok{relation=}\StringTok{"free"}\NormalTok{))}
\KeywordTok{featurePlot}\NormalTok{(}\DataTypeTok{x=}\NormalTok{x, }\DataTypeTok{y=}\NormalTok{y, }\DataTypeTok{plot=}\StringTok{"density"}\NormalTok{, }\DataTypeTok{scales=}\NormalTok{scales)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{algo-comparison_320-classification_iris-multi_files/figure-latex/unnamed-chunk-15-1} \end{center}

\hypertarget{evaluate-algorithms}{%
\section{Evaluate algorithms}\label{evaluate-algorithms}}

\hypertarget{split-and-metrics}{%
\subsection{split and metrics}\label{split-and-metrics}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Run algorithms using 10-fold cross-validation}
\NormalTok{trainControl <-}\StringTok{ }\KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method=}\StringTok{"cv"}\NormalTok{, }\DataTypeTok{number=}\DecValTok{10}\NormalTok{)}
\NormalTok{metric <-}\StringTok{ "Accuracy"}
\end{Highlighting}
\end{Shaded}

\hypertarget{build-models}{%
\subsection{build models}\label{build-models}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# LDA}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{7}\NormalTok{)}
\NormalTok{fit.lda <-}\StringTok{ }\KeywordTok{train}\NormalTok{(Species}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{dataset, }\DataTypeTok{method =} \StringTok{"lda"}\NormalTok{, }
                 \DataTypeTok{metric=}\NormalTok{metric, }\DataTypeTok{trControl=}\NormalTok{trainControl)}
\CommentTok{# CART}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{7}\NormalTok{)}
\NormalTok{fit.cart <-}\StringTok{ }\KeywordTok{train}\NormalTok{(Species}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{dataset, }\DataTypeTok{method =} \StringTok{"rpart"}\NormalTok{, }
                  \DataTypeTok{metric=}\NormalTok{metric, }\DataTypeTok{trControl=}\NormalTok{trainControl)}
\CommentTok{# KNN}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{7}\NormalTok{)}
\NormalTok{fit.knn <-}\StringTok{ }\KeywordTok{train}\NormalTok{(Species}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{dataset, }\DataTypeTok{method =} \StringTok{"knn"}\NormalTok{, }
                 \DataTypeTok{metric=}\NormalTok{metric, }\DataTypeTok{trControl=}\NormalTok{trainControl)}
\CommentTok{# SVM}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{7}\NormalTok{)}
\NormalTok{fit.svm <-}\StringTok{ }\KeywordTok{train}\NormalTok{(Species}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{dataset, }\DataTypeTok{method =} \StringTok{"svmRadial"}\NormalTok{, }
                 \DataTypeTok{metric=}\NormalTok{metric, }\DataTypeTok{trControl=}\NormalTok{trainControl)}
\CommentTok{# Random Forest}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{7}\NormalTok{)}
\NormalTok{fit.rf <-}\StringTok{ }\KeywordTok{train}\NormalTok{(Species}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{dataset, }\DataTypeTok{method =} \StringTok{"rf"}\NormalTok{, }
                \DataTypeTok{metric=}\NormalTok{metric, }\DataTypeTok{trControl=}\NormalTok{trainControl)}
\end{Highlighting}
\end{Shaded}

\hypertarget{compare}{%
\subsection{compare}\label{compare}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#summarize accuracy of models}
\NormalTok{results <-}\StringTok{ }\KeywordTok{resamples}\NormalTok{(}\KeywordTok{list}\NormalTok{(}\DataTypeTok{lda  =}\NormalTok{ fit.lda, }
                          \DataTypeTok{cart =}\NormalTok{ fit.cart, }
                          \DataTypeTok{knn  =}\NormalTok{ fit.knn, }
                          \DataTypeTok{svm  =}\NormalTok{ fit.svm, }
                          \DataTypeTok{rf   =}\NormalTok{ fit.rf))}
\KeywordTok{summary}\NormalTok{(results)}
\CommentTok{#> }
\CommentTok{#> Call:}
\CommentTok{#> summary.resamples(object = results)}
\CommentTok{#> }
\CommentTok{#> Models: lda, cart, knn, svm, rf }
\CommentTok{#> Number of resamples: 10 }
\CommentTok{#> }
\CommentTok{#> Accuracy }
\CommentTok{#>       Min. 1st Qu. Median  Mean 3rd Qu. Max. NA's}
\CommentTok{#> lda  0.917   1.000      1 0.992       1    1    0}
\CommentTok{#> cart 0.917   0.917      1 0.967       1    1    0}
\CommentTok{#> knn  0.917   0.938      1 0.975       1    1    0}
\CommentTok{#> svm  0.833   0.917      1 0.958       1    1    0}
\CommentTok{#> rf   0.917   0.917      1 0.967       1    1    0}
\CommentTok{#> }
\CommentTok{#> Kappa }
\CommentTok{#>       Min. 1st Qu. Median  Mean 3rd Qu. Max. NA's}
\CommentTok{#> lda  0.875   1.000      1 0.987       1    1    0}
\CommentTok{#> cart 0.875   0.875      1 0.950       1    1    0}
\CommentTok{#> knn  0.875   0.906      1 0.962       1    1    0}
\CommentTok{#> svm  0.750   0.875      1 0.937       1    1    0}
\CommentTok{#> rf   0.875   0.875      1 0.950       1    1    0}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# compare accuracy of models}
\KeywordTok{dotplot}\NormalTok{(results)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{algo-comparison_320-classification_iris-multi_files/figure-latex/unnamed-chunk-19-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# summarize Best Model}
\KeywordTok{print}\NormalTok{(fit.lda)}
\CommentTok{#> Linear Discriminant Analysis }
\CommentTok{#> }
\CommentTok{#> 120 samples}
\CommentTok{#>   4 predictor}
\CommentTok{#>   3 classes: 'setosa', 'versicolor', 'virginica' }
\CommentTok{#> }
\CommentTok{#> No pre-processing}
\CommentTok{#> Resampling: Cross-Validated (10 fold) }
\CommentTok{#> Summary of sample sizes: 108, 108, 108, 108, 108, 108, ... }
\CommentTok{#> Resampling results:}
\CommentTok{#> }
\CommentTok{#>   Accuracy  Kappa}
\CommentTok{#>   0.992     0.987}
\end{Highlighting}
\end{Shaded}

\hypertarget{make-predictions}{%
\section{Make predictions}\label{make-predictions}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# estimate skill of LDA on the validation dataset}
\NormalTok{predictions <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(fit.lda, validation)}
\KeywordTok{confusionMatrix}\NormalTok{(predictions, validation}\OperatorTok{$}\NormalTok{Species)}
\CommentTok{#> Confusion Matrix and Statistics}
\CommentTok{#> }
\CommentTok{#>             Reference}
\CommentTok{#> Prediction   setosa versicolor virginica}
\CommentTok{#>   setosa         10          0         0}
\CommentTok{#>   versicolor      0          9         1}
\CommentTok{#>   virginica       0          1         9}
\CommentTok{#> }
\CommentTok{#> Overall Statistics}
\CommentTok{#>                                         }
\CommentTok{#>                Accuracy : 0.933         }
\CommentTok{#>                  95% CI : (0.779, 0.992)}
\CommentTok{#>     No Information Rate : 0.333         }
\CommentTok{#>     P-Value [Acc > NIR] : 8.75e-12      }
\CommentTok{#>                                         }
\CommentTok{#>                   Kappa : 0.9           }
\CommentTok{#>                                         }
\CommentTok{#>  Mcnemar's Test P-Value : NA            }
\CommentTok{#> }
\CommentTok{#> Statistics by Class:}
\CommentTok{#> }
\CommentTok{#>                      Class: setosa Class: versicolor Class: virginica}
\CommentTok{#> Sensitivity                  1.000             0.900            0.900}
\CommentTok{#> Specificity                  1.000             0.950            0.950}
\CommentTok{#> Pos Pred Value               1.000             0.900            0.900}
\CommentTok{#> Neg Pred Value               1.000             0.950            0.950}
\CommentTok{#> Prevalence                   0.333             0.333            0.333}
\CommentTok{#> Detection Rate               0.333             0.300            0.300}
\CommentTok{#> Detection Prevalence         0.333             0.333            0.333}
\CommentTok{#> Balanced Accuracy            1.000             0.925            0.925}
\end{Highlighting}
\end{Shaded}

\hypertarget{regression-algorithms-comparison.-boston-dataset.-lm-gkm-glmnet-svm-cart-knn}{%
\chapter{\texorpdfstring{Regression algorithms comparison. Boston dataset. (\emph{LM, GKM, GLMNET, SVM, CART, KNN})}{Regression algorithms comparison. Boston dataset. (LM, GKM, GLMNET, SVM, CART, KNN)}}\label{regression-algorithms-comparison.-boston-dataset.-lm-gkm-glmnet-svm-cart-knn}}

\hypertarget{boston-dataset}{%
\section{\texorpdfstring{\texttt{Boston} dataset}{Boston dataset}}\label{boston-dataset}}

\begin{itemize}
\tightlist
\item
  Comparison of various algorithms.
\end{itemize}

\hypertarget{introduction-3}{%
\section{Introduction}\label{introduction-3}}

These are the algorithms used:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  LM
\item
  GLM
\item
  GLMNET
\item
  SVM
\item
  CART
\item
  KNN
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load packages}
\KeywordTok{library}\NormalTok{(mlbench)}
\KeywordTok{library}\NormalTok{(caret)}
\CommentTok{#> Loading required package: lattice}
\CommentTok{#> Loading required package: ggplot2}
\CommentTok{#> Registered S3 methods overwritten by 'ggplot2':}
\CommentTok{#>   method         from }
\CommentTok{#>   [.quosures     rlang}
\CommentTok{#>   c.quosures     rlang}
\CommentTok{#>   print.quosures rlang}
\KeywordTok{library}\NormalTok{(corrplot)}
\CommentTok{#> corrplot 0.84 loaded}

\CommentTok{# attach the BostonHousing dataset}
\KeywordTok{data}\NormalTok{(BostonHousing)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{glimpse}\NormalTok{(BostonHousing)}
\CommentTok{#> Observations: 506}
\CommentTok{#> Variables: 14}
\CommentTok{#> $ crim    <dbl> 0.00632, 0.02731, 0.02729, 0.03237, 0.06905, 0.02985, ...}
\CommentTok{#> $ zn      <dbl> 18.0, 0.0, 0.0, 0.0, 0.0, 0.0, 12.5, 12.5, 12.5, 12.5,...}
\CommentTok{#> $ indus   <dbl> 2.31, 7.07, 7.07, 2.18, 2.18, 2.18, 7.87, 7.87, 7.87, ...}
\CommentTok{#> $ chas    <fct> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...}
\CommentTok{#> $ nox     <dbl> 0.538, 0.469, 0.469, 0.458, 0.458, 0.458, 0.524, 0.524...}
\CommentTok{#> $ rm      <dbl> 6.58, 6.42, 7.18, 7.00, 7.15, 6.43, 6.01, 6.17, 5.63, ...}
\CommentTok{#> $ age     <dbl> 65.2, 78.9, 61.1, 45.8, 54.2, 58.7, 66.6, 96.1, 100.0,...}
\CommentTok{#> $ dis     <dbl> 4.09, 4.97, 4.97, 6.06, 6.06, 6.06, 5.56, 5.95, 6.08, ...}
\CommentTok{#> $ rad     <dbl> 1, 2, 2, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 4, 4, 4, 4, 4, ...}
\CommentTok{#> $ tax     <dbl> 296, 242, 242, 222, 222, 222, 311, 311, 311, 311, 311,...}
\CommentTok{#> $ ptratio <dbl> 15.3, 17.8, 17.8, 18.7, 18.7, 18.7, 15.2, 15.2, 15.2, ...}
\CommentTok{#> $ b       <dbl> 397, 397, 393, 395, 397, 394, 396, 397, 387, 387, 393,...}
\CommentTok{#> $ lstat   <dbl> 4.98, 9.14, 4.03, 2.94, 5.33, 5.21, 12.43, 19.15, 29.9...}
\CommentTok{#> $ medv    <dbl> 24.0, 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, ...}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tibble}\OperatorTok{::}\KeywordTok{as_tibble}\NormalTok{(BostonHousing)}
\CommentTok{#> # A tibble: 506 x 14}
\CommentTok{#>      crim    zn indus chas    nox    rm   age   dis   rad   tax ptratio}
\CommentTok{#>     <dbl> <dbl> <dbl> <fct> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>   <dbl>}
\CommentTok{#> 1 0.00632    18  2.31 0     0.538  6.58  65.2  4.09     1   296    15.3}
\CommentTok{#> 2 0.0273      0  7.07 0     0.469  6.42  78.9  4.97     2   242    17.8}
\CommentTok{#> 3 0.0273      0  7.07 0     0.469  7.18  61.1  4.97     2   242    17.8}
\CommentTok{#> 4 0.0324      0  2.18 0     0.458  7.00  45.8  6.06     3   222    18.7}
\CommentTok{#> 5 0.0690      0  2.18 0     0.458  7.15  54.2  6.06     3   222    18.7}
\CommentTok{#> 6 0.0298      0  2.18 0     0.458  6.43  58.7  6.06     3   222    18.7}
\CommentTok{#> # ... with 500 more rows, and 3 more variables: b <dbl>, lstat <dbl>,}
\CommentTok{#> #   medv <dbl>}
\end{Highlighting}
\end{Shaded}

\hypertarget{workflow-2}{%
\section{Workflow}\label{workflow-2}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Load dataset
\item
  Create train and test datasets, 80/20
\item
  Inspect dataset:
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Dimension
\item
  classes
\item
  \texttt{skimr}
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Analyze features
\end{enumerate}

\begin{itemize}
\tightlist
\item
  correlation
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Visualize features
\end{enumerate}

\begin{itemize}
\tightlist
\item
  histograms
\item
  density plots
\item
  pairwise
\item
  correlogram
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Train as-is\\
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Set the train control to

  \begin{itemize}
  \tightlist
  \item
    10 cross-validations
  \item
    3 repetitions
  \item
    Metric: RMSE
  \end{itemize}
\item
  Train the models
\item
  Compare accuracy of models
\item
  Visual comparison

  \begin{itemize}
  \tightlist
  \item
    dot plot
  \end{itemize}
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  Train with Feature selection
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Feature selection

  \begin{itemize}
  \tightlist
  \item
    \texttt{findCorrelation}
  \item
    generate new dataset
  \end{itemize}
\item
  Train models again\\
\item
  Compare RMSE again
\item
  Visual comparison

  \begin{itemize}
  \tightlist
  \item
    dot plot\\
  \end{itemize}
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{6}
\tightlist
\item
  Train with dataset transformation
\end{enumerate}

\begin{itemize}
\tightlist
\item
  data transformatiom

  \begin{itemize}
  \tightlist
  \item
    Center
  \item
    Scale
  \item
    BoxCox
  \end{itemize}
\item
  Train models
\item
  Compare RMSE
\item
  Visual comparison

  \begin{itemize}
  \tightlist
  \item
    dot plot
  \end{itemize}
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{7}
\tightlist
\item
  Tune the best model
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Set the train control to

  \begin{itemize}
  \tightlist
  \item
    10 cross-validations
  \item
    3 repetitions
  \item
    Metric: RMSE
  \end{itemize}
\item
  Train the models

  \begin{itemize}
  \tightlist
  \item
    Radial SVM
  \item
    Sigma vector
  \item
    .C
  \item
    BoxCox
    9, Ensembling
  \end{itemize}
\item
  Select the algorithms

  \begin{itemize}
  \tightlist
  \item
    Random Forest
  \item
    Stochastic Gradient Boosting
  \item
    Cubist
  \end{itemize}
\item
  Numeric comparison

  \begin{itemize}
  \tightlist
  \item
    resample
  \item
    summary
  \end{itemize}
\item
  Visual comparison
\item
  dot plot
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{9}
\tightlist
\item
  Tune the best model: Cubist
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Set the train control to

  \begin{itemize}
  \tightlist
  \item
    10 cross-validations
  \item
    3 repetitions
  \item
    Metric: RMSE
  \end{itemize}
\item
  Train the models

  \begin{itemize}
  \tightlist
  \item
    Cubist
  \item
    \texttt{.committees}
  \item
    \texttt{.neighbors}
  \item
    BoxCox
  \end{itemize}
\item
  Evaluate the tuning parameters

  \begin{itemize}
  \tightlist
  \item
    Numeric comparison

    \begin{itemize}
    \tightlist
    \item
      print tuned model
    \end{itemize}
  \item
    Visual comparison

    \begin{itemize}
    \tightlist
    \item
      scatter plot
    \end{itemize}
  \end{itemize}
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{10}
\tightlist
\item
  Finalize the model
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Back transformation
\item
  Summary
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{11}
\tightlist
\item
  Apply model to validation set

  \begin{itemize}
  \tightlist
  \item
    Transform the dataset
  \item
    Make prediction
  \item
    Calculate the RMSE
  \end{itemize}
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Split out validation dataset}
\CommentTok{# create a list of 80% of the rows in the original dataset we can use for training}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{7}\NormalTok{)}
\NormalTok{validationIndex <-}\StringTok{ }\KeywordTok{createDataPartition}\NormalTok{(BostonHousing}\OperatorTok{$}\NormalTok{medv, }
                                       \DataTypeTok{p=}\FloatTok{0.80}\NormalTok{, }\DataTypeTok{list=}\OtherTok{FALSE}\NormalTok{)}

\CommentTok{# select 20% of the data for validation}
\NormalTok{validation <-}\StringTok{ }\NormalTok{BostonHousing[}\OperatorTok{-}\NormalTok{validationIndex,]}

\CommentTok{# use the remaining 80% of data to training and testing the models}
\NormalTok{dataset <-}\StringTok{ }\NormalTok{BostonHousing[validationIndex,]}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# dimensions of dataset}
\KeywordTok{dim}\NormalTok{(validation)}
\CommentTok{#> [1] 99 14}
\KeywordTok{dim}\NormalTok{(dataset)}
\CommentTok{#> [1] 407  14}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# list types for each attribute}
\KeywordTok{sapply}\NormalTok{(dataset, class)}
\CommentTok{#>      crim        zn     indus      chas       nox        rm       age }
\CommentTok{#> "numeric" "numeric" "numeric"  "factor" "numeric" "numeric" "numeric" }
\CommentTok{#>       dis       rad       tax   ptratio         b     lstat      medv }
\CommentTok{#> "numeric" "numeric" "numeric" "numeric" "numeric" "numeric" "numeric"}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# take a peek at the first 20 rows of the data}
\KeywordTok{head}\NormalTok{(dataset, }\DataTypeTok{n=}\DecValTok{20}\NormalTok{)}
\CommentTok{#>       crim   zn indus chas   nox   rm   age  dis rad tax ptratio   b lstat}
\CommentTok{#> 1  0.00632 18.0  2.31    0 0.538 6.58  65.2 4.09   1 296    15.3 397  4.98}
\CommentTok{#> 2  0.02731  0.0  7.07    0 0.469 6.42  78.9 4.97   2 242    17.8 397  9.14}
\CommentTok{#> 3  0.02729  0.0  7.07    0 0.469 7.18  61.1 4.97   2 242    17.8 393  4.03}
\CommentTok{#> 4  0.03237  0.0  2.18    0 0.458 7.00  45.8 6.06   3 222    18.7 395  2.94}
\CommentTok{#> 5  0.06905  0.0  2.18    0 0.458 7.15  54.2 6.06   3 222    18.7 397  5.33}
\CommentTok{#> 6  0.02985  0.0  2.18    0 0.458 6.43  58.7 6.06   3 222    18.7 394  5.21}
\CommentTok{#> 7  0.08829 12.5  7.87    0 0.524 6.01  66.6 5.56   5 311    15.2 396 12.43}
\CommentTok{#> 10 0.17004 12.5  7.87    0 0.524 6.00  85.9 6.59   5 311    15.2 387 17.10}
\CommentTok{#> 11 0.22489 12.5  7.87    0 0.524 6.38  94.3 6.35   5 311    15.2 393 20.45}
\CommentTok{#> 12 0.11747 12.5  7.87    0 0.524 6.01  82.9 6.23   5 311    15.2 397 13.27}
\CommentTok{#> 13 0.09378 12.5  7.87    0 0.524 5.89  39.0 5.45   5 311    15.2 390 15.71}
\CommentTok{#> 14 0.62976  0.0  8.14    0 0.538 5.95  61.8 4.71   4 307    21.0 397  8.26}
\CommentTok{#> 15 0.63796  0.0  8.14    0 0.538 6.10  84.5 4.46   4 307    21.0 380 10.26}
\CommentTok{#> 17 1.05393  0.0  8.14    0 0.538 5.93  29.3 4.50   4 307    21.0 387  6.58}
\CommentTok{#> 20 0.72580  0.0  8.14    0 0.538 5.73  69.5 3.80   4 307    21.0 391 11.28}
\CommentTok{#> 21 1.25179  0.0  8.14    0 0.538 5.57  98.1 3.80   4 307    21.0 377 21.02}
\CommentTok{#> 22 0.85204  0.0  8.14    0 0.538 5.96  89.2 4.01   4 307    21.0 393 13.83}
\CommentTok{#> 23 1.23247  0.0  8.14    0 0.538 6.14  91.7 3.98   4 307    21.0 397 18.72}
\CommentTok{#> 24 0.98843  0.0  8.14    0 0.538 5.81 100.0 4.10   4 307    21.0 395 19.88}
\CommentTok{#> 25 0.75026  0.0  8.14    0 0.538 5.92  94.1 4.40   4 307    21.0 394 16.30}
\CommentTok{#>    medv}
\CommentTok{#> 1  24.0}
\CommentTok{#> 2  21.6}
\CommentTok{#> 3  34.7}
\CommentTok{#> 4  33.4}
\CommentTok{#> 5  36.2}
\CommentTok{#> 6  28.7}
\CommentTok{#> 7  22.9}
\CommentTok{#> 10 18.9}
\CommentTok{#> 11 15.0}
\CommentTok{#> 12 18.9}
\CommentTok{#> 13 21.7}
\CommentTok{#> 14 20.4}
\CommentTok{#> 15 18.2}
\CommentTok{#> 17 23.1}
\CommentTok{#> 20 18.2}
\CommentTok{#> 21 13.6}
\CommentTok{#> 22 19.6}
\CommentTok{#> 23 15.2}
\CommentTok{#> 24 14.5}
\CommentTok{#> 25 15.6}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(skimr)}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'skimr'}
\CommentTok{#> The following object is masked from 'package:stats':}
\CommentTok{#> }
\CommentTok{#>     filter}
\KeywordTok{skim_with}\NormalTok{(}\DataTypeTok{numeric =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{hist =} \OtherTok{NULL}\NormalTok{))}
\KeywordTok{skim}\NormalTok{(dataset)}
\CommentTok{#> Skim summary statistics}
\CommentTok{#>  n obs: 407 }
\CommentTok{#>  n variables: 14 }
\CommentTok{#> }
\CommentTok{#> -- Variable type:factor --------------------------------------------------}
\CommentTok{#>  variable missing complete   n n_unique           top_counts ordered}
\CommentTok{#>      chas       0      407 407        2 0: 378, 1: 29, NA: 0   FALSE}
\CommentTok{#> }
\CommentTok{#> -- Variable type:numeric -------------------------------------------------}
\CommentTok{#>  variable missing complete   n   mean     sd       p0    p25    p50    p75}
\CommentTok{#>       age       0      407 407  68.38  28.16   6.2     42.7   77.3   94.2 }
\CommentTok{#>         b       0      407 407 357.19  89.67   0.32   373.81 391.27 396.02}
\CommentTok{#>      crim       0      407 407   3.64   8.8    0.0063   0.08   0.27   3.69}
\CommentTok{#>       dis       0      407 407   3.82   2.12   1.13     2.11   3.15   5.21}
\CommentTok{#>     indus       0      407 407  11      6.87   0.74     4.93   8.56  18.1 }
\CommentTok{#>     lstat       0      407 407  12.56   7.03   1.92     7.06  11.32  16.45}
\CommentTok{#>      medv       0      407 407  22.52   8.96   5       17.05  21.2   25   }
\CommentTok{#>       nox       0      407 407   0.55   0.12   0.39     0.45   0.54   0.63}
\CommentTok{#>   ptratio       0      407 407  18.42   2.18  12.6     17     19     20.2 }
\CommentTok{#>       rad       0      407 407   9.59   8.77   1        4      5     24   }
\CommentTok{#>        rm       0      407 407   6.29   0.7    3.56     5.89   6.21   6.62}
\CommentTok{#>       tax       0      407 407 408.75 168.72 187      280.5  334    666   }
\CommentTok{#>        zn       0      407 407  11.92  24.19   0        0      0     15   }
\CommentTok{#>    p100}
\CommentTok{#>  100   }
\CommentTok{#>  396.9 }
\CommentTok{#>   88.98}
\CommentTok{#>   12.13}
\CommentTok{#>   27.74}
\CommentTok{#>   37.97}
\CommentTok{#>   50   }
\CommentTok{#>    0.87}
\CommentTok{#>   22   }
\CommentTok{#>   24   }
\CommentTok{#>    8.78}
\CommentTok{#>  711   }
\CommentTok{#>  100}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dataset[,}\DecValTok{4}\NormalTok{] <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{as.character}\NormalTok{(dataset[,}\DecValTok{4}\NormalTok{]))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{skim}\NormalTok{(dataset)}
\CommentTok{#> Skim summary statistics}
\CommentTok{#>  n obs: 407 }
\CommentTok{#>  n variables: 14 }
\CommentTok{#> }
\CommentTok{#> -- Variable type:numeric -------------------------------------------------}
\CommentTok{#>  variable missing complete   n    mean     sd       p0    p25    p50}
\CommentTok{#>       age       0      407 407  68.38   28.16   6.2     42.7   77.3 }
\CommentTok{#>         b       0      407 407 357.19   89.67   0.32   373.81 391.27}
\CommentTok{#>      chas       0      407 407   0.071   0.26   0        0      0   }
\CommentTok{#>      crim       0      407 407   3.64    8.8    0.0063   0.08   0.27}
\CommentTok{#>       dis       0      407 407   3.82    2.12   1.13     2.11   3.15}
\CommentTok{#>     indus       0      407 407  11       6.87   0.74     4.93   8.56}
\CommentTok{#>     lstat       0      407 407  12.56    7.03   1.92     7.06  11.32}
\CommentTok{#>      medv       0      407 407  22.52    8.96   5       17.05  21.2 }
\CommentTok{#>       nox       0      407 407   0.55    0.12   0.39     0.45   0.54}
\CommentTok{#>   ptratio       0      407 407  18.42    2.18  12.6     17     19   }
\CommentTok{#>       rad       0      407 407   9.59    8.77   1        4      5   }
\CommentTok{#>        rm       0      407 407   6.29    0.7    3.56     5.89   6.21}
\CommentTok{#>       tax       0      407 407 408.75  168.72 187      280.5  334   }
\CommentTok{#>        zn       0      407 407  11.92   24.19   0        0      0   }
\CommentTok{#>     p75   p100}
\CommentTok{#>   94.2  100   }
\CommentTok{#>  396.02 396.9 }
\CommentTok{#>    0      1   }
\CommentTok{#>    3.69  88.98}
\CommentTok{#>    5.21  12.13}
\CommentTok{#>   18.1   27.74}
\CommentTok{#>   16.45  37.97}
\CommentTok{#>   25     50   }
\CommentTok{#>    0.63   0.87}
\CommentTok{#>   20.2   22   }
\CommentTok{#>   24     24   }
\CommentTok{#>    6.62   8.78}
\CommentTok{#>  666    711   }
\CommentTok{#>   15    100}
\end{Highlighting}
\end{Shaded}

\begin{quote}
no more factors or character variables
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# find correlation between variables}
\KeywordTok{cor}\NormalTok{(dataset[,}\DecValTok{1}\OperatorTok{:}\DecValTok{13}\NormalTok{])}
\CommentTok{#>            crim      zn   indus     chas     nox     rm     age    dis}
\CommentTok{#> crim     1.0000 -0.1996  0.4076 -0.05507  0.4099 -0.194  0.3524 -0.376}
\CommentTok{#> zn      -0.1996  1.0000 -0.5314 -0.02987 -0.5202  0.311 -0.5845  0.680}
\CommentTok{#> indus    0.4076 -0.5314  1.0000  0.06583  0.7733 -0.383  0.6512 -0.711}
\CommentTok{#> chas    -0.0551 -0.0299  0.0658  1.00000  0.0934  0.127  0.0735 -0.099}
\CommentTok{#> nox      0.4099 -0.5202  0.7733  0.09340  1.0000 -0.296  0.7338 -0.769}
\CommentTok{#> rm      -0.1940  0.3111 -0.3826  0.12677 -0.2961  1.000 -0.2262  0.207}
\CommentTok{#> age      0.3524 -0.5845  0.6512  0.07350  0.7338 -0.226  1.0000 -0.749}
\CommentTok{#> dis     -0.3756  0.6799 -0.7113 -0.09905 -0.7693  0.207 -0.7492  1.000}
\CommentTok{#> rad      0.6083 -0.3227  0.6200 -0.00245  0.6276 -0.221  0.4690 -0.504}
\CommentTok{#> tax      0.5711 -0.3184  0.7185 -0.03064  0.6758 -0.295  0.5058 -0.526}
\CommentTok{#> ptratio  0.2897 -0.3888  0.3782 -0.12283  0.1888 -0.365  0.2709 -0.228}
\CommentTok{#> b       -0.3442  0.1747 -0.3644  0.03782 -0.3684  0.126 -0.2742  0.284}
\CommentTok{#> lstat    0.4229 -0.4219  0.6136 -0.08430  0.5839 -0.612  0.6066 -0.501}
\CommentTok{#>              rad     tax ptratio       b   lstat}
\CommentTok{#> crim     0.60834  0.5711   0.290 -0.3442  0.4229}
\CommentTok{#> zn      -0.32273 -0.3184  -0.389  0.1747 -0.4219}
\CommentTok{#> indus    0.61998  0.7185   0.378 -0.3644  0.6136}
\CommentTok{#> chas    -0.00245 -0.0306  -0.123  0.0378 -0.0843}
\CommentTok{#> nox      0.62760  0.6758   0.189 -0.3684  0.5839}
\CommentTok{#> rm      -0.22126 -0.2953  -0.365  0.1260 -0.6120}
\CommentTok{#> age      0.46896  0.5058   0.271 -0.2742  0.6066}
\CommentTok{#> dis     -0.50372 -0.5264  -0.228  0.2843 -0.5013}
\CommentTok{#> rad      1.00000  0.9201   0.480 -0.4231  0.5025}
\CommentTok{#> tax      0.92005  1.0000   0.469 -0.4303  0.5538}
\CommentTok{#> ptratio  0.47971  0.4691   1.000 -0.1700  0.4093}
\CommentTok{#> b       -0.42314 -0.4303  -0.170  1.0000 -0.3509}
\CommentTok{#> lstat    0.50251  0.5538   0.409 -0.3509  1.0000}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(dplyr)}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'dplyr'}
\CommentTok{#> The following objects are masked from 'package:stats':}
\CommentTok{#> }
\CommentTok{#>     filter, lag}
\CommentTok{#> The following objects are masked from 'package:base':}
\CommentTok{#> }
\CommentTok{#>     intersect, setdiff, setequal, union}

\NormalTok{m <-}\StringTok{ }\KeywordTok{cor}\NormalTok{(dataset[,}\DecValTok{1}\OperatorTok{:}\DecValTok{13}\NormalTok{])}
\KeywordTok{diag}\NormalTok{(m) <-}\StringTok{ }\DecValTok{0}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# select variables with correlation 0.7 and above}
\NormalTok{threshold <-}\StringTok{ }\FloatTok{0.7}
\NormalTok{ok <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(}\KeywordTok{abs}\NormalTok{(m) }\OperatorTok{>=}\StringTok{ }\NormalTok{threshold, }\DecValTok{1}\NormalTok{, any)}
\NormalTok{m[ok, ok]}
\CommentTok{#>        indus    nox    age    dis    rad    tax}
\CommentTok{#> indus  0.000  0.773  0.651 -0.711  0.620  0.719}
\CommentTok{#> nox    0.773  0.000  0.734 -0.769  0.628  0.676}
\CommentTok{#> age    0.651  0.734  0.000 -0.749  0.469  0.506}
\CommentTok{#> dis   -0.711 -0.769 -0.749  0.000 -0.504 -0.526}
\CommentTok{#> rad    0.620  0.628  0.469 -0.504  0.000  0.920}
\CommentTok{#> tax    0.719  0.676  0.506 -0.526  0.920  0.000}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# values of correlation >= 0.7}
\NormalTok{ind <-}\StringTok{ }\KeywordTok{sapply}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{13}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{abs}\NormalTok{(m[, x]) }\OperatorTok{>}\StringTok{ }\FloatTok{0.7}\NormalTok{)}
\NormalTok{m[ind]}
\CommentTok{#>  [1]  0.773 -0.711  0.719  0.773  0.734 -0.769  0.734 -0.749 -0.711 -0.769}
\CommentTok{#> [11] -0.749  0.920  0.719  0.920}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# defining a index for selecting if the condition is met}
\NormalTok{cind <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(m, }\DecValTok{2}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{any}\NormalTok{(}\KeywordTok{abs}\NormalTok{(x) }\OperatorTok{>}\StringTok{ }\FloatTok{0.7}\NormalTok{))}
\NormalTok{cm <-}\StringTok{ }\NormalTok{m[, cind] }\CommentTok{# since col6 only has values less than 0.5 it is not taken}
\NormalTok{cm}
\CommentTok{#>           indus     nox     age    dis      rad     tax}
\CommentTok{#> crim     0.4076  0.4099  0.3524 -0.376  0.60834  0.5711}
\CommentTok{#> zn      -0.5314 -0.5202 -0.5845  0.680 -0.32273 -0.3184}
\CommentTok{#> indus    0.0000  0.7733  0.6512 -0.711  0.61998  0.7185}
\CommentTok{#> chas     0.0658  0.0934  0.0735 -0.099 -0.00245 -0.0306}
\CommentTok{#> nox      0.7733  0.0000  0.7338 -0.769  0.62760  0.6758}
\CommentTok{#> rm      -0.3826 -0.2961 -0.2262  0.207 -0.22126 -0.2953}
\CommentTok{#> age      0.6512  0.7338  0.0000 -0.749  0.46896  0.5058}
\CommentTok{#> dis     -0.7113 -0.7693 -0.7492  0.000 -0.50372 -0.5264}
\CommentTok{#> rad      0.6200  0.6276  0.4690 -0.504  0.00000  0.9201}
\CommentTok{#> tax      0.7185  0.6758  0.5058 -0.526  0.92005  0.0000}
\CommentTok{#> ptratio  0.3782  0.1888  0.2709 -0.228  0.47971  0.4691}
\CommentTok{#> b       -0.3644 -0.3684 -0.2742  0.284 -0.42314 -0.4303}
\CommentTok{#> lstat    0.6136  0.5839  0.6066 -0.501  0.50251  0.5538}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rind <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(cm, }\DecValTok{1}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{any}\NormalTok{(}\KeywordTok{abs}\NormalTok{(x) }\OperatorTok{>}\StringTok{ }\FloatTok{0.7}\NormalTok{))  }
\NormalTok{rm <-}\StringTok{ }\NormalTok{cm[rind, ]}
\NormalTok{rm}
\CommentTok{#>        indus    nox    age    dis    rad    tax}
\CommentTok{#> indus  0.000  0.773  0.651 -0.711  0.620  0.719}
\CommentTok{#> nox    0.773  0.000  0.734 -0.769  0.628  0.676}
\CommentTok{#> age    0.651  0.734  0.000 -0.749  0.469  0.506}
\CommentTok{#> dis   -0.711 -0.769 -0.749  0.000 -0.504 -0.526}
\CommentTok{#> rad    0.620  0.628  0.469 -0.504  0.000  0.920}
\CommentTok{#> tax    0.719  0.676  0.506 -0.526  0.920  0.000}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# histograms for each attribute}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{5}\NormalTok{))}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{13}\NormalTok{) \{}
    \KeywordTok{hist}\NormalTok{(dataset[,i], }\DataTypeTok{main=}\KeywordTok{names}\NormalTok{(dataset)[i])}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{algo-comparison_321-regression_boston-multi_files/figure-latex/plot-histograms-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# density plot for each attribute}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{5}\NormalTok{))}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{13}\NormalTok{) \{}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{density}\NormalTok{(dataset[,i]), }\DataTypeTok{main=}\KeywordTok{names}\NormalTok{(dataset)[i])}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{algo-comparison_321-regression_boston-multi_files/figure-latex/plot-density-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# boxplots for each attribute}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{5}\NormalTok{))}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{13}\NormalTok{) \{}
\KeywordTok{boxplot}\NormalTok{(dataset[,i], }\DataTypeTok{main=}\KeywordTok{names}\NormalTok{(dataset)[i])}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{algo-comparison_321-regression_boston-multi_files/figure-latex/plot-boxplots, -1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# scatter plot matrix}
\KeywordTok{pairs}\NormalTok{(dataset[,}\DecValTok{1}\OperatorTok{:}\DecValTok{13}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{algo-comparison_321-regression_boston-multi_files/figure-latex/unnamed-chunk-18-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# correlation plot}
\NormalTok{correlations <-}\StringTok{ }\KeywordTok{cor}\NormalTok{(dataset[,}\DecValTok{1}\OperatorTok{:}\DecValTok{13}\NormalTok{])}
\KeywordTok{corrplot}\NormalTok{(correlations, }\DataTypeTok{method=}\StringTok{"circle"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{algo-comparison_321-regression_boston-multi_files/figure-latex/plot-correlation-1} \end{center}

\hypertarget{evaluation}{%
\section{Evaluation}\label{evaluation}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Run algorithms using 10-fold cross-validation}
\NormalTok{trainControl <-}\StringTok{ }\KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method=}\StringTok{"repeatedcv"}\NormalTok{, }\DataTypeTok{number=}\DecValTok{10}\NormalTok{, }\DataTypeTok{repeats=}\DecValTok{3}\NormalTok{)}
\NormalTok{metric <-}\StringTok{ "RMSE"}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# LM}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{7}\NormalTok{)}
\NormalTok{fit.lm <-}\StringTok{ }\KeywordTok{train}\NormalTok{(medv}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{dataset, }\DataTypeTok{method=}\StringTok{"lm"}\NormalTok{, }
                \DataTypeTok{metric=}\NormalTok{metric, }\DataTypeTok{preProc=}\KeywordTok{c}\NormalTok{(}\StringTok{"center"}\NormalTok{, }\StringTok{"scale"}\NormalTok{), }
                \DataTypeTok{trControl=}\NormalTok{trainControl)}
\CommentTok{# GLM}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{7}\NormalTok{)}
\NormalTok{fit.glm <-}\StringTok{ }\KeywordTok{train}\NormalTok{(medv}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{dataset, }\DataTypeTok{method=}\StringTok{"glm"}\NormalTok{, }
                 \DataTypeTok{metric=}\NormalTok{metric, }\DataTypeTok{preProc=}\KeywordTok{c}\NormalTok{(}\StringTok{"center"}\NormalTok{, }\StringTok{"scale"}\NormalTok{), }
                 \DataTypeTok{trControl=}\NormalTok{trainControl)}
\CommentTok{# GLMNET}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{7}\NormalTok{)}
\NormalTok{fit.glmnet <-}\StringTok{ }\KeywordTok{train}\NormalTok{(medv}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{dataset, }\DataTypeTok{method=}\StringTok{"glmnet"}\NormalTok{, }
                    \DataTypeTok{metric=}\NormalTok{metric, }
                    \DataTypeTok{preProc=}\KeywordTok{c}\NormalTok{(}\StringTok{"center"}\NormalTok{, }\StringTok{"scale"}\NormalTok{), }
                    \DataTypeTok{trControl=}\NormalTok{trainControl)}
\CommentTok{# SVM}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{7}\NormalTok{)}
\NormalTok{fit.svm <-}\StringTok{ }\KeywordTok{train}\NormalTok{(medv}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{dataset, }\DataTypeTok{method=}\StringTok{"svmRadial"}\NormalTok{, }
                 \DataTypeTok{metric=}\NormalTok{metric, }
                 \DataTypeTok{preProc=}\KeywordTok{c}\NormalTok{(}\StringTok{"center"}\NormalTok{, }\StringTok{"scale"}\NormalTok{), }
                 \DataTypeTok{trControl=}\NormalTok{trainControl)}
\CommentTok{# CART}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{7}\NormalTok{)}
\NormalTok{grid <-}\StringTok{ }\KeywordTok{expand.grid}\NormalTok{(}\DataTypeTok{.cp=}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.05}\NormalTok{, }\FloatTok{0.1}\NormalTok{))}
\NormalTok{fit.cart <-}\StringTok{ }\KeywordTok{train}\NormalTok{(medv}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{dataset, }\DataTypeTok{method=}\StringTok{"rpart"}\NormalTok{, }
                  \DataTypeTok{metric=}\NormalTok{metric, }\DataTypeTok{tuneGrid=}\NormalTok{grid, }
                  \DataTypeTok{preProc=}\KeywordTok{c}\NormalTok{(}\StringTok{"center"}\NormalTok{, }\StringTok{"scale"}\NormalTok{), }
                  \DataTypeTok{trControl=}\NormalTok{trainControl)}
\CommentTok{# KNN}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{7}\NormalTok{)}
\NormalTok{fit.knn <-}\StringTok{ }\KeywordTok{train}\NormalTok{(medv}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{dataset, }\DataTypeTok{method=}\StringTok{"knn"}\NormalTok{, }
                 \DataTypeTok{metric=}\NormalTok{metric, }\DataTypeTok{preProc=}\KeywordTok{c}\NormalTok{(}\StringTok{"center"}\NormalTok{, }\StringTok{"scale"}\NormalTok{), }
                 \DataTypeTok{trControl=}\NormalTok{trainControl)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Compare algorithms}
\NormalTok{results <-}\StringTok{ }\KeywordTok{resamples}\NormalTok{(}\KeywordTok{list}\NormalTok{(}\DataTypeTok{LM     =}\NormalTok{ fit.lm, }
                          \DataTypeTok{GLM    =}\NormalTok{ fit.glm, }
                          \DataTypeTok{GLMNET =}\NormalTok{ fit.glmnet, }
                          \DataTypeTok{SVM    =}\NormalTok{ fit.svm, }
                          \DataTypeTok{CART   =}\NormalTok{ fit.cart, }
                          \DataTypeTok{KNN    =}\NormalTok{ fit.knn))}
\KeywordTok{summary}\NormalTok{(results)}
\CommentTok{#> }
\CommentTok{#> Call:}
\CommentTok{#> summary.resamples(object = results)}
\CommentTok{#> }
\CommentTok{#> Models: LM, GLM, GLMNET, SVM, CART, KNN }
\CommentTok{#> Number of resamples: 30 }
\CommentTok{#> }
\CommentTok{#> MAE }
\CommentTok{#>        Min. 1st Qu. Median Mean 3rd Qu. Max. NA's}
\CommentTok{#> LM     2.30    2.90   3.37 3.32    3.70 4.64    0}
\CommentTok{#> GLM    2.30    2.90   3.37 3.32    3.70 4.64    0}
\CommentTok{#> GLMNET 2.30    2.88   3.34 3.30    3.70 4.63    0}
\CommentTok{#> SVM    1.42    1.99   2.52 2.39    2.65 3.35    0}
\CommentTok{#> CART   2.22    2.62   2.88 2.93    3.08 4.16    0}
\CommentTok{#> KNN    1.98    2.69   2.87 2.95    3.24 4.00    0}
\CommentTok{#> }
\CommentTok{#> RMSE }
\CommentTok{#>        Min. 1st Qu. Median Mean 3rd Qu. Max. NA's}
\CommentTok{#> LM     2.99    3.87   4.63 4.63    5.32 6.69    0}
\CommentTok{#> GLM    2.99    3.87   4.63 4.63    5.32 6.69    0}
\CommentTok{#> GLMNET 2.99    3.88   4.62 4.62    5.32 6.69    0}
\CommentTok{#> SVM    2.05    2.95   3.81 3.91    4.46 6.98    0}
\CommentTok{#> CART   2.77    3.38   4.00 4.20    4.60 7.09    0}
\CommentTok{#> KNN    2.65    3.74   4.42 4.48    5.06 6.98    0}
\CommentTok{#> }
\CommentTok{#> Rsquared }
\CommentTok{#>         Min. 1st Qu. Median  Mean 3rd Qu.  Max. NA's}
\CommentTok{#> LM     0.505   0.674  0.747 0.740   0.813 0.900    0}
\CommentTok{#> GLM    0.505   0.674  0.747 0.740   0.813 0.900    0}
\CommentTok{#> GLMNET 0.503   0.673  0.747 0.741   0.816 0.904    0}
\CommentTok{#> SVM    0.519   0.762  0.845 0.810   0.896 0.970    0}
\CommentTok{#> CART   0.514   0.737  0.816 0.778   0.842 0.899    0}
\CommentTok{#> KNN    0.519   0.748  0.804 0.770   0.829 0.931    0}
\KeywordTok{dotplot}\NormalTok{(results)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{algo-comparison_321-regression_boston-multi_files/figure-latex/unnamed-chunk-20-1} \end{center}

\hypertarget{feature-selection}{%
\section{Feature selection}\label{feature-selection}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# remove correlated attributes}
\CommentTok{# find attributes that are highly correlated}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{7}\NormalTok{)}
\NormalTok{cutoff <-}\StringTok{ }\FloatTok{0.70}
\NormalTok{correlations <-}\StringTok{ }\KeywordTok{cor}\NormalTok{(dataset[,}\DecValTok{1}\OperatorTok{:}\DecValTok{13}\NormalTok{])}
\NormalTok{highlyCorrelated <-}\StringTok{ }\KeywordTok{findCorrelation}\NormalTok{(correlations, }\DataTypeTok{cutoff=}\NormalTok{cutoff)}

\ControlFlowTok{for}\NormalTok{ (value }\ControlFlowTok{in}\NormalTok{ highlyCorrelated) \{}
    \KeywordTok{print}\NormalTok{(}\KeywordTok{names}\NormalTok{(dataset)[value])}
\NormalTok{\}}
\CommentTok{#> [1] "indus"}
\CommentTok{#> [1] "nox"}
\CommentTok{#> [1] "tax"}
\CommentTok{#> [1] "dis"}

\CommentTok{# create a new dataset without highly correlated features}
\NormalTok{datasetFeatures <-}\StringTok{ }\NormalTok{dataset[,}\OperatorTok{-}\NormalTok{highlyCorrelated]}
\KeywordTok{dim}\NormalTok{(datasetFeatures)}
\CommentTok{#> [1] 407  10}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Run algorithms using 10-fold cross-validation}
\NormalTok{trainControl <-}\StringTok{ }\KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method=}\StringTok{"repeatedcv"}\NormalTok{, }\DataTypeTok{number=}\DecValTok{10}\NormalTok{, }\DataTypeTok{repeats=}\DecValTok{3}\NormalTok{)}
\NormalTok{metric <-}\StringTok{ "RMSE"}

\CommentTok{# LM}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{7}\NormalTok{)}
\NormalTok{fit.lm <-}\StringTok{ }\KeywordTok{train}\NormalTok{(medv}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{dataset, }\DataTypeTok{method=}\StringTok{"lm"}\NormalTok{, }
                \DataTypeTok{metric=}\NormalTok{metric, }\DataTypeTok{preProc=}\KeywordTok{c}\NormalTok{(}\StringTok{"center"}\NormalTok{, }\StringTok{"scale"}\NormalTok{), }
                \DataTypeTok{trControl=}\NormalTok{trainControl)}
\CommentTok{# GLM}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{7}\NormalTok{)}
\NormalTok{fit.glm <-}\StringTok{ }\KeywordTok{train}\NormalTok{(medv}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{dataset, }\DataTypeTok{method=}\StringTok{"glm"}\NormalTok{, }
                 \DataTypeTok{metric=}\NormalTok{metric, }\DataTypeTok{preProc=}\KeywordTok{c}\NormalTok{(}\StringTok{"center"}\NormalTok{, }\StringTok{"scale"}\NormalTok{), }
                 \DataTypeTok{trControl=}\NormalTok{trainControl)}
\CommentTok{# GLMNET}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{7}\NormalTok{)}
\NormalTok{fit.glmnet <-}\StringTok{ }\KeywordTok{train}\NormalTok{(medv}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{dataset, }\DataTypeTok{method=}\StringTok{"glmnet"}\NormalTok{, }
                    \DataTypeTok{metric=}\NormalTok{metric, }
                    \DataTypeTok{preProc=}\KeywordTok{c}\NormalTok{(}\StringTok{"center"}\NormalTok{, }\StringTok{"scale"}\NormalTok{), }
                    \DataTypeTok{trControl=}\NormalTok{trainControl)}
\CommentTok{# SVM}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{7}\NormalTok{)}
\NormalTok{fit.svm <-}\StringTok{ }\KeywordTok{train}\NormalTok{(medv}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{dataset, }\DataTypeTok{method=}\StringTok{"svmRadial"}\NormalTok{, }
                 \DataTypeTok{metric=}\NormalTok{metric, }
                 \DataTypeTok{preProc=}\KeywordTok{c}\NormalTok{(}\StringTok{"center"}\NormalTok{, }\StringTok{"scale"}\NormalTok{), }
                 \DataTypeTok{trControl=}\NormalTok{trainControl)}
\CommentTok{# CART}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{7}\NormalTok{)}
\NormalTok{grid <-}\StringTok{ }\KeywordTok{expand.grid}\NormalTok{(}\DataTypeTok{.cp=}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.05}\NormalTok{, }\FloatTok{0.1}\NormalTok{))}
\NormalTok{fit.cart <-}\StringTok{ }\KeywordTok{train}\NormalTok{(medv}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{dataset, }\DataTypeTok{method=}\StringTok{"rpart"}\NormalTok{, }
                  \DataTypeTok{metric=}\NormalTok{metric, }\DataTypeTok{tuneGrid=}\NormalTok{grid, }
                  \DataTypeTok{preProc=}\KeywordTok{c}\NormalTok{(}\StringTok{"center"}\NormalTok{, }\StringTok{"scale"}\NormalTok{), }
                  \DataTypeTok{trControl=}\NormalTok{trainControl)}
\CommentTok{# KNN}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{7}\NormalTok{)}
\NormalTok{fit.knn <-}\StringTok{ }\KeywordTok{train}\NormalTok{(medv}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{dataset, }\DataTypeTok{method=}\StringTok{"knn"}\NormalTok{, }
                 \DataTypeTok{metric=}\NormalTok{metric, }\DataTypeTok{preProc=}\KeywordTok{c}\NormalTok{(}\StringTok{"center"}\NormalTok{, }\StringTok{"scale"}\NormalTok{), }
                 \DataTypeTok{trControl=}\NormalTok{trainControl)}

\CommentTok{# Compare algorithms}
\NormalTok{feature_results <-}\StringTok{ }\KeywordTok{resamples}\NormalTok{(}\KeywordTok{list}\NormalTok{(}\DataTypeTok{LM     =}\NormalTok{ fit.lm, }
                                  \DataTypeTok{GLM    =}\NormalTok{ fit.glm, }
                                  \DataTypeTok{GLMNET =}\NormalTok{ fit.glmnet, }
                                  \DataTypeTok{SVM    =}\NormalTok{ fit.svm, }
                                  \DataTypeTok{CART   =}\NormalTok{ fit.cart, }
                                  \DataTypeTok{KNN    =}\NormalTok{ fit.knn))}
\KeywordTok{summary}\NormalTok{(feature_results)}
\CommentTok{#> }
\CommentTok{#> Call:}
\CommentTok{#> summary.resamples(object = feature_results)}
\CommentTok{#> }
\CommentTok{#> Models: LM, GLM, GLMNET, SVM, CART, KNN }
\CommentTok{#> Number of resamples: 30 }
\CommentTok{#> }
\CommentTok{#> MAE }
\CommentTok{#>        Min. 1st Qu. Median Mean 3rd Qu. Max. NA's}
\CommentTok{#> LM     2.30    2.90   3.37 3.32    3.70 4.64    0}
\CommentTok{#> GLM    2.30    2.90   3.37 3.32    3.70 4.64    0}
\CommentTok{#> GLMNET 2.30    2.88   3.34 3.30    3.70 4.63    0}
\CommentTok{#> SVM    1.42    1.99   2.52 2.39    2.65 3.35    0}
\CommentTok{#> CART   2.22    2.62   2.88 2.93    3.08 4.16    0}
\CommentTok{#> KNN    1.98    2.69   2.87 2.95    3.24 4.00    0}
\CommentTok{#> }
\CommentTok{#> RMSE }
\CommentTok{#>        Min. 1st Qu. Median Mean 3rd Qu. Max. NA's}
\CommentTok{#> LM     2.99    3.87   4.63 4.63    5.32 6.69    0}
\CommentTok{#> GLM    2.99    3.87   4.63 4.63    5.32 6.69    0}
\CommentTok{#> GLMNET 2.99    3.88   4.62 4.62    5.32 6.69    0}
\CommentTok{#> SVM    2.05    2.95   3.81 3.91    4.46 6.98    0}
\CommentTok{#> CART   2.77    3.38   4.00 4.20    4.60 7.09    0}
\CommentTok{#> KNN    2.65    3.74   4.42 4.48    5.06 6.98    0}
\CommentTok{#> }
\CommentTok{#> Rsquared }
\CommentTok{#>         Min. 1st Qu. Median  Mean 3rd Qu.  Max. NA's}
\CommentTok{#> LM     0.505   0.674  0.747 0.740   0.813 0.900    0}
\CommentTok{#> GLM    0.505   0.674  0.747 0.740   0.813 0.900    0}
\CommentTok{#> GLMNET 0.503   0.673  0.747 0.741   0.816 0.904    0}
\CommentTok{#> SVM    0.519   0.762  0.845 0.810   0.896 0.970    0}
\CommentTok{#> CART   0.514   0.737  0.816 0.778   0.842 0.899    0}
\CommentTok{#> KNN    0.519   0.748  0.804 0.770   0.829 0.931    0}
\KeywordTok{dotplot}\NormalTok{(feature_results)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{algo-comparison_321-regression_boston-multi_files/figure-latex/models-second-run-removeFeatures-1} \end{center}

Comparing the results, we can see that this has made the RMSE worse for the linear and the nonlinear algorithms. The correlated attributes we removed are contributing to the accuracy of the models.

\hypertarget{evaluate-algorithms-box-cox-transform}{%
\section{Evaluate Algorithms: Box-Cox Transform}\label{evaluate-algorithms-box-cox-transform}}

We know that some of the attributes have a skew and others perhaps have an
exponential distribution. One option would be to explore squaring and log
transforms respectively (you could try this!). Another approach would be to use a power transform and let it figure out the amount to correct each attribute. One example is the \texttt{Box-Cox} power transform. Let's try using this transform to rescale the original data and evaluate the effect on the same 6 algorithms. We will also leave in the centering and scaling for the benefit of the instance-based methods.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Run algorithms using 10-fold cross-validation}
\NormalTok{trainControl <-}\StringTok{ }\KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method=}\StringTok{"repeatedcv"}\NormalTok{, }\DataTypeTok{number=}\DecValTok{10}\NormalTok{, }\DataTypeTok{repeats=}\DecValTok{3}\NormalTok{)}
\NormalTok{metric <-}\StringTok{ "RMSE"}

\CommentTok{# lm}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{7}\NormalTok{)}
\NormalTok{fit.lm <-}\StringTok{ }\KeywordTok{train}\NormalTok{(medv}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{dataset, }\DataTypeTok{method=}\StringTok{"lm"}\NormalTok{, }\DataTypeTok{metric=}\NormalTok{metric, }
                \DataTypeTok{preProc=}\KeywordTok{c}\NormalTok{(}\StringTok{"center"}\NormalTok{, }\StringTok{"scale"}\NormalTok{, }\StringTok{"BoxCox"}\NormalTok{), }
                \DataTypeTok{trControl=}\NormalTok{trainControl)}
\CommentTok{# GLM}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{7}\NormalTok{)}
\NormalTok{fit.glm <-}\StringTok{ }\KeywordTok{train}\NormalTok{(medv}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{dataset, }\DataTypeTok{method=}\StringTok{"glm"}\NormalTok{, }\DataTypeTok{metric=}\NormalTok{metric, }
                 \DataTypeTok{preProc=}\KeywordTok{c}\NormalTok{(}\StringTok{"center"}\NormalTok{, }\StringTok{"scale"}\NormalTok{, }\StringTok{"BoxCox"}\NormalTok{), }
                 \DataTypeTok{trControl=}\NormalTok{trainControl)}
\CommentTok{# GLMNET}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{7}\NormalTok{)}
\NormalTok{fit.glmnet <-}\StringTok{ }\KeywordTok{train}\NormalTok{(medv}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{dataset, }\DataTypeTok{method=}\StringTok{"glmnet"}\NormalTok{, }\DataTypeTok{metric=}\NormalTok{metric, }
                    \DataTypeTok{preProc=}\KeywordTok{c}\NormalTok{(}\StringTok{"center"}\NormalTok{, }\StringTok{"scale"}\NormalTok{, }\StringTok{"BoxCox"}\NormalTok{),}
                    \DataTypeTok{trControl=}\NormalTok{trainControl)}
\CommentTok{# SVM}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{7}\NormalTok{)}
\NormalTok{fit.svm <-}\StringTok{ }\KeywordTok{train}\NormalTok{(medv}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{dataset, }\DataTypeTok{method=}\StringTok{"svmRadial"}\NormalTok{, }\DataTypeTok{metric=}\NormalTok{metric, }
                 \DataTypeTok{preProc=}\KeywordTok{c}\NormalTok{(}\StringTok{"center"}\NormalTok{, }\StringTok{"scale"}\NormalTok{, }\StringTok{"BoxCox"}\NormalTok{),}
                 \DataTypeTok{trControl=}\NormalTok{trainControl)}
\CommentTok{# CART}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{7}\NormalTok{)}
\NormalTok{grid <-}\StringTok{ }\KeywordTok{expand.grid}\NormalTok{(}\DataTypeTok{.cp=}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.05}\NormalTok{, }\FloatTok{0.1}\NormalTok{))}
\NormalTok{fit.cart <-}\StringTok{ }\KeywordTok{train}\NormalTok{(medv}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{dataset, }\DataTypeTok{method=}\StringTok{"rpart"}\NormalTok{, }\DataTypeTok{metric=}\NormalTok{metric,}
                  \DataTypeTok{tuneGrid=}\NormalTok{grid,}
                  \DataTypeTok{preProc=}\KeywordTok{c}\NormalTok{(}\StringTok{"center"}\NormalTok{, }\StringTok{"scale"}\NormalTok{, }\StringTok{"BoxCox"}\NormalTok{),}
                  \DataTypeTok{trControl=}\NormalTok{trainControl)}
\CommentTok{# KNN}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{7}\NormalTok{)}
\NormalTok{fit.knn <-}\StringTok{ }\KeywordTok{train}\NormalTok{(medv}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{dataset, }\DataTypeTok{method=}\StringTok{"knn"}\NormalTok{, }\DataTypeTok{metric=}\NormalTok{metric, }
                 \DataTypeTok{preProc=}\KeywordTok{c}\NormalTok{(}\StringTok{"center"}\NormalTok{, }\StringTok{"scale"}\NormalTok{, }\StringTok{"BoxCox"}\NormalTok{), }
                 \DataTypeTok{trControl=}\NormalTok{trainControl)}

\CommentTok{# Compare algorithms}
\NormalTok{transformResults <-}\StringTok{ }\KeywordTok{resamples}\NormalTok{(}\KeywordTok{list}\NormalTok{(}\DataTypeTok{LM     =}\NormalTok{ fit.lm, }
                                  \DataTypeTok{GLM    =}\NormalTok{ fit.glm, }
                                  \DataTypeTok{GLMNET =}\NormalTok{ fit.glmnet, }
                                  \DataTypeTok{SVM    =}\NormalTok{ fit.svm, }
                                  \DataTypeTok{CART   =}\NormalTok{ fit.cart, }
                                  \DataTypeTok{KNN    =}\NormalTok{ fit.knn))}
\KeywordTok{summary}\NormalTok{(transformResults)}
\CommentTok{#> }
\CommentTok{#> Call:}
\CommentTok{#> summary.resamples(object = transformResults)}
\CommentTok{#> }
\CommentTok{#> Models: LM, GLM, GLMNET, SVM, CART, KNN }
\CommentTok{#> Number of resamples: 30 }
\CommentTok{#> }
\CommentTok{#> MAE }
\CommentTok{#>        Min. 1st Qu. Median Mean 3rd Qu. Max. NA's}
\CommentTok{#> LM     2.10    2.80   3.21 3.18    3.45 4.44    0}
\CommentTok{#> GLM    2.10    2.80   3.21 3.18    3.45 4.44    0}
\CommentTok{#> GLMNET 2.11    2.80   3.21 3.17    3.45 4.43    0}
\CommentTok{#> SVM    1.30    1.95   2.25 2.26    2.48 3.19    0}
\CommentTok{#> CART   2.22    2.62   2.89 2.94    3.11 4.16    0}
\CommentTok{#> KNN    2.33    2.66   2.82 2.97    3.27 3.96    0}
\CommentTok{#> }
\CommentTok{#> RMSE }
\CommentTok{#>        Min. 1st Qu. Median Mean 3rd Qu. Max. NA's}
\CommentTok{#> LM     2.82    3.81   4.43 4.37    5.00 6.16    0}
\CommentTok{#> GLM    2.82    3.81   4.43 4.37    5.00 6.16    0}
\CommentTok{#> GLMNET 2.83    3.79   4.42 4.36    5.00 6.18    0}
\CommentTok{#> SVM    1.80    2.73   3.41 3.70    4.24 6.73    0}
\CommentTok{#> CART   2.77    3.38   4.00 4.23    4.83 7.09    0}
\CommentTok{#> KNN    3.01    3.73   4.37 4.52    5.02 7.30    0}
\CommentTok{#> }
\CommentTok{#> Rsquared }
\CommentTok{#>         Min. 1st Qu. Median  Mean 3rd Qu.  Max. NA's}
\CommentTok{#> LM     0.560   0.712  0.775 0.766   0.825 0.910    0}
\CommentTok{#> GLM    0.560   0.712  0.775 0.766   0.825 0.910    0}
\CommentTok{#> GLMNET 0.556   0.713  0.775 0.766   0.826 0.910    0}
\CommentTok{#> SVM    0.524   0.778  0.854 0.827   0.907 0.979    0}
\CommentTok{#> CART   0.514   0.727  0.816 0.774   0.843 0.899    0}
\CommentTok{#> KNN    0.492   0.723  0.792 0.762   0.842 0.937    0}
\KeywordTok{dotplot}\NormalTok{(transformResults)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{algo-comparison_321-regression_boston-multi_files/figure-latex/model-third-run-boxcox-1} \end{center}

\hypertarget{tune-svm}{%
\section{Tune SVM}\label{tune-svm}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{print}\NormalTok{(fit.svm)}
\CommentTok{#> Support Vector Machines with Radial Basis Function Kernel }
\CommentTok{#> }
\CommentTok{#> 407 samples}
\CommentTok{#>  13 predictor}
\CommentTok{#> }
\CommentTok{#> Pre-processing: centered (13), scaled (13), Box-Cox transformation (11) }
\CommentTok{#> Resampling: Cross-Validated (10 fold, repeated 3 times) }
\CommentTok{#> Summary of sample sizes: 365, 366, 366, 367, 366, 366, ... }
\CommentTok{#> Resampling results across tuning parameters:}
\CommentTok{#> }
\CommentTok{#>   C     RMSE  Rsquared  MAE }
\CommentTok{#>   0.25  4.54  0.772     2.73}
\CommentTok{#>   0.50  4.07  0.802     2.46}
\CommentTok{#>   1.00  3.70  0.827     2.26}
\CommentTok{#> }
\CommentTok{#> Tuning parameter 'sigma' was held constant at a value of 0.116}
\CommentTok{#> RMSE was used to select the optimal model using the smallest value.}
\CommentTok{#> The final values used for the model were sigma = 0.116 and C = 1.}
\end{Highlighting}
\end{Shaded}

Let's design a grid search around a C value of 1. We might see a small trend of decreasing RMSE with increasing C, so let's try all integer C values between 1 and 10. Another parameter that caret let us tune is the sigma parameter. This is a smoothing parameter. Good sigma values often start around 0.1, so we will try numbers before and after.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# tune SVM sigma and C parametres}
\NormalTok{trainControl <-}\StringTok{ }\KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method=}\StringTok{"repeatedcv"}\NormalTok{, }\DataTypeTok{number=}\DecValTok{10}\NormalTok{, }\DataTypeTok{repeats=}\DecValTok{3}\NormalTok{)}
\NormalTok{metric <-}\StringTok{ "RMSE"}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{7}\NormalTok{)}

\NormalTok{grid <-}\StringTok{ }\KeywordTok{expand.grid}\NormalTok{(}\DataTypeTok{.sigma =} \KeywordTok{c}\NormalTok{(}\FloatTok{0.025}\NormalTok{, }\FloatTok{0.05}\NormalTok{, }\FloatTok{0.1}\NormalTok{, }\FloatTok{0.15}\NormalTok{), }
                    \DataTypeTok{.C =} \KeywordTok{seq}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DataTypeTok{by=}\DecValTok{1}\NormalTok{))}

\NormalTok{fit.svm <-}\StringTok{ }\KeywordTok{train}\NormalTok{(medv}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{dataset, }\DataTypeTok{method=}\StringTok{"svmRadial"}\NormalTok{, }\DataTypeTok{metric=}\NormalTok{metric, }
                 \DataTypeTok{tuneGrid=}\NormalTok{grid, }
                 \DataTypeTok{preProc=}\KeywordTok{c}\NormalTok{(}\StringTok{"BoxCox"}\NormalTok{), }\DataTypeTok{trControl=}\NormalTok{trainControl)}
\KeywordTok{print}\NormalTok{(fit.svm)}
\CommentTok{#> Support Vector Machines with Radial Basis Function Kernel }
\CommentTok{#> }
\CommentTok{#> 407 samples}
\CommentTok{#>  13 predictor}
\CommentTok{#> }
\CommentTok{#> Pre-processing: Box-Cox transformation (11) }
\CommentTok{#> Resampling: Cross-Validated (10 fold, repeated 3 times) }
\CommentTok{#> Summary of sample sizes: 365, 366, 366, 367, 366, 366, ... }
\CommentTok{#> Resampling results across tuning parameters:}
\CommentTok{#> }
\CommentTok{#>   sigma  C   RMSE  Rsquared  MAE }
\CommentTok{#>   0.025   1  3.67  0.830     2.34}
\CommentTok{#>   0.025   2  3.49  0.840     2.21}
\CommentTok{#>   0.025   3  3.45  0.842     2.17}
\CommentTok{#>   0.025   4  3.42  0.844     2.14}
\CommentTok{#>   0.025   5  3.41  0.845     2.13}
\CommentTok{#>   0.025   6  3.40  0.846     2.12}
\CommentTok{#>   0.025   7  3.39  0.846     2.11}
\CommentTok{#>   0.025   8  3.39  0.846     2.11}
\CommentTok{#>   0.025   9  3.38  0.846     2.11}
\CommentTok{#>   0.025  10  3.37  0.847     2.10}
\CommentTok{#>   0.050   1  3.61  0.833     2.25}
\CommentTok{#>   0.050   2  3.44  0.843     2.17}
\CommentTok{#>   0.050   3  3.36  0.848     2.11}
\CommentTok{#>   0.050   4  3.30  0.852     2.08}
\CommentTok{#>   0.050   5  3.25  0.856     2.05}
\CommentTok{#>   0.050   6  3.20  0.860     2.03}
\CommentTok{#>   0.050   7  3.16  0.862     2.02}
\CommentTok{#>   0.050   8  3.13  0.865     2.02}
\CommentTok{#>   0.050   9  3.11  0.866     2.01}
\CommentTok{#>   0.050  10  3.10  0.867     2.01}
\CommentTok{#>   0.100   1  3.68  0.829     2.26}
\CommentTok{#>   0.100   2  3.37  0.848     2.12}
\CommentTok{#>   0.100   3  3.23  0.858     2.06}
\CommentTok{#>   0.100   4  3.17  0.862     2.04}
\CommentTok{#>   0.100   5  3.14  0.865     2.04}
\CommentTok{#>   0.100   6  3.11  0.866     2.04}
\CommentTok{#>   0.100   7  3.09  0.868     2.04}
\CommentTok{#>   0.100   8  3.09  0.868     2.04}
\CommentTok{#>   0.100   9  3.08  0.868     2.04}
\CommentTok{#>   0.100  10  3.08  0.868     2.05}
\CommentTok{#>   0.150   1  3.79  0.822     2.30}
\CommentTok{#>   0.150   2  3.42  0.846     2.14}
\CommentTok{#>   0.150   3  3.30  0.854     2.09}
\CommentTok{#>   0.150   4  3.26  0.857     2.09}
\CommentTok{#>   0.150   5  3.24  0.858     2.09}
\CommentTok{#>   0.150   6  3.23  0.858     2.10}
\CommentTok{#>   0.150   7  3.23  0.857     2.12}
\CommentTok{#>   0.150   8  3.24  0.856     2.13}
\CommentTok{#>   0.150   9  3.26  0.855     2.15}
\CommentTok{#>   0.150  10  3.27  0.854     2.17}
\CommentTok{#> }
\CommentTok{#> RMSE was used to select the optimal model using the smallest value.}
\CommentTok{#> The final values used for the model were sigma = 0.1 and C = 9.}
\KeywordTok{plot}\NormalTok{(fit.svm)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{algo-comparison_321-regression_boston-multi_files/figure-latex/model-svm-run-1} \end{center}

\hypertarget{ensembling}{%
\section{Ensembling}\label{ensembling}}

We can try some ensemble methods on the problem and see if we can get a further decrease in our RMSE.

\begin{itemize}
\tightlist
\item
  Random Forest, bagging (RF).
\item
  Gradient Boosting Machines (GBM).
\item
  Cubist, boosting (CUBIST).
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# try ensembles}
\NormalTok{seed <-}\StringTok{ }\DecValTok{7}
\NormalTok{trainControl <-}\StringTok{ }\KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method=}\StringTok{"repeatedcv"}\NormalTok{, }\DataTypeTok{number=}\DecValTok{10}\NormalTok{, }\DataTypeTok{repeats=}\DecValTok{3}\NormalTok{)}
\NormalTok{metric <-}\StringTok{ "RMSE"}

\CommentTok{# Random Forest}
\KeywordTok{set.seed}\NormalTok{(seed)}
\NormalTok{fit.rf <-}\StringTok{ }\KeywordTok{train}\NormalTok{(medv}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{dataset, }\DataTypeTok{method=}\StringTok{"rf"}\NormalTok{, }\DataTypeTok{metric=}\NormalTok{metric, }
                \DataTypeTok{preProc=}\KeywordTok{c}\NormalTok{(}\StringTok{"BoxCox"}\NormalTok{),}
                \DataTypeTok{trControl=}\NormalTok{trainControl)}

\CommentTok{# Stochastic Gradient Boosting}
\KeywordTok{set.seed}\NormalTok{(seed)}
\NormalTok{fit.gbm <-}\StringTok{ }\KeywordTok{train}\NormalTok{(medv}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{dataset, }\DataTypeTok{method=}\StringTok{"gbm"}\NormalTok{, }\DataTypeTok{metric=}\NormalTok{metric, }
                 \DataTypeTok{preProc=}\KeywordTok{c}\NormalTok{(}\StringTok{"BoxCox"}\NormalTok{), }
                 \DataTypeTok{trControl=}\NormalTok{trainControl, }\DataTypeTok{verbose=}\OtherTok{FALSE}\NormalTok{)}
\CommentTok{# Cubist}
\KeywordTok{set.seed}\NormalTok{(seed)}
\NormalTok{fit.cubist <-}\StringTok{ }\KeywordTok{train}\NormalTok{(medv}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{dataset, }\DataTypeTok{method=}\StringTok{"cubist"}\NormalTok{, }\DataTypeTok{metric=}\NormalTok{metric, }
                    \DataTypeTok{preProc=}\KeywordTok{c}\NormalTok{(}\StringTok{"BoxCox"}\NormalTok{), }\DataTypeTok{trControl=}\NormalTok{trainControl)}
\CommentTok{# Compare algorithms}
\NormalTok{ensembleResults <-}\StringTok{ }\KeywordTok{resamples}\NormalTok{(}\KeywordTok{list}\NormalTok{(}\DataTypeTok{RF  =}\NormalTok{ fit.rf, }
                                  \DataTypeTok{GBM =}\NormalTok{ fit.gbm, }
                                  \DataTypeTok{CUBIST =}\NormalTok{ fit.cubist))}
\KeywordTok{summary}\NormalTok{(ensembleResults)}
\CommentTok{#> }
\CommentTok{#> Call:}
\CommentTok{#> summary.resamples(object = ensembleResults)}
\CommentTok{#> }
\CommentTok{#> Models: RF, GBM, CUBIST }
\CommentTok{#> Number of resamples: 30 }
\CommentTok{#> }
\CommentTok{#> MAE }
\CommentTok{#>        Min. 1st Qu. Median Mean 3rd Qu. Max. NA's}
\CommentTok{#> RF     1.64    1.98   2.20 2.21    2.30 3.22    0}
\CommentTok{#> GBM    1.65    1.98   2.27 2.33    2.55 3.75    0}
\CommentTok{#> CUBIST 1.31    1.75   1.95 2.00    2.17 2.89    0}
\CommentTok{#> }
\CommentTok{#> RMSE }
\CommentTok{#>        Min. 1st Qu. Median Mean 3rd Qu. Max. NA's}
\CommentTok{#> RF     2.11    2.58   3.08 3.22    3.70 6.52    0}
\CommentTok{#> GBM    1.92    2.54   3.31 3.38    3.67 6.85    0}
\CommentTok{#> CUBIST 1.79    2.38   2.74 3.09    3.78 5.79    0}
\CommentTok{#> }
\CommentTok{#> Rsquared }
\CommentTok{#>         Min. 1st Qu. Median  Mean 3rd Qu.  Max. NA's}
\CommentTok{#> RF     0.597   0.852  0.900 0.869   0.919 0.972    0}
\CommentTok{#> GBM    0.558   0.815  0.889 0.851   0.912 0.964    0}
\CommentTok{#> CUBIST 0.681   0.818  0.909 0.875   0.932 0.970    0}
\KeywordTok{dotplot}\NormalTok{(ensembleResults)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{algo-comparison_321-regression_boston-multi_files/figure-latex/models-ensembling-run-1} \end{center}

Let's dive deeper into Cubist and see if we can tune it further and get more skill out of it. Cubist has two parameters that are tunable with caret: committees which is the number of boosting operations and neighbors which is used during prediction and is the number of instances used to correct the rule-based prediction (although the documentation is perhaps a little ambiguous on this).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# look at parameters used for Cubist}
\KeywordTok{print}\NormalTok{(fit.cubist)}
\CommentTok{#> Cubist }
\CommentTok{#> }
\CommentTok{#> 407 samples}
\CommentTok{#>  13 predictor}
\CommentTok{#> }
\CommentTok{#> Pre-processing: Box-Cox transformation (11) }
\CommentTok{#> Resampling: Cross-Validated (10 fold, repeated 3 times) }
\CommentTok{#> Summary of sample sizes: 365, 366, 366, 367, 366, 366, ... }
\CommentTok{#> Resampling results across tuning parameters:}
\CommentTok{#> }
\CommentTok{#>   committees  neighbors  RMSE  Rsquared  MAE }
\CommentTok{#>    1          0          3.94  0.805     2.50}
\CommentTok{#>    1          5          3.66  0.828     2.24}
\CommentTok{#>    1          9          3.69  0.825     2.26}
\CommentTok{#>   10          0          3.45  0.848     2.29}
\CommentTok{#>   10          5          3.19  0.868     2.04}
\CommentTok{#>   10          9          3.23  0.864     2.07}
\CommentTok{#>   20          0          3.34  0.858     2.25}
\CommentTok{#>   20          5          3.09  0.875     2.00}
\CommentTok{#>   20          9          3.12  0.872     2.03}
\CommentTok{#> }
\CommentTok{#> RMSE was used to select the optimal model using the smallest value.}
\CommentTok{#> The final values used for the model were committees = 20 and neighbors = 5.}
\end{Highlighting}
\end{Shaded}

Let's use a grid search to tune around those values. We'll try all committees between 15 and 25 and spot-check a neighbors value above and below 5.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(Cubist)}
\CommentTok{# Tune the Cubist algorithm}
\NormalTok{trainControl <-}\StringTok{ }\KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method=}\StringTok{"repeatedcv"}\NormalTok{, }\DataTypeTok{number=}\DecValTok{10}\NormalTok{, }\DataTypeTok{repeats=}\DecValTok{3}\NormalTok{)}
\NormalTok{metric <-}\StringTok{ "RMSE"}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{7}\NormalTok{)}
\NormalTok{grid <-}\StringTok{ }\KeywordTok{expand.grid}\NormalTok{(}\DataTypeTok{.committees =} \KeywordTok{seq}\NormalTok{(}\DecValTok{15}\NormalTok{, }\DecValTok{25}\NormalTok{, }\DataTypeTok{by=}\DecValTok{1}\NormalTok{), }
                    \DataTypeTok{.neighbors =} \KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{7}\NormalTok{))}

\NormalTok{tune.cubist <-}\StringTok{ }\KeywordTok{train}\NormalTok{(medv}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{dataset, }\DataTypeTok{method =} \StringTok{"cubist"}\NormalTok{, }\DataTypeTok{metric=}\NormalTok{metric, }
                     \DataTypeTok{preProc=}\KeywordTok{c}\NormalTok{(}\StringTok{"BoxCox"}\NormalTok{), }
                     \DataTypeTok{tuneGrid=}\NormalTok{grid, }\DataTypeTok{trControl=}\NormalTok{trainControl)}
\KeywordTok{print}\NormalTok{(tune.cubist)}
\CommentTok{#> Cubist }
\CommentTok{#> }
\CommentTok{#> 407 samples}
\CommentTok{#>  13 predictor}
\CommentTok{#> }
\CommentTok{#> Pre-processing: Box-Cox transformation (11) }
\CommentTok{#> Resampling: Cross-Validated (10 fold, repeated 3 times) }
\CommentTok{#> Summary of sample sizes: 365, 366, 366, 367, 366, 366, ... }
\CommentTok{#> Resampling results across tuning parameters:}
\CommentTok{#> }
\CommentTok{#>   committees  neighbors  RMSE  Rsquared  MAE }
\CommentTok{#>   15          3          3.07  0.877     2.00}
\CommentTok{#>   15          5          3.13  0.873     2.02}
\CommentTok{#>   15          7          3.14  0.871     2.03}
\CommentTok{#>   16          3          3.05  0.878     1.99}
\CommentTok{#>   16          5          3.11  0.874     2.01}
\CommentTok{#>   16          7          3.12  0.872     2.02}
\CommentTok{#>   17          3          3.04  0.879     1.98}
\CommentTok{#>   17          5          3.09  0.875     2.00}
\CommentTok{#>   17          7          3.11  0.873     2.01}
\CommentTok{#>   18          3          3.03  0.880     1.97}
\CommentTok{#>   18          5          3.08  0.876     2.00}
\CommentTok{#>   18          7          3.10  0.874     2.01}
\CommentTok{#>   19          3          3.03  0.880     1.97}
\CommentTok{#>   19          5          3.08  0.876     1.99}
\CommentTok{#>   19          7          3.10  0.874     2.01}
\CommentTok{#>   20          3          3.03  0.879     1.98}
\CommentTok{#>   20          5          3.09  0.875     2.00}
\CommentTok{#>   20          7          3.10  0.874     2.01}
\CommentTok{#>   21          3          3.03  0.879     1.98}
\CommentTok{#>   21          5          3.09  0.876     2.00}
\CommentTok{#>   21          7          3.10  0.874     2.02}
\CommentTok{#>   22          3          3.03  0.879     1.98}
\CommentTok{#>   22          5          3.09  0.875     2.00}
\CommentTok{#>   22          7          3.10  0.874     2.02}
\CommentTok{#>   23          3          3.03  0.880     1.98}
\CommentTok{#>   23          5          3.09  0.876     2.01}
\CommentTok{#>   23          7          3.10  0.874     2.02}
\CommentTok{#>   24          3          3.03  0.879     1.98}
\CommentTok{#>   24          5          3.09  0.875     2.01}
\CommentTok{#>   24          7          3.11  0.873     2.02}
\CommentTok{#>   25          3          3.03  0.880     1.98}
\CommentTok{#>   25          5          3.09  0.876     2.01}
\CommentTok{#>   25          7          3.10  0.874     2.02}
\CommentTok{#> }
\CommentTok{#> RMSE was used to select the optimal model using the smallest value.}
\CommentTok{#> The final values used for the model were committees = 25 and neighbors = 3.}
\KeywordTok{plot}\NormalTok{(tune.cubist)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{algo-comparison_321-regression_boston-multi_files/figure-latex/model-cubist-run-1} \end{center}

\begin{quote}
We can see that we have achieved a more accurate model again with an RMSE of 2.822 using committees = 18 and neighbors = 3.
\end{quote}

It looks like the results for the Cubist algorithm are the most accurate. Let's finalize it by creating a new standalone Cubist model with the parameters above trained using the whole dataset. We must also use the Box-Cox power transform.

\hypertarget{finalize-the-model}{%
\section{Finalize the model}\label{finalize-the-model}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# prepare the data transform using training data}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{7}\NormalTok{)}
\NormalTok{x <-}\StringTok{ }\NormalTok{dataset[,}\DecValTok{1}\OperatorTok{:}\DecValTok{13}\NormalTok{]}
\NormalTok{y <-}\StringTok{ }\NormalTok{dataset[,}\DecValTok{14}\NormalTok{]}

\CommentTok{# transform}
\NormalTok{preprocessParams <-}\StringTok{ }\KeywordTok{preProcess}\NormalTok{(x, }\DataTypeTok{method=}\KeywordTok{c}\NormalTok{(}\StringTok{"BoxCox"}\NormalTok{))}
\NormalTok{transX <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(preprocessParams, x)}

\CommentTok{# train the final model}
\NormalTok{finalModel <-}\StringTok{ }\KeywordTok{cubist}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ transX, }\DataTypeTok{y=}\NormalTok{y, }\DataTypeTok{committees=}\DecValTok{18}\NormalTok{)}
\KeywordTok{summary}\NormalTok{(finalModel)}
\CommentTok{#> }
\CommentTok{#> Call:}
\CommentTok{#> cubist.default(x = transX, y = y, committees = 18)}
\CommentTok{#> }
\CommentTok{#> }
\CommentTok{#> Cubist [Release 2.07 GPL Edition]  Fri Sep 20 14:28:05 2019}
\CommentTok{#> ---------------------------------}
\CommentTok{#> }
\CommentTok{#>     Target attribute `outcome'}
\CommentTok{#> }
\CommentTok{#> Read 407 cases (14 attributes) from undefined.data}
\CommentTok{#> }
\CommentTok{#> Model 1:}
\CommentTok{#> }
\CommentTok{#>   Rule 1/1: [84 cases, mean 14.29, range 5 to 27.5, est err 1.97]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  nox > -0.4864544}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 35.08 - 2.45 crim - 4.31 lstat + 2.1e-05 b}
\CommentTok{#> }
\CommentTok{#>   Rule 1/2: [163 cases, mean 19.37, range 7 to 31, est err 2.10]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  nox <= -0.4864544}
\CommentTok{#>  lstat > 2.848535}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 186.8 - 2.34 lstat - 3.3 dis - 88 tax + 2 rad + 4.4 rm}
\CommentTok{#>            - 0.033 ptratio - 0.0116 age + 3.3e-05 b}
\CommentTok{#> }
\CommentTok{#>   Rule 1/3: [24 cases, mean 21.65, range 18.2 to 25.3, est err 1.19]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  rm <= 3.326479}
\CommentTok{#>  dis > 1.345056}
\CommentTok{#>  lstat <= 2.848535}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 43.83 + 14.5 rm - 2.29 lstat - 3.8 dis - 30 tax}
\CommentTok{#>            - 0.014 ptratio - 1.4 nox + 0.017 zn + 0.4 rad + 0.15 crim}
\CommentTok{#>            - 0.0025 age + 8e-06 b}
\CommentTok{#> }
\CommentTok{#>   Rule 1/4: [7 cases, mean 27.66, range 20.7 to 50, est err 7.89]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  rm > 3.326479}
\CommentTok{#>  ptratio > 193.545}
\CommentTok{#>  lstat <= 2.848535}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 19.64 + 7.8 rm - 3.4 dis - 1.62 lstat + 0.27 crim - 0.006 age}
\CommentTok{#>            + 0.023 zn - 7 tax - 0.003 ptratio}
\CommentTok{#> }
\CommentTok{#>   Rule 1/5: [141 cases, mean 30.60, range 15 to 50, est err 2.09]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  rm > 3.326479}
\CommentTok{#>  ptratio <= 193.545}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 137.95 + 21.7 rm - 3.43 lstat - 4.9 dis - 87 tax - 0.0162 age}
\CommentTok{#>            - 0.039 ptratio + 0.06 crim + 0.005 zn}
\CommentTok{#> }
\CommentTok{#>   Rule 1/6: [8 cases, mean 32.16, range 22.1 to 50, est err 8.67]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  rm <= 3.326479}
\CommentTok{#>  dis <= 1.345056}
\CommentTok{#>  lstat <= 2.848535}
\CommentTok{#>     then}
\CommentTok{#>  outcome = -19.71 + 18.58 lstat - 15.9 dis + 5.6 rm}
\CommentTok{#> }
\CommentTok{#> Model 2:}
\CommentTok{#> }
\CommentTok{#>   Rule 2/1: [23 cases, mean 10.57, range 5 to 15, est err 3.06]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  crim > 2.086391}
\CommentTok{#>  dis <= 0.6604174}
\CommentTok{#>  b > 67032.41}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 37.22 - 4.83 crim - 7 dis - 1.9 lstat - 1.9e-05 b - 0.7 rm}
\CommentTok{#> }
\CommentTok{#>   Rule 2/2: [70 cases, mean 14.82, range 5 to 50, est err 3.90]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  rm <= 3.620525}
\CommentTok{#>  dis <= 0.6604174}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 74.6 - 21 dis - 5.09 lstat - 15 tax - 0.0017 age + 6e-06 b}
\CommentTok{#> }
\CommentTok{#>   Rule 2/3: [18 cases, mean 18.03, range 7.5 to 50, est err 6.81]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  crim > 2.086391}
\CommentTok{#>  dis <= 0.6604174}
\CommentTok{#>  b <= 67032.41}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 94.95 - 40.1 dis - 8.15 crim - 7.14 lstat - 3.5e-05 b - 1.3 rm}
\CommentTok{#> }
\CommentTok{#>   Rule 2/4: [258 cases, mean 20.74, range 9.5 to 36.2, est err 1.92]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  rm <= 3.620525}
\CommentTok{#>  dis > 0.6604174}
\CommentTok{#>  lstat > 1.805082}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 61.89 - 2.56 lstat + 5.5 rm - 2.8 dis + 7.3e-05 b - 0.0132 age}
\CommentTok{#>            - 26 tax - 0.11 indus - 0.004 ptratio + 0.05 crim}
\CommentTok{#> }
\CommentTok{#>   Rule 2/5: [37 cases, mean 31.66, range 10.4 to 50, est err 3.70]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  rm > 3.620525}
\CommentTok{#>  lstat > 1.805082}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 370.03 - 180 tax - 2.19 lstat - 1.7 dis + 2.6 rm}
\CommentTok{#>            - 0.016 ptratio - 0.25 indus + 0.12 crim - 0.0021 age}
\CommentTok{#>            + 9e-06 b - 0.5 nox}
\CommentTok{#> }
\CommentTok{#>   Rule 2/6: [42 cases, mean 38.23, range 22.8 to 50, est err 3.70]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  lstat <= 1.805082}
\CommentTok{#>     then}
\CommentTok{#>  outcome = -73.87 + 32.4 rm - 9.4e-05 b - 1.8 dis + 0.028 zn}
\CommentTok{#>            - 0.013 ptratio}
\CommentTok{#> }
\CommentTok{#>   Rule 2/7: [4 cases, mean 40.20, range 37.6 to 42.8, est err 7.33]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  rm > 4.151791}
\CommentTok{#>  dis > 1.114486}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 35.8}
\CommentTok{#> }
\CommentTok{#>   Rule 2/8: [8 cases, mean 47.45, range 41.3 to 50, est err 10.01]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  dis <= 1.114486}
\CommentTok{#>  lstat <= 1.805082}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 48.96 + 7.53 crim - 4.1e-05 b - 0.8 dis + 1.2 rm + 0.008 zn}
\CommentTok{#> }
\CommentTok{#> Model 3:}
\CommentTok{#> }
\CommentTok{#>   Rule 3/1: [81 cases, mean 13.93, range 5 to 23.2, est err 2.24]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  nox > -0.4864544}
\CommentTok{#>  lstat > 2.848535}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 55.03 - 0.0631 age - 2.11 crim + 12 nox - 4.16 lstat}
\CommentTok{#>            + 3.2e-05 b}
\CommentTok{#> }
\CommentTok{#>   Rule 3/2: [163 cases, mean 19.37, range 7 to 31, est err 2.29]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  nox <= -0.4864544}
\CommentTok{#>  lstat > 2.848535}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 77.73 - 0.059 ptratio + 5.8 rm - 3.2 dis - 0.0139 age}
\CommentTok{#>            - 1.15 lstat - 30 tax - 1.1 nox + 0.4 rad}
\CommentTok{#> }
\CommentTok{#>   Rule 3/3: [62 cases, mean 24.01, range 18.2 to 50, est err 3.56]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  rm <= 3.448196}
\CommentTok{#>  lstat <= 2.848535}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 94.86 + 18.2 rm + 0.63 crim - 68 tax - 2.3 dis - 3 nox}
\CommentTok{#>            - 0.0098 age - 0.41 indus - 0.011 ptratio}
\CommentTok{#> }
\CommentTok{#>   Rule 3/4: [143 cases, mean 28.76, range 16.5 to 50, est err 2.53]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  dis > 0.9547035}
\CommentTok{#>  lstat <= 2.848535}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 269.46 + 17.9 rm - 6.1 dis - 153 tax + 0.96 crim - 0.0217 age}
\CommentTok{#>            - 5.5 nox - 0.62 indus - 0.028 ptratio - 0.89 lstat + 0.4 rad}
\CommentTok{#>            + 0.004 zn}
\CommentTok{#> }
\CommentTok{#>   Rule 3/5: [10 cases, mean 35.13, range 21.9 to 50, est err 9.31]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  dis <= 0.6492998}
\CommentTok{#>  lstat <= 2.848535}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 58.69 - 56.8 dis - 8.4 nox}
\CommentTok{#> }
\CommentTok{#>   Rule 3/6: [10 cases, mean 41.67, range 22 to 50, est err 9.89]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  dis > 0.6492998}
\CommentTok{#>  dis <= 0.9547035}
\CommentTok{#>  lstat <= 2.848535}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 47.93}
\CommentTok{#> }
\CommentTok{#> Model 4:}
\CommentTok{#> }
\CommentTok{#>   Rule 4/1: [69 cases, mean 12.69, range 5 to 27.5, est err 2.55]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  dis <= 0.719156}
\CommentTok{#>  lstat > 3.508535}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 180.13 - 7.2 dis + 0.039 age - 3.78 lstat - 83 tax}
\CommentTok{#> }
\CommentTok{#>   Rule 4/2: [164 cases, mean 19.42, range 12 to 31, est err 1.96]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  dis > 0.719156}
\CommentTok{#>  lstat > 2.848535}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 52.75 + 7.1 rm - 2.05 lstat - 3.6 dis + 8.2e-05 b - 0.0152 age}
\CommentTok{#>            - 25 tax + 0.5 rad - 1.2 nox - 0.008 ptratio}
\CommentTok{#> }
\CommentTok{#>   Rule 4/3: [11 cases, mean 20.39, range 15 to 27.9, est err 3.51]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  dis <= 0.719156}
\CommentTok{#>  lstat > 2.848535}
\CommentTok{#>  lstat <= 3.508535}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 21.69}
\CommentTok{#> }
\CommentTok{#>   Rule 4/4: [63 cases, mean 23.22, range 16.5 to 31.5, est err 1.67]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  rm <= 3.483629}
\CommentTok{#>  dis > 0.9731624}
\CommentTok{#>  lstat <= 2.848535}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 59.35 - 3.96 lstat - 3.1 dis + 1 rm - 14 tax + 0.3 rad}
\CommentTok{#>            - 0.7 nox - 0.005 ptratio + 6e-06 b}
\CommentTok{#> }
\CommentTok{#>   Rule 4/5: [8 cases, mean 33.08, range 22 to 50, est err 23.91]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  rm > 3.369183}
\CommentTok{#>  dis <= 0.9731624}
\CommentTok{#>  lstat > 2.254579}
\CommentTok{#>  lstat <= 2.848535}
\CommentTok{#>     then}
\CommentTok{#>  outcome = -322.28 + 64.9 lstat + 56.8 rm - 30.2 dis}
\CommentTok{#> }
\CommentTok{#>   Rule 4/6: [7 cases, mean 33.87, range 22.1 to 50, est err 13.21]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  rm <= 3.369183}
\CommentTok{#>  dis <= 0.9731624}
\CommentTok{#>  lstat <= 2.848535}
\CommentTok{#>     then}
\CommentTok{#>  outcome = -52.11 + 43.45 lstat - 30.8 dis}
\CommentTok{#> }
\CommentTok{#>   Rule 4/7: [91 cases, mean 34.43, range 21.9 to 50, est err 3.32]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  rm > 3.483629}
\CommentTok{#>  lstat <= 2.848535}
\CommentTok{#>     then}
\CommentTok{#>  outcome = -33.09 + 22 rm - 5.02 lstat - 0.038 ptratio - 0.9 dis}
\CommentTok{#>            + 0.005 zn}
\CommentTok{#> }
\CommentTok{#>   Rule 4/8: [22 cases, mean 36.99, range 21.9 to 50, est err 13.21]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  dis <= 0.9731624}
\CommentTok{#>  lstat <= 2.848535}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 80.3 - 17.43 lstat - 0.134 ptratio + 2.5 rm - 1.2 dis}
\CommentTok{#>            + 0.008 zn}
\CommentTok{#> }
\CommentTok{#> Model 5:}
\CommentTok{#> }
\CommentTok{#>   Rule 5/1: [84 cases, mean 14.29, range 5 to 27.5, est err 2.81]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  nox > -0.4864544}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 56.48 + 28.5 nox - 0.0875 age - 3.58 crim - 5.9 dis}
\CommentTok{#>            - 2.96 lstat + 0.073 ptratio + 1.7e-05 b}
\CommentTok{#> }
\CommentTok{#>   Rule 5/2: [163 cases, mean 19.37, range 7 to 31, est err 2.38]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  nox <= -0.4864544}
\CommentTok{#>  lstat > 2.848535}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 61.59 - 0.064 ptratio + 5.9 rm - 3.1 dis - 0.0142 age}
\CommentTok{#>            - 0.77 lstat - 21 tax}
\CommentTok{#> }
\CommentTok{#>   Rule 5/3: [163 cases, mean 29.94, range 16.5 to 50, est err 3.65]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  lstat <= 2.848535}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 264.17 + 21.9 rm - 8 dis - 155 tax - 0.0317 age}
\CommentTok{#>            - 0.032 ptratio + 0.29 crim - 1.6 nox - 0.25 indus}
\CommentTok{#> }
\CommentTok{#>   Rule 5/4: [10 cases, mean 35.13, range 21.9 to 50, est err 11.79]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  dis <= 0.6492998}
\CommentTok{#>  lstat <= 2.848535}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 68.19 - 73.4 dis + 1.1 rm + 0.11 crim - 0.6 nox - 0.1 indus}
\CommentTok{#>            - 0.0017 age - 0.12 lstat}
\CommentTok{#> }
\CommentTok{#> Model 6:}
\CommentTok{#> }
\CommentTok{#>   Rule 6/1: [71 cases, mean 15.57, range 5 to 50, est err 4.42]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  dis <= 0.6443245}
\CommentTok{#>  lstat > 1.793385}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 45.7 - 20.6 dis - 5.38 lstat}
\CommentTok{#> }
\CommentTok{#>   Rule 6/2: [159 cases, mean 19.53, range 8.3 to 36.2, est err 2.08]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  rm <= 3.329365}
\CommentTok{#>  dis > 0.6443245}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 24.33 + 8.8 rm + 0.000118 b - 0.0146 age - 2.5 dis}
\CommentTok{#>            - 0.95 lstat + 0.37 crim - 0.32 indus + 0.02 zn - 16 tax}
\CommentTok{#>            + 0.2 rad - 0.5 nox - 0.004 ptratio}
\CommentTok{#> }
\CommentTok{#>   Rule 6/3: [175 cases, mean 27.80, range 9.5 to 50, est err 2.95]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  rm > 3.329365}
\CommentTok{#>  dis > 0.6443245}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 0.11 + 18.7 rm - 3.11 lstat + 8.1e-05 b - 1.1 dis + 0.19 crim}
\CommentTok{#>            - 20 tax - 0.19 indus + 0.3 rad - 0.7 nox - 0.005 ptratio}
\CommentTok{#>            + 0.006 zn}
\CommentTok{#> }
\CommentTok{#>   Rule 6/4: [8 cases, mean 32.50, range 21.9 to 50, est err 10.34]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  dis <= 0.6443245}
\CommentTok{#>  lstat > 1.793385}
\CommentTok{#>  lstat <= 2.894121}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 69.38 - 71.2 dis - 0.14 lstat}
\CommentTok{#> }
\CommentTok{#>   Rule 6/5: [34 cases, mean 37.55, range 22.8 to 50, est err 3.55]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  rm <= 4.151791}
\CommentTok{#>  lstat <= 1.793385}
\CommentTok{#>     then}
\CommentTok{#>  outcome = -125.14 + 41.7 rm + 4.3 rad + 1.48 indus - 0.014 ptratio}
\CommentTok{#> }
\CommentTok{#>   Rule 6/6: [7 cases, mean 43.66, range 37.6 to 50, est err 3.12]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  rm > 4.151791}
\CommentTok{#>  lstat <= 1.793385}
\CommentTok{#>     then}
\CommentTok{#>  outcome = -137.67 + 44.6 rm - 0.064 ptratio}
\CommentTok{#> }
\CommentTok{#> Model 7:}
\CommentTok{#> }
\CommentTok{#>   Rule 7/1: [84 cases, mean 14.29, range 5 to 27.5, est err 2.91]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  nox > -0.4864544}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 46.85 - 3.45 crim - 0.0621 age + 14.2 nox + 4.4 dis}
\CommentTok{#>            - 2.01 lstat + 2.5e-05 b}
\CommentTok{#> }
\CommentTok{#>   Rule 7/2: [323 cases, mean 24.66, range 7 to 50, est err 3.68]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  nox <= -0.4864544}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 57.59 - 0.065 ptratio - 4.4 dis + 6.8 rm - 0.0143 age}
\CommentTok{#>            - 1.36 lstat - 19 tax - 0.8 nox - 0.12 crim + 0.09 indus}
\CommentTok{#> }
\CommentTok{#>   Rule 7/3: [132 cases, mean 28.24, range 16.5 to 50, est err 2.55]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  dis > 1.063503}
\CommentTok{#>  lstat <= 2.848535}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 270.92 + 24.5 rm - 0.0418 age - 165 tax - 5.7 dis}
\CommentTok{#>            - 0.028 ptratio + 0.26 crim + 0.017 zn}
\CommentTok{#> }
\CommentTok{#>   Rule 7/4: [7 cases, mean 36.01, range 23.3 to 50, est err 3.87]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  dis <= 0.6002641}
\CommentTok{#>  lstat <= 2.848535}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 57.18 - 69.5 dis - 6.5 nox + 1.9 rm - 0.015 ptratio}
\CommentTok{#> }
\CommentTok{#>   Rule 7/5: [24 cases, mean 37.55, range 21.9 to 50, est err 8.66]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  dis > 0.6002641}
\CommentTok{#>  dis <= 1.063503}
\CommentTok{#>  lstat <= 2.848535}
\CommentTok{#>     then}
\CommentTok{#>  outcome = -3.76 - 14.8 dis - 2.93 crim - 0.16 ptratio + 17.5 rm - 15 nox}
\CommentTok{#> }
\CommentTok{#> Model 8:}
\CommentTok{#> }
\CommentTok{#>   Rule 8/1: [80 cases, mean 13.75, range 5 to 27.9, est err 3.51]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  dis <= 0.719156}
\CommentTok{#>  lstat > 2.848535}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 123.46 - 11.3 dis - 5.06 lstat - 45 tax + 0.9 rad + 1.7e-05 b}
\CommentTok{#> }
\CommentTok{#>   Rule 8/2: [164 cases, mean 19.42, range 12 to 31, est err 2.05]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  dis > 0.719156}
\CommentTok{#>  lstat > 2.848535}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 227.11 - 120 tax + 6.4 rm + 9.3e-05 b - 3.3 dis + 2 rad}
\CommentTok{#>            - 0.0183 age - 0.93 lstat + 0.05 crim - 0.3 nox}
\CommentTok{#> }
\CommentTok{#>   Rule 8/3: [163 cases, mean 29.94, range 16.5 to 50, est err 3.54]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  lstat <= 2.848535}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 158.14 - 5.73 lstat + 10.8 rm - 4 dis - 83 tax - 4.1 nox}
\CommentTok{#>            + 0.61 crim - 0.54 indus + 1 rad + 3.6e-05 b}
\CommentTok{#> }
\CommentTok{#>   Rule 8/4: [7 cases, mean 36.01, range 23.3 to 50, est err 11.44]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  dis <= 0.6002641}
\CommentTok{#>  lstat <= 2.848535}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 72.89 - 87.2 dis + 0.6 rm - 0.13 lstat}
\CommentTok{#> }
\CommentTok{#>   Rule 8/5: [47 cases, mean 38.44, range 15 to 50, est err 5.71]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  rm > 3.726352}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 602.95 - 10.4 lstat + 21 rm - 326 tax - 0.093 ptratio}
\CommentTok{#> }
\CommentTok{#> Model 9:}
\CommentTok{#> }
\CommentTok{#>   Rule 9/1: [81 cases, mean 13.93, range 5 to 23.2, est err 2.91]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  nox > -0.4864544}
\CommentTok{#>  lstat > 2.848535}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 41.11 - 3.98 crim - 4.42 lstat + 6.7 nox}
\CommentTok{#> }
\CommentTok{#>   Rule 9/2: [163 cases, mean 19.37, range 7 to 31, est err 2.49]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  nox <= -0.4864544}
\CommentTok{#>  lstat > 2.848535}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 44.98 - 0.068 ptratio - 4.4 dis + 6.6 rm - 1.25 lstat}
\CommentTok{#>            - 0.0118 age - 0.9 nox - 12 tax - 0.08 crim + 0.06 indus}
\CommentTok{#> }
\CommentTok{#>   Rule 9/3: [132 cases, mean 28.24, range 16.5 to 50, est err 2.35]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  dis > 1.063503}
\CommentTok{#>  lstat <= 2.848535}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 157.67 + 22.2 rm - 0.0383 age - 104 tax - 0.033 ptratio}
\CommentTok{#>            - 2.2 dis}
\CommentTok{#> }
\CommentTok{#>   Rule 9/4: [7 cases, mean 30.76, range 21.9 to 50, est err 6.77]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  dis <= 1.063503}
\CommentTok{#>  b <= 66469.73}
\CommentTok{#>  lstat <= 2.848535}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 48.52 - 56.1 dis - 12.9 nox - 0.032 ptratio + 2.7 rm}
\CommentTok{#> }
\CommentTok{#>   Rule 9/5: [24 cases, mean 39.09, range 22 to 50, est err 6.20]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  dis <= 1.063503}
\CommentTok{#>  b > 66469.73}
\CommentTok{#>  lstat <= 2.848535}
\CommentTok{#>     then}
\CommentTok{#>  outcome = -5.49 - 34.8 dis - 20.7 nox + 18.2 rm - 0.051 ptratio}
\CommentTok{#> }
\CommentTok{#> Model 10:}
\CommentTok{#> }
\CommentTok{#>   Rule 10/1: [327 cases, mean 19.45, range 5 to 50, est err 2.77]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  rm <= 3.617282}
\CommentTok{#>  lstat > 1.805082}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 270.78 - 4.09 lstat - 131 tax + 2.9 rad + 5.3e-05 b - 0.6 dis}
\CommentTok{#>            - 0.16 indus + 0.7 rm - 0.3 nox}
\CommentTok{#> }
\CommentTok{#>   Rule 10/2: [38 cases, mean 31.57, range 10.4 to 50, est err 4.71]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  rm > 3.617282}
\CommentTok{#>  lstat > 1.805082}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 308.44 - 150 tax - 2.63 lstat + 1.6 rad - 1.9 dis - 0.49 indus}
\CommentTok{#>            + 2.5 rm + 3e-05 b - 1.2 nox + 0.14 crim - 0.005 ptratio}
\CommentTok{#> }
\CommentTok{#>   Rule 10/3: [35 cases, mean 37.15, range 22.8 to 50, est err 2.76]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  rm <= 4.151791}
\CommentTok{#>  lstat <= 1.805082}
\CommentTok{#>     then}
\CommentTok{#>  outcome = -71.65 + 33.4 rm - 0.017 ptratio - 0.34 lstat + 0.2 rad}
\CommentTok{#>            - 0.3 dis - 7 tax - 0.4 nox}
\CommentTok{#> }
\CommentTok{#>   Rule 10/4: [10 cases, mean 42.63, range 21.9 to 50, est err 7.11]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  rm > 4.151791}
\CommentTok{#>     then}
\CommentTok{#>  outcome = -92.51 + 32.8 rm - 0.03 ptratio}
\CommentTok{#> }
\CommentTok{#> Model 11:}
\CommentTok{#> }
\CommentTok{#>   Rule 11/1: [84 cases, mean 14.29, range 5 to 27.5, est err 4.13]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  nox > -0.4864544}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 42.75 - 4.12 crim + 18.1 nox - 0.045 age + 6.8 dis}
\CommentTok{#>            - 1.86 lstat}
\CommentTok{#> }
\CommentTok{#>   Rule 11/2: [244 cases, mean 17.56, range 5 to 31, est err 4.29]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  lstat > 2.848535}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 34.83 - 5.2 dis - 0.058 ptratio - 0.0228 age + 5.8 rm}
\CommentTok{#>            - 0.56 lstat - 0.07 crim - 0.4 nox - 5 tax}
\CommentTok{#> }
\CommentTok{#>   Rule 11/3: [163 cases, mean 29.94, range 16.5 to 50, est err 3.49]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  lstat <= 2.848535}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 151.5 + 23.3 rm - 5.5 dis + 1.01 crim - 0.0211 age}
\CommentTok{#>            - 0.052 ptratio - 98 tax + 0.031 zn}
\CommentTok{#> }
\CommentTok{#>   Rule 11/4: [10 cases, mean 35.13, range 21.9 to 50, est err 25.19]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  dis <= 0.6492998}
\CommentTok{#>  lstat <= 2.848535}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 130.87 - 157.1 dis - 15.76 crim}
\CommentTok{#> }
\CommentTok{#> Model 12:}
\CommentTok{#> }
\CommentTok{#>   Rule 12/1: [80 cases, mean 13.75, range 5 to 27.9, est err 4.76]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  dis <= 0.719156}
\CommentTok{#>  lstat > 2.894121}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 182.68 - 6.03 lstat - 7.6 dis - 76 tax + 1.3 rad - 0.52 indus}
\CommentTok{#>            + 2.6e-05 b}
\CommentTok{#> }
\CommentTok{#>   Rule 12/2: [300 cases, mean 19.10, range 5 to 50, est err 2.76]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  rm <= 3.50716}
\CommentTok{#>  lstat > 1.793385}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 83.61 - 3 lstat + 9.6e-05 b - 0.0072 age - 33 tax + 0.7 rad}
\CommentTok{#>            + 0.32 indus}
\CommentTok{#> }
\CommentTok{#>   Rule 12/3: [10 cases, mean 24.25, range 15.7 to 36.2, est err 13.88]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  rm <= 3.50716}
\CommentTok{#>  tax <= 1.865769}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 35.46}
\CommentTok{#> }
\CommentTok{#>   Rule 12/4: [10 cases, mean 32.66, range 21.9 to 50, est err 6.28]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  dis <= 0.719156}
\CommentTok{#>  lstat > 1.793385}
\CommentTok{#>  lstat <= 2.894121}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 82.78 - 69.5 dis - 3.66 indus}
\CommentTok{#> }
\CommentTok{#>   Rule 12/5: [89 cases, mean 32.75, range 13.4 to 50, est err 3.39]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  rm > 3.50716}
\CommentTok{#>  dis > 0.719156}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 313.22 + 13.7 rm - 174 tax - 3.06 lstat + 4.8e-05 b - 1.5 dis}
\CommentTok{#>            - 0.41 indus + 0.7 rad - 0.0055 age + 0.22 crim}
\CommentTok{#> }
\CommentTok{#>   Rule 12/6: [34 cases, mean 37.55, range 22.8 to 50, est err 3.25]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  rm <= 4.151791}
\CommentTok{#>  lstat <= 1.793385}
\CommentTok{#>     then}
\CommentTok{#>  outcome = -86.8 + 36 rm - 0.3 lstat - 5 tax}
\CommentTok{#> }
\CommentTok{#>   Rule 12/7: [7 cases, mean 43.66, range 37.6 to 50, est err 5.79]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  rm > 4.151791}
\CommentTok{#>  lstat <= 1.793385}
\CommentTok{#>     then}
\CommentTok{#>  outcome = -158.68 + 47.4 rm - 0.02 ptratio}
\CommentTok{#> }
\CommentTok{#> Model 13:}
\CommentTok{#> }
\CommentTok{#>   Rule 13/1: [84 cases, mean 14.29, range 5 to 27.5, est err 2.87]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  nox > -0.4864544}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 54.69 - 3.79 crim - 0.0644 age + 11.4 nox - 2.53 lstat}
\CommentTok{#> }
\CommentTok{#>   Rule 13/2: [8 cases, mean 17.76, range 7 to 27.9, est err 13.69]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  nox <= -0.4864544}
\CommentTok{#>  age > 296.3423}
\CommentTok{#>  b <= 60875.57}
\CommentTok{#>     then}
\CommentTok{#>  outcome = -899.55 + 3.0551 age}
\CommentTok{#> }
\CommentTok{#>   Rule 13/3: [31 cases, mean 17.94, range 7 to 27.9, est err 5.15]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  nox <= -0.4864544}
\CommentTok{#>  b <= 60875.57}
\CommentTok{#>  lstat > 2.848535}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 44.43 - 3.51 lstat - 0.054 ptratio - 1.4 dis - 0.26 crim}
\CommentTok{#>            - 0.0042 age - 0.21 indus + 0.9 rm}
\CommentTok{#> }
\CommentTok{#>   Rule 13/4: [163 cases, mean 19.37, range 7 to 31, est err 3.37]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  nox <= -0.4864544}
\CommentTok{#>  lstat > 2.848535}
\CommentTok{#>     then}
\CommentTok{#>  outcome = -5.76 + 0.000242 b + 8.9 rm - 5.2 dis - 0.0209 age}
\CommentTok{#>            - 0.042 ptratio - 0.63 indus}
\CommentTok{#> }
\CommentTok{#>   Rule 13/5: [163 cases, mean 29.94, range 16.5 to 50, est err 3.45]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  lstat <= 2.848535}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 178.84 + 23.8 rm - 0.0343 age - 4.5 dis - 114 tax + 0.88 crim}
\CommentTok{#>            - 0.048 ptratio + 0.026 zn}
\CommentTok{#> }
\CommentTok{#>   Rule 13/6: [7 cases, mean 36.01, range 23.3 to 50, est err 14.09]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  dis <= 0.6002641}
\CommentTok{#>  lstat <= 2.848535}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 45.82 - 70.3 dis - 9.9 nox + 5.1 rm + 1.5 rad}
\CommentTok{#> }
\CommentTok{#>   Rule 13/7: [31 cases, mean 37.21, range 21.9 to 50, est err 7.73]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  dis <= 1.063503}
\CommentTok{#>  lstat <= 2.848535}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 95.05 - 4.52 lstat - 7.5 dis + 8.8 rm - 0.064 ptratio}
\CommentTok{#>            - 6.2 nox - 36 tax}
\CommentTok{#> }
\CommentTok{#> Model 14:}
\CommentTok{#> }
\CommentTok{#>   Rule 14/1: [49 cases, mean 16.06, range 8.4 to 22.7, est err 3.17]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  nox > -0.4205732}
\CommentTok{#>  lstat > 2.848535}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 12.83 + 42.3 nox - 4.77 lstat + 9.7 rm + 7.8e-05 b}
\CommentTok{#> }
\CommentTok{#>   Rule 14/2: [78 cases, mean 16.36, range 5 to 50, est err 5.17]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  dis <= 0.6604174}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 110.6 - 10.4 dis - 4.85 lstat + 0.0446 age - 46 tax + 0.8 rad}
\CommentTok{#> }
\CommentTok{#>   Rule 14/3: [57 cases, mean 18.40, range 9.5 to 31, est err 2.43]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  nox > -0.9365134}
\CommentTok{#>  nox <= -0.4205732}
\CommentTok{#>  age > 245.2507}
\CommentTok{#>  dis > 0.6604174}
\CommentTok{#>  lstat > 2.848535}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 206.69 - 0.1012 age - 7.05 lstat + 12.2 nox - 67 tax + 0.3 rad}
\CommentTok{#>            + 0.5 rm - 0.3 dis}
\CommentTok{#> }
\CommentTok{#>   Rule 14/4: [230 cases, mean 20.19, range 9.5 to 36.2, est err 2.09]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  rm <= 3.483629}
\CommentTok{#>  dis > 0.6492998}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 119.15 - 2.61 lstat + 5.2 rm - 57 tax - 1.8 dis - 2.4 nox}
\CommentTok{#>            + 0.7 rad + 0.24 crim + 0.003 age - 0.007 ptratio + 9e-06 b}
\CommentTok{#> }
\CommentTok{#>   Rule 14/5: [48 cases, mean 20.28, range 10.2 to 24.5, est err 2.13]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  nox > -0.9365134}
\CommentTok{#>  nox <= -0.4205732}
\CommentTok{#>  age <= 245.2507}
\CommentTok{#>  dis > 0.6604174}
\CommentTok{#>  lstat > 2.848535}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 19.4 - 1.91 lstat + 1.02 indus - 0.013 age + 2.7 rm + 2.6 nox}
\CommentTok{#>            - 0.009 ptratio}
\CommentTok{#> }
\CommentTok{#>   Rule 14/6: [44 cases, mean 20.69, range 14.4 to 29.6, est err 2.26]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  nox <= -0.9365134}
\CommentTok{#>  lstat > 2.848535}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 87.55 - 0.000315 b - 6.5 dis + 2.6 rad - 0.59 lstat - 18 tax}
\CommentTok{#> }
\CommentTok{#>   Rule 14/7: [102 cases, mean 32.44, range 13.4 to 50, est err 3.35]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  rm > 3.483629}
\CommentTok{#>  dis > 0.6492998}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 126.92 + 22.7 rm - 4.68 lstat - 85 tax - 0.036 ptratio}
\CommentTok{#>            - 1.1 dis + 0.007 zn}
\CommentTok{#> }
\CommentTok{#>   Rule 14/8: [84 cases, mean 33.40, range 21 to 50, est err 2.44]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  rm > 3.483629}
\CommentTok{#>  tax <= 1.896025}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 347.12 + 25.2 rm - 213 tax - 3.5 lstat - 0.013 ptratio}
\CommentTok{#> }
\CommentTok{#>   Rule 14/9: [10 cases, mean 35.13, range 21.9 to 50, est err 12.13]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  dis <= 0.6492998}
\CommentTok{#>  lstat <= 2.848535}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 72.65 - 77.8 dis}
\CommentTok{#> }
\CommentTok{#> Model 15:}
\CommentTok{#> }
\CommentTok{#>   Rule 15/1: [28 cases, mean 12.35, range 5 to 27.9, est err 4.09]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  crim > 2.405809}
\CommentTok{#>  b > 16084.5}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 53.45 - 7.8 crim - 3.5 lstat - 0.0189 age}
\CommentTok{#> }
\CommentTok{#>   Rule 15/2: [11 cases, mean 13.56, range 8.3 to 27.5, est err 5.99]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  crim > 2.405809}
\CommentTok{#>  b <= 16084.5}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 8.73 + 0.001756 b}
\CommentTok{#> }
\CommentTok{#>   Rule 15/3: [244 cases, mean 17.56, range 5 to 31, est err 2.73]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  lstat > 2.848535}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 103.02 - 0.0251 age - 2.37 lstat - 3.5 dis + 6.8e-05 b + 4 rm}
\CommentTok{#>            - 0.035 ptratio - 41 tax - 0.25 crim}
\CommentTok{#> }
\CommentTok{#>   Rule 15/4: [131 cases, mean 28.22, range 16.5 to 50, est err 2.59]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  dis > 1.086337}
\CommentTok{#>  lstat <= 2.848535}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 267.07 + 17.7 rm - 0.0421 age - 150 tax - 5.5 dis + 0.88 crim}
\CommentTok{#>            - 0.035 ptratio + 0.031 zn - 0.12 lstat - 0.3 nox}
\CommentTok{#> }
\CommentTok{#>   Rule 15/5: [13 cases, mean 33.08, range 22 to 50, est err 4.44]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  nox <= -0.7229691}
\CommentTok{#>  dis <= 1.086337}
\CommentTok{#>  lstat <= 2.848535}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 148.52 - 0.002365 b - 85.9 nox - 1 dis + 0.16 crim + 0.8 rm}
\CommentTok{#>            + 0.007 zn - 0.0016 age - 7 tax - 0.003 ptratio}
\CommentTok{#> }
\CommentTok{#>   Rule 15/6: [7 cases, mean 36.01, range 23.3 to 50, est err 7.00]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  dis <= 0.6002641}
\CommentTok{#>  lstat <= 2.848535}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 50.55 - 68.1 dis - 11.4 nox + 0.00012 b + 1 rm - 0.008 ptratio}
\CommentTok{#> }
\CommentTok{#>   Rule 15/7: [12 cases, mean 41.77, range 21.9 to 50, est err 9.73]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  nox > -0.7229691}
\CommentTok{#>  dis > 0.6002641}
\CommentTok{#>  lstat <= 2.848535}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 13.74 - 92 nox - 40.5 dis - 0.023 ptratio + 2.6 rm}
\CommentTok{#> }
\CommentTok{#> Model 16:}
\CommentTok{#> }
\CommentTok{#>   Rule 16/1: [60 cases, mean 15.95, range 7.2 to 27.5, est err 3.16]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  nox > -0.4344906}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 46.98 - 6.53 lstat - 6.9 dis - 1.1 rm}
\CommentTok{#> }
\CommentTok{#>   Rule 16/2: [45 cases, mean 16.89, range 5 to 50, est err 5.45]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  nox <= -0.4344906}
\CommentTok{#>  dis <= 0.6557049}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 35.33 - 37 dis - 51.7 nox - 7.38 lstat - 0.4 rm}
\CommentTok{#> }
\CommentTok{#>   Rule 16/3: [128 cases, mean 19.97, range 9.5 to 36.2, est err 2.52]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  rm <= 3.626081}
\CommentTok{#>  dis > 0.6557049}
\CommentTok{#>  dis <= 1.298828}
\CommentTok{#>  lstat > 2.133251}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 61.65 - 3.35 lstat + 4.9 dis + 1.6 rm - 1.3 nox - 22 tax}
\CommentTok{#>            + 0.5 rad + 1.8e-05 b + 0.09 crim - 0.004 ptratio}
\CommentTok{#> }
\CommentTok{#>   Rule 16/4: [140 cases, mean 21.93, range 12.7 to 35.1, est err 2.19]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  rm <= 3.626081}
\CommentTok{#>  dis > 1.298828}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 54.16 - 3.58 lstat + 2.2 rad - 1.6 dis - 1.9 nox + 1.8 rm}
\CommentTok{#>            - 17 tax + 1.3e-05 b + 0.06 crim - 0.003 ptratio}
\CommentTok{#> }
\CommentTok{#>   Rule 16/5: [30 cases, mean 21.97, range 14.4 to 29.1, est err 2.41]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  rm <= 3.626081}
\CommentTok{#>  dis > 1.298828}
\CommentTok{#>  tax <= 1.879832}
\CommentTok{#>  lstat > 2.133251}
\CommentTok{#>     then}
\CommentTok{#>  outcome = -1065.35 + 566 tax + 8.7 rm - 0.13 lstat - 0.2 dis - 0.3 nox}
\CommentTok{#> }
\CommentTok{#>   Rule 16/6: [22 cases, mean 30.88, range 10.4 to 50, est err 4.51]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  rm > 3.626081}
\CommentTok{#>  lstat > 2.133251}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 42.24 + 18.7 rm - 1.5 indus - 1.84 lstat - 2.5 nox - 1.6 dis}
\CommentTok{#>            - 39 tax + 0.7 rad - 0.012 ptratio + 0.0035 age + 1.2e-05 b}
\CommentTok{#>            + 0.11 crim}
\CommentTok{#> }
\CommentTok{#>   Rule 16/7: [73 cases, mean 34.52, range 20.6 to 50, est err 3.36]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  lstat <= 2.133251}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 50.6 + 19.6 rm - 2.77 lstat - 3.2 nox - 1.7 dis - 45 tax}
\CommentTok{#>            + 1 rad + 0.007 age - 0.014 ptratio}
\CommentTok{#> }
\CommentTok{#> Model 17:}
\CommentTok{#> }
\CommentTok{#>   Rule 17/1: [116 cases, mean 15.37, range 5 to 27.9, est err 2.55]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  crim > 0.4779842}
\CommentTok{#>  lstat > 2.944963}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 35.96 - 3.68 crim - 3.41 lstat + 0.3 nox}
\CommentTok{#> }
\CommentTok{#>   Rule 17/2: [112 cases, mean 19.13, range 7 to 31, est err 2.14]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  crim <= 0.4779842}
\CommentTok{#>  lstat > 2.944963}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 184.65 - 0.0365 age + 9 rm - 4.1 dis - 97 tax + 8.4e-05 b}
\CommentTok{#>            - 0.024 ptratio}
\CommentTok{#> }
\CommentTok{#>   Rule 17/3: [9 cases, mean 28.37, range 15 to 50, est err 11.17]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  dis <= 0.9547035}
\CommentTok{#>  b <= 66469.73}
\CommentTok{#>  lstat <= 2.944963}
\CommentTok{#>     then}
\CommentTok{#>  outcome = -1.12 + 0.000454 b}
\CommentTok{#> }
\CommentTok{#>   Rule 17/4: [179 cases, mean 29.28, range 15 to 50, est err 3.35]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  lstat <= 2.944963}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 278.16 + 20 rm - 7.4 dis - 0.0356 age - 161 tax + 0.051 zn}
\CommentTok{#>            - 0.61 lstat + 0.17 crim - 0.008 ptratio}
\CommentTok{#> }
\CommentTok{#>   Rule 17/5: [23 cases, mean 36.10, range 15 to 50, est err 10.83]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  dis <= 0.9547035}
\CommentTok{#>  lstat <= 2.944963}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 233.74 - 8.5 dis + 12.1 rm + 1.15 crim - 2.42 lstat - 113 tax}
\CommentTok{#>            - 0.0221 age + 0.068 zn - 0.031 ptratio}
\CommentTok{#> }
\CommentTok{#> Model 18:}
\CommentTok{#> }
\CommentTok{#>   Rule 18/1: [84 cases, mean 14.29, range 5 to 27.5, est err 2.44]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  nox > -0.4864544}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 41.55 - 6.2 lstat + 14.6 nox + 3.8e-05 b}
\CommentTok{#> }
\CommentTok{#>   Rule 18/2: [163 cases, mean 19.37, range 7 to 31, est err 2.44]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  nox <= -0.4864544}
\CommentTok{#>  lstat > 2.848535}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 172.79 - 3.67 lstat + 3.1 rad - 3.5 dis - 72 tax - 0.72 indus}
\CommentTok{#>            - 0.033 ptratio - 1.2 nox + 0.0027 age + 0.6 rm + 0.05 crim}
\CommentTok{#>            + 5e-06 b}
\CommentTok{#> }
\CommentTok{#>   Rule 18/3: [106 cases, mean 25.41, range 16.5 to 50, est err 2.76]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  rm <= 3.626081}
\CommentTok{#>  lstat <= 2.848535}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 10.71 - 4.6 dis - 2.21 lstat + 2.3 rad + 5.5 rm - 5.3 nox}
\CommentTok{#>            - 0.83 indus - 0.003 ptratio}
\CommentTok{#> }
\CommentTok{#>   Rule 18/4: [4 cases, mean 33.47, range 30.1 to 36.2, est err 5.61]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  rm <= 3.626081}
\CommentTok{#>  tax <= 1.863917}
\CommentTok{#>  lstat <= 2.848535}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 36.84}
\CommentTok{#> }
\CommentTok{#>   Rule 18/5: [10 cases, mean 35.13, range 21.9 to 50, est err 17.40]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  dis <= 0.6492998}
\CommentTok{#>  lstat <= 2.848535}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 84.58 - 94.7 dis - 0.15 lstat}
\CommentTok{#> }
\CommentTok{#>   Rule 18/6: [57 cases, mean 38.38, range 21.9 to 50, est err 3.97]}
\CommentTok{#> }
\CommentTok{#>     if}
\CommentTok{#>  rm > 3.626081}
\CommentTok{#>  lstat <= 2.848535}
\CommentTok{#>     then}
\CommentTok{#>  outcome = 100.34 + 22.3 rm - 5.79 lstat - 0.062 ptratio - 69 tax}
\CommentTok{#>            + 0.3 rad - 0.5 nox - 0.3 dis + 0.0011 age}
\CommentTok{#> }
\CommentTok{#> }
\CommentTok{#> Evaluation on training data (407 cases):}
\CommentTok{#> }
\CommentTok{#>     Average  |error|               1.72}
\CommentTok{#>     Relative |error|               0.26}
\CommentTok{#>     Correlation coefficient        0.96}
\CommentTok{#> }
\CommentTok{#> }
\CommentTok{#>  Attribute usage:}
\CommentTok{#>    Conds  Model}
\CommentTok{#> }
\CommentTok{#>     72%    84%    lstat}
\CommentTok{#>     38%    85%    dis}
\CommentTok{#>     35%    80%    rm}
\CommentTok{#>     27%    55%    nox}
\CommentTok{#>      4%    58%    crim}
\CommentTok{#>      2%    49%    b}
\CommentTok{#>      2%    68%    ptratio}
\CommentTok{#>      1%    78%    tax}
\CommentTok{#>      1%    67%    age}
\CommentTok{#>            41%    rad}
\CommentTok{#>            36%    indus}
\CommentTok{#>            20%    zn}
\CommentTok{#> }
\CommentTok{#> }
\CommentTok{#> Time: 0.2 secs}
\end{Highlighting}
\end{Shaded}

We can now use this model to evaluate our held-out validation dataset. Again, we must prepare the input data using the same Box-Cox transform.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# transform the validation dataset}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{7}\NormalTok{)}
\NormalTok{valX <-}\StringTok{ }\NormalTok{validation[,}\DecValTok{1}\OperatorTok{:}\DecValTok{13}\NormalTok{]}
\NormalTok{trans_valX <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(preprocessParams, valX)}
\NormalTok{valY <-}\StringTok{ }\NormalTok{validation[,}\DecValTok{14}\NormalTok{]}

\CommentTok{# use final model to make predictions on the validation dataset}
\NormalTok{predictions <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(finalModel, }\DataTypeTok{newdata =}\NormalTok{ trans_valX, }\DataTypeTok{neighbors=}\DecValTok{3}\NormalTok{)}

\CommentTok{# calculate RMSE}
\NormalTok{rmse <-}\StringTok{ }\KeywordTok{RMSE}\NormalTok{(predictions, valY)}
\NormalTok{r2 <-}\StringTok{ }\KeywordTok{R2}\NormalTok{(predictions, valY)}
\KeywordTok{print}\NormalTok{(rmse)}
\CommentTok{#> [1] 3.24}
\end{Highlighting}
\end{Shaded}

\begin{quote}
We can see that the estimated RMSE on this unseen data is about 2.666, lower but not too dissimilar from our expected RMSE of 2.822.
\end{quote}

\hypertarget{classification-algorithms-comparison.-breast-cancer-dataset-lg-lda-glmnet-knn-cartm-nb-svm}{%
\chapter{\texorpdfstring{Classification algorithms comparison. Breast cancer dataset, (\emph{LG, LDA, GLMNET, KNN, CARTM NB, SVM})}{Classification algorithms comparison. Breast cancer dataset, (LG, LDA, GLMNET, KNN, CARTM NB, SVM)}}\label{classification-algorithms-comparison.-breast-cancer-dataset-lg-lda-glmnet-knn-cartm-nb-svm}}

\hypertarget{breastcancer-dataset}{%
\section{\texorpdfstring{\texttt{BreastCancer} dataset}{BreastCancer dataset}}\label{breastcancer-dataset}}

\hypertarget{introduction-4}{%
\section{Introduction}\label{introduction-4}}

In this classification problem we apply these algorithms:

\begin{itemize}
\tightlist
\item
  Linear

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    LG (logistic regression)
  \item
    LDA (linear discriminant analysis)
  \item
    GLMNET (Regularized logistic regression)
  \end{enumerate}
\item
  Non-linear

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \setcounter{enumi}{3}
  \tightlist
  \item
    KNN (k-Nearest Neighbors)
  \item
    CART (Classification and Regression Trees)
  \item
    NB (Naive Bayes)
  \item
    SVM (Support Vector Machines)
  \end{enumerate}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load packages}
\KeywordTok{library}\NormalTok{(mlbench)}
\KeywordTok{library}\NormalTok{(caret)}
\CommentTok{#> Loading required package: lattice}
\CommentTok{#> Loading required package: ggplot2}
\CommentTok{#> Registered S3 methods overwritten by 'ggplot2':}
\CommentTok{#>   method         from }
\CommentTok{#>   [.quosures     rlang}
\CommentTok{#>   c.quosures     rlang}
\CommentTok{#>   print.quosures rlang}
\KeywordTok{library}\NormalTok{(tictoc)}

\CommentTok{# Load data}
\KeywordTok{data}\NormalTok{(BreastCancer)}
\end{Highlighting}
\end{Shaded}

\hypertarget{workflow-3}{%
\section{Workflow}\label{workflow-3}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Load dataset
\item
  Create train and validation datasets, 80/20
\item
  Inspect dataset:
\end{enumerate}

\begin{itemize}
\tightlist
\item
  dimension
\item
  class of variables
\item
  \texttt{skimr}
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Clean up features
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Convert character to numeric
\item
  Frequency table on class
\item
  remove NAs
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Visualize features
\end{enumerate}

\begin{itemize}
\tightlist
\item
  histograms (loop on variables)
\item
  density plots (loop)
\item
  boxplot (loop)
\item
  Pairwise jittered plot
\item
  Barplots for all features (loop)
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  Train as-is\\
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Set the train control to

  \begin{itemize}
  \tightlist
  \item
    10 cross-validations
  \item
    3 repetitions
  \item
    Metric: Accuracy
  \end{itemize}
\item
  Train the models
\item
  Numeric comparison of model results
\item
  Visual comparison

  \begin{itemize}
  \tightlist
  \item
    dot plot
  \end{itemize}
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{6}
\tightlist
\item
  Train with data transformation
\end{enumerate}

\begin{itemize}
\tightlist
\item
  data transformatiom

  \begin{itemize}
  \tightlist
  \item
    BoxCox
  \end{itemize}
\item
  Train models
\item
  Numeric comparison
\item
  Visual comparison

  \begin{itemize}
  \tightlist
  \item
    dot plot
  \end{itemize}
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{7}
\tightlist
\item
  Tune the best model: SVM
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Set the train control to

  \begin{itemize}
  \tightlist
  \item
    10 cross-validations
  \item
    3 repetitions
  \item
    Metric: Accuracy
  \end{itemize}
\item
  Train the models

  \begin{itemize}
  \tightlist
  \item
    Radial SVM
  \item
    Sigma vector
  \item
    \texttt{.C}
  \item
    BoxCox
  \end{itemize}
\item
  Evaluate tuning parameters
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{8}
\tightlist
\item
  Tune the best model: KNN
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Set the train control to

  \begin{itemize}
  \tightlist
  \item
    10 cross-validations
  \item
    3 repetitions
  \item
    Metric: Accuracy
  \end{itemize}
\item
  Train the models

  \begin{itemize}
  \tightlist
  \item
    .k
  \item
    BoxCox
  \end{itemize}
\item
  Evaluate tuning parameters

  \begin{itemize}
  \tightlist
  \item
    Scatter plot
    10, Ensembling
  \end{itemize}
\item
  Select the algorithms

  \begin{itemize}
  \tightlist
  \item
    Bagged CART
  \item
    Random Forest
  \item
    Stochastic Gradient Boosting
  \item
    C5.0
  \end{itemize}
\item
  Numeric comparison

  \begin{itemize}
  \tightlist
  \item
    resamples
  \item
    summary
  \end{itemize}
\item
  Visual comparison

  \begin{itemize}
  \tightlist
  \item
    dot plot\\
  \end{itemize}
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{10}
\tightlist
\item
  Finalize the model
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Back transformation

  \begin{itemize}
  \tightlist
  \item
    \texttt{preProcess}
  \item
    \texttt{predict}
  \end{itemize}
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{11}
\tightlist
\item
  Apply model to validation set
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Prepare validation set
\item
  Transform the dataset
\item
  Make prediction

  \begin{itemize}
  \tightlist
  \item
    \texttt{knn3Train}
  \end{itemize}
\item
  Calculate accuracy

  \begin{itemize}
  \tightlist
  \item
    Confusion Matrix
  \end{itemize}
\end{itemize}

\hypertarget{inspect-the-dataset}{%
\section{Inspect the dataset}\label{inspect-the-dataset}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{glimpse}\NormalTok{(BreastCancer)}
\CommentTok{#> Observations: 699}
\CommentTok{#> Variables: 11}
\CommentTok{#> $ Id              <chr> "1000025", "1002945", "1015425", "1016277", "1...}
\CommentTok{#> $ Cl.thickness    <ord> 5, 5, 3, 6, 4, 8, 1, 2, 2, 4, 1, 2, 5, 1, 8, 7...}
\CommentTok{#> $ Cell.size       <ord> 1, 4, 1, 8, 1, 10, 1, 1, 1, 2, 1, 1, 3, 1, 7, ...}
\CommentTok{#> $ Cell.shape      <ord> 1, 4, 1, 8, 1, 10, 1, 2, 1, 1, 1, 1, 3, 1, 5, ...}
\CommentTok{#> $ Marg.adhesion   <ord> 1, 5, 1, 1, 3, 8, 1, 1, 1, 1, 1, 1, 3, 1, 10, ...}
\CommentTok{#> $ Epith.c.size    <ord> 2, 7, 2, 3, 2, 7, 2, 2, 2, 2, 1, 2, 2, 2, 7, 6...}
\CommentTok{#> $ Bare.nuclei     <fct> 1, 10, 2, 4, 1, 10, 10, 1, 1, 1, 1, 1, 3, 3, 9...}
\CommentTok{#> $ Bl.cromatin     <fct> 3, 3, 3, 3, 3, 9, 3, 3, 1, 2, 3, 2, 4, 3, 5, 4...}
\CommentTok{#> $ Normal.nucleoli <fct> 1, 2, 1, 7, 1, 7, 1, 1, 1, 1, 1, 1, 4, 1, 5, 3...}
\CommentTok{#> $ Mitoses         <fct> 1, 1, 1, 1, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 4, 1...}
\CommentTok{#> $ Class           <fct> benign, benign, benign, benign, benign, malign...}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tibble}\OperatorTok{::}\KeywordTok{as_tibble}\NormalTok{(BreastCancer)}
\CommentTok{#> # A tibble: 699 x 11}
\CommentTok{#>   Id    Cl.thickness Cell.size Cell.shape Marg.adhesion Epith.c.size}
\CommentTok{#>   <chr> <ord>        <ord>     <ord>      <ord>         <ord>       }
\CommentTok{#> 1 1000~ 5            1         1          1             2           }
\CommentTok{#> 2 1002~ 5            4         4          5             7           }
\CommentTok{#> 3 1015~ 3            1         1          1             2           }
\CommentTok{#> 4 1016~ 6            8         8          1             3           }
\CommentTok{#> 5 1017~ 4            1         1          3             2           }
\CommentTok{#> 6 1017~ 8            10        10         8             7           }
\CommentTok{#> # ... with 693 more rows, and 5 more variables: Bare.nuclei <fct>,}
\CommentTok{#> #   Bl.cromatin <fct>, Normal.nucleoli <fct>, Mitoses <fct>, Class <fct>}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Split out validation dataset}
\CommentTok{# create a list of 80% of the rows in the original dataset we can use for training}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{7}\NormalTok{)}
\NormalTok{validationIndex <-}\StringTok{ }\KeywordTok{createDataPartition}\NormalTok{(BreastCancer}\OperatorTok{$}\NormalTok{Class, }
                                       \DataTypeTok{p=}\FloatTok{0.80}\NormalTok{, }
                                       \DataTypeTok{list=}\OtherTok{FALSE}\NormalTok{)}

\CommentTok{# select 20% of the data for validation}
\NormalTok{validation <-}\StringTok{ }\NormalTok{BreastCancer[}\OperatorTok{-}\NormalTok{validationIndex,]}
\CommentTok{# use the remaining 80% of data to training and testing the models}
\NormalTok{dataset <-}\StringTok{ }\NormalTok{BreastCancer[validationIndex,]}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# dimensions of dataset}
\KeywordTok{dim}\NormalTok{(validation)}
\CommentTok{#> [1] 139  11}
\KeywordTok{dim}\NormalTok{(dataset)}
\CommentTok{#> [1] 560  11}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# peek}
\KeywordTok{head}\NormalTok{(dataset, }\DataTypeTok{n=}\DecValTok{20}\NormalTok{)}
\CommentTok{#>         Id Cl.thickness Cell.size Cell.shape Marg.adhesion Epith.c.size}
\CommentTok{#> 1  1000025            5         1          1             1            2}
\CommentTok{#> 2  1002945            5         4          4             5            7}
\CommentTok{#> 3  1015425            3         1          1             1            2}
\CommentTok{#> 5  1017023            4         1          1             3            2}
\CommentTok{#> 6  1017122            8        10         10             8            7}
\CommentTok{#> 7  1018099            1         1          1             1            2}
\CommentTok{#> 8  1018561            2         1          2             1            2}
\CommentTok{#> 9  1033078            2         1          1             1            2}
\CommentTok{#> 10 1033078            4         2          1             1            2}
\CommentTok{#> 11 1035283            1         1          1             1            1}
\CommentTok{#> 13 1041801            5         3          3             3            2}
\CommentTok{#> 14 1043999            1         1          1             1            2}
\CommentTok{#> 15 1044572            8         7          5            10            7}
\CommentTok{#> 16 1047630            7         4          6             4            6}
\CommentTok{#> 18 1049815            4         1          1             1            2}
\CommentTok{#> 19 1050670           10         7          7             6            4}
\CommentTok{#> 21 1054590            7         3          2            10            5}
\CommentTok{#> 22 1054593           10         5          5             3            6}
\CommentTok{#> 23 1056784            3         1          1             1            2}
\CommentTok{#> 24 1057013            8         4          5             1            2}
\CommentTok{#>    Bare.nuclei Bl.cromatin Normal.nucleoli Mitoses     Class}
\CommentTok{#> 1            1           3               1       1    benign}
\CommentTok{#> 2           10           3               2       1    benign}
\CommentTok{#> 3            2           3               1       1    benign}
\CommentTok{#> 5            1           3               1       1    benign}
\CommentTok{#> 6           10           9               7       1 malignant}
\CommentTok{#> 7           10           3               1       1    benign}
\CommentTok{#> 8            1           3               1       1    benign}
\CommentTok{#> 9            1           1               1       5    benign}
\CommentTok{#> 10           1           2               1       1    benign}
\CommentTok{#> 11           1           3               1       1    benign}
\CommentTok{#> 13           3           4               4       1 malignant}
\CommentTok{#> 14           3           3               1       1    benign}
\CommentTok{#> 15           9           5               5       4 malignant}
\CommentTok{#> 16           1           4               3       1 malignant}
\CommentTok{#> 18           1           3               1       1    benign}
\CommentTok{#> 19          10           4               1       2 malignant}
\CommentTok{#> 21          10           5               4       4 malignant}
\CommentTok{#> 22           7           7              10       1 malignant}
\CommentTok{#> 23           1           2               1       1    benign}
\CommentTok{#> 24        <NA>           7               3       1 malignant}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(skimr)}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'skimr'}
\CommentTok{#> The following object is masked from 'package:stats':}
\CommentTok{#> }
\CommentTok{#>     filter}
\KeywordTok{skim}\NormalTok{(dataset)}
\CommentTok{#> Skim summary statistics}
\CommentTok{#>  n obs: 560 }
\CommentTok{#>  n variables: 11 }
\CommentTok{#> }
\CommentTok{#> -- Variable type:character -----------------------------------------------}
\CommentTok{#>  variable missing complete   n min max empty n_unique}
\CommentTok{#>        Id       0      560 560   5   8     0      523}
\CommentTok{#> }
\CommentTok{#> -- Variable type:factor --------------------------------------------------}
\CommentTok{#>         variable missing complete   n n_unique}
\CommentTok{#>      Bare.nuclei      12      548 560       10}
\CommentTok{#>      Bl.cromatin       0      560 560       10}
\CommentTok{#>       Cell.shape       0      560 560       10}
\CommentTok{#>        Cell.size       0      560 560       10}
\CommentTok{#>     Cl.thickness       0      560 560       10}
\CommentTok{#>            Class       0      560 560        2}
\CommentTok{#>     Epith.c.size       0      560 560       10}
\CommentTok{#>    Marg.adhesion       0      560 560       10}
\CommentTok{#>          Mitoses       0      560 560        9}
\CommentTok{#>  Normal.nucleoli       0      560 560       10}
\CommentTok{#>                     top_counts ordered}
\CommentTok{#>  1: 327, 10: 102, 2: 28, 5: 24   FALSE}
\CommentTok{#>  3: 136, 2: 134, 1: 118, 7: 64   FALSE}
\CommentTok{#>   1: 281, 10: 48, 3: 47, 2: 46    TRUE}
\CommentTok{#>   1: 307, 10: 55, 3: 43, 2: 36    TRUE}
\CommentTok{#>    1: 113, 3: 94, 5: 94, 4: 62    TRUE}
\CommentTok{#>      ben: 367, mal: 193, NA: 0   FALSE}
\CommentTok{#>    2: 304, 3: 60, 4: 40, 1: 38    TRUE}
\CommentTok{#>   1: 331, 2: 44, 10: 44, 3: 42    TRUE}
\CommentTok{#>    1: 466, 3: 29, 2: 23, 4: 10   FALSE}
\CommentTok{#>   1: 347, 10: 50, 3: 36, 2: 29   FALSE}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# types}
\KeywordTok{sapply}\NormalTok{(dataset, class)}
\CommentTok{#> $Id}
\CommentTok{#> [1] "character"}
\CommentTok{#> }
\CommentTok{#> $Cl.thickness}
\CommentTok{#> [1] "ordered" "factor" }
\CommentTok{#> }
\CommentTok{#> $Cell.size}
\CommentTok{#> [1] "ordered" "factor" }
\CommentTok{#> }
\CommentTok{#> $Cell.shape}
\CommentTok{#> [1] "ordered" "factor" }
\CommentTok{#> }
\CommentTok{#> $Marg.adhesion}
\CommentTok{#> [1] "ordered" "factor" }
\CommentTok{#> }
\CommentTok{#> $Epith.c.size}
\CommentTok{#> [1] "ordered" "factor" }
\CommentTok{#> }
\CommentTok{#> $Bare.nuclei}
\CommentTok{#> [1] "factor"}
\CommentTok{#> }
\CommentTok{#> $Bl.cromatin}
\CommentTok{#> [1] "factor"}
\CommentTok{#> }
\CommentTok{#> $Normal.nucleoli}
\CommentTok{#> [1] "factor"}
\CommentTok{#> }
\CommentTok{#> $Mitoses}
\CommentTok{#> [1] "factor"}
\CommentTok{#> }
\CommentTok{#> $Class}
\CommentTok{#> [1] "factor"}
\end{Highlighting}
\end{Shaded}

We can see that besides the Id, the attributes are factors. This makes sense. I
think for modeling it may be more useful to work with the data as numbers than
factors. Factors might make things easier for decision tree algorithms (or not).
Given that there is an ordinal relationship between the levels we can expose
that structure to other algorithms better if we work directly with the integer
numbers.

\hypertarget{clean-up}{%
\section{clean up}\label{clean-up}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Remove redundant variable Id}
\NormalTok{dataset <-}\StringTok{ }\NormalTok{dataset[,}\OperatorTok{-}\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# convert input values to numeric}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{9}\NormalTok{) \{}
\NormalTok{    dataset[,i] <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{as.character}\NormalTok{(dataset[,i]))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# summary}
\KeywordTok{summary}\NormalTok{(dataset)}
\CommentTok{#>   Cl.thickness     Cell.size       Cell.shape    Marg.adhesion  }
\CommentTok{#>  Min.   : 1.00   Min.   : 1.00   Min.   : 1.00   Min.   : 1.00  }
\CommentTok{#>  1st Qu.: 2.00   1st Qu.: 1.00   1st Qu.: 1.00   1st Qu.: 1.00  }
\CommentTok{#>  Median : 4.00   Median : 1.00   Median : 1.00   Median : 1.00  }
\CommentTok{#>  Mean   : 4.44   Mean   : 3.14   Mean   : 3.21   Mean   : 2.79  }
\CommentTok{#>  3rd Qu.: 6.00   3rd Qu.: 5.00   3rd Qu.: 5.00   3rd Qu.: 4.00  }
\CommentTok{#>  Max.   :10.00   Max.   :10.00   Max.   :10.00   Max.   :10.00  }
\CommentTok{#>                                                                 }
\CommentTok{#>   Epith.c.size    Bare.nuclei     Bl.cromatin    Normal.nucleoli}
\CommentTok{#>  Min.   : 1.00   Min.   : 1.00   Min.   : 1.00   Min.   : 1.00  }
\CommentTok{#>  1st Qu.: 2.00   1st Qu.: 1.00   1st Qu.: 2.00   1st Qu.: 1.00  }
\CommentTok{#>  Median : 2.00   Median : 1.00   Median : 3.00   Median : 1.00  }
\CommentTok{#>  Mean   : 3.23   Mean   : 3.48   Mean   : 3.45   Mean   : 2.94  }
\CommentTok{#>  3rd Qu.: 4.00   3rd Qu.: 5.25   3rd Qu.: 5.00   3rd Qu.: 4.00  }
\CommentTok{#>  Max.   :10.00   Max.   :10.00   Max.   :10.00   Max.   :10.00  }
\CommentTok{#>                  NA's   :12                                     }
\CommentTok{#>     Mitoses            Class    }
\CommentTok{#>  Min.   : 1.00   benign   :367  }
\CommentTok{#>  1st Qu.: 1.00   malignant:193  }
\CommentTok{#>  Median : 1.00                  }
\CommentTok{#>  Mean   : 1.59                  }
\CommentTok{#>  3rd Qu.: 1.00                  }
\CommentTok{#>  Max.   :10.00                  }
\CommentTok{#> }
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{skim}\NormalTok{(dataset)}
\CommentTok{#> Skim summary statistics}
\CommentTok{#>  n obs: 560 }
\CommentTok{#>  n variables: 10 }
\CommentTok{#> }
\CommentTok{#> -- Variable type:factor --------------------------------------------------}
\CommentTok{#>  variable missing complete   n n_unique                top_counts ordered}
\CommentTok{#>     Class       0      560 560        2 ben: 367, mal: 193, NA: 0   FALSE}
\CommentTok{#> }
\CommentTok{#> -- Variable type:numeric -------------------------------------------------}
\CommentTok{#>         variable missing complete   n mean   sd p0 p25 p50  p75 p100}
\CommentTok{#>      Bare.nuclei      12      548 560 3.48 3.62  1   1   1 5.25   10}
\CommentTok{#>      Bl.cromatin       0      560 560 3.45 2.43  1   2   3 5      10}
\CommentTok{#>       Cell.shape       0      560 560 3.21 2.97  1   1   1 5      10}
\CommentTok{#>        Cell.size       0      560 560 3.14 3.07  1   1   1 5      10}
\CommentTok{#>     Cl.thickness       0      560 560 4.44 2.83  1   2   4 6      10}
\CommentTok{#>     Epith.c.size       0      560 560 3.23 2.22  1   2   2 4      10}
\CommentTok{#>    Marg.adhesion       0      560 560 2.79 2.85  1   1   1 4      10}
\CommentTok{#>          Mitoses       0      560 560 1.59 1.7   1   1   1 1      10}
\CommentTok{#>  Normal.nucleoli       0      560 560 2.94 3.08  1   1   1 4      10}
\CommentTok{#>      hist}
\CommentTok{#>  }
\CommentTok{#>  }
\CommentTok{#>  }
\CommentTok{#>  }
\CommentTok{#>  }
\CommentTok{#>  }
\CommentTok{#>  }
\CommentTok{#>  }
\CommentTok{#>  }
\end{Highlighting}
\end{Shaded}

\begin{quote}
we can see we have 13 NA values for the Bare.nuclei attribute. This suggests
we may need to remove the records (or impute values) with NA values for some analysis and modeling techniques.
\end{quote}

\hypertarget{analyze-the-class-variable}{%
\section{Analyze the class variable}\label{analyze-the-class-variable}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# class distribution}
\KeywordTok{cbind}\NormalTok{(}\DataTypeTok{freq =} \KeywordTok{table}\NormalTok{(dataset}\OperatorTok{$}\NormalTok{Class), }
      \DataTypeTok{percentage =} \KeywordTok{prop.table}\NormalTok{(}\KeywordTok{table}\NormalTok{(dataset}\OperatorTok{$}\NormalTok{Class))}\OperatorTok{*}\DecValTok{100}\NormalTok{)}
\CommentTok{#>           freq percentage}
\CommentTok{#> benign     367       65.5}
\CommentTok{#> malignant  193       34.5}
\end{Highlighting}
\end{Shaded}

\begin{quote}
There is indeed a 65\% to 35\% split for benign-malignant in the class values which is imbalanced, but not so much that we need to be thinking about rebalancing the dataset, at least not yet.
\end{quote}

\hypertarget{remove-nas}{%
\subsection{remove NAs}\label{remove-nas}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# summarize correlations between input variables}
\NormalTok{complete_cases <-}\StringTok{ }\KeywordTok{complete.cases}\NormalTok{(dataset)}
\KeywordTok{cor}\NormalTok{(dataset[complete_cases,}\DecValTok{1}\OperatorTok{:}\DecValTok{9}\NormalTok{])}
\CommentTok{#>                 Cl.thickness Cell.size Cell.shape Marg.adhesion}
\CommentTok{#> Cl.thickness           1.000     0.665      0.667         0.486}
\CommentTok{#> Cell.size              0.665     1.000      0.904         0.722}
\CommentTok{#> Cell.shape             0.667     0.904      1.000         0.694}
\CommentTok{#> Marg.adhesion          0.486     0.722      0.694         1.000}
\CommentTok{#> Epith.c.size           0.543     0.773      0.739         0.643}
\CommentTok{#> Bare.nuclei            0.598     0.700      0.721         0.669}
\CommentTok{#> Bl.cromatin            0.565     0.752      0.739         0.692}
\CommentTok{#> Normal.nucleoli        0.570     0.737      0.741         0.644}
\CommentTok{#> Mitoses                0.347     0.453      0.432         0.419}
\CommentTok{#>                 Epith.c.size Bare.nuclei Bl.cromatin Normal.nucleoli}
\CommentTok{#> Cl.thickness           0.543       0.598       0.565           0.570}
\CommentTok{#> Cell.size              0.773       0.700       0.752           0.737}
\CommentTok{#> Cell.shape             0.739       0.721       0.739           0.741}
\CommentTok{#> Marg.adhesion          0.643       0.669       0.692           0.644}
\CommentTok{#> Epith.c.size           1.000       0.614       0.628           0.642}
\CommentTok{#> Bare.nuclei            0.614       1.000       0.685           0.605}
\CommentTok{#> Bl.cromatin            0.628       0.685       1.000           0.692}
\CommentTok{#> Normal.nucleoli        0.642       0.605       0.692           1.000}
\CommentTok{#> Mitoses                0.485       0.351       0.356           0.432}
\CommentTok{#>                 Mitoses}
\CommentTok{#> Cl.thickness      0.347}
\CommentTok{#> Cell.size         0.453}
\CommentTok{#> Cell.shape        0.432}
\CommentTok{#> Marg.adhesion     0.419}
\CommentTok{#> Epith.c.size      0.485}
\CommentTok{#> Bare.nuclei       0.351}
\CommentTok{#> Bl.cromatin       0.356}
\CommentTok{#> Normal.nucleoli   0.432}
\CommentTok{#> Mitoses           1.000}
\end{Highlighting}
\end{Shaded}

\begin{quote}
We can see some modest to high correlation between some of the attributes. For example between cell shape and cell size at 0.90 correlation.
\end{quote}

\hypertarget{unimodal-visualization}{%
\section{Unimodal visualization}\label{unimodal-visualization}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# histograms each attribute}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{))}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{9}\NormalTok{) \{}
    \KeywordTok{hist}\NormalTok{(dataset[,i], }\DataTypeTok{main=}\KeywordTok{names}\NormalTok{(dataset)[i])}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{algo-comparison_322-classification_BreastCancer-mlbench-multi_files/figure-latex/unnamed-chunk-16-1} \end{center}

\begin{quote}
We can see that almost all of the distributions have an exponential or bimodal shape to them.
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# density plot for each attribute}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{))}
\NormalTok{complete_cases <-}\StringTok{ }\KeywordTok{complete.cases}\NormalTok{(dataset)}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{9}\NormalTok{) \{}
    \KeywordTok{plot}\NormalTok{(}\KeywordTok{density}\NormalTok{(dataset[complete_cases,i]), }\DataTypeTok{main=}\KeywordTok{names}\NormalTok{(dataset)[i])}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{algo-comparison_322-classification_BreastCancer-mlbench-multi_files/figure-latex/unnamed-chunk-17-1} \end{center}

\begin{quote}
These plots add more support to our initial ideas. We can see bimodal distributions (two bumps) and exponential-looking distributions.
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# boxplots for each attribute}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{))}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{9}\NormalTok{) \{}
    \KeywordTok{boxplot}\NormalTok{(dataset[,i], }\DataTypeTok{main=}\KeywordTok{names}\NormalTok{(dataset)[i])}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{algo-comparison_322-classification_BreastCancer-mlbench-multi_files/figure-latex/unnamed-chunk-18-1} \end{center}

\hypertarget{multimodal-visualization}{%
\section{Multimodal visualization}\label{multimodal-visualization}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# scatter plot matrix}
\NormalTok{jittered_x <-}\StringTok{ }\KeywordTok{sapply}\NormalTok{(dataset[,}\DecValTok{1}\OperatorTok{:}\DecValTok{9}\NormalTok{], jitter)}
\KeywordTok{pairs}\NormalTok{(jittered_x, }\KeywordTok{names}\NormalTok{(dataset[,}\DecValTok{1}\OperatorTok{:}\DecValTok{9}\NormalTok{]), }\DataTypeTok{col=}\NormalTok{dataset}\OperatorTok{$}\NormalTok{Class)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{algo-comparison_322-classification_BreastCancer-mlbench-multi_files/figure-latex/unnamed-chunk-19-1} \end{center}

\begin{quote}
We can see that the black (benign) a part to be clustered around the bottom-right corner (smaller values) and red (malignant) are all over the place.
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# bar plots of each variable by class}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{))}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{9}\NormalTok{) \{}
    \KeywordTok{barplot}\NormalTok{(}\KeywordTok{table}\NormalTok{(dataset}\OperatorTok{$}\NormalTok{Class,dataset[,i]), }\DataTypeTok{main=}\KeywordTok{names}\NormalTok{(dataset)[i], }
            \DataTypeTok{legend.text=}\KeywordTok{unique}\NormalTok{(dataset}\OperatorTok{$}\NormalTok{Class))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{algo-comparison_322-classification_BreastCancer-mlbench-multi_files/figure-latex/unnamed-chunk-20-1} \end{center}

\hypertarget{algorithms-evaluation}{%
\section{Algorithms Evaluation}\label{algorithms-evaluation}}

\begin{itemize}
\item
  Linear Algorithms: Logistic Regression (LG), Linear Discriminate Analysis (LDA) and Regularized Logistic Regression (GLMNET).
\item
  Nonlinear Algorithms: k-Nearest Neighbors (KNN), Classication and Regression Trees (CART), Naive Bayes (NB) and Support Vector Machines with Radial Basis Functions (SVM).
\end{itemize}

For simplicity, we will use Accuracy and Kappa metrics. Given
that it is a medical test, we could have gone with the Area Under ROC Curve (AUC) and looked at the sensitivity and specicity to select the best algorithms.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# 10-fold cross-validation with 3 repeats}
\NormalTok{trainControl <-}\StringTok{ }\KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"repeatedcv"}\NormalTok{, }\DataTypeTok{number=}\DecValTok{10}\NormalTok{, }\DataTypeTok{repeats=}\DecValTok{3}\NormalTok{)}
\NormalTok{metric <-}\StringTok{ "Accuracy"}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{tic}\NormalTok{()}
\CommentTok{# LG}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{7}\NormalTok{)}
\NormalTok{fit.glm <-}\StringTok{ }\KeywordTok{train}\NormalTok{(Class}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{dataset, }\DataTypeTok{method=}\StringTok{"glm"}\NormalTok{, }\DataTypeTok{metric=}\NormalTok{metric,}
                 \DataTypeTok{trControl=}\NormalTok{trainControl, }\DataTypeTok{na.action=}\NormalTok{na.omit)}
\CommentTok{# LDA}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{7}\NormalTok{)}
\NormalTok{fit.lda <-}\StringTok{ }\KeywordTok{train}\NormalTok{(Class}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{dataset, }\DataTypeTok{method=}\StringTok{"lda"}\NormalTok{, }\DataTypeTok{metric=}\NormalTok{metric,}
                 \DataTypeTok{trControl=}\NormalTok{trainControl, }\DataTypeTok{na.action=}\NormalTok{na.omit)}
\CommentTok{# GLMNET}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{7}\NormalTok{)}
\NormalTok{fit.glmnet <-}\StringTok{ }\KeywordTok{train}\NormalTok{(Class}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{dataset, }\DataTypeTok{method=}\StringTok{"glmnet"}\NormalTok{, }\DataTypeTok{metric=}\NormalTok{metric,}
                    \DataTypeTok{trControl=}\NormalTok{trainControl, }\DataTypeTok{na.action=}\NormalTok{na.omit)}
\CommentTok{# KNN}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{7}\NormalTok{)}
\NormalTok{fit.knn <-}\StringTok{ }\KeywordTok{train}\NormalTok{(Class}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{dataset, }\DataTypeTok{method=}\StringTok{"knn"}\NormalTok{, }\DataTypeTok{metric=}\NormalTok{metric, }
                 \DataTypeTok{trControl=}\NormalTok{trainControl, }\DataTypeTok{na.action=}\NormalTok{na.omit)}
\CommentTok{# CART}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{7}\NormalTok{)}
\NormalTok{fit.cart <-}\StringTok{ }\KeywordTok{train}\NormalTok{(Class}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{dataset, }\DataTypeTok{method=}\StringTok{"rpart"}\NormalTok{, }\DataTypeTok{metric=}\NormalTok{metric, }
                  \DataTypeTok{trControl=}\NormalTok{trainControl, }\DataTypeTok{na.action=}\NormalTok{na.omit)}
\CommentTok{# Naive Bayes}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{7}\NormalTok{)}
\NormalTok{fit.nb <-}\StringTok{ }\KeywordTok{train}\NormalTok{(Class}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{dataset, }\DataTypeTok{method=}\StringTok{"nb"}\NormalTok{, }\DataTypeTok{metric=}\NormalTok{metric, }
                \DataTypeTok{trControl=}\NormalTok{trainControl, }\DataTypeTok{na.action=}\NormalTok{na.omit)}
\CommentTok{# SVM}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{7}\NormalTok{)}
\NormalTok{fit.svm <-}\StringTok{ }\KeywordTok{train}\NormalTok{(Class}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{dataset, }\DataTypeTok{method=}\StringTok{"svmRadial"}\NormalTok{, }\DataTypeTok{metric=}\NormalTok{metric, }
                 \DataTypeTok{trControl=}\NormalTok{trainControl, }\DataTypeTok{na.action=}\NormalTok{na.omit)}

\CommentTok{# Compare algorithms}
\NormalTok{results <-}\StringTok{ }\KeywordTok{resamples}\NormalTok{(}\KeywordTok{list}\NormalTok{(}\DataTypeTok{LG     =}\NormalTok{ fit.glm, }
                          \DataTypeTok{LDA    =}\NormalTok{ fit.lda, }
                          \DataTypeTok{GLMNET =}\NormalTok{ fit.glmnet, }
                          \DataTypeTok{KNN    =}\NormalTok{ fit.knn, }
                          \DataTypeTok{CART   =}\NormalTok{ fit.cart, }
                          \DataTypeTok{NB     =}\NormalTok{ fit.nb, }
                          \DataTypeTok{SVM    =}\NormalTok{ fit.svm))}
\KeywordTok{toc}\NormalTok{()}
\CommentTok{#> 13.846 sec elapsed}
\KeywordTok{summary}\NormalTok{(results)}
\CommentTok{#> }
\CommentTok{#> Call:}
\CommentTok{#> summary.resamples(object = results)}
\CommentTok{#> }
\CommentTok{#> Models: LG, LDA, GLMNET, KNN, CART, NB, SVM }
\CommentTok{#> Number of resamples: 30 }
\CommentTok{#> }
\CommentTok{#> Accuracy }
\CommentTok{#>         Min. 1st Qu. Median  Mean 3rd Qu. Max. NA's}
\CommentTok{#> LG     0.909   0.945  0.964 0.968   0.995    1    0}
\CommentTok{#> LDA    0.907   0.945  0.964 0.963   0.982    1    0}
\CommentTok{#> GLMNET 0.927   0.964  0.964 0.973   0.995    1    0}
\CommentTok{#> KNN    0.927   0.964  0.982 0.976   1.000    1    0}
\CommentTok{#> CART   0.833   0.927  0.945 0.943   0.964    1    0}
\CommentTok{#> NB     0.927   0.963  0.981 0.970   0.982    1    0}
\CommentTok{#> SVM    0.907   0.945  0.964 0.965   0.982    1    0}
\CommentTok{#> }
\CommentTok{#> Kappa }
\CommentTok{#>         Min. 1st Qu. Median  Mean 3rd Qu. Max. NA's}
\CommentTok{#> LG     0.806   0.880  0.921 0.930   0.990    1    0}
\CommentTok{#> LDA    0.786   0.880  0.918 0.917   0.959    1    0}
\CommentTok{#> GLMNET 0.843   0.918  0.922 0.940   0.990    1    0}
\CommentTok{#> KNN    0.843   0.920  0.959 0.947   1.000    1    0}
\CommentTok{#> CART   0.630   0.840  0.879 0.876   0.920    1    0}
\CommentTok{#> NB     0.835   0.918  0.959 0.934   0.960    1    0}
\CommentTok{#> SVM    0.804   0.883  0.922 0.926   0.960    1    0}
\KeywordTok{dotplot}\NormalTok{(results)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{algo-comparison_322-classification_BreastCancer-mlbench-multi_files/figure-latex/models-first-run-1} \end{center}

\begin{quote}
We can see good accuracy across the board. All algorithms have a mean accuracy
above 90\%, well above the baseline of 65\% if we just predicted benign. The
problem is learnable. We can see that KNN (97.08\%) and logistic regression (NB
was 96.2\% and GLMNET was 96.4\%) had the highest accuracy on the problem.
\end{quote}

\hypertarget{data-transform}{%
\section{Data transform}\label{data-transform}}

We know we have some skewed distributions. There are transform methods that we can use to adjust and normalize these distributions. A favorite for positive input attributes (which we have in this case) is the Box-Cox transform.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# 10-fold cross-validation with 3 repeats}
\NormalTok{trainControl <-}\StringTok{ }\KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method=}\StringTok{"repeatedcv"}\NormalTok{, }\DataTypeTok{number=}\DecValTok{10}\NormalTok{, }\DataTypeTok{repeats=}\DecValTok{3}\NormalTok{)}
\NormalTok{metric <-}\StringTok{ "Accuracy"}

\CommentTok{# LG}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{7}\NormalTok{)}
\NormalTok{fit.glm <-}\StringTok{ }\KeywordTok{train}\NormalTok{(Class}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{dataset, }\DataTypeTok{method=}\StringTok{"glm"}\NormalTok{, }\DataTypeTok{metric=}\NormalTok{metric, }
                 \DataTypeTok{preProc=}\KeywordTok{c}\NormalTok{(}\StringTok{"BoxCox"}\NormalTok{), }\DataTypeTok{trControl=}\NormalTok{trainControl, }\DataTypeTok{na.action=}\NormalTok{na.omit)}
\CommentTok{# LDA}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{7}\NormalTok{)}
\NormalTok{fit.lda <-}\StringTok{ }\KeywordTok{train}\NormalTok{(Class}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{dataset, }\DataTypeTok{method=}\StringTok{"lda"}\NormalTok{, }\DataTypeTok{metric=}\NormalTok{metric,}
                 \DataTypeTok{preProc=}\KeywordTok{c}\NormalTok{(}\StringTok{"BoxCox"}\NormalTok{), }\DataTypeTok{trControl=}\NormalTok{trainControl, }\DataTypeTok{na.action=}\NormalTok{na.omit)}
\CommentTok{# GLMNET}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{7}\NormalTok{)}
\NormalTok{fit.glmnet <-}\StringTok{ }\KeywordTok{train}\NormalTok{(Class}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{dataset, }\DataTypeTok{method=}\StringTok{"glmnet"}\NormalTok{, }\DataTypeTok{metric=}\NormalTok{metric, }
                    \DataTypeTok{preProc=}\KeywordTok{c}\NormalTok{(}\StringTok{"BoxCox"}\NormalTok{), }\DataTypeTok{trControl=}\NormalTok{trainControl, }
                    \DataTypeTok{na.action=}\NormalTok{na.omit)}
\CommentTok{# KNN}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{7}\NormalTok{)}
\NormalTok{fit.knn <-}\StringTok{ }\KeywordTok{train}\NormalTok{(Class}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{dataset, }\DataTypeTok{method=}\StringTok{"knn"}\NormalTok{, }\DataTypeTok{metric=}\NormalTok{metric, }
                 \DataTypeTok{preProc=}\KeywordTok{c}\NormalTok{(}\StringTok{"BoxCox"}\NormalTok{), }\DataTypeTok{trControl=}\NormalTok{trainControl, }\DataTypeTok{na.action=}\NormalTok{na.omit)}
\CommentTok{# CART}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{7}\NormalTok{)}
\NormalTok{fit.cart <-}\StringTok{ }\KeywordTok{train}\NormalTok{(Class}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{dataset, }\DataTypeTok{method=}\StringTok{"rpart"}\NormalTok{, }\DataTypeTok{metric=}\NormalTok{metric, }
                  \DataTypeTok{preProc=}\KeywordTok{c}\NormalTok{(}\StringTok{"BoxCox"}\NormalTok{), }\DataTypeTok{trControl=}\NormalTok{trainControl, }
                  \DataTypeTok{na.action=}\NormalTok{na.omit)}
\CommentTok{# Naive Bayes}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{7}\NormalTok{)}
\NormalTok{fit.nb <-}\StringTok{ }\KeywordTok{train}\NormalTok{(Class}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{dataset, }\DataTypeTok{method=}\StringTok{"nb"}\NormalTok{, }\DataTypeTok{metric=}\NormalTok{metric, }
                \DataTypeTok{preProc=}\KeywordTok{c}\NormalTok{(}\StringTok{"BoxCox"}\NormalTok{), }\DataTypeTok{trControl=}\NormalTok{trainControl, }\DataTypeTok{na.action=}\NormalTok{na.omit)}
\CommentTok{#> Warning in FUN(X[[i]], ...): Numerical 0 probability for all classes with}
\CommentTok{#> observation 1}
\CommentTok{#> Warning in FUN(X[[i]], ...): Numerical 0 probability for all classes with}
\CommentTok{#> observation 24}
\CommentTok{#> Warning in FUN(X[[i]], ...): Numerical 0 probability for all classes with}
\CommentTok{#> observation 28}
\CommentTok{#> Warning in FUN(X[[i]], ...): Numerical 0 probability for all classes with}
\CommentTok{#> observation 20}
\CommentTok{#> Warning in FUN(X[[i]], ...): Numerical 0 probability for all classes with}
\CommentTok{#> observation 11}
\CommentTok{#> Warning in FUN(X[[i]], ...): Numerical 0 probability for all classes with}
\CommentTok{#> observation 18}
\CommentTok{#> Warning in FUN(X[[i]], ...): Numerical 0 probability for all classes with}
\CommentTok{#> observation 54}
\CommentTok{#> Warning in FUN(X[[i]], ...): Numerical 0 probability for all classes with}
\CommentTok{#> observation 3}
\CommentTok{#> Warning in FUN(X[[i]], ...): Numerical 0 probability for all classes with}
\CommentTok{#> observation 23}
\CommentTok{#> Warning in FUN(X[[i]], ...): Numerical 0 probability for all classes with}
\CommentTok{#> observation 21}
\CommentTok{#> Warning in FUN(X[[i]], ...): Numerical 0 probability for all classes with}
\CommentTok{#> observation 27}
\CommentTok{#> Warning in FUN(X[[i]], ...): Numerical 0 probability for all classes with}
\CommentTok{#> observation 53}
\CommentTok{#> Warning in FUN(X[[i]], ...): Numerical 0 probability for all classes with}
\CommentTok{#> observation 12}
\CommentTok{#> Warning in FUN(X[[i]], ...): Numerical 0 probability for all classes with}
\CommentTok{#> observation 9}
\CommentTok{#> Warning in FUN(X[[i]], ...): Numerical 0 probability for all classes with}
\CommentTok{#> observation 2}
\CommentTok{#> Warning in FUN(X[[i]], ...): Numerical 0 probability for all classes with}
\CommentTok{#> observation 17}
\CommentTok{#> Warning in FUN(X[[i]], ...): Numerical 0 probability for all classes with}
\CommentTok{#> observation 9}
\CommentTok{#> Warning in FUN(X[[i]], ...): Numerical 0 probability for all classes with}
\CommentTok{#> observation 55}
\CommentTok{#> Warning in FUN(X[[i]], ...): Numerical 0 probability for all classes with}
\CommentTok{#> observation 23}
\CommentTok{#> Warning in FUN(X[[i]], ...): Numerical 0 probability for all classes with}
\CommentTok{#> observation 32}
\CommentTok{# SVM}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{7}\NormalTok{)}
\NormalTok{fit.svm <-}\StringTok{ }\KeywordTok{train}\NormalTok{(Class}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{dataset, }\DataTypeTok{method=}\StringTok{"svmRadial"}\NormalTok{, }\DataTypeTok{metric=}\NormalTok{metric, }
                 \DataTypeTok{preProc=}\KeywordTok{c}\NormalTok{(}\StringTok{"BoxCox"}\NormalTok{), }\DataTypeTok{trControl=}\NormalTok{trainControl, }\DataTypeTok{na.action=}\NormalTok{na.omit)}

\CommentTok{# Compare algorithms}
\NormalTok{transformResults <-}\StringTok{ }\KeywordTok{resamples}\NormalTok{(}\KeywordTok{list}\NormalTok{(}\DataTypeTok{LG     =}\NormalTok{ fit.glm, }
                                  \DataTypeTok{LDA    =}\NormalTok{ fit.lda, }
                                  \DataTypeTok{GLMNET =}\NormalTok{ fit.glmnet, }
                                  \DataTypeTok{KNN    =}\NormalTok{ fit.knn, }
                                  \DataTypeTok{CART   =}\NormalTok{ fit.cart, }
                                  \DataTypeTok{NB     =}\NormalTok{ fit.nb, }
                                  \DataTypeTok{SVM    =}\NormalTok{ fit.svm))}
\KeywordTok{summary}\NormalTok{(transformResults)}
\CommentTok{#> }
\CommentTok{#> Call:}
\CommentTok{#> summary.resamples(object = transformResults)}
\CommentTok{#> }
\CommentTok{#> Models: LG, LDA, GLMNET, KNN, CART, NB, SVM }
\CommentTok{#> Number of resamples: 30 }
\CommentTok{#> }
\CommentTok{#> Accuracy }
\CommentTok{#>         Min. 1st Qu. Median  Mean 3rd Qu. Max. NA's}
\CommentTok{#> LG     0.909   0.963  0.982 0.973   0.996    1    0}
\CommentTok{#> LDA    0.927   0.964  0.981 0.974   0.982    1    0}
\CommentTok{#> GLMNET 0.944   0.964  0.982 0.980   1.000    1    0}
\CommentTok{#> KNN    0.909   0.964  0.981 0.976   0.982    1    0}
\CommentTok{#> CART   0.833   0.927  0.945 0.943   0.964    1    0}
\CommentTok{#> NB     0.927   0.964  0.982 0.978   1.000    1    0}
\CommentTok{#> SVM    0.927   0.964  0.982 0.980   1.000    1    0}
\CommentTok{#> }
\CommentTok{#> Kappa }
\CommentTok{#>         Min. 1st Qu. Median  Mean 3rd Qu. Max. NA's}
\CommentTok{#> LG     0.806   0.919  0.959 0.941   0.990    1    0}
\CommentTok{#> LDA    0.847   0.921  0.959 0.945   0.961    1    0}
\CommentTok{#> GLMNET 0.878   0.922  0.960 0.957   1.000    1    0}
\CommentTok{#> KNN    0.806   0.922  0.959 0.949   0.961    1    0}
\CommentTok{#> CART   0.630   0.840  0.879 0.876   0.920    1    0}
\CommentTok{#> NB     0.843   0.922  0.960 0.953   1.000    1    0}
\CommentTok{#> SVM    0.847   0.922  0.960 0.957   1.000    1    0}
\KeywordTok{dotplot}\NormalTok{(transformResults)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{algo-comparison_322-classification_BreastCancer-mlbench-multi_files/figure-latex/models-second-run-BoxCox-1} \end{center}

\begin{quote}
We can see that the accuracy of the previous best algorithm KNN was elevated to 97.14\%. We have a new ranking, showing SVM with the most accurate mean accuracy at 97.20\%.
\end{quote}

\hypertarget{tuning-svm}{%
\section{Tuning SVM}\label{tuning-svm}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# 10-fold cross-validation with 3 repeats}
\NormalTok{trainControl <-}\StringTok{ }\KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method=}\StringTok{"repeatedcv"}\NormalTok{, }\DataTypeTok{number=}\DecValTok{10}\NormalTok{, }\DataTypeTok{repeats=}\DecValTok{3}\NormalTok{)}
\NormalTok{metric <-}\StringTok{ "Accuracy"}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{7}\NormalTok{)}

\NormalTok{grid <-}\StringTok{ }\KeywordTok{expand.grid}\NormalTok{(}\DataTypeTok{.sigma =} \KeywordTok{c}\NormalTok{(}\FloatTok{0.025}\NormalTok{, }\FloatTok{0.05}\NormalTok{, }\FloatTok{0.1}\NormalTok{, }\FloatTok{0.15}\NormalTok{), }
                    \DataTypeTok{.C =} \KeywordTok{seq}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DataTypeTok{by=}\DecValTok{1}\NormalTok{))}

\NormalTok{fit.svm <-}\StringTok{ }\KeywordTok{train}\NormalTok{(Class}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{dataset, }\DataTypeTok{method=}\StringTok{"svmRadial"}\NormalTok{, }\DataTypeTok{metric=}\NormalTok{metric, }
                 \DataTypeTok{tuneGrid=}\NormalTok{grid, }
                 \DataTypeTok{preProc=}\KeywordTok{c}\NormalTok{(}\StringTok{"BoxCox"}\NormalTok{), }\DataTypeTok{trControl=}\NormalTok{trainControl, }
                 \DataTypeTok{na.action=}\NormalTok{na.omit)}
\KeywordTok{print}\NormalTok{(fit.svm)}
\CommentTok{#> Support Vector Machines with Radial Basis Function Kernel }
\CommentTok{#> }
\CommentTok{#> 560 samples}
\CommentTok{#>   9 predictor}
\CommentTok{#>   2 classes: 'benign', 'malignant' }
\CommentTok{#> }
\CommentTok{#> Pre-processing: Box-Cox transformation (9) }
\CommentTok{#> Resampling: Cross-Validated (10 fold, repeated 3 times) }
\CommentTok{#> Summary of sample sizes: 493, 492, 493, 493, 493, 494, ... }
\CommentTok{#> Resampling results across tuning parameters:}
\CommentTok{#> }
\CommentTok{#>   sigma  C   Accuracy  Kappa}
\CommentTok{#>   0.025   1  0.979     0.956}
\CommentTok{#>   0.025   2  0.979     0.954}
\CommentTok{#>   0.025   3  0.979     0.956}
\CommentTok{#>   0.025   4  0.977     0.950}
\CommentTok{#>   0.025   5  0.977     0.950}
\CommentTok{#>   0.025   6  0.978     0.951}
\CommentTok{#>   0.025   7  0.979     0.954}
\CommentTok{#>   0.025   8  0.979     0.956}
\CommentTok{#>   0.025   9  0.979     0.956}
\CommentTok{#>   0.025  10  0.979     0.956}
\CommentTok{#>   0.050   1  0.980     0.957}
\CommentTok{#>   0.050   2  0.980     0.957}
\CommentTok{#>   0.050   3  0.979     0.956}
\CommentTok{#>   0.050   4  0.980     0.957}
\CommentTok{#>   0.050   5  0.980     0.957}
\CommentTok{#>   0.050   6  0.980     0.957}
\CommentTok{#>   0.050   7  0.979     0.956}
\CommentTok{#>   0.050   8  0.979     0.954}
\CommentTok{#>   0.050   9  0.979     0.954}
\CommentTok{#>   0.050  10  0.979     0.954}
\CommentTok{#>   0.100   1  0.980     0.957}
\CommentTok{#>   0.100   2  0.980     0.957}
\CommentTok{#>   0.100   3  0.979     0.956}
\CommentTok{#>   0.100   4  0.978     0.953}
\CommentTok{#>   0.100   5  0.978     0.952}
\CommentTok{#>   0.100   6  0.975     0.946}
\CommentTok{#>   0.100   7  0.976     0.948}
\CommentTok{#>   0.100   8  0.976     0.948}
\CommentTok{#>   0.100   9  0.976     0.948}
\CommentTok{#>   0.100  10  0.975     0.946}
\CommentTok{#>   0.150   1  0.980     0.957}
\CommentTok{#>   0.150   2  0.978     0.953}
\CommentTok{#>   0.150   3  0.978     0.953}
\CommentTok{#>   0.150   4  0.976     0.949}
\CommentTok{#>   0.150   5  0.976     0.948}
\CommentTok{#>   0.150   6  0.975     0.946}
\CommentTok{#>   0.150   7  0.974     0.944}
\CommentTok{#>   0.150   8  0.972     0.939}
\CommentTok{#>   0.150   9  0.970     0.934}
\CommentTok{#>   0.150  10  0.968     0.930}
\CommentTok{#> }
\CommentTok{#> Accuracy was used to select the optimal model using the largest value.}
\CommentTok{#> The final values used for the model were sigma = 0.15 and C = 1.}
\KeywordTok{plot}\NormalTok{(fit.svm)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{algo-comparison_322-classification_BreastCancer-mlbench-multi_files/figure-latex/model-svm-run-takes_a_while-1} \end{center}

\begin{quote}
We can see that we have made very little dierence to the results. The most accurate model had a score of 97.31\% (the same as our previously rounded score of 97.20\%) using a sigma = 0.1 and C = 1. We could tune further, but I don't expect a payo.
\end{quote}

\hypertarget{tuning-knn}{%
\section{Tuning KNN}\label{tuning-knn}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# 10-fold cross-validation with 3 repeats}
\NormalTok{trainControl <-}\StringTok{ }\KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method=}\StringTok{"repeatedcv"}\NormalTok{, }\DataTypeTok{number=}\DecValTok{10}\NormalTok{, }\DataTypeTok{repeats=}\DecValTok{3}\NormalTok{)}
\NormalTok{metric <-}\StringTok{ "Accuracy"}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{7}\NormalTok{)}

\NormalTok{grid <-}\StringTok{ }\KeywordTok{expand.grid}\NormalTok{(}\DataTypeTok{.k =} \KeywordTok{seq}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{20}\NormalTok{, }\DataTypeTok{by=}\DecValTok{1}\NormalTok{))}
\NormalTok{fit.knn <-}\StringTok{ }\KeywordTok{train}\NormalTok{(Class}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{dataset, }\DataTypeTok{method=}\StringTok{"knn"}\NormalTok{, }\DataTypeTok{metric=}\NormalTok{metric, }
                 \DataTypeTok{tuneGrid=}\NormalTok{grid, }
                 \DataTypeTok{preProc=}\KeywordTok{c}\NormalTok{(}\StringTok{"BoxCox"}\NormalTok{), }\DataTypeTok{trControl=}\NormalTok{trainControl, }
                 \DataTypeTok{na.action=}\NormalTok{na.omit)}
\KeywordTok{print}\NormalTok{(fit.knn)}
\CommentTok{#> k-Nearest Neighbors }
\CommentTok{#> }
\CommentTok{#> 560 samples}
\CommentTok{#>   9 predictor}
\CommentTok{#>   2 classes: 'benign', 'malignant' }
\CommentTok{#> }
\CommentTok{#> Pre-processing: Box-Cox transformation (9) }
\CommentTok{#> Resampling: Cross-Validated (10 fold, repeated 3 times) }
\CommentTok{#> Summary of sample sizes: 493, 492, 493, 493, 493, 494, ... }
\CommentTok{#> Resampling results across tuning parameters:}
\CommentTok{#> }
\CommentTok{#>   k   Accuracy  Kappa}
\CommentTok{#>    1  0.958     0.908}
\CommentTok{#>    2  0.960     0.912}
\CommentTok{#>    3  0.968     0.931}
\CommentTok{#>    4  0.970     0.935}
\CommentTok{#>    5  0.972     0.939}
\CommentTok{#>    6  0.973     0.942}
\CommentTok{#>    7  0.976     0.949}
\CommentTok{#>    8  0.975     0.946}
\CommentTok{#>    9  0.976     0.947}
\CommentTok{#>   10  0.976     0.949}
\CommentTok{#>   11  0.976     0.949}
\CommentTok{#>   12  0.976     0.947}
\CommentTok{#>   13  0.974     0.945}
\CommentTok{#>   14  0.975     0.946}
\CommentTok{#>   15  0.976     0.947}
\CommentTok{#>   16  0.975     0.946}
\CommentTok{#>   17  0.976     0.947}
\CommentTok{#>   18  0.976     0.947}
\CommentTok{#>   19  0.978     0.951}
\CommentTok{#>   20  0.977     0.950}
\CommentTok{#> }
\CommentTok{#> Accuracy was used to select the optimal model using the largest value.}
\CommentTok{#> The final value used for the model was k = 19.}
\KeywordTok{plot}\NormalTok{(fit.knn)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{algo-comparison_322-classification_BreastCancer-mlbench-multi_files/figure-latex/model-knn-run-1} \end{center}

\begin{quote}
We can see again that tuning has made little dierence, settling on a value of k = 7 with an accuracy of 97.19\%. This is higher than the previous 97.14\%, but very similar (or perhaps identical!) to the result achieved by the tuned SVM.
\end{quote}

\hypertarget{ensemble}{%
\section{Ensemble}\label{ensemble}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# 10-fold cross-validation with 3 repeats}
\NormalTok{trainControl <-}\StringTok{ }\KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method=}\StringTok{"repeatedcv"}\NormalTok{, }\DataTypeTok{number=}\DecValTok{10}\NormalTok{, }\DataTypeTok{repeats=}\DecValTok{3}\NormalTok{)}
\NormalTok{metric <-}\StringTok{ "Accuracy"}

\CommentTok{# Bagged CART}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{7}\NormalTok{)}
\NormalTok{fit.treebag <-}\StringTok{ }\KeywordTok{train}\NormalTok{(Class}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{dataset, }\DataTypeTok{method=}\StringTok{"treebag"}\NormalTok{, }\DataTypeTok{metric=}\NormalTok{metric, }
                     \DataTypeTok{trControl=}\NormalTok{trainControl, }\DataTypeTok{na.action=}\NormalTok{na.omit)}

\CommentTok{# Random Forest}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{7}\NormalTok{)}
\NormalTok{fit.rf <-}\StringTok{ }\KeywordTok{train}\NormalTok{(Class}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{dataset, }\DataTypeTok{method=}\StringTok{"rf"}\NormalTok{, }\DataTypeTok{metric=}\NormalTok{metric, }
                \DataTypeTok{preProc=}\KeywordTok{c}\NormalTok{(}\StringTok{"BoxCox"}\NormalTok{), }\DataTypeTok{trControl=}\NormalTok{trainControl, }\DataTypeTok{na.action=}\NormalTok{na.omit)}

\CommentTok{# Stochastic Gradient Boosting}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{7}\NormalTok{)}
\NormalTok{fit.gbm <-}\StringTok{ }\KeywordTok{train}\NormalTok{(Class}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{dataset, }\DataTypeTok{method=}\StringTok{"gbm"}\NormalTok{, }\DataTypeTok{metric=}\NormalTok{metric, }
                 \DataTypeTok{preProc=}\KeywordTok{c}\NormalTok{(}\StringTok{"BoxCox"}\NormalTok{), }\DataTypeTok{trControl=}\NormalTok{trainControl, }\DataTypeTok{verbose=}\OtherTok{FALSE}\NormalTok{, }\DataTypeTok{na.action=}\NormalTok{na.omit)}

\CommentTok{# C5.0}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{7}\NormalTok{)}
\NormalTok{fit.c50 <-}\StringTok{ }\KeywordTok{train}\NormalTok{(Class}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{dataset, }\DataTypeTok{method=}\StringTok{"C5.0"}\NormalTok{, }\DataTypeTok{metric=}\NormalTok{metric, }
                 \DataTypeTok{preProc=}\KeywordTok{c}\NormalTok{(}\StringTok{"BoxCox"}\NormalTok{), }\DataTypeTok{trControl=}\NormalTok{trainControl, }\DataTypeTok{na.action=}\NormalTok{na.omit)}

\CommentTok{# Compare results}
\NormalTok{ensembleResults <-}\StringTok{ }\KeywordTok{resamples}\NormalTok{(}\KeywordTok{list}\NormalTok{(}\DataTypeTok{BAG =}\NormalTok{ fit.treebag, }
                                  \DataTypeTok{RF  =}\NormalTok{ fit.rf, }
                                  \DataTypeTok{GBM =}\NormalTok{ fit.gbm, }
                                  \DataTypeTok{C50 =}\NormalTok{ fit.c50))}
\KeywordTok{summary}\NormalTok{(ensembleResults)}
\CommentTok{#> }
\CommentTok{#> Call:}
\CommentTok{#> summary.resamples(object = ensembleResults)}
\CommentTok{#> }
\CommentTok{#> Models: BAG, RF, GBM, C50 }
\CommentTok{#> Number of resamples: 30 }
\CommentTok{#> }
\CommentTok{#> Accuracy }
\CommentTok{#>      Min. 1st Qu. Median  Mean 3rd Qu. Max. NA's}
\CommentTok{#> BAG 0.907   0.963  0.982 0.973   0.982    1    0}
\CommentTok{#> RF  0.926   0.981  0.982 0.979   0.995    1    0}
\CommentTok{#> GBM 0.929   0.963  0.981 0.973   0.995    1    0}
\CommentTok{#> C50 0.907   0.964  0.982 0.972   0.982    1    0}
\CommentTok{#> }
\CommentTok{#> Kappa }
\CommentTok{#>      Min. 1st Qu. Median  Mean 3rd Qu. Max. NA's}
\CommentTok{#> BAG 0.804   0.920  0.959 0.942    0.96    1    0}
\CommentTok{#> RF  0.841   0.959  0.960 0.955    0.99    1    0}
\CommentTok{#> GBM 0.844   0.919  0.959 0.941    0.99    1    0}
\CommentTok{#> C50 0.795   0.918  0.959 0.939    0.96    1    0}
\KeywordTok{dotplot}\NormalTok{(ensembleResults)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{algo-comparison_322-classification_BreastCancer-mlbench-multi_files/figure-latex/models-ensembling-run-1} \end{center}

\begin{quote}
We see that Random Forest was the most accurate with a score of 97.26\%. Very similar to our tuned models above. We could spend time tuning the parameters of Random Forest (e.g.~increasing the number of trees) and the other ensemble methods, but I don't expect to see better accuracy scores other than random statistical uctuations.
\end{quote}

\hypertarget{finalize-model}{%
\section{Finalize model}\label{finalize-model}}

We now need to nalize the model, which really means choose which model we would like to use. For simplicity I would probably select the KNN method, at the expense of the memory required to store the training dataset. SVM would be a good choice to trade-o space and time complexity. I probably would not select the Random Forest algorithm given the complexity of the model. It seems overkill for this dataset, lots of trees with little benet in Accuracy.

Let's go with the KNN algorithm. This is really simple, as we do not need to store a model. We do need to capture the parameters of the Box-Cox transform though. And we also need to prepare the data by removing the unused Id attribute and converting all of the inputs to numeric format.

The implementation of KNN (knn3()) belongs to the caret package and does not support missing values. We will have to remove the rows with missing values from the training dataset as well as the validation dataset. The code below shows the preparation of the pre-processing parameters using the training dataset.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# prepare parameters for data transform}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{7}\NormalTok{)}

\NormalTok{datasetNoMissing <-}\StringTok{ }\NormalTok{dataset[}\KeywordTok{complete.cases}\NormalTok{(dataset),]}
\NormalTok{x <-}\StringTok{ }\NormalTok{datasetNoMissing[,}\DecValTok{1}\OperatorTok{:}\DecValTok{9}\NormalTok{]}

\CommentTok{# transform}
\NormalTok{preprocessParams <-}\StringTok{ }\KeywordTok{preProcess}\NormalTok{(x, }\DataTypeTok{method=}\KeywordTok{c}\NormalTok{(}\StringTok{"BoxCox"}\NormalTok{))}
\NormalTok{x <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(preprocessParams, x)}
\end{Highlighting}
\end{Shaded}

\hypertarget{prepare-the-validation-set}{%
\section{Prepare the validation set}\label{prepare-the-validation-set}}

Next we need to prepare the validation dataset for making a prediction. We must:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Remove the Id attribute.
\item
  Remove those rows with missing data.
\item
  Convert all input attributes to numeric.
\item
  Apply the Box-Cox transform to the input attributes using parameters prepared on the training dataset.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# prepare the validation dataset}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{7}\NormalTok{)}

\CommentTok{# remove id column}
\NormalTok{validation <-}\StringTok{ }\NormalTok{validation[,}\OperatorTok{-}\DecValTok{1}\NormalTok{]}

\CommentTok{# remove missing values (not allowed in this implementation of knn)}
\NormalTok{validation <-}\StringTok{ }\NormalTok{validation[}\KeywordTok{complete.cases}\NormalTok{(validation),]}

\CommentTok{# convert to numeric}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{9}\NormalTok{) \{}
\NormalTok{    validation[,i] <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{as.character}\NormalTok{(validation[,i]))}
\NormalTok{\}}

\CommentTok{# transform the validation dataset}
\NormalTok{validationX <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(preprocessParams, validation[,}\DecValTok{1}\OperatorTok{:}\DecValTok{9}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# make predictions}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{7}\NormalTok{)}
\CommentTok{# knn3Train(train, test, cl, k = 1, l = 0, prob = TRUE, use.all = TRUE)}
\CommentTok{# k: number of neighbours considered.}
\NormalTok{predictions <-}\StringTok{ }\KeywordTok{knn3Train}\NormalTok{(x, validationX, datasetNoMissing}\OperatorTok{$}\NormalTok{Class, }
                         \DataTypeTok{k =} \DecValTok{9}\NormalTok{, }
                         \DataTypeTok{prob =} \OtherTok{FALSE}\NormalTok{)}

\CommentTok{# convert }
\KeywordTok{confusionMatrix}\NormalTok{(}\KeywordTok{as.factor}\NormalTok{(predictions), validation}\OperatorTok{$}\NormalTok{Class)}
\CommentTok{#> Confusion Matrix and Statistics}
\CommentTok{#> }
\CommentTok{#>            Reference}
\CommentTok{#> Prediction  benign malignant}
\CommentTok{#>   benign        83         1}
\CommentTok{#>   malignant      4        47}
\CommentTok{#>                                         }
\CommentTok{#>                Accuracy : 0.963         }
\CommentTok{#>                  95% CI : (0.916, 0.988)}
\CommentTok{#>     No Information Rate : 0.644         }
\CommentTok{#>     P-Value [Acc > NIR] : <2e-16        }
\CommentTok{#>                                         }
\CommentTok{#>                   Kappa : 0.92          }
\CommentTok{#>                                         }
\CommentTok{#>  Mcnemar's Test P-Value : 0.371         }
\CommentTok{#>                                         }
\CommentTok{#>             Sensitivity : 0.954         }
\CommentTok{#>             Specificity : 0.979         }
\CommentTok{#>          Pos Pred Value : 0.988         }
\CommentTok{#>          Neg Pred Value : 0.922         }
\CommentTok{#>              Prevalence : 0.644         }
\CommentTok{#>          Detection Rate : 0.615         }
\CommentTok{#>    Detection Prevalence : 0.622         }
\CommentTok{#>       Balanced Accuracy : 0.967         }
\CommentTok{#>                                         }
\CommentTok{#>        'Positive' Class : benign        }
\CommentTok{#> }
\end{Highlighting}
\end{Shaded}

\begin{quote}
We can see that the accuracy of the final model on the validation dataset is 99.26\%. This is optimistic because there is only 136 rows, but it does show that we have an accurate standalone model that we could use on other unclassied data.
\end{quote}

\hypertarget{classification-algorithms-comparison.-outbreaks-dataset.-rf-glmnet-knn-pda-lda-nsc-c5-pls}{%
\chapter{\texorpdfstring{Classification algorithms comparison. outbreaks dataset. (\emph{RF, GLMNET, KNN, PDA, LDA, NSC, C5, PLS})}{Classification algorithms comparison. outbreaks dataset. (RF, GLMNET, KNN, PDA, LDA, NSC, C5, PLS)}}\label{classification-algorithms-comparison.-outbreaks-dataset.-rf-glmnet-knn-pda-lda-nsc-c5-pls}}

\hypertarget{introduction-5}{%
\section{Introduction}\label{introduction-5}}

Among the many nice R packages containing data collections is the \href{https://mran.microsoft.com/web/packages/outbreaks/outbreaks.pdf}{outbreaks} package. It contains datsets on epidemics and among them is data from the 2013 outbreak of \href{http://www.who.int/influenza/human_animal_interface/faq_H7N9/en/}{influenza A H7N9} in \href{http://www.who.int/influenza/human_animal_interface/influenza_h7n9/ChinaH7N9JointMissionReport2013u.pdf?ua=1}{China} as analysed by Kucharski et al. (2014):

\begin{quote}
A. Kucharski, H. Mills, A. Pinsent, C. Fraser, M. Van Kerkhove, C. A. Donnelly, and S. Riley. 2014. Distinguishing between reservoir exposure and human-to-human transmission for emerging pathogens using case onset data. PLOS Currents Outbreaks. Mar 7, edition 1. doi: 10.1371/currents.outbreaks.e1473d9bfc99d080ca242139a06c455f.
\end{quote}

\begin{quote}
A. Kucharski, H. Mills, A. Pinsent, C. Fraser, M. Van Kerkhove, C. A. Donnelly, and S. Riley. 2014. Data from: Distinguishing between reservoir exposure and human-to-human transmission for emerging pathogens using case onset data. Dryad Digital Repository. \url{http://dx.doi.org/10.5061/dryad.2g43n}.
\end{quote}

\hypertarget{algorithms-1}{%
\subsection{Algorithms}\label{algorithms-1}}

We compare these classification algorithms:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Random Forest
\item
  GLM net
\item
  k-Nearest Neighbors
\item
  Penalized Discriminant Analysis
\item
  Stabilized Linear Discriminant Analysis
\item
  Nearest Shrunken Centroids
\item
  Single C5.0 Tree
\item
  Partial Least Squares
\end{enumerate}

\hypertarget{workflow-4}{%
\subsection{Workflow}\label{workflow-4}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Load dataset
\item
  Data wrangling
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Many dates as one variable: \texttt{gather}
\item
  Convert group to factor: \texttt{factor}
\item
  Change dates descriptions: \texttt{dplyr::mapvalues}
\item
  Set several provinces as Other: \texttt{mapvalues}
\item
  Convert gender to factor
\item
  Convert province to factor
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Features visualization
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Area density plot of Month vs Age by Province, by date group, by outcome, by gender
\item
  Number of flu cases by gender, by province
\item
  Age density plot of flu cases by outcome
\item
  Days passed from outset by province, by age, by gender, by outcome
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Feature engineering
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Generate new features
\item
  Gender to numeric
\item
  Provinces to binary classifiers
\item
  Outcome to binary classifier
\item
  Impute missing data: \texttt{mice}
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Split dataset in training, validation and test sets\\
\item
  Feature importance
\end{enumerate}

\begin{itemize}
\tightlist
\item
  with Decision Trees
\item
  with Random Forest
\item
  Wrangling dataset
\item
  Plot Importance vs Variable
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{6}
\tightlist
\item
  Impact on Datasets\\
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Density plot of training, validation and test datasets
\item
  Features vs outcome by age, days onset to hospital, day onset to outcome
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{7}
\tightlist
\item
  Train models on training validation dataset
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Numeric and visual Comparison of models

  \begin{itemize}
  \tightlist
  \item
    Accuracy
  \item
    Kappa
  \end{itemize}
\item
  Compare predictions on training validation set

  \begin{itemize}
  \tightlist
  \item
    Create dataset for results
  \item
    New variable for outcome
  \end{itemize}
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{10}
\tightlist
\item
  Predicting on unknown outcomes on training data\\
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Numeric and visual Comparison of models

  \begin{itemize}
  \tightlist
  \item
    Accuracy
  \item
    Kappa
  \end{itemize}
\item
  Compare predictions on training validation set

  \begin{itemize}
  \tightlist
  \item
    Create dataset for results
  \item
    New variable for outcome
  \end{itemize}
\item
  Calculate predicted outcome
\item
  Save to CSV file
\item
  Calculate recovery cases
\item
  Summarize outcome
\item
  Plot month vs log2-ratio of recovery vs death, by gender, by age, by date group
\end{itemize}

\hypertarget{can-we-predict-flu-outcome-with-machine-learning-in-r}{%
\subsection{Can we predict flu outcome with Machine Learning in R?}\label{can-we-predict-flu-outcome-with-machine-learning-in-r}}

I will be using their data as an example to show how to use Machine Learning algorithms for predicting disease outcome.

To do so, I selected and extracted features from the raw data, including age, days between onset and outcome, gender, whether the patients were hospitalised, etc. Missing values were imputed and different model algorithms were used to predict outcome (death or recovery). The prediction accuracy, sensitivity and specificity. The thus prepared dataset was devided into training and testing subsets. The test subset contained all cases with an unknown outcome. Before I applied the models to the test data, I further split the training data into validation subsets.

The tested modeling algorithms were similarly successful at predicting the outcomes of the validation data. To decide on final classifications, I compared predictions from all models and defined the outcome ``Death'' or ``Recovery'' as a function of all models, whereas classifications with a low prediction probability were flagged as ``uncertain''. Accounting for this uncertainty led to a 100\% correct classification of the validation test set.

The training cases with unknown outcome were then classified based on the same algorithms. From 57 unknown cases, 14 were classified as ``Recovery'', 10 as ``Death'' and 33 as uncertain.

In a Part 2, I am looking at how extreme gradient boosting performs on this dataset.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Disclaimer:} I am not an expert in Machine Learning. Everything I know, I tought myself during the last months. So, if you see any mistakes or have tips and tricks for improvement, please don't hesitate to let me know! Thanks. :-)

\hypertarget{the-data}{%
\section{The data}\label{the-data}}

The dataset contains case ID, date of onset, date of hospitalisation, date of outcome, gender, age, province and of course the outcome: Death or Recovery. I can already see that there are a couple of missing values in the data, which I will deal with later.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{options}\NormalTok{(}\DataTypeTok{width =} \DecValTok{1000}\NormalTok{)}

\ControlFlowTok{if}\NormalTok{ (}\OperatorTok{!}\KeywordTok{require}\NormalTok{(}\StringTok{"outbreaks"}\NormalTok{)) }\KeywordTok{install.packages}\NormalTok{(}\StringTok{"outbreaks"}\NormalTok{)}
\CommentTok{#> Loading required package: outbreaks}
\KeywordTok{library}\NormalTok{(outbreaks)}

\NormalTok{fluH7N9_china_}\DecValTok{2013}\NormalTok{_backup <-}\StringTok{ }\NormalTok{fluH7N9_china_}\DecValTok{2013}

\NormalTok{fluH7N9_china_}\DecValTok{2013}\OperatorTok{$}\NormalTok{age[}\KeywordTok{which}\NormalTok{(fluH7N9_china_}\DecValTok{2013}\OperatorTok{$}\NormalTok{age }\OperatorTok{==}\StringTok{ "?"}\NormalTok{)] <-}\StringTok{ }\OtherTok{NA}
\NormalTok{fluH7N9_china_}\DecValTok{2013}\OperatorTok{$}\NormalTok{case_ID <-}\StringTok{ }\KeywordTok{paste}\NormalTok{(}\StringTok{"case"}\NormalTok{, }
\NormalTok{                                    fluH7N9_china_}\DecValTok{2013}\OperatorTok{$}\NormalTok{case_id, }
                                    \DataTypeTok{sep =} \StringTok{"_"}\NormalTok{)}
\KeywordTok{head}\NormalTok{(fluH7N9_china_}\DecValTok{2013}\NormalTok{)}
\CommentTok{#>   case_id date_of_onset date_of_hospitalisation date_of_outcome outcome gender age province case_ID}
\CommentTok{#> 1       1    2013-02-19                    <NA>      2013-03-04   Death      m  87 Shanghai  case_1}
\CommentTok{#> 2       2    2013-02-27              2013-03-03      2013-03-10   Death      m  27 Shanghai  case_2}
\CommentTok{#> 3       3    2013-03-09              2013-03-19      2013-04-09   Death      f  35    Anhui  case_3}
\CommentTok{#> 4       4    2013-03-19              2013-03-27            <NA>    <NA>      f  45  Jiangsu  case_4}
\CommentTok{#> 5       5    2013-03-19              2013-03-30      2013-05-15 Recover      f  48  Jiangsu  case_5}
\CommentTok{#> 6       6    2013-03-21              2013-03-28      2013-04-26   Death      f  32  Jiangsu  case_6}
\end{Highlighting}
\end{Shaded}

Before I start preparing the data for Machine Learning, I want to get an idea of the distribution of the data points and their different variables by plotting.

Most provinces have only a handful of cases, so I am combining them into the category ``other'' and keep only Jiangsu, Shanghai and Zhejian and separate provinces.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# make tidy data, clean up and simplify}
\KeywordTok{library}\NormalTok{(tidyr)}

\CommentTok{# put different type of dates in one variable (Date)}
\NormalTok{fluH7N9_china_}\DecValTok{2013}\NormalTok{_gather <-}\StringTok{ }\NormalTok{fluH7N9_china_}\DecValTok{2013} \OperatorTok{%>%}
\StringTok{  }\KeywordTok{gather}\NormalTok{(Group, Date, date_of_onset}\OperatorTok{:}\NormalTok{date_of_outcome)}

\CommentTok{# convert Group to factor}
\NormalTok{fluH7N9_china_}\DecValTok{2013}\NormalTok{_gather}\OperatorTok{$}\NormalTok{Group <-}\StringTok{ }\KeywordTok{factor}\NormalTok{(fluH7N9_china_}\DecValTok{2013}\NormalTok{_gather}\OperatorTok{$}\NormalTok{Group, }
                                          \DataTypeTok{levels =} \KeywordTok{c}\NormalTok{(}\StringTok{"date_of_onset"}\NormalTok{, }
                                                     \StringTok{"date_of_hospitalisation"}\NormalTok{, }
                                                     \StringTok{"date_of_outcome"}\NormalTok{))}

\CommentTok{# change the dates description with plyr::mapvalues}
\KeywordTok{library}\NormalTok{(plyr)}
\NormalTok{from <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"date_of_onset"}\NormalTok{, }\StringTok{"date_of_hospitalisation"}\NormalTok{, }\StringTok{"date_of_outcome"}\NormalTok{)}
\NormalTok{to <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Date of onset"}\NormalTok{, }\StringTok{"Date of hospitalisation"}\NormalTok{, }\StringTok{"Date of outcome"}\NormalTok{)}
\NormalTok{fluH7N9_china_}\DecValTok{2013}\NormalTok{_gather}\OperatorTok{$}\NormalTok{Group <-}\StringTok{ }\KeywordTok{mapvalues}\NormalTok{(fluH7N9_china_}\DecValTok{2013}\NormalTok{_gather}\OperatorTok{$}\NormalTok{Group, }
                                             \DataTypeTok{from =}\NormalTok{ from, }\DataTypeTok{to =}\NormalTok{ to)}

\CommentTok{# change additional provinces to Other}
\NormalTok{from <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Anhui"}\NormalTok{, }\StringTok{"Beijing"}\NormalTok{, }\StringTok{"Fujian"}\NormalTok{, }\StringTok{"Guangdong"}\NormalTok{, }\StringTok{"Hebei"}\NormalTok{, }\StringTok{"Henan"}\NormalTok{, }
          \StringTok{"Hunan"}\NormalTok{, }\StringTok{"Jiangxi"}\NormalTok{, }\StringTok{"Shandong"}\NormalTok{, }\StringTok{"Taiwan"}\NormalTok{)}
\NormalTok{to <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\StringTok{"Other"}\NormalTok{, }\DecValTok{10}\NormalTok{)}
\NormalTok{fluH7N9_china_}\DecValTok{2013}\NormalTok{_gather}\OperatorTok{$}\NormalTok{province <-}\StringTok{ }
\StringTok{  }\KeywordTok{mapvalues}\NormalTok{(fluH7N9_china_}\DecValTok{2013}\NormalTok{_gather}\OperatorTok{$}\NormalTok{province, }
            \DataTypeTok{from =}\NormalTok{ from, }\DataTypeTok{to =}\NormalTok{ to)}

\CommentTok{# convert gender to factor}
\KeywordTok{levels}\NormalTok{(fluH7N9_china_}\DecValTok{2013}\NormalTok{_gather}\OperatorTok{$}\NormalTok{gender) <-}\StringTok{ }
\StringTok{  }\KeywordTok{c}\NormalTok{(}\KeywordTok{levels}\NormalTok{(fluH7N9_china_}\DecValTok{2013}\NormalTok{_gather}\OperatorTok{$}\NormalTok{gender), }\StringTok{"unknown"}\NormalTok{)}

\CommentTok{# replace NA in gender by "unknown}
\NormalTok{is_na <-}\StringTok{ }\KeywordTok{is.na}\NormalTok{(fluH7N9_china_}\DecValTok{2013}\NormalTok{_gather}\OperatorTok{$}\NormalTok{gender)}
\NormalTok{fluH7N9_china_}\DecValTok{2013}\NormalTok{_gather}\OperatorTok{$}\NormalTok{gender[is_na] <-}\StringTok{ "unknown"}

\CommentTok{# convert province to factor}
\NormalTok{fluH7N9_china_}\DecValTok{2013}\NormalTok{_gather}\OperatorTok{$}\NormalTok{province <-}\StringTok{ }\KeywordTok{factor}\NormalTok{(fluH7N9_china_}\DecValTok{2013}\NormalTok{_gather}\OperatorTok{$}\NormalTok{province, }
                                             \DataTypeTok{levels =} \KeywordTok{c}\NormalTok{(}\StringTok{"Jiangsu"}\NormalTok{,  }\StringTok{"Shanghai"}\NormalTok{, }
                                                        \StringTok{"Zhejiang"}\NormalTok{, }\StringTok{"Other"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(ggplot2)}
\CommentTok{#> Registered S3 methods overwritten by 'ggplot2':}
\CommentTok{#>   method         from }
\CommentTok{#>   [.quosures     rlang}
\CommentTok{#>   c.quosures     rlang}
\CommentTok{#>   print.quosures rlang}
\NormalTok{my_theme <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(}\DataTypeTok{base_size =} \DecValTok{12}\NormalTok{, }\DataTypeTok{base_family =} \StringTok{"sans"}\NormalTok{)\{}
  \KeywordTok{theme_minimal}\NormalTok{(}\DataTypeTok{base_size =}\NormalTok{ base_size, }\DataTypeTok{base_family =}\NormalTok{ base_family) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}
    \DataTypeTok{axis.text =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{size =} \DecValTok{12}\NormalTok{),}
    \DataTypeTok{axis.text.x =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{angle =} \DecValTok{45}\NormalTok{, }\DataTypeTok{vjust =} \FloatTok{0.5}\NormalTok{, }\DataTypeTok{hjust =} \FloatTok{0.5}\NormalTok{),}
    \DataTypeTok{axis.title =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{size =} \DecValTok{14}\NormalTok{),}
    \DataTypeTok{panel.grid.major =} \KeywordTok{element_line}\NormalTok{(}\DataTypeTok{color =} \StringTok{"grey"}\NormalTok{),}
    \DataTypeTok{panel.grid.minor =} \KeywordTok{element_blank}\NormalTok{(),}
    \DataTypeTok{panel.background =} \KeywordTok{element_rect}\NormalTok{(}\DataTypeTok{fill =} \StringTok{"aliceblue"}\NormalTok{),}
    \DataTypeTok{strip.background =} \KeywordTok{element_rect}\NormalTok{(}\DataTypeTok{fill =} \StringTok{"lightgrey"}\NormalTok{, }\DataTypeTok{color =} \StringTok{"grey"}\NormalTok{, }
                                    \DataTypeTok{size =} \DecValTok{1}\NormalTok{),}
    \DataTypeTok{strip.text =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{face =} \StringTok{"bold"}\NormalTok{, }\DataTypeTok{size =} \DecValTok{12}\NormalTok{, }\DataTypeTok{color =} \StringTok{"black"}\NormalTok{),}
    \DataTypeTok{legend.position =} \StringTok{"bottom"}\NormalTok{,}
    \DataTypeTok{legend.justification =} \StringTok{"top"}\NormalTok{, }
    \DataTypeTok{legend.box =} \StringTok{"horizontal"}\NormalTok{,}
    \DataTypeTok{legend.box.background =} \KeywordTok{element_rect}\NormalTok{(}\DataTypeTok{colour =} \StringTok{"grey50"}\NormalTok{),}
    \DataTypeTok{legend.background =} \KeywordTok{element_blank}\NormalTok{(),}
    \DataTypeTok{panel.border =} \KeywordTok{element_rect}\NormalTok{(}\DataTypeTok{color =} \StringTok{"grey"}\NormalTok{, }\DataTypeTok{fill =} \OtherTok{NA}\NormalTok{, }\DataTypeTok{size =} \FloatTok{0.5}\NormalTok{)}
\NormalTok{  )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ fluH7N9_china_}\DecValTok{2013}\NormalTok{_gather, }
       \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Date, }\DataTypeTok{y =} \KeywordTok{as.numeric}\NormalTok{(age), }\DataTypeTok{fill =}\NormalTok{ outcome)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{stat_density2d}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{alpha =}\NormalTok{ ..level..), }\DataTypeTok{geom =} \StringTok{"polygon"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_jitter}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{color =}\NormalTok{ outcome, }\DataTypeTok{shape =}\NormalTok{ gender), }\DataTypeTok{size =} \FloatTok{1.5}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_rug}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{color =}\NormalTok{ outcome)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}
    \DataTypeTok{fill =} \StringTok{"Outcome"}\NormalTok{,}
    \DataTypeTok{color =} \StringTok{"Outcome"}\NormalTok{,}
    \DataTypeTok{alpha =} \StringTok{"Level"}\NormalTok{,}
    \DataTypeTok{shape =} \StringTok{"Gender"}\NormalTok{,}
    \DataTypeTok{x =} \StringTok{"Date in 2013"}\NormalTok{,}
    \DataTypeTok{y =} \StringTok{"Age"}\NormalTok{,}
    \DataTypeTok{title =} \StringTok{"2013 Influenza A H7N9 cases in China"}\NormalTok{,}
    \DataTypeTok{subtitle =} \StringTok{"Dataset from 'outbreaks' package (Kucharski et al. 2014)"}\NormalTok{,}
    \DataTypeTok{caption =} \StringTok{""}
\NormalTok{  ) }\OperatorTok{+}
\StringTok{  }\KeywordTok{facet_grid}\NormalTok{(Group }\OperatorTok{~}\StringTok{ }\NormalTok{province) }\OperatorTok{+}
\StringTok{  }\KeywordTok{my_theme}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_shape_manual}\NormalTok{(}\DataTypeTok{values =} \KeywordTok{c}\NormalTok{(}\DecValTok{15}\NormalTok{, }\DecValTok{16}\NormalTok{, }\DecValTok{17}\NormalTok{)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_color_brewer}\NormalTok{(}\DataTypeTok{palette=}\StringTok{"Set1"}\NormalTok{, }\DataTypeTok{na.value =} \StringTok{"grey50"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_fill_brewer}\NormalTok{(}\DataTypeTok{palette=}\StringTok{"Set1"}\NormalTok{)}
\CommentTok{#> Warning: Removed 149 rows containing non-finite values (stat_density2d).}
\CommentTok{#> Warning: Removed 149 rows containing missing values (geom_point).}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{algo-comparison_330-classification_flu_outcome-multi-sglander_files/figure-latex/unnamed-chunk-6-1} \end{center}

This plot shows the dates of onset, hospitalisation and outcome (if known) of each data point. Outcome is marked by color and age shown on the y-axis. Gender is marked by point shape.

The density distribution of date by age for the cases seems to indicate that older people died more frequently in the Jiangsu and Zhejiang province than in Shanghai and in other provinces.

When we look at the distribution of points along the time axis, it suggests that their might be a positive correlation between the likelihood of death and an early onset or early outcome.

I also want to know how many cases there are for each gender and province and compare the genders' age distribution.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# more tidy data. first remove age}
\NormalTok{fluH7N9_china_}\DecValTok{2013}\NormalTok{_gather_}\DecValTok{2}\NormalTok{ <-}\StringTok{ }\NormalTok{fluH7N9_china_}\DecValTok{2013}\NormalTok{_gather[, }\DecValTok{-4}\NormalTok{] }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{gather}\NormalTok{(group_}\DecValTok{2}\NormalTok{, value, gender}\OperatorTok{:}\NormalTok{province)}
\CommentTok{#> Warning: attributes are not identical across measure variables;}
\CommentTok{#> they will be dropped}

\CommentTok{# change descriptions}
\NormalTok{from <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"m"}\NormalTok{, }\StringTok{"f"}\NormalTok{, }\StringTok{"unknown"}\NormalTok{, }\StringTok{"Other"}\NormalTok{)}
\NormalTok{to <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Male"}\NormalTok{, }\StringTok{"Female"}\NormalTok{, }\StringTok{"Unknown gender"}\NormalTok{, }\StringTok{"Other province"}\NormalTok{)}
\NormalTok{fluH7N9_china_}\DecValTok{2013}\NormalTok{_gather_}\DecValTok{2}\OperatorTok{$}\NormalTok{value <-}\StringTok{ }\KeywordTok{mapvalues}\NormalTok{(fluH7N9_china_}\DecValTok{2013}\NormalTok{_gather_}\DecValTok{2}\OperatorTok{$}\NormalTok{value, }
                                               \DataTypeTok{from =}\NormalTok{ from, }\DataTypeTok{to =}\NormalTok{ to)}
\CommentTok{# convert to factor}
\NormalTok{fluH7N9_china_}\DecValTok{2013}\NormalTok{_gather_}\DecValTok{2}\OperatorTok{$}\NormalTok{value <-}\StringTok{ }\KeywordTok{factor}\NormalTok{(fluH7N9_china_}\DecValTok{2013}\NormalTok{_gather_}\DecValTok{2}\OperatorTok{$}\NormalTok{value, }
                                            \DataTypeTok{levels =} \KeywordTok{c}\NormalTok{(}\StringTok{"Female"}\NormalTok{, }\StringTok{"Male"}\NormalTok{, }
                                                       \StringTok{"Unknown gender"}\NormalTok{, }
                                                       \StringTok{"Jiangsu"}\NormalTok{, }\StringTok{"Shanghai"}\NormalTok{, }
                                                       \StringTok{"Zhejiang"}\NormalTok{, }\StringTok{"Other }
\StringTok{                                                       province"}\NormalTok{))}

\NormalTok{p1 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ fluH7N9_china_}\DecValTok{2013}\NormalTok{_gather_}\DecValTok{2}\NormalTok{, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ value, }
                                                     \DataTypeTok{fill =}\NormalTok{ outcome, }
                                                     \DataTypeTok{color =}\NormalTok{ outcome)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{position =} \StringTok{"dodge"}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{0.7}\NormalTok{, }\DataTypeTok{size =} \DecValTok{1}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{my_theme}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_fill_brewer}\NormalTok{(}\DataTypeTok{palette=}\StringTok{"Set1"}\NormalTok{, }\DataTypeTok{na.value =} \StringTok{"grey50"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_color_brewer}\NormalTok{(}\DataTypeTok{palette=}\StringTok{"Set1"}\NormalTok{, }\DataTypeTok{na.value =} \StringTok{"grey50"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}
    \DataTypeTok{color =} \StringTok{""}\NormalTok{,}
    \DataTypeTok{fill =} \StringTok{""}\NormalTok{,}
    \DataTypeTok{x =} \StringTok{""}\NormalTok{,}
    \DataTypeTok{y =} \StringTok{"Count"}\NormalTok{,}
    \DataTypeTok{title =} \StringTok{"2013 Influenza A H7N9 cases in China"}\NormalTok{,}
    \DataTypeTok{subtitle =} \StringTok{"Gender and Province numbers of flu cases"}\NormalTok{,}
    \DataTypeTok{caption =} \StringTok{""}
\NormalTok{  )}

\NormalTok{p2 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ fluH7N9_china_}\DecValTok{2013}\NormalTok{_gather, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{as.numeric}\NormalTok{(age), }
                                                   \DataTypeTok{fill =}\NormalTok{ outcome, }
                                                   \DataTypeTok{color =}\NormalTok{ outcome)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_density}\NormalTok{(}\DataTypeTok{alpha =} \FloatTok{0.3}\NormalTok{, }\DataTypeTok{size =} \DecValTok{1}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_rug}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_color_brewer}\NormalTok{(}\DataTypeTok{palette=}\StringTok{"Set1"}\NormalTok{, }\DataTypeTok{na.value =} \StringTok{"grey50"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_fill_brewer}\NormalTok{(}\DataTypeTok{palette=}\StringTok{"Set1"}\NormalTok{, }\DataTypeTok{na.value =} \StringTok{"grey50"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{my_theme}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}
    \DataTypeTok{color =} \StringTok{""}\NormalTok{,}
    \DataTypeTok{fill =} \StringTok{""}\NormalTok{,}
    \DataTypeTok{x =} \StringTok{"Age"}\NormalTok{,}
    \DataTypeTok{y =} \StringTok{"Density"}\NormalTok{,}
    \DataTypeTok{title =} \StringTok{""}\NormalTok{,}
    \DataTypeTok{subtitle =} \StringTok{"Age distribution of flu cases"}\NormalTok{,}
    \DataTypeTok{caption =} \StringTok{""}
\NormalTok{  )}

\KeywordTok{library}\NormalTok{(gridExtra)}
\KeywordTok{library}\NormalTok{(grid)}

\KeywordTok{grid.arrange}\NormalTok{(p1, p2, }\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{)}
\CommentTok{#> Warning: Removed 6 rows containing non-finite values (stat_density).}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{algo-comparison_330-classification_flu_outcome-multi-sglander_files/figure-latex/unnamed-chunk-7-1} \end{center}

In the dataset, there are more male than female cases and correspondingly, we see more deaths, recoveries and unknown outcomes in men than in women. This is potentially a problem later on for modeling because the inherent likelihoods for outcome are not directly comparable between the sexes.

Most unknown outcomes were recorded in Zhejiang. Similarly to gender, we don't have an equal distribution of data points across provinces either.

When we look at the age distribution it is obvious that people who died tended to be slightly older than those who recovered. The density curve of unknown outcomes is more similar to that of death than of recovery, suggesting that among these people there might have been more deaths than recoveries.

And lastly, I want to plot how many days passed between onset, hospitalisation and outcome for each case.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ fluH7N9_china_}\DecValTok{2013}\NormalTok{_gather, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Date, }\DataTypeTok{y =} \KeywordTok{as.numeric}\NormalTok{(age), }
                                             \DataTypeTok{color =}\NormalTok{ outcome)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{color =}\NormalTok{ outcome, }\DataTypeTok{shape =}\NormalTok{ gender), }\DataTypeTok{size =} \FloatTok{1.5}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{0.6}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_path}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{group =}\NormalTok{ case_ID)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{facet_wrap}\NormalTok{( }\OperatorTok{~}\StringTok{ }\NormalTok{province, }\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{my_theme}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_shape_manual}\NormalTok{(}\DataTypeTok{values =} \KeywordTok{c}\NormalTok{(}\DecValTok{15}\NormalTok{, }\DecValTok{16}\NormalTok{, }\DecValTok{17}\NormalTok{)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_color_brewer}\NormalTok{(}\DataTypeTok{palette=}\StringTok{"Set1"}\NormalTok{, }\DataTypeTok{na.value =} \StringTok{"grey50"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_fill_brewer}\NormalTok{(}\DataTypeTok{palette=}\StringTok{"Set1"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}
    \DataTypeTok{color =} \StringTok{"Outcome"}\NormalTok{,}
    \DataTypeTok{shape =} \StringTok{"Gender"}\NormalTok{,}
    \DataTypeTok{x =} \StringTok{"Date in 2013"}\NormalTok{,}
    \DataTypeTok{y =} \StringTok{"Age"}\NormalTok{,}
    \DataTypeTok{title =} \StringTok{"2013 Influenza A H7N9 cases in China"}\NormalTok{,}
    \DataTypeTok{subtitle =} \StringTok{"Dataset from 'outbreaks' package (Kucharski et al. 2014)"}\NormalTok{,}
    \DataTypeTok{caption =} \StringTok{"}\CharTok{\textbackslash{}n}\StringTok{Time from onset of flu to outcome."}
\NormalTok{  )}
\CommentTok{#> Warning: Removed 149 rows containing missing values (geom_point).}
\CommentTok{#> Warning: Removed 122 rows containing missing values (geom_path).}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{algo-comparison_330-classification_flu_outcome-multi-sglander_files/figure-latex/unnamed-chunk-8-1} \end{center}

This plot shows that there are many missing values in the dates, so it is hard to draw a general conclusion.

\hypertarget{features}{%
\section{Features}\label{features}}

In Machine Learning-speak features are the variables used for model training. Using the right features dramatically influences the accuracy of the model.

Because we don't have many features, I am keeping age as it is, but I am also generating new features:

\begin{itemize}
\item
  from the date information I am calculating the days between onset and outcome and between onset and hospitalisation
\item
  I am converting gender into numeric values with 1 for female and 0 for male
\item
  similarly, I am converting provinces to binary classifiers (yes == 1, no == 0) for Shanghai, Zhejiang, Jiangsu and other provinces
\item
  the same binary classification is given for whether a case was hospitalised, and whether they had an early onset or early outcome (earlier than the median date)
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# convert gender, provinces to discrete and numeric values}
\KeywordTok{library}\NormalTok{(dplyr)}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'dplyr'}
\CommentTok{#> The following object is masked from 'package:gridExtra':}
\CommentTok{#> }
\CommentTok{#>     combine}
\CommentTok{#> The following objects are masked from 'package:plyr':}
\CommentTok{#> }
\CommentTok{#>     arrange, count, desc, failwith, id, mutate, rename, summarise, summarize}
\CommentTok{#> The following objects are masked from 'package:stats':}
\CommentTok{#> }
\CommentTok{#>     filter, lag}
\CommentTok{#> The following objects are masked from 'package:base':}
\CommentTok{#> }
\CommentTok{#>     intersect, setdiff, setequal, union}

\NormalTok{dataset <-}\StringTok{ }\NormalTok{fluH7N9_china_}\DecValTok{2013} \OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{hospital =} \KeywordTok{as.factor}\NormalTok{(}\KeywordTok{ifelse}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(date_of_hospitalisation), }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)),}
         \DataTypeTok{gender_f =} \KeywordTok{as.factor}\NormalTok{(}\KeywordTok{ifelse}\NormalTok{(gender }\OperatorTok{==}\StringTok{ "f"}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)),}
         \DataTypeTok{province_Jiangsu =} \KeywordTok{as.factor}\NormalTok{(}\KeywordTok{ifelse}\NormalTok{(province }\OperatorTok{==}\StringTok{ "Jiangsu"}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)),}
         \DataTypeTok{province_Shanghai =} \KeywordTok{as.factor}\NormalTok{(}\KeywordTok{ifelse}\NormalTok{(province }\OperatorTok{==}\StringTok{ "Shanghai"}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)),}
         \DataTypeTok{province_Zhejiang =} \KeywordTok{as.factor}\NormalTok{(}\KeywordTok{ifelse}\NormalTok{(province }\OperatorTok{==}\StringTok{ "Zhejiang"}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)),}
         \DataTypeTok{province_other =} \KeywordTok{as.factor}\NormalTok{(}\KeywordTok{ifelse}\NormalTok{(province }\OperatorTok{==}\StringTok{ "Zhejiang"} \OperatorTok{|}\StringTok{ }
\StringTok{                                             }\NormalTok{province }\OperatorTok{==}\StringTok{ "Jiangsu"} \OperatorTok{|}\StringTok{ }
\StringTok{                                             }\NormalTok{province }\OperatorTok{==}\StringTok{ "Shanghai"}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)),}
         \DataTypeTok{days_onset_to_outcome =} \KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{as.character}\NormalTok{(}\KeywordTok{gsub}\NormalTok{(}\StringTok{" days"}\NormalTok{, }\StringTok{""}\NormalTok{,}
                \KeywordTok{as.Date}\NormalTok{(}\KeywordTok{as.character}\NormalTok{(date_of_outcome), }\DataTypeTok{format =} \StringTok{"%Y-%m-%d"}\NormalTok{) }\OperatorTok{-}\StringTok{ }
\StringTok{                }\KeywordTok{as.Date}\NormalTok{(}\KeywordTok{as.character}\NormalTok{(date_of_onset), }\DataTypeTok{format =} \StringTok{"%Y-%m-%d"}\NormalTok{)))),}
         
         \DataTypeTok{days_onset_to_hospital =} \KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{as.character}\NormalTok{(}\KeywordTok{gsub}\NormalTok{(}\StringTok{" days"}\NormalTok{, }\StringTok{""}\NormalTok{,}
            \KeywordTok{as.Date}\NormalTok{(}\KeywordTok{as.character}\NormalTok{(date_of_hospitalisation), }\DataTypeTok{format =} \StringTok{"%Y-%m-%d"}\NormalTok{) }\OperatorTok{-}\StringTok{ }
\StringTok{            }\KeywordTok{as.Date}\NormalTok{(}\KeywordTok{as.character}\NormalTok{(date_of_onset), }\DataTypeTok{format =} \StringTok{"%Y-%m-%d"}\NormalTok{)))),}
         \DataTypeTok{age =} \KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{as.character}\NormalTok{(age)),}
         \DataTypeTok{early_onset =} \KeywordTok{as.factor}\NormalTok{(}\KeywordTok{ifelse}\NormalTok{(date_of_onset }\OperatorTok{<}\StringTok{ }
\StringTok{                        }\KeywordTok{summary}\NormalTok{(date_of_onset)[[}\DecValTok{3}\NormalTok{]], }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)),}
         \DataTypeTok{early_outcome =} \KeywordTok{as.factor}\NormalTok{(}\KeywordTok{ifelse}\NormalTok{(date_of_outcome }\OperatorTok{<}\StringTok{ }
\StringTok{                        }\KeywordTok{summary}\NormalTok{(date_of_outcome)[[}\DecValTok{3}\NormalTok{]], }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{))) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{subset}\NormalTok{(}\DataTypeTok{select =} \OperatorTok{-}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\OperatorTok{:}\DecValTok{4}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{8}\NormalTok{))}
  \CommentTok{# print}

\KeywordTok{rownames}\NormalTok{(dataset) <-}\StringTok{ }\NormalTok{dataset}\OperatorTok{$}\NormalTok{case_ID}
\NormalTok{dataset <-}\StringTok{ }\NormalTok{dataset[, }\OperatorTok{-}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{4}\NormalTok{)]}
\KeywordTok{head}\NormalTok{(dataset)}
\CommentTok{#>        outcome age hospital gender_f province_Jiangsu province_Shanghai province_Zhejiang province_other days_onset_to_outcome days_onset_to_hospital early_onset early_outcome}
\CommentTok{#> case_1   Death  87        0        0                0                 1                 0              0                    13                     NA           1             1}
\CommentTok{#> case_2   Death  27        1        0                0                 1                 0              0                    11                      4           1             1}
\CommentTok{#> case_3   Death  35        1        1                0                 0                 0              1                    31                     10           1             1}
\CommentTok{#> case_4    <NA>  45        1        1                1                 0                 0              0                    NA                      8           1          <NA>}
\CommentTok{#> case_5 Recover  48        1        1                1                 0                 0              0                    57                     11           1             0}
\CommentTok{#> case_6   Death  32        1        1                1                 0                 0              0                    36                      7           1             1}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(dataset}\OperatorTok{$}\NormalTok{outcome)}
\CommentTok{#>   Death Recover    NA's }
\CommentTok{#>      32      47      57}
\end{Highlighting}
\end{Shaded}

\hypertarget{imputing-missing-values}{%
\subsection{Imputing missing values}\label{imputing-missing-values}}

\url{https://www.r-bloggers.com/imputing-missing-data-with-r-mice-package/}

When looking at the dataset I created for modeling, it is obvious that we have quite a few missing values.

The missing values from the outcome column are what I want to predict but for the rest I would either have to remove the entire row from the data or impute the missing information. I decided to try the latter with the \texttt{mice} package.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(mice)}
\CommentTok{#> Loading required package: lattice}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'mice'}
\CommentTok{#> The following object is masked from 'package:tidyr':}
\CommentTok{#> }
\CommentTok{#>     complete}
\CommentTok{#> The following objects are masked from 'package:base':}
\CommentTok{#> }
\CommentTok{#>     cbind, rbind}

\NormalTok{dataset_impute  <-}\StringTok{ }\KeywordTok{mice}\NormalTok{(dataset[, }\DecValTok{-1}\NormalTok{],  }\DataTypeTok{print =} \OtherTok{FALSE}\NormalTok{)  }\CommentTok{# remove outcome}
\CommentTok{#> Warning: Number of logged events: 150}
\NormalTok{impute_complete <-}\StringTok{ }\NormalTok{mice}\OperatorTok{::}\KeywordTok{complete}\NormalTok{(dataset_impute, }\DecValTok{1}\NormalTok{) }\CommentTok{# return 1st imputed dataset}
\KeywordTok{rownames}\NormalTok{(impute_complete) <-}\StringTok{ }\KeywordTok{row.names}\NormalTok{(dataset)}
\NormalTok{impute_complete}
\CommentTok{#>          age hospital gender_f province_Jiangsu province_Shanghai province_Zhejiang province_other days_onset_to_outcome days_onset_to_hospital early_onset early_outcome}
\CommentTok{#> case_1    87        0        0                0                 1                 0              0                    13                      6           1             1}
\CommentTok{#> case_2    27        1        0                0                 1                 0              0                    11                      4           1             1}
\CommentTok{#> case_3    35        1        1                0                 0                 0              1                    31                     10           1             1}
\CommentTok{#> case_4    45        1        1                1                 0                 0              0                    14                      8           1             1}
\CommentTok{#> case_5    48        1        1                1                 0                 0              0                    57                     11           1             0}
\CommentTok{#> case_6    32        1        1                1                 0                 0              0                    36                      7           1             1}
\CommentTok{#> case_7    83        1        0                1                 0                 0              0                    20                      9           1             1}
\CommentTok{#> case_8    38        1        0                0                 0                 1              0                    20                     11           1             1}
\CommentTok{#> case_9    67        1        0                0                 0                 1              0                     7                      0           1             1}
\CommentTok{#> case_10   48        1        0                0                 1                 0              0                     6                      4           1             1}
\CommentTok{#> case_11   64        1        0                0                 0                 1              0                     6                      2           1             1}
\CommentTok{#> case_12   52        0        1                0                 1                 0              0                     7                      7           1             1}
\CommentTok{#> case_13   67        1        1                0                 1                 0              0                    12                      3           1             1}
\CommentTok{#> case_14    4        0        0                0                 1                 0              0                    10                      5           1             1}
\CommentTok{#> case_15   61        0        1                1                 0                 0              0                    32                      4           1             0}
\CommentTok{#> case_16   79        0        0                1                 0                 0              0                    38                      4           1             0}
\CommentTok{#> case_17   74        1        0                0                 1                 0              0                    14                      6           1             1}
\CommentTok{#> case_18   66        1        0                0                 1                 0              0                    20                      4           1             1}
\CommentTok{#> case_19   59        1        0                0                 1                 0              0                    67                      5           1             0}
\CommentTok{#> case_20   55        1        0                0                 0                 0              1                    22                      4           1             1}
\CommentTok{#> case_21   67        1        0                0                 1                 0              0                    23                      1           1             1}
\CommentTok{#> case_22   85        1        0                1                 0                 0              0                    31                      4           1             0}
\CommentTok{#> case_23   25        1        1                1                 0                 0              0                    46                      0           1             0}
\CommentTok{#> case_24   64        0        0                0                 1                 0              0                     6                      5           1             1}
\CommentTok{#> case_25   62        1        0                0                 1                 0              0                    35                      0           1             0}
\CommentTok{#> case_26   77        1        0                0                 1                 0              0                    11                      4           1             1}
\CommentTok{#> case_27   51        1        1                0                 0                 1              0                    37                     27           1             1}
\CommentTok{#> case_28   79        0        0                0                 0                 1              0                    32                      0           1             0}
\CommentTok{#> case_29   76        1        1                0                 1                 0              0                    32                      4           1             0}
\CommentTok{#> case_30   81        0        1                0                 1                 0              0                    23                      0           1             0}
\CommentTok{#> case_31   70        0        0                1                 0                 0              0                    32                      4           1             0}
\CommentTok{#> case_32   74        0        0                1                 0                 0              0                    38                      4           1             0}
\CommentTok{#> case_33   65        0        0                0                 0                 1              0                    24                      8           1             0}
\CommentTok{#> case_34   74        1        0                0                 1                 0              0                    11                      5           1             1}
\CommentTok{#> case_35   83        1        1                0                 1                 0              0                    38                      5           1             0}
\CommentTok{#> case_36   68        0        0                0                 1                 0              0                    17                      4           1             1}
\CommentTok{#> case_37   31        0        0                1                 0                 0              0                    45                      7           1             0}
\CommentTok{#> case_38   56        0        0                1                 0                 0              0                    23                      7           1             0}
\CommentTok{#> case_39   66        1        0                0                 0                 1              0                    13                      0           0             1}
\CommentTok{#> case_40   74        1        0                0                 0                 1              0                     6                      5           1             1}
\CommentTok{#> case_41   54        1        1                0                 0                 1              0                    23                      6           1             1}
\CommentTok{#> case_42   53        1        0                0                 1                 0              0                    11                      7           1             1}
\CommentTok{#> case_43   86        1        0                0                 1                 0              0                    18                      3           1             1}
\CommentTok{#> case_44    7        1        1                0                 0                 0              1                     6                      0           0             1}
\CommentTok{#> case_45   56        1        0                0                 1                 0              0                    86                      3           1             0}
\CommentTok{#> case_46   77        0        1                1                 0                 0              0                     9                      0           1             1}
\CommentTok{#> case_47   72        0        0                1                 0                 0              0                    32                      5           1             0}
\CommentTok{#> case_48   65        1        0                0                 0                 1              0                    20                      6           1             1}
\CommentTok{#> case_49   38        1        0                0                 0                 1              0                    28                      5           1             0}
\CommentTok{#> case_50   34        1        0                0                 0                 0              1                    17                      3           1             0}
\CommentTok{#> case_51   65        1        0                0                 0                 0              1                    15                      2           0             1}
\CommentTok{#> case_52   64        0        1                0                 0                 1              0                    17                      7           1             1}
\CommentTok{#> case_53   62        0        1                0                 0                 1              0                    22                      7           1             1}
\CommentTok{#> case_54   75        0        0                0                 0                 1              0                    37                      7           1             0}
\CommentTok{#> case_55   79        0        0                0                 0                 1              0                    18                      7           0             0}
\CommentTok{#> case_56   73        1        0                0                 1                 0              0                    11                      6           1             1}
\CommentTok{#> case_57   54        1        0                0                 1                 0              0                    13                      4           0             1}
\CommentTok{#> case_58   78        1        0                0                 1                 0              0                    31                      4           1             0}
\CommentTok{#> case_59   50        0        0                1                 0                 0              0                    37                      7           1             0}
\CommentTok{#> case_60   26        0        0                1                 0                 0              0                    18                      4           0             1}
\CommentTok{#> case_61   60        0        0                1                 0                 0              0                    10                      4           1             1}
\CommentTok{#> case_62   68        0        1                0                 0                 1              0                    11                      5           1             1}
\CommentTok{#> case_63   60        0        0                0                 0                 0              1                    26                      3           0             0}
\CommentTok{#> case_64   56        0        0                1                 0                 0              0                    16                      4           0             0}
\CommentTok{#> case_65   21        0        1                1                 0                 0              0                    37                      4           1             0}
\CommentTok{#> case_66   72        0        0                1                 0                 0              0                    32                      3           0             0}
\CommentTok{#> case_67   56        0        0                0                 0                 1              0                    31                      3           0             0}
\CommentTok{#> case_68   57        0        0                0                 0                 1              0                     6                     10           0             1}
\CommentTok{#> case_69   62        0        0                0                 0                 1              0                     8                      7           0             1}
\CommentTok{#> case_70   58        0        1                0                 0                 1              0                    10                     11           0             1}
\CommentTok{#> case_71   72        0        1                0                 0                 1              0                    13                      6           0             0}
\CommentTok{#> case_72   47        1        0                0                 1                 0              0                    17                      5           0             0}
\CommentTok{#> case_73   69        0        0                0                 1                 0              0                     7                      4           0             1}
\CommentTok{#> case_74   54        0        0                0                 1                 0              0                    17                      6           1             0}
\CommentTok{#> case_75   83        0        0                0                 1                 0              0                     6                      4           1             1}
\CommentTok{#> case_76   55        0        0                0                 1                 0              0                     6                      7           1             1}
\CommentTok{#> case_77    2        0        0                0                 1                 0              0                    13                      7           1             0}
\CommentTok{#> case_78   89        1        0                0                 1                 0              0                    17                      4           0             0}
\CommentTok{#> case_79   37        0        1                0                 0                 1              0                    16                      0           0             0}
\CommentTok{#> case_80   74        0        0                0                 0                 1              0                    22                      3           0             0}
\CommentTok{#> case_81   86        0        0                0                 0                 1              0                    10                      6           0             1}
\CommentTok{#> case_82   41        0        0                0                 0                 1              0                     8                     10           0             1}
\CommentTok{#> case_83   38        0        0                0                 0                 0              1                    23                      4           0             0}
\CommentTok{#> case_84   26        0        1                1                 0                 0              0                    13                      1           1             1}
\CommentTok{#> case_85   80        1        1                0                 1                 0              0                    13                      7           0             1}
\CommentTok{#> case_86   54        0        1                0                 0                 1              0                    10                      3           0             1}
\CommentTok{#> case_87   69        0        0                0                 0                 1              0                     8                      3           0             1}
\CommentTok{#> case_88    4        0        0                0                 0                 0              1                    11                      0           0             0}
\CommentTok{#> case_89   54        1        0                1                 0                 0              0                    36                      6           0             0}
\CommentTok{#> case_90   43        1        0                0                 0                 1              0                     9                      3           0             1}
\CommentTok{#> case_91   48        1        0                0                 0                 1              0                    16                      6           0             0}
\CommentTok{#> case_92   66        1        1                0                 0                 1              0                    13                      7           0             1}
\CommentTok{#> case_93   56        0        0                0                 0                 1              0                    17                      0           0             0}
\CommentTok{#> case_94   35        0        1                0                 0                 1              0                    13                      6           0             0}
\CommentTok{#> case_95   37        0        0                0                 0                 1              0                     7                      4           1             1}
\CommentTok{#> case_96   43        0        0                1                 0                 0              0                    37                      4           1             0}
\CommentTok{#> case_97   75        1        1                0                 1                 0              0                    22                      5           0             0}
\CommentTok{#> case_98   76        0        0                0                 0                 1              0                    13                      3           0             1}
\CommentTok{#> case_99   68        0        1                0                 0                 1              0                     6                      7           0             1}
\CommentTok{#> case_100  58        0        0                0                 0                 1              0                    20                      7           0             0}
\CommentTok{#> case_101  79        0        1                0                 0                 1              0                    13                      3           0             1}
\CommentTok{#> case_102  81        0        0                0                 0                 1              0                    32                      6           0             0}
\CommentTok{#> case_103  68        1        0                1                 0                 0              0                    37                      6           0             0}
\CommentTok{#> case_104  54        0        1                0                 0                 1              0                    20                      7           0             0}
\CommentTok{#> case_105  32        0        0                0                 0                 1              0                    22                      6           0             0}
\CommentTok{#> case_106  36        1        0                0                 0                 0              1                    30                      6           0             0}
\CommentTok{#> case_107  91        0        0                0                 0                 0              1                    21                     10           0             0}
\CommentTok{#> case_108  84        0        0                0                 0                 1              0                    13                      3           0             0}
\CommentTok{#> case_109  62        0        0                0                 0                 1              0                     6                      0           0             1}
\CommentTok{#> case_110  53        1        0                0                 0                 0              1                     9                      4           0             0}
\CommentTok{#> case_111  56        0        0                0                 0                 0              1                    22                      5           0             0}
\CommentTok{#> case_112  69        0        0                0                 0                 0              1                    10                      1           0             0}
\CommentTok{#> case_113  60        0        1                0                 0                 1              0                    13                      3           0             0}
\CommentTok{#> case_114  50        0        1                0                 0                 1              0                    10                      6           0             1}
\CommentTok{#> case_115  38        0        0                0                 0                 1              0                    16                      5           0             0}
\CommentTok{#> case_116  65        1        0                0                 0                 0              1                    21                      5           0             0}
\CommentTok{#> case_117  76        0        1                0                 0                 0              1                    28                      5           0             0}
\CommentTok{#> case_118  49        0        0                1                 0                 0              0                    18                      5           0             1}
\CommentTok{#> case_119  36        0        0                1                 0                 0              0                    26                      4           0             0}
\CommentTok{#> case_120  60        0        0                1                 0                 0              0                    31                      7           1             0}
\CommentTok{#> case_121  64        1        1                0                 0                 0              1                    30                      5           0             0}
\CommentTok{#> case_122  38        0        0                0                 0                 1              0                     2                      3           0             1}
\CommentTok{#> case_123  54        1        0                0                 0                 0              1                    16                      7           0             0}
\CommentTok{#> case_124  80        0        0                0                 0                 0              1                    17                      7           0             0}
\CommentTok{#> case_125  31        0        1                0                 0                 0              1                    20                      0           0             0}
\CommentTok{#> case_126  80        1        0                0                 0                 0              1                    24                     10           0             0}
\CommentTok{#> case_127   4        0        0                0                 0                 0              1                     9                      4           0             0}
\CommentTok{#> case_128  58        1        0                0                 0                 0              1                    22                      7           0             0}
\CommentTok{#> case_129  69        1        0                0                 0                 0              1                    28                      4           0             0}
\CommentTok{#> case_130  69        1        0                0                 0                 0              1                    30                      1           0             0}
\CommentTok{#> case_131   9        1        0                0                 0                 0              1                    11                      1           0             0}
\CommentTok{#> case_132  79        1        1                0                 0                 0              1                    18                      0           0             1}
\CommentTok{#> case_133   6        1        0                0                 0                 0              1                     2                      0           0             0}
\CommentTok{#> case_134  15        1        0                1                 0                 0              0                     7                      1           0             0}
\CommentTok{#> case_135  61        1        1                0                 0                 0              1                    32                      3           0             0}
\CommentTok{#> case_136  51        1        1                0                 0                 0              1                     8                      1           0             1}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dataset_complete <-}\StringTok{ }\KeywordTok{merge}\NormalTok{(dataset[, }\DecValTok{1}\NormalTok{, }\DataTypeTok{drop =} \OtherTok{FALSE}\NormalTok{], }
                          \CommentTok{# mice::complete(dataset_impute, 1), }
\NormalTok{                          impute_complete,}
                          \DataTypeTok{by =} \StringTok{"row.names"}\NormalTok{, }\DataTypeTok{all =} \OtherTok{TRUE}\NormalTok{)}
\KeywordTok{rownames}\NormalTok{(dataset_complete) <-}\StringTok{ }\NormalTok{dataset_complete}\OperatorTok{$}\NormalTok{Row.names}
\NormalTok{dataset_complete <-}\StringTok{ }\NormalTok{dataset_complete[, }\DecValTok{-1}\NormalTok{]}
\NormalTok{dataset_complete}
\CommentTok{#>          outcome age hospital gender_f province_Jiangsu province_Shanghai province_Zhejiang province_other days_onset_to_outcome days_onset_to_hospital early_onset early_outcome}
\CommentTok{#> case_1     Death  87        0        0                0                 1                 0              0                    13                      6           1             1}
\CommentTok{#> case_10    Death  48        1        0                0                 1                 0              0                     6                      4           1             1}
\CommentTok{#> case_100    <NA>  58        0        0                0                 0                 1              0                    20                      7           0             0}
\CommentTok{#> case_101    <NA>  79        0        1                0                 0                 1              0                    13                      3           0             1}
\CommentTok{#> case_102    <NA>  81        0        0                0                 0                 1              0                    32                      6           0             0}
\CommentTok{#> case_103    <NA>  68        1        0                1                 0                 0              0                    37                      6           0             0}
\CommentTok{#> case_104    <NA>  54        0        1                0                 0                 1              0                    20                      7           0             0}
\CommentTok{#> case_105    <NA>  32        0        0                0                 0                 1              0                    22                      6           0             0}
\CommentTok{#> case_106 Recover  36        1        0                0                 0                 0              1                    30                      6           0             0}
\CommentTok{#> case_107   Death  91        0        0                0                 0                 0              1                    21                     10           0             0}
\CommentTok{#> case_108    <NA>  84        0        0                0                 0                 1              0                    13                      3           0             0}
\CommentTok{#> case_109    <NA>  62        0        0                0                 0                 1              0                     6                      0           0             1}
\CommentTok{#> case_11    Death  64        1        0                0                 0                 1              0                     6                      2           1             1}
\CommentTok{#> case_110    <NA>  53        1        0                0                 0                 0              1                     9                      4           0             0}
\CommentTok{#> case_111   Death  56        0        0                0                 0                 0              1                    22                      5           0             0}
\CommentTok{#> case_112    <NA>  69        0        0                0                 0                 0              1                    10                      1           0             0}
\CommentTok{#> case_113    <NA>  60        0        1                0                 0                 1              0                    13                      3           0             0}
\CommentTok{#> case_114    <NA>  50        0        1                0                 0                 1              0                    10                      6           0             1}
\CommentTok{#> case_115    <NA>  38        0        0                0                 0                 1              0                    16                      5           0             0}
\CommentTok{#> case_116 Recover  65        1        0                0                 0                 0              1                    21                      5           0             0}
\CommentTok{#> case_117 Recover  76        0        1                0                 0                 0              1                    28                      5           0             0}
\CommentTok{#> case_118    <NA>  49        0        0                1                 0                 0              0                    18                      5           0             1}
\CommentTok{#> case_119 Recover  36        0        0                1                 0                 0              0                    26                      4           0             0}
\CommentTok{#> case_12    Death  52        0        1                0                 1                 0              0                     7                      7           1             1}
\CommentTok{#> case_120    <NA>  60        0        0                1                 0                 0              0                    31                      7           1             0}
\CommentTok{#> case_121   Death  64        1        1                0                 0                 0              1                    30                      5           0             0}
\CommentTok{#> case_122    <NA>  38        0        0                0                 0                 1              0                     2                      3           0             1}
\CommentTok{#> case_123   Death  54        1        0                0                 0                 0              1                    16                      7           0             0}
\CommentTok{#> case_124 Recover  80        0        0                0                 0                 0              1                    17                      7           0             0}
\CommentTok{#> case_125 Recover  31        0        1                0                 0                 0              1                    20                      0           0             0}
\CommentTok{#> case_126    <NA>  80        1        0                0                 0                 0              1                    24                     10           0             0}
\CommentTok{#> case_127 Recover   4        0        0                0                 0                 0              1                     9                      4           0             0}
\CommentTok{#> case_128 Recover  58        1        0                0                 0                 0              1                    22                      7           0             0}
\CommentTok{#> case_129 Recover  69        1        0                0                 0                 0              1                    28                      4           0             0}
\CommentTok{#> case_13    Death  67        1        1                0                 1                 0              0                    12                      3           1             1}
\CommentTok{#> case_130    <NA>  69        1        0                0                 0                 0              1                    30                      1           0             0}
\CommentTok{#> case_131 Recover   9        1        0                0                 0                 0              1                    11                      1           0             0}
\CommentTok{#> case_132    <NA>  79        1        1                0                 0                 0              1                    18                      0           0             1}
\CommentTok{#> case_133 Recover   6        1        0                0                 0                 0              1                     2                      0           0             0}
\CommentTok{#> case_134 Recover  15        1        0                1                 0                 0              0                     7                      1           0             0}
\CommentTok{#> case_135   Death  61        1        1                0                 0                 0              1                    32                      3           0             0}
\CommentTok{#> case_136    <NA>  51        1        1                0                 0                 0              1                     8                      1           0             1}
\CommentTok{#> case_14  Recover   4        0        0                0                 1                 0              0                    10                      5           1             1}
\CommentTok{#> case_15     <NA>  61        0        1                1                 0                 0              0                    32                      4           1             0}
\CommentTok{#> case_16     <NA>  79        0        0                1                 0                 0              0                    38                      4           1             0}
\CommentTok{#> case_17    Death  74        1        0                0                 1                 0              0                    14                      6           1             1}
\CommentTok{#> case_18  Recover  66        1        0                0                 1                 0              0                    20                      4           1             1}
\CommentTok{#> case_19    Death  59        1        0                0                 1                 0              0                    67                      5           1             0}
\CommentTok{#> case_2     Death  27        1        0                0                 1                 0              0                    11                      4           1             1}
\CommentTok{#> case_20  Recover  55        1        0                0                 0                 0              1                    22                      4           1             1}
\CommentTok{#> case_21  Recover  67        1        0                0                 1                 0              0                    23                      1           1             1}
\CommentTok{#> case_22     <NA>  85        1        0                1                 0                 0              0                    31                      4           1             0}
\CommentTok{#> case_23  Recover  25        1        1                1                 0                 0              0                    46                      0           1             0}
\CommentTok{#> case_24    Death  64        0        0                0                 1                 0              0                     6                      5           1             1}
\CommentTok{#> case_25  Recover  62        1        0                0                 1                 0              0                    35                      0           1             0}
\CommentTok{#> case_26    Death  77        1        0                0                 1                 0              0                    11                      4           1             1}
\CommentTok{#> case_27  Recover  51        1        1                0                 0                 1              0                    37                     27           1             1}
\CommentTok{#> case_28     <NA>  79        0        0                0                 0                 1              0                    32                      0           1             0}
\CommentTok{#> case_29  Recover  76        1        1                0                 1                 0              0                    32                      4           1             0}
\CommentTok{#> case_3     Death  35        1        1                0                 0                 0              1                    31                     10           1             1}
\CommentTok{#> case_30  Recover  81        0        1                0                 1                 0              0                    23                      0           1             0}
\CommentTok{#> case_31     <NA>  70        0        0                1                 0                 0              0                    32                      4           1             0}
\CommentTok{#> case_32     <NA>  74        0        0                1                 0                 0              0                    38                      4           1             0}
\CommentTok{#> case_33  Recover  65        0        0                0                 0                 1              0                    24                      8           1             0}
\CommentTok{#> case_34    Death  74        1        0                0                 1                 0              0                    11                      5           1             1}
\CommentTok{#> case_35    Death  83        1        1                0                 1                 0              0                    38                      5           1             0}
\CommentTok{#> case_36  Recover  68        0        0                0                 1                 0              0                    17                      4           1             1}
\CommentTok{#> case_37  Recover  31        0        0                1                 0                 0              0                    45                      7           1             0}
\CommentTok{#> case_38     <NA>  56        0        0                1                 0                 0              0                    23                      7           1             0}
\CommentTok{#> case_39     <NA>  66        1        0                0                 0                 1              0                    13                      0           0             1}
\CommentTok{#> case_4      <NA>  45        1        1                1                 0                 0              0                    14                      8           1             1}
\CommentTok{#> case_40     <NA>  74        1        0                0                 0                 1              0                     6                      5           1             1}
\CommentTok{#> case_41     <NA>  54        1        1                0                 0                 1              0                    23                      6           1             1}
\CommentTok{#> case_42     <NA>  53        1        0                0                 1                 0              0                    11                      7           1             1}
\CommentTok{#> case_43    Death  86        1        0                0                 1                 0              0                    18                      3           1             1}
\CommentTok{#> case_44  Recover   7        1        1                0                 0                 0              1                     6                      0           0             1}
\CommentTok{#> case_45    Death  56        1        0                0                 1                 0              0                    86                      3           1             0}
\CommentTok{#> case_46    Death  77        0        1                1                 0                 0              0                     9                      0           1             1}
\CommentTok{#> case_47     <NA>  72        0        0                1                 0                 0              0                    32                      5           1             0}
\CommentTok{#> case_48     <NA>  65        1        0                0                 0                 1              0                    20                      6           1             1}
\CommentTok{#> case_49  Recover  38        1        0                0                 0                 1              0                    28                      5           1             0}
\CommentTok{#> case_5   Recover  48        1        1                1                 0                 0              0                    57                     11           1             0}
\CommentTok{#> case_50  Recover  34        1        0                0                 0                 0              1                    17                      3           1             0}
\CommentTok{#> case_51  Recover  65        1        0                0                 0                 0              1                    15                      2           0             1}
\CommentTok{#> case_52     <NA>  64        0        1                0                 0                 1              0                    17                      7           1             1}
\CommentTok{#> case_53    Death  62        0        1                0                 0                 1              0                    22                      7           1             1}
\CommentTok{#> case_54     <NA>  75        0        0                0                 0                 1              0                    37                      7           1             0}
\CommentTok{#> case_55  Recover  79        0        0                0                 0                 1              0                    18                      7           0             0}
\CommentTok{#> case_56     <NA>  73        1        0                0                 1                 0              0                    11                      6           1             1}
\CommentTok{#> case_57  Recover  54        1        0                0                 1                 0              0                    13                      4           0             1}
\CommentTok{#> case_58  Recover  78        1        0                0                 1                 0              0                    31                      4           1             0}
\CommentTok{#> case_59  Recover  50        0        0                1                 0                 0              0                    37                      7           1             0}
\CommentTok{#> case_6     Death  32        1        1                1                 0                 0              0                    36                      7           1             1}
\CommentTok{#> case_60  Recover  26        0        0                1                 0                 0              0                    18                      4           0             1}
\CommentTok{#> case_61    Death  60        0        0                1                 0                 0              0                    10                      4           1             1}
\CommentTok{#> case_62     <NA>  68        0        1                0                 0                 1              0                    11                      5           1             1}
\CommentTok{#> case_63     <NA>  60        0        0                0                 0                 0              1                    26                      3           0             0}
\CommentTok{#> case_64  Recover  56        0        0                1                 0                 0              0                    16                      4           0             0}
\CommentTok{#> case_65  Recover  21        0        1                1                 0                 0              0                    37                      4           1             0}
\CommentTok{#> case_66     <NA>  72        0        0                1                 0                 0              0                    32                      3           0             0}
\CommentTok{#> case_67     <NA>  56        0        0                0                 0                 1              0                    31                      3           0             0}
\CommentTok{#> case_68     <NA>  57        0        0                0                 0                 1              0                     6                     10           0             1}
\CommentTok{#> case_69     <NA>  62        0        0                0                 0                 1              0                     8                      7           0             1}
\CommentTok{#> case_7     Death  83        1        0                1                 0                 0              0                    20                      9           1             1}
\CommentTok{#> case_70     <NA>  58        0        1                0                 0                 1              0                    10                     11           0             1}
\CommentTok{#> case_71     <NA>  72        0        1                0                 0                 1              0                    13                      6           0             0}
\CommentTok{#> case_72  Recover  47        1        0                0                 1                 0              0                    17                      5           0             0}
\CommentTok{#> case_73  Recover  69        0        0                0                 1                 0              0                     7                      4           0             1}
\CommentTok{#> case_74  Recover  54        0        0                0                 1                 0              0                    17                      6           1             0}
\CommentTok{#> case_75    Death  83        0        0                0                 1                 0              0                     6                      4           1             1}
\CommentTok{#> case_76    Death  55        0        0                0                 1                 0              0                     6                      7           1             1}
\CommentTok{#> case_77  Recover   2        0        0                0                 1                 0              0                    13                      7           1             0}
\CommentTok{#> case_78    Death  89        1        0                0                 1                 0              0                    17                      4           0             0}
\CommentTok{#> case_79  Recover  37        0        1                0                 0                 1              0                    16                      0           0             0}
\CommentTok{#> case_8     Death  38        1        0                0                 0                 1              0                    20                     11           1             1}
\CommentTok{#> case_80     <NA>  74        0        0                0                 0                 1              0                    22                      3           0             0}
\CommentTok{#> case_81    Death  86        0        0                0                 0                 1              0                    10                      6           0             1}
\CommentTok{#> case_82  Recover  41        0        0                0                 0                 1              0                     8                     10           0             1}
\CommentTok{#> case_83  Recover  38        0        0                0                 0                 0              1                    23                      4           0             0}
\CommentTok{#> case_84     <NA>  26        0        1                1                 0                 0              0                    13                      1           1             1}
\CommentTok{#> case_85     <NA>  80        1        1                0                 1                 0              0                    13                      7           0             1}
\CommentTok{#> case_86     <NA>  54        0        1                0                 0                 1              0                    10                      3           0             1}
\CommentTok{#> case_87    Death  69        0        0                0                 0                 1              0                     8                      3           0             1}
\CommentTok{#> case_88     <NA>   4        0        0                0                 0                 0              1                    11                      0           0             0}
\CommentTok{#> case_89  Recover  54        1        0                1                 0                 0              0                    36                      6           0             0}
\CommentTok{#> case_9      <NA>  67        1        0                0                 0                 1              0                     7                      0           1             1}
\CommentTok{#> case_90     <NA>  43        1        0                0                 0                 1              0                     9                      3           0             1}
\CommentTok{#> case_91  Recover  48        1        0                0                 0                 1              0                    16                      6           0             0}
\CommentTok{#> case_92     <NA>  66        1        1                0                 0                 1              0                    13                      7           0             1}
\CommentTok{#> case_93     <NA>  56        0        0                0                 0                 1              0                    17                      0           0             0}
\CommentTok{#> case_94  Recover  35        0        1                0                 0                 1              0                    13                      6           0             0}
\CommentTok{#> case_95     <NA>  37        0        0                0                 0                 1              0                     7                      4           1             1}
\CommentTok{#> case_96     <NA>  43        0        0                1                 0                 0              0                    37                      4           1             0}
\CommentTok{#> case_97  Recover  75        1        1                0                 1                 0              0                    22                      5           0             0}
\CommentTok{#> case_98    Death  76        0        0                0                 0                 1              0                    13                      3           0             1}
\CommentTok{#> case_99     <NA>  68        0        1                0                 0                 1              0                     6                      7           0             1}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{cat}\NormalTok{(}\StringTok{"NAs before imput: "}\NormalTok{, }\KeywordTok{sum}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(dataset)), }\StringTok{"}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{#> NAs before imput:  277}
\KeywordTok{summary}\NormalTok{(dataset}\OperatorTok{$}\NormalTok{outcome)}
\CommentTok{#>   Death Recover    NA's }
\CommentTok{#>      32      47      57}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{cat}\NormalTok{(}\StringTok{"NAs after imput: "}\NormalTok{, }\KeywordTok{sum}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(dataset_complete)), }\StringTok{"}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{#> NAs after imput:  57}
\KeywordTok{summary}\NormalTok{(dataset_complete}\OperatorTok{$}\NormalTok{outcome)}
\CommentTok{#>   Death Recover    NA's }
\CommentTok{#>      32      47      57}
\end{Highlighting}
\end{Shaded}

\hypertarget{test-train-and-validation-data-sets}{%
\section{Test, train and validation data sets}\label{test-train-and-validation-data-sets}}

For building the model, I am separating the imputed data frame into training and test data. Test data are the 57 cases with unknown outcome.

The training data will be further devided for validation of the models: 70\% of the training data will be kept for model building and the remaining 30\% will be used for model testing.

I am using the caret package for modeling.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{train_index <-}\StringTok{ }\KeywordTok{which}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(dataset_complete}\OperatorTok{$}\NormalTok{outcome))}
\NormalTok{train_data <-}\StringTok{ }\NormalTok{dataset_complete[}\OperatorTok{-}\NormalTok{train_index, ]     }\CommentTok{# 79x12}
\NormalTok{test_data  <-}\StringTok{ }\NormalTok{dataset_complete[train_index, }\DecValTok{-1}\NormalTok{]    }\CommentTok{# 57x11}

\KeywordTok{library}\NormalTok{(caret)}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{27}\NormalTok{)}
\NormalTok{val_index <-}\StringTok{ }\KeywordTok{createDataPartition}\NormalTok{(train_data}\OperatorTok{$}\NormalTok{outcome, }\DataTypeTok{p =} \FloatTok{0.7}\NormalTok{, }\DataTypeTok{list=}\OtherTok{FALSE}\NormalTok{)}
\NormalTok{val_train_data <-}\StringTok{ }\NormalTok{train_data[val_index, ]}
\NormalTok{val_test_data  <-}\StringTok{ }\NormalTok{train_data[}\OperatorTok{-}\NormalTok{val_index, ]}
\NormalTok{val_train_X <-}\StringTok{ }\NormalTok{val_train_data[,}\OperatorTok{-}\DecValTok{1}\NormalTok{]}
\NormalTok{val_test_X <-}\StringTok{ }\NormalTok{val_test_data[,}\OperatorTok{-}\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\hypertarget{decision-trees}{%
\subsection{Decision trees}\label{decision-trees}}

To get an idea about how each feature contributes to the prediction of the outcome, I first built a decision tree based on the training data using \texttt{rpart} and \texttt{rattle}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(rpart)}
\KeywordTok{library}\NormalTok{(rattle)}
\CommentTok{#> Rattle: A free graphical interface for data science with R.}
\CommentTok{#> Version 5.2.0 Copyright (c) 2006-2018 Togaware Pty Ltd.}
\CommentTok{#> Type 'rattle()' to shake, rattle, and roll your data.}
\KeywordTok{library}\NormalTok{(rpart.plot)}
\KeywordTok{library}\NormalTok{(RColorBrewer)}

\KeywordTok{set.seed}\NormalTok{(}\DecValTok{27}\NormalTok{)}
\NormalTok{fit <-}\StringTok{ }\KeywordTok{rpart}\NormalTok{(outcome }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ train_data, }\DataTypeTok{method =} \StringTok{"class"}\NormalTok{, }
             \DataTypeTok{control =} \KeywordTok{rpart.control}\NormalTok{(}\DataTypeTok{xval =} \DecValTok{10}\NormalTok{, }\DataTypeTok{minbucket =} \DecValTok{2}\NormalTok{, }\DataTypeTok{cp =} \DecValTok{0}\NormalTok{), }
             \DataTypeTok{parms =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{split =} \StringTok{"information"}\NormalTok{))}

\KeywordTok{fancyRpartPlot}\NormalTok{(fit)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{algo-comparison_330-classification_flu_outcome-multi-sglander_files/figure-latex/model-rpart-run-1} \end{center}

This randomly generated decision tree shows that cases with an early outcome were most likely to die when they were 68 or older, when they also had an early onset and when they were sick for fewer than 13 days. If a person was not among the first cases and was younger than 52, they had a good chance of recovering, but if they were 82 or older, they were more likely to die from the flu.

\hypertarget{feature-importance}{%
\subsection{Feature Importance}\label{feature-importance}}

Not all of the features I created will be equally important to the model. The decision tree already gave me an idea of which features might be most important but I also want to estimate feature importance using a Random Forest approach with repeated cross validation.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# prepare training scheme}
\NormalTok{control <-}\StringTok{ }\KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"repeatedcv"}\NormalTok{, }\DataTypeTok{number =} \DecValTok{10}\NormalTok{, }\DataTypeTok{repeats =} \DecValTok{10}\NormalTok{)}

\CommentTok{# train the model}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{27}\NormalTok{)}
\NormalTok{model <-}\StringTok{ }\KeywordTok{train}\NormalTok{(outcome }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ train_data, }\DataTypeTok{method =} \StringTok{"rf"}\NormalTok{, }
               \DataTypeTok{preProcess =} \OtherTok{NULL}\NormalTok{, }\DataTypeTok{trControl =}\NormalTok{ control)}

\CommentTok{# estimate variable importance}
\NormalTok{importance <-}\StringTok{ }\KeywordTok{varImp}\NormalTok{(model, }\DataTypeTok{scale=}\OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{from <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"age"}\NormalTok{, }\StringTok{"hospital1"}\NormalTok{, }\StringTok{"gender_f1"}\NormalTok{, }\StringTok{"province_Jiangsu1"}\NormalTok{, }
          \StringTok{"province_Shanghai1"}\NormalTok{, }\StringTok{"province_Zhejiang1"}\NormalTok{, }\StringTok{"province_other1"}\NormalTok{, }
          \StringTok{"days_onset_to_outcome"}\NormalTok{, }\StringTok{"days_onset_to_hospital"}\NormalTok{, }\StringTok{"early_onset1"}\NormalTok{, }
          \StringTok{"early_outcome1"}\NormalTok{ )}

\NormalTok{to <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Age"}\NormalTok{, }\StringTok{"Hospital"}\NormalTok{, }\StringTok{"Female"}\NormalTok{, }\StringTok{"Jiangsu"}\NormalTok{, }\StringTok{"Shanghai"}\NormalTok{, }\StringTok{"Zhejiang"}\NormalTok{,}
        \StringTok{"Other province"}\NormalTok{, }\StringTok{"Days onset to outcome"}\NormalTok{, }\StringTok{"Days onset to hospital"}\NormalTok{, }
        \StringTok{"Early onset"}\NormalTok{, }\StringTok{"Early outcome"}\NormalTok{ )}

\NormalTok{importance_df_}\DecValTok{1}\NormalTok{ <-}\StringTok{ }\NormalTok{importance}\OperatorTok{$}\NormalTok{importance}
\NormalTok{importance_df_}\DecValTok{1}\OperatorTok{$}\NormalTok{group <-}\StringTok{ }\KeywordTok{rownames}\NormalTok{(importance_df_}\DecValTok{1}\NormalTok{)}

\NormalTok{importance_df_}\DecValTok{1}\OperatorTok{$}\NormalTok{group <-}\StringTok{ }\KeywordTok{mapvalues}\NormalTok{(importance_df_}\DecValTok{1}\OperatorTok{$}\NormalTok{group, }
                                           \DataTypeTok{from =}\NormalTok{ from, }
                                           \DataTypeTok{to =}\NormalTok{ to)}
\NormalTok{f =}\StringTok{ }\NormalTok{importance_df_}\DecValTok{1}\NormalTok{[}\KeywordTok{order}\NormalTok{(importance_df_}\DecValTok{1}\OperatorTok{$}\NormalTok{Overall, }\DataTypeTok{decreasing =} \OtherTok{FALSE}\NormalTok{), }\StringTok{"group"}\NormalTok{]}

\NormalTok{importance_df_}\DecValTok{2}\NormalTok{ <-}\StringTok{ }\NormalTok{importance_df_}\DecValTok{1}
\NormalTok{importance_df_}\DecValTok{2}\OperatorTok{$}\NormalTok{Overall <-}\StringTok{ }\DecValTok{0}

\NormalTok{importance_df <-}\StringTok{ }\KeywordTok{rbind}\NormalTok{(importance_df_}\DecValTok{1}\NormalTok{, importance_df_}\DecValTok{2}\NormalTok{)}

\CommentTok{# setting factor levels}
\NormalTok{importance_df <-}\StringTok{ }\KeywordTok{within}\NormalTok{(importance_df, group <-}\StringTok{ }\KeywordTok{factor}\NormalTok{(group, }\DataTypeTok{levels =}\NormalTok{ f))}
\NormalTok{importance_df_}\DecValTok{1}\NormalTok{ <-}\StringTok{ }\KeywordTok{within}\NormalTok{(importance_df_}\DecValTok{1}\NormalTok{, group <-}\StringTok{ }\KeywordTok{factor}\NormalTok{(group, }\DataTypeTok{levels =}\NormalTok{ f))}

\KeywordTok{ggplot}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ importance_df_}\DecValTok{1}\NormalTok{, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Overall, }\DataTypeTok{y =}\NormalTok{ group, }
                                         \DataTypeTok{color =}\NormalTok{ group), }\DataTypeTok{size =} \DecValTok{2}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_path}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ importance_df, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Overall, }\DataTypeTok{y =}\NormalTok{ group, }\DataTypeTok{color =}\NormalTok{ group, }
                                      \DataTypeTok{group =}\NormalTok{ group), }\DataTypeTok{size =} \DecValTok{1}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_color_manual}\NormalTok{(}\DataTypeTok{values =} \KeywordTok{rep}\NormalTok{(}\KeywordTok{brewer.pal}\NormalTok{(}\DecValTok{1}\NormalTok{, }\StringTok{"Set1"}\NormalTok{)[}\DecValTok{1}\NormalTok{], }\DecValTok{11}\NormalTok{)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{my_theme}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{legend.position =} \StringTok{"none"}\NormalTok{,}
        \DataTypeTok{axis.text.x =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{angle =} \DecValTok{0}\NormalTok{, }\DataTypeTok{vjust =} \FloatTok{0.5}\NormalTok{, }\DataTypeTok{hjust =} \FloatTok{0.5}\NormalTok{)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}
    \DataTypeTok{x =} \StringTok{"Importance"}\NormalTok{,}
    \DataTypeTok{y =} \StringTok{""}\NormalTok{,}
    \DataTypeTok{title =} \StringTok{"2013 Influenza A H7N9 cases in China"}\NormalTok{,}
    \DataTypeTok{subtitle =} \StringTok{"Scaled feature importance"}\NormalTok{,}
    \DataTypeTok{caption =} \StringTok{"}\CharTok{\textbackslash{}n}\StringTok{Determined with Random Forest and}
\StringTok{    repeated cross validation (10 repeats, 10 times)"}
\NormalTok{  )}
\CommentTok{#> Warning in brewer.pal(1, "Set1"): minimal value for n is 3, returning requested palette with 3 different levels}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{algo-comparison_330-classification_flu_outcome-multi-sglander_files/figure-latex/plot-var-importance-1} \end{center}

This tells me that age is the most important determining factor for predicting disease outcome, followed by days between onset an outcome, early outcome and days between onset and hospitalisation.

\hypertarget{feature-plot}{%
\subsection{Feature Plot}\label{feature-plot}}

Before I start actually building models, I want to check whether the distribution of feature values is comparable between training, validation and test datasets.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# tidy dataframe of 11 variables for plotting features for all datasets}
\CommentTok{# dataset_complete: 136x12. test + val_train + val_test}
\NormalTok{dataset_complete_gather <-}\StringTok{ }\NormalTok{dataset_complete }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{set =} \KeywordTok{ifelse}\NormalTok{(}\KeywordTok{rownames}\NormalTok{(dataset_complete) }\OperatorTok{%in%}\StringTok{ }\KeywordTok{rownames}\NormalTok{(test_data), }
                      \StringTok{"Test Data"}\NormalTok{, }
               \KeywordTok{ifelse}\NormalTok{(}\KeywordTok{rownames}\NormalTok{(dataset_complete) }\OperatorTok{%in%}\StringTok{ }\KeywordTok{rownames}\NormalTok{(val_train_data), }
                      \StringTok{"Validation Train Data"}\NormalTok{,}
               \KeywordTok{ifelse}\NormalTok{(}\KeywordTok{rownames}\NormalTok{(dataset_complete) }\OperatorTok{%in%}\StringTok{ }\KeywordTok{rownames}\NormalTok{(val_test_data), }
                      \StringTok{"Validation Test Data"}\NormalTok{, }\StringTok{"NA"}\NormalTok{))),}
         \DataTypeTok{case_ID =} \KeywordTok{rownames}\NormalTok{(.)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{gather}\NormalTok{(group, value, age}\OperatorTok{:}\NormalTok{early_outcome)}
\CommentTok{#> Warning: attributes are not identical across measure variables;}
\CommentTok{#> they will be dropped}

\CommentTok{# map values in group to more readable}
\NormalTok{from <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"age"}\NormalTok{, }\StringTok{"hospital"}\NormalTok{, }\StringTok{"gender_f"}\NormalTok{, }\StringTok{"province_Jiangsu"}\NormalTok{, }\StringTok{"province_Shanghai"}\NormalTok{, }
          \StringTok{"province_Zhejiang"}\NormalTok{, }\StringTok{"province_other"}\NormalTok{, }\StringTok{"days_onset_to_outcome"}\NormalTok{, }
          \StringTok{"days_onset_to_hospital"}\NormalTok{, }\StringTok{"early_onset"}\NormalTok{,  }\StringTok{"early_outcome"}\NormalTok{ )}
\NormalTok{to <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Age"}\NormalTok{, }\StringTok{"Hospital"}\NormalTok{, }\StringTok{"Female"}\NormalTok{, }\StringTok{"Jiangsu"}\NormalTok{, }\StringTok{"Shanghai"}\NormalTok{, }\StringTok{"Zhejiang"}\NormalTok{, }
        \StringTok{"Other province"}\NormalTok{, }\StringTok{"Days onset to outcome"}\NormalTok{, }\StringTok{"Days onset to hospital"}\NormalTok{, }
        \StringTok{"Early onset"}\NormalTok{, }\StringTok{"Early outcome"}\NormalTok{ )}
\NormalTok{dataset_complete_gather}\OperatorTok{$}\NormalTok{group <-}\StringTok{ }\KeywordTok{mapvalues}\NormalTok{(dataset_complete_gather}\OperatorTok{$}\NormalTok{group, }
                      \DataTypeTok{from =}\NormalTok{ from, }
                      \DataTypeTok{to =}\NormalTok{ to)}
\end{Highlighting}
\end{Shaded}

\hypertarget{plot-distribution-of-features-in-each-dataset}{%
\subsection{Plot distribution of features in each dataset}\label{plot-distribution-of-features-in-each-dataset}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot features all datasets}
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ dataset_complete_gather, }
       \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{as.numeric}\NormalTok{(value), }\DataTypeTok{fill =}\NormalTok{ outcome, }\DataTypeTok{color =}\NormalTok{ outcome)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_density}\NormalTok{(}\DataTypeTok{alpha =} \FloatTok{0.2}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_rug}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_color_brewer}\NormalTok{(}\DataTypeTok{palette=}\StringTok{"Set1"}\NormalTok{, }\DataTypeTok{na.value =} \StringTok{"grey50"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_fill_brewer}\NormalTok{(}\DataTypeTok{palette=}\StringTok{"Set1"}\NormalTok{, }\DataTypeTok{na.value =} \StringTok{"grey50"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{my_theme}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{facet_wrap}\NormalTok{(set }\OperatorTok{~}\StringTok{ }\NormalTok{group, }\DataTypeTok{ncol =} \DecValTok{11}\NormalTok{, }\DataTypeTok{scales =} \StringTok{"free"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}
    \DataTypeTok{x =} \StringTok{"Value"}\NormalTok{,}
    \DataTypeTok{y =} \StringTok{"Density"}\NormalTok{,}
    \DataTypeTok{title =} \StringTok{"2013 Influenza A H7N9 cases in China"}\NormalTok{,}
    \DataTypeTok{subtitle =} \StringTok{"Features for classifying outcome"}\NormalTok{,}
    \DataTypeTok{caption =} \StringTok{"}\CharTok{\textbackslash{}n}\StringTok{Density distribution of all features used for }
\StringTok{    classification of flu outcome."}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{algo-comparison_330-classification_flu_outcome-multi-sglander_files/figure-latex/plot-datasets-distribution-1} \end{center}

\hypertarget{plot-3-features-vs-outcome-all-datasets}{%
\subsection{Plot 3 features vs outcome, all datasets}\label{plot-3-features-vs-outcome-all-datasets}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot three groups vs outcome}
\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{subset}\NormalTok{(dataset_complete_gather, }
\NormalTok{              group }\OperatorTok{==}\StringTok{ "Age"} \OperatorTok{|}\StringTok{ }
\StringTok{              }\NormalTok{group }\OperatorTok{==}\StringTok{ "Days onset to hospital"} \OperatorTok{|}\StringTok{ }
\StringTok{              }\NormalTok{group }\OperatorTok{==}\StringTok{ "Days onset to outcome"}\NormalTok{), }
       \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{outcome, }\DataTypeTok{y=}\KeywordTok{as.numeric}\NormalTok{(value), }\DataTypeTok{fill=}\NormalTok{set)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_boxplot}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{my_theme}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_fill_brewer}\NormalTok{(}\DataTypeTok{palette=}\StringTok{"Set1"}\NormalTok{, }\DataTypeTok{type =} \StringTok{"div "}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{facet_wrap}\NormalTok{( }\OperatorTok{~}\StringTok{ }\NormalTok{group, }\DataTypeTok{ncol =} \DecValTok{3}\NormalTok{, }\DataTypeTok{scales =} \StringTok{"free"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}
    \DataTypeTok{fill =} \StringTok{""}\NormalTok{,}
    \DataTypeTok{x =} \StringTok{"Outcome"}\NormalTok{,}
    \DataTypeTok{y =} \StringTok{"Value"}\NormalTok{,}
    \DataTypeTok{title =} \StringTok{"2013 Influenza A H7N9 cases in China"}\NormalTok{,}
    \DataTypeTok{subtitle =} \StringTok{"Features for classifying outcome"}\NormalTok{,}
    \DataTypeTok{caption =} \StringTok{"}\CharTok{\textbackslash{}n}\StringTok{Boxplot of the features age, days from onset to }
\StringTok{    hospitalisation and days from onset to outcome."}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{algo-comparison_330-classification_flu_outcome-multi-sglander_files/figure-latex/unnamed-chunk-16-1} \end{center}

Luckily, the distributions looks reasonably similar between the validation and test data, except for a few outliers.

\hypertarget{comparing-machine-learning-algorithms}{%
\section{Comparing Machine Learning algorithms}\label{comparing-machine-learning-algorithms}}

Before I try to predict the outcome of the unknown cases, I am testing the models' accuracy with the validation datasets on a couple of algorithms. I have chosen only a few more well known algorithms, but \texttt{caret} implements many more.

I have chosen to not do any preprocessing because I was worried that the different data distributions with continuous variables (e.g.~age) and binary variables (i.e.~0, 1 classification of e.g.~hospitalisation) would lead to problems.

\begin{itemize}
\tightlist
\item
  Random Forest
\item
  GLM net
\item
  k-Nearest Neighbors
\item
  Penalized Discriminant Analysis
\item
  Stabilized Linear Discriminant Analysis
\item
  Nearest Shrunken Centroids
\item
  Single C5.0 Tree
\item
  Partial Least Squares
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{train_control <-}\StringTok{ }\KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"repeatedcv"}\NormalTok{, }
                              \DataTypeTok{number =} \DecValTok{10}\NormalTok{, }
                              \DataTypeTok{repeats =} \DecValTok{10}\NormalTok{, }
                              \DataTypeTok{verboseIter =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{random-forest-1}{%
\subsection{Random Forest}\label{random-forest-1}}

Random Forests predictions are based on the generation of multiple classification trees.

This model classified 14 out of 23 cases correctly.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{27}\NormalTok{)}
\NormalTok{model_rf <-}\StringTok{ }\NormalTok{caret}\OperatorTok{::}\KeywordTok{train}\NormalTok{(outcome }\OperatorTok{~}\StringTok{ }\NormalTok{.,}
                             \DataTypeTok{data =}\NormalTok{ val_train_data,}
                             \DataTypeTok{method =} \StringTok{"rf"}\NormalTok{,}
                             \DataTypeTok{preProcess =} \OtherTok{NULL}\NormalTok{,}
                             \DataTypeTok{trControl =}\NormalTok{ train_control)}
\NormalTok{model_rf}
\CommentTok{#> Random Forest }
\CommentTok{#> }
\CommentTok{#> 56 samples}
\CommentTok{#> 11 predictors}
\CommentTok{#>  2 classes: 'Death', 'Recover' }
\CommentTok{#> }
\CommentTok{#> No pre-processing}
\CommentTok{#> Resampling: Cross-Validated (10 fold, repeated 10 times) }
\CommentTok{#> Summary of sample sizes: 51, 49, 50, 51, 49, 51, ... }
\CommentTok{#> Resampling results across tuning parameters:}
\CommentTok{#> }
\CommentTok{#>   mtry  Accuracy  Kappa}
\CommentTok{#>    2    0.687     0.340}
\CommentTok{#>    6    0.732     0.432}
\CommentTok{#>   11    0.726     0.423}
\CommentTok{#> }
\CommentTok{#> Accuracy was used to select the optimal model using the largest value.}
\CommentTok{#> The final value used for the model was mtry = 6.}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{confusionMatrix}\NormalTok{(}\KeywordTok{predict}\NormalTok{(model_rf, val_test_data[, }\DecValTok{-1}\NormalTok{]), val_test_data}\OperatorTok{$}\NormalTok{outcome)}
\CommentTok{#> Confusion Matrix and Statistics}
\CommentTok{#> }
\CommentTok{#>           Reference}
\CommentTok{#> Prediction Death Recover}
\CommentTok{#>    Death       3       0}
\CommentTok{#>    Recover     6      14}
\CommentTok{#>                                         }
\CommentTok{#>                Accuracy : 0.739         }
\CommentTok{#>                  95% CI : (0.516, 0.898)}
\CommentTok{#>     No Information Rate : 0.609         }
\CommentTok{#>     P-Value [Acc > NIR] : 0.1421        }
\CommentTok{#>                                         }
\CommentTok{#>                   Kappa : 0.378         }
\CommentTok{#>                                         }
\CommentTok{#>  Mcnemar's Test P-Value : 0.0412        }
\CommentTok{#>                                         }
\CommentTok{#>             Sensitivity : 0.333         }
\CommentTok{#>             Specificity : 1.000         }
\CommentTok{#>          Pos Pred Value : 1.000         }
\CommentTok{#>          Neg Pred Value : 0.700         }
\CommentTok{#>              Prevalence : 0.391         }
\CommentTok{#>          Detection Rate : 0.130         }
\CommentTok{#>    Detection Prevalence : 0.130         }
\CommentTok{#>       Balanced Accuracy : 0.667         }
\CommentTok{#>                                         }
\CommentTok{#>        'Positive' Class : Death         }
\CommentTok{#> }
\end{Highlighting}
\end{Shaded}

\hypertarget{glm-net}{%
\subsection{GLM net}\label{glm-net}}

Lasso or elastic net regularization for generalized linear model regression are based on linear regression models and is useful when we have feature correlation in our model.

This model classified 13 out of 23 cases correctly.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{27}\NormalTok{)}
\NormalTok{model_glmnet <-}\StringTok{ }\NormalTok{caret}\OperatorTok{::}\KeywordTok{train}\NormalTok{(outcome }\OperatorTok{~}\StringTok{ }\NormalTok{.,}
                             \DataTypeTok{data =}\NormalTok{ val_train_data,}
                             \DataTypeTok{method =} \StringTok{"glmnet"}\NormalTok{,}
                             \DataTypeTok{preProcess =} \OtherTok{NULL}\NormalTok{,}
                             \DataTypeTok{trControl =}\NormalTok{ train_control)}
\NormalTok{model_glmnet}
\CommentTok{#> glmnet }
\CommentTok{#> }
\CommentTok{#> 56 samples}
\CommentTok{#> 11 predictors}
\CommentTok{#>  2 classes: 'Death', 'Recover' }
\CommentTok{#> }
\CommentTok{#> No pre-processing}
\CommentTok{#> Resampling: Cross-Validated (10 fold, repeated 10 times) }
\CommentTok{#> Summary of sample sizes: 51, 49, 50, 51, 49, 51, ... }
\CommentTok{#> Resampling results across tuning parameters:}
\CommentTok{#> }
\CommentTok{#>   alpha  lambda    Accuracy  Kappa}
\CommentTok{#>   0.10   0.000491  0.671     0.324}
\CommentTok{#>   0.10   0.004909  0.669     0.318}
\CommentTok{#>   0.10   0.049093  0.680     0.339}
\CommentTok{#>   0.55   0.000491  0.671     0.324}
\CommentTok{#>   0.55   0.004909  0.671     0.322}
\CommentTok{#>   0.55   0.049093  0.695     0.365}
\CommentTok{#>   1.00   0.000491  0.671     0.324}
\CommentTok{#>   1.00   0.004909  0.672     0.326}
\CommentTok{#>   1.00   0.049093  0.714     0.414}
\CommentTok{#> }
\CommentTok{#> Accuracy was used to select the optimal model using the largest value.}
\CommentTok{#> The final values used for the model were alpha = 1 and lambda = 0.0491.}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{confusionMatrix}\NormalTok{(}\KeywordTok{predict}\NormalTok{(model_glmnet, val_test_data[, }\DecValTok{-1}\NormalTok{]), val_test_data}\OperatorTok{$}\NormalTok{outcome)}
\CommentTok{#> Confusion Matrix and Statistics}
\CommentTok{#> }
\CommentTok{#>           Reference}
\CommentTok{#> Prediction Death Recover}
\CommentTok{#>    Death       3       2}
\CommentTok{#>    Recover     6      12}
\CommentTok{#>                                         }
\CommentTok{#>                Accuracy : 0.652         }
\CommentTok{#>                  95% CI : (0.427, 0.836)}
\CommentTok{#>     No Information Rate : 0.609         }
\CommentTok{#>     P-Value [Acc > NIR] : 0.422         }
\CommentTok{#>                                         }
\CommentTok{#>                   Kappa : 0.207         }
\CommentTok{#>                                         }
\CommentTok{#>  Mcnemar's Test P-Value : 0.289         }
\CommentTok{#>                                         }
\CommentTok{#>             Sensitivity : 0.333         }
\CommentTok{#>             Specificity : 0.857         }
\CommentTok{#>          Pos Pred Value : 0.600         }
\CommentTok{#>          Neg Pred Value : 0.667         }
\CommentTok{#>              Prevalence : 0.391         }
\CommentTok{#>          Detection Rate : 0.130         }
\CommentTok{#>    Detection Prevalence : 0.217         }
\CommentTok{#>       Balanced Accuracy : 0.595         }
\CommentTok{#>                                         }
\CommentTok{#>        'Positive' Class : Death         }
\CommentTok{#> }
\end{Highlighting}
\end{Shaded}

\hypertarget{k-nearest-neighbors}{%
\subsection{k-Nearest Neighbors}\label{k-nearest-neighbors}}

k-nearest neighbors predicts based on point distances with predefined constants.

This model classified 14 out of 23 cases correctly.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{27}\NormalTok{)}
\NormalTok{model_kknn <-}\StringTok{ }\NormalTok{caret}\OperatorTok{::}\KeywordTok{train}\NormalTok{(outcome }\OperatorTok{~}\StringTok{ }\NormalTok{.,}
                             \DataTypeTok{data =}\NormalTok{ val_train_data,}
                             \DataTypeTok{method =} \StringTok{"kknn"}\NormalTok{,}
                             \DataTypeTok{preProcess =} \OtherTok{NULL}\NormalTok{,}
                             \DataTypeTok{trControl =}\NormalTok{ train_control)}
\NormalTok{model_kknn}
\CommentTok{#> k-Nearest Neighbors }
\CommentTok{#> }
\CommentTok{#> 56 samples}
\CommentTok{#> 11 predictors}
\CommentTok{#>  2 classes: 'Death', 'Recover' }
\CommentTok{#> }
\CommentTok{#> No pre-processing}
\CommentTok{#> Resampling: Cross-Validated (10 fold, repeated 10 times) }
\CommentTok{#> Summary of sample sizes: 51, 49, 50, 51, 49, 51, ... }
\CommentTok{#> Resampling results across tuning parameters:}
\CommentTok{#> }
\CommentTok{#>   kmax  Accuracy  Kappa}
\CommentTok{#>   5     0.666     0.313}
\CommentTok{#>   7     0.653     0.274}
\CommentTok{#>   9     0.648     0.263}
\CommentTok{#> }
\CommentTok{#> Tuning parameter 'distance' was held constant at a value of 2}
\CommentTok{#> Tuning parameter 'kernel' was held constant at a value of optimal}
\CommentTok{#> Accuracy was used to select the optimal model using the largest value.}
\CommentTok{#> The final values used for the model were kmax = 5, distance = 2 and kernel = optimal.}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{confusionMatrix}\NormalTok{(}\KeywordTok{predict}\NormalTok{(model_kknn, val_test_data[, }\DecValTok{-1}\NormalTok{]), val_test_data}\OperatorTok{$}\NormalTok{outcome)}
\CommentTok{#> Confusion Matrix and Statistics}
\CommentTok{#> }
\CommentTok{#>           Reference}
\CommentTok{#> Prediction Death Recover}
\CommentTok{#>    Death       5       3}
\CommentTok{#>    Recover     4      11}
\CommentTok{#>                                         }
\CommentTok{#>                Accuracy : 0.696         }
\CommentTok{#>                  95% CI : (0.471, 0.868)}
\CommentTok{#>     No Information Rate : 0.609         }
\CommentTok{#>     P-Value [Acc > NIR] : 0.264         }
\CommentTok{#>                                         }
\CommentTok{#>                   Kappa : 0.348         }
\CommentTok{#>                                         }
\CommentTok{#>  Mcnemar's Test P-Value : 1.000         }
\CommentTok{#>                                         }
\CommentTok{#>             Sensitivity : 0.556         }
\CommentTok{#>             Specificity : 0.786         }
\CommentTok{#>          Pos Pred Value : 0.625         }
\CommentTok{#>          Neg Pred Value : 0.733         }
\CommentTok{#>              Prevalence : 0.391         }
\CommentTok{#>          Detection Rate : 0.217         }
\CommentTok{#>    Detection Prevalence : 0.348         }
\CommentTok{#>       Balanced Accuracy : 0.671         }
\CommentTok{#>                                         }
\CommentTok{#>        'Positive' Class : Death         }
\CommentTok{#> }
\end{Highlighting}
\end{Shaded}

\hypertarget{penalized-discriminant-analysis}{%
\subsection{Penalized Discriminant Analysis}\label{penalized-discriminant-analysis}}

Penalized Discriminant Analysis is the penalized linear discriminant analysis and is also useful for when we have highly correlated features.

This model classified 14 out of 23 cases correctly.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{27}\NormalTok{)}
\NormalTok{model_pda <-}\StringTok{ }\NormalTok{caret}\OperatorTok{::}\KeywordTok{train}\NormalTok{(outcome }\OperatorTok{~}\StringTok{ }\NormalTok{.,}
                             \DataTypeTok{data =}\NormalTok{ val_train_data,}
                             \DataTypeTok{method =} \StringTok{"pda"}\NormalTok{,}
                             \DataTypeTok{preProcess =} \OtherTok{NULL}\NormalTok{,}
                             \DataTypeTok{trControl =}\NormalTok{ train_control)}
\CommentTok{#> Warning: predictions failed for Fold01.Rep01: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold02.Rep01: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold03.Rep01: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold04.Rep01: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold05.Rep01: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold06.Rep01: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold07.Rep01: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold08.Rep01: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold09.Rep01: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold10.Rep01: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold01.Rep02: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold02.Rep02: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold03.Rep02: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold04.Rep02: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold05.Rep02: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold06.Rep02: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold07.Rep02: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold08.Rep02: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold09.Rep02: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold10.Rep02: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold01.Rep03: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold02.Rep03: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold03.Rep03: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold04.Rep03: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold05.Rep03: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold06.Rep03: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold07.Rep03: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold08.Rep03: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold09.Rep03: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold10.Rep03: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold01.Rep04: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold02.Rep04: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold03.Rep04: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold04.Rep04: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold05.Rep04: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold06.Rep04: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold07.Rep04: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold08.Rep04: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold09.Rep04: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold10.Rep04: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold01.Rep05: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold02.Rep05: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold03.Rep05: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold04.Rep05: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold05.Rep05: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold06.Rep05: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold07.Rep05: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold08.Rep05: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold09.Rep05: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold10.Rep05: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold01.Rep06: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold02.Rep06: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold03.Rep06: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold04.Rep06: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold05.Rep06: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold06.Rep06: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold07.Rep06: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold08.Rep06: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold09.Rep06: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold10.Rep06: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold01.Rep07: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold02.Rep07: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold03.Rep07: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold04.Rep07: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold05.Rep07: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold06.Rep07: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold07.Rep07: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold08.Rep07: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold09.Rep07: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold10.Rep07: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold01.Rep08: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold02.Rep08: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold03.Rep08: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold04.Rep08: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold05.Rep08: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold06.Rep08: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold07.Rep08: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold08.Rep08: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold09.Rep08: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold10.Rep08: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold01.Rep09: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold02.Rep09: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold03.Rep09: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold04.Rep09: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold05.Rep09: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold06.Rep09: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold07.Rep09: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold08.Rep09: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold09.Rep09: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold10.Rep09: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold01.Rep10: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold02.Rep10: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold03.Rep10: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold04.Rep10: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold05.Rep10: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold06.Rep10: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold07.Rep10: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold08.Rep10: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold09.Rep10: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold10.Rep10: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, : There were missing values in resampled performance measures.}
\CommentTok{#> Warning in train.default(x, y, weights = w, ...): missing values found in aggregated results}
\NormalTok{model_pda}
\CommentTok{#> Penalized Discriminant Analysis }
\CommentTok{#> }
\CommentTok{#> 56 samples}
\CommentTok{#> 11 predictors}
\CommentTok{#>  2 classes: 'Death', 'Recover' }
\CommentTok{#> }
\CommentTok{#> No pre-processing}
\CommentTok{#> Resampling: Cross-Validated (10 fold, repeated 10 times) }
\CommentTok{#> Summary of sample sizes: 51, 49, 50, 51, 49, 51, ... }
\CommentTok{#> Resampling results across tuning parameters:}
\CommentTok{#> }
\CommentTok{#>   lambda  Accuracy  Kappa}
\CommentTok{#>   0e+00     NaN       NaN}
\CommentTok{#>   1e-04   0.681     0.343}
\CommentTok{#>   1e-01   0.681     0.343}
\CommentTok{#> }
\CommentTok{#> Accuracy was used to select the optimal model using the largest value.}
\CommentTok{#> The final value used for the model was lambda = 1e-04.}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{confusionMatrix}\NormalTok{(}\KeywordTok{predict}\NormalTok{(model_pda, val_test_data[, }\DecValTok{-1}\NormalTok{]), val_test_data}\OperatorTok{$}\NormalTok{outcome)}
\CommentTok{#> Confusion Matrix and Statistics}
\CommentTok{#> }
\CommentTok{#>           Reference}
\CommentTok{#> Prediction Death Recover}
\CommentTok{#>    Death       3       2}
\CommentTok{#>    Recover     6      12}
\CommentTok{#>                                         }
\CommentTok{#>                Accuracy : 0.652         }
\CommentTok{#>                  95% CI : (0.427, 0.836)}
\CommentTok{#>     No Information Rate : 0.609         }
\CommentTok{#>     P-Value [Acc > NIR] : 0.422         }
\CommentTok{#>                                         }
\CommentTok{#>                   Kappa : 0.207         }
\CommentTok{#>                                         }
\CommentTok{#>  Mcnemar's Test P-Value : 0.289         }
\CommentTok{#>                                         }
\CommentTok{#>             Sensitivity : 0.333         }
\CommentTok{#>             Specificity : 0.857         }
\CommentTok{#>          Pos Pred Value : 0.600         }
\CommentTok{#>          Neg Pred Value : 0.667         }
\CommentTok{#>              Prevalence : 0.391         }
\CommentTok{#>          Detection Rate : 0.130         }
\CommentTok{#>    Detection Prevalence : 0.217         }
\CommentTok{#>       Balanced Accuracy : 0.595         }
\CommentTok{#>                                         }
\CommentTok{#>        'Positive' Class : Death         }
\CommentTok{#> }
\end{Highlighting}
\end{Shaded}

\hypertarget{stabilized-linear-discriminant-analysis}{%
\subsection{Stabilized Linear Discriminant Analysis}\label{stabilized-linear-discriminant-analysis}}

Stabilized Linear Discriminant Analysis is designed for high-dimensional data and correlated co-variables.

This model classified 15 out of 23 cases correctly.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{27}\NormalTok{)}
\NormalTok{model_slda <-}\StringTok{ }\NormalTok{caret}\OperatorTok{::}\KeywordTok{train}\NormalTok{(outcome }\OperatorTok{~}\StringTok{ }\NormalTok{.,}
                             \DataTypeTok{data =}\NormalTok{ val_train_data,}
                             \DataTypeTok{method =} \StringTok{"slda"}\NormalTok{,}
                             \DataTypeTok{preProcess =} \OtherTok{NULL}\NormalTok{,}
                             \DataTypeTok{trControl =}\NormalTok{ train_control)}
\NormalTok{model_slda}
\CommentTok{#> Stabilized Linear Discriminant Analysis }
\CommentTok{#> }
\CommentTok{#> 56 samples}
\CommentTok{#> 11 predictors}
\CommentTok{#>  2 classes: 'Death', 'Recover' }
\CommentTok{#> }
\CommentTok{#> No pre-processing}
\CommentTok{#> Resampling: Cross-Validated (10 fold, repeated 10 times) }
\CommentTok{#> Summary of sample sizes: 51, 49, 50, 51, 49, 51, ... }
\CommentTok{#> Resampling results:}
\CommentTok{#> }
\CommentTok{#>   Accuracy  Kappa}
\CommentTok{#>   0.682     0.358}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{confusionMatrix}\NormalTok{(}\KeywordTok{predict}\NormalTok{(model_slda, val_test_data[, }\DecValTok{-1}\NormalTok{]), val_test_data}\OperatorTok{$}\NormalTok{outcome)}
\CommentTok{#> Confusion Matrix and Statistics}
\CommentTok{#> }
\CommentTok{#>           Reference}
\CommentTok{#> Prediction Death Recover}
\CommentTok{#>    Death       3       3}
\CommentTok{#>    Recover     6      11}
\CommentTok{#>                                         }
\CommentTok{#>                Accuracy : 0.609         }
\CommentTok{#>                  95% CI : (0.385, 0.803)}
\CommentTok{#>     No Information Rate : 0.609         }
\CommentTok{#>     P-Value [Acc > NIR] : 0.590         }
\CommentTok{#>                                         }
\CommentTok{#>                   Kappa : 0.127         }
\CommentTok{#>                                         }
\CommentTok{#>  Mcnemar's Test P-Value : 0.505         }
\CommentTok{#>                                         }
\CommentTok{#>             Sensitivity : 0.333         }
\CommentTok{#>             Specificity : 0.786         }
\CommentTok{#>          Pos Pred Value : 0.500         }
\CommentTok{#>          Neg Pred Value : 0.647         }
\CommentTok{#>              Prevalence : 0.391         }
\CommentTok{#>          Detection Rate : 0.130         }
\CommentTok{#>    Detection Prevalence : 0.261         }
\CommentTok{#>       Balanced Accuracy : 0.560         }
\CommentTok{#>                                         }
\CommentTok{#>        'Positive' Class : Death         }
\CommentTok{#> }
\end{Highlighting}
\end{Shaded}

\hypertarget{nearest-shrunken-centroids}{%
\subsection{Nearest Shrunken Centroids}\label{nearest-shrunken-centroids}}

Nearest Shrunken Centroids computes a standardized centroid for each class and shrinks each centroid toward the overall centroid for all classes.

This model classified 15 out of 23 cases correctly.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{27}\NormalTok{)}
\NormalTok{model_pam <-}\StringTok{ }\NormalTok{caret}\OperatorTok{::}\KeywordTok{train}\NormalTok{(outcome }\OperatorTok{~}\StringTok{ }\NormalTok{.,}
                             \DataTypeTok{data =}\NormalTok{ val_train_data,}
                             \DataTypeTok{method =} \StringTok{"pam"}\NormalTok{,}
                             \DataTypeTok{preProcess =} \OtherTok{NULL}\NormalTok{,}
                             \DataTypeTok{trControl =}\NormalTok{ train_control)}
\CommentTok{#> 12345678910111213141516171819202122232425262728293011111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111}
\NormalTok{model_pam}
\CommentTok{#> Nearest Shrunken Centroids }
\CommentTok{#> }
\CommentTok{#> 56 samples}
\CommentTok{#> 11 predictors}
\CommentTok{#>  2 classes: 'Death', 'Recover' }
\CommentTok{#> }
\CommentTok{#> No pre-processing}
\CommentTok{#> Resampling: Cross-Validated (10 fold, repeated 10 times) }
\CommentTok{#> Summary of sample sizes: 51, 49, 50, 51, 49, 51, ... }
\CommentTok{#> Resampling results across tuning parameters:}
\CommentTok{#> }
\CommentTok{#>   threshold  Accuracy  Kappa}
\CommentTok{#>   0.142      0.709     0.416}
\CommentTok{#>   2.065      0.714     0.382}
\CommentTok{#>   3.987      0.590     0.000}
\CommentTok{#> }
\CommentTok{#> Accuracy was used to select the optimal model using the largest value.}
\CommentTok{#> The final value used for the model was threshold = 2.06.}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{confusionMatrix}\NormalTok{(}\KeywordTok{predict}\NormalTok{(model_pam, val_test_data[, }\DecValTok{-1}\NormalTok{]), val_test_data}\OperatorTok{$}\NormalTok{outcome)}
\CommentTok{#> Confusion Matrix and Statistics}
\CommentTok{#> }
\CommentTok{#>           Reference}
\CommentTok{#> Prediction Death Recover}
\CommentTok{#>    Death       1       3}
\CommentTok{#>    Recover     8      11}
\CommentTok{#>                                         }
\CommentTok{#>                Accuracy : 0.522         }
\CommentTok{#>                  95% CI : (0.306, 0.732)}
\CommentTok{#>     No Information Rate : 0.609         }
\CommentTok{#>     P-Value [Acc > NIR] : 0.857         }
\CommentTok{#>                                         }
\CommentTok{#>                   Kappa : -0.115        }
\CommentTok{#>                                         }
\CommentTok{#>  Mcnemar's Test P-Value : 0.228         }
\CommentTok{#>                                         }
\CommentTok{#>             Sensitivity : 0.1111        }
\CommentTok{#>             Specificity : 0.7857        }
\CommentTok{#>          Pos Pred Value : 0.2500        }
\CommentTok{#>          Neg Pred Value : 0.5789        }
\CommentTok{#>              Prevalence : 0.3913        }
\CommentTok{#>          Detection Rate : 0.0435        }
\CommentTok{#>    Detection Prevalence : 0.1739        }
\CommentTok{#>       Balanced Accuracy : 0.4484        }
\CommentTok{#>                                         }
\CommentTok{#>        'Positive' Class : Death         }
\CommentTok{#> }
\end{Highlighting}
\end{Shaded}

\hypertarget{single-c5.0-tree}{%
\subsection{Single C5.0 Tree}\label{single-c5.0-tree}}

C5.0 is another tree-based modeling algorithm.

This model classified 15 out of 23 cases correctly.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{27}\NormalTok{)}
\NormalTok{model_C5}\FloatTok{.0}\NormalTok{Tree <-}\StringTok{ }\NormalTok{caret}\OperatorTok{::}\KeywordTok{train}\NormalTok{(outcome }\OperatorTok{~}\StringTok{ }\NormalTok{.,}
                             \DataTypeTok{data =}\NormalTok{ val_train_data,}
                             \DataTypeTok{method =} \StringTok{"C5.0Tree"}\NormalTok{,}
                             \DataTypeTok{preProcess =} \OtherTok{NULL}\NormalTok{,}
                             \DataTypeTok{trControl =}\NormalTok{ train_control)}
\NormalTok{model_C5}\FloatTok{.0}\NormalTok{Tree}
\CommentTok{#> Single C5.0 Tree }
\CommentTok{#> }
\CommentTok{#> 56 samples}
\CommentTok{#> 11 predictors}
\CommentTok{#>  2 classes: 'Death', 'Recover' }
\CommentTok{#> }
\CommentTok{#> No pre-processing}
\CommentTok{#> Resampling: Cross-Validated (10 fold, repeated 10 times) }
\CommentTok{#> Summary of sample sizes: 51, 49, 50, 51, 49, 51, ... }
\CommentTok{#> Resampling results:}
\CommentTok{#> }
\CommentTok{#>   Accuracy  Kappa}
\CommentTok{#>   0.696     0.359}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{confusionMatrix}\NormalTok{(}\KeywordTok{predict}\NormalTok{(model_C5}\FloatTok{.0}\NormalTok{Tree, val_test_data[, }\DecValTok{-1}\NormalTok{]), val_test_data}\OperatorTok{$}\NormalTok{outcome)}
\CommentTok{#> Confusion Matrix and Statistics}
\CommentTok{#> }
\CommentTok{#>           Reference}
\CommentTok{#> Prediction Death Recover}
\CommentTok{#>    Death       4       1}
\CommentTok{#>    Recover     5      13}
\CommentTok{#>                                         }
\CommentTok{#>                Accuracy : 0.739         }
\CommentTok{#>                  95% CI : (0.516, 0.898)}
\CommentTok{#>     No Information Rate : 0.609         }
\CommentTok{#>     P-Value [Acc > NIR] : 0.142         }
\CommentTok{#>                                         }
\CommentTok{#>                   Kappa : 0.405         }
\CommentTok{#>                                         }
\CommentTok{#>  Mcnemar's Test P-Value : 0.221         }
\CommentTok{#>                                         }
\CommentTok{#>             Sensitivity : 0.444         }
\CommentTok{#>             Specificity : 0.929         }
\CommentTok{#>          Pos Pred Value : 0.800         }
\CommentTok{#>          Neg Pred Value : 0.722         }
\CommentTok{#>              Prevalence : 0.391         }
\CommentTok{#>          Detection Rate : 0.174         }
\CommentTok{#>    Detection Prevalence : 0.217         }
\CommentTok{#>       Balanced Accuracy : 0.687         }
\CommentTok{#>                                         }
\CommentTok{#>        'Positive' Class : Death         }
\CommentTok{#> }
\end{Highlighting}
\end{Shaded}

\hypertarget{partial-least-squares}{%
\subsection{Partial Least Squares}\label{partial-least-squares}}

modeling with correlated features.

This model classified 15 out of 23 cases correctly.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{27}\NormalTok{)}
\NormalTok{model_pls <-}\StringTok{ }\NormalTok{caret}\OperatorTok{::}\KeywordTok{train}\NormalTok{(outcome }\OperatorTok{~}\StringTok{ }\NormalTok{.,}
                             \DataTypeTok{data =}\NormalTok{ val_train_data,}
                             \DataTypeTok{method =} \StringTok{"pls"}\NormalTok{,}
                             \DataTypeTok{preProcess =} \OtherTok{NULL}\NormalTok{,}
                             \DataTypeTok{trControl =}\NormalTok{ train_control)}
\NormalTok{model_pls}
\CommentTok{#> Partial Least Squares }
\CommentTok{#> }
\CommentTok{#> 56 samples}
\CommentTok{#> 11 predictors}
\CommentTok{#>  2 classes: 'Death', 'Recover' }
\CommentTok{#> }
\CommentTok{#> No pre-processing}
\CommentTok{#> Resampling: Cross-Validated (10 fold, repeated 10 times) }
\CommentTok{#> Summary of sample sizes: 51, 49, 50, 51, 49, 51, ... }
\CommentTok{#> Resampling results across tuning parameters:}
\CommentTok{#> }
\CommentTok{#>   ncomp  Accuracy  Kappa}
\CommentTok{#>   1      0.663     0.315}
\CommentTok{#>   2      0.676     0.341}
\CommentTok{#>   3      0.691     0.376}
\CommentTok{#> }
\CommentTok{#> Accuracy was used to select the optimal model using the largest value.}
\CommentTok{#> The final value used for the model was ncomp = 3.}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{confusionMatrix}\NormalTok{(}\KeywordTok{predict}\NormalTok{(model_pls, val_test_data[, }\DecValTok{-1}\NormalTok{]), val_test_data}\OperatorTok{$}\NormalTok{outcome)}
\CommentTok{#> Confusion Matrix and Statistics}
\CommentTok{#> }
\CommentTok{#>           Reference}
\CommentTok{#> Prediction Death Recover}
\CommentTok{#>    Death       2       3}
\CommentTok{#>    Recover     7      11}
\CommentTok{#>                                         }
\CommentTok{#>                Accuracy : 0.565         }
\CommentTok{#>                  95% CI : (0.345, 0.768)}
\CommentTok{#>     No Information Rate : 0.609         }
\CommentTok{#>     P-Value [Acc > NIR] : 0.742         }
\CommentTok{#>                                         }
\CommentTok{#>                   Kappa : 0.009         }
\CommentTok{#>                                         }
\CommentTok{#>  Mcnemar's Test P-Value : 0.343         }
\CommentTok{#>                                         }
\CommentTok{#>             Sensitivity : 0.222         }
\CommentTok{#>             Specificity : 0.786         }
\CommentTok{#>          Pos Pred Value : 0.400         }
\CommentTok{#>          Neg Pred Value : 0.611         }
\CommentTok{#>              Prevalence : 0.391         }
\CommentTok{#>          Detection Rate : 0.087         }
\CommentTok{#>    Detection Prevalence : 0.217         }
\CommentTok{#>       Balanced Accuracy : 0.504         }
\CommentTok{#>                                         }
\CommentTok{#>        'Positive' Class : Death         }
\CommentTok{#> }
\end{Highlighting}
\end{Shaded}

\hypertarget{comparing-accuracy-of-models}{%
\section{Comparing accuracy of models}\label{comparing-accuracy-of-models}}

All models were similarly accurate.

\hypertarget{summary-accuracy-and-kappa}{%
\subsection{Summary Accuracy and Kappa}\label{summary-accuracy-and-kappa}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Create a list of models}
\NormalTok{models <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}\DataTypeTok{rf       =}\NormalTok{ model_rf, }
               \DataTypeTok{glmnet   =}\NormalTok{ model_glmnet, }
               \DataTypeTok{kknn     =}\NormalTok{ model_kknn, }
               \DataTypeTok{pda      =}\NormalTok{ model_pda, }
               \DataTypeTok{slda     =}\NormalTok{ model_slda,}
               \DataTypeTok{pam      =}\NormalTok{ model_pam, }
               \DataTypeTok{C5.0Tree =}\NormalTok{ model_C5}\FloatTok{.0}\NormalTok{Tree, }
               \DataTypeTok{pls      =}\NormalTok{ model_pls)}

\CommentTok{# Resample the models}
\NormalTok{resample_results <-}\StringTok{ }\KeywordTok{resamples}\NormalTok{(models)}

\CommentTok{# Generate a summary}
\KeywordTok{summary}\NormalTok{(resample_results, }\DataTypeTok{metric =} \KeywordTok{c}\NormalTok{(}\StringTok{"Kappa"}\NormalTok{, }\StringTok{"Accuracy"}\NormalTok{))}
\CommentTok{#> }
\CommentTok{#> Call:}
\CommentTok{#> summary.resamples(object = resample_results, metric = c("Kappa", "Accuracy"))}
\CommentTok{#> }
\CommentTok{#> Models: rf, glmnet, kknn, pda, slda, pam, C5.0Tree, pls }
\CommentTok{#> Number of resamples: 100 }
\CommentTok{#> }
\CommentTok{#> Kappa }
\CommentTok{#>            Min. 1st Qu. Median  Mean 3rd Qu. Max. NA's}
\CommentTok{#> rf       -0.500   0.167  0.545 0.432   0.667    1    0}
\CommentTok{#> glmnet   -0.667   0.167  0.545 0.414   0.667    1    0}
\CommentTok{#> kknn     -0.667   0.125  0.333 0.313   0.615    1    0}
\CommentTok{#> pda      -0.667   0.142  0.333 0.343   0.615    1    0}
\CommentTok{#> slda     -0.667   0.167  0.367 0.358   0.615    1    0}
\CommentTok{#> pam      -0.667   0.167  0.545 0.382   0.571    1    0}
\CommentTok{#> C5.0Tree -0.667   0.167  0.333 0.359   0.615    1    0}
\CommentTok{#> pls      -0.667   0.167  0.333 0.376   0.667    1    0}
\CommentTok{#> }
\CommentTok{#> Accuracy }
\CommentTok{#>           Min. 1st Qu. Median  Mean 3rd Qu. Max. NA's}
\CommentTok{#> rf       0.333   0.600  0.800 0.732   0.833    1    0}
\CommentTok{#> glmnet   0.167   0.600  0.800 0.714   0.833    1    0}
\CommentTok{#> kknn     0.167   0.600  0.667 0.666   0.800    1    0}
\CommentTok{#> pda      0.167   0.593  0.667 0.681   0.800    1    0}
\CommentTok{#> slda     0.167   0.600  0.667 0.682   0.800    1    0}
\CommentTok{#> pam      0.200   0.600  0.800 0.714   0.800    1    0}
\CommentTok{#> C5.0Tree 0.167   0.600  0.667 0.696   0.800    1    0}
\CommentTok{#> pls      0.167   0.600  0.667 0.691   0.833    1    0}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{bwplot}\NormalTok{(resample_results , }\DataTypeTok{metric =} \KeywordTok{c}\NormalTok{(}\StringTok{"Kappa"}\NormalTok{,}\StringTok{"Accuracy"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{algo-comparison_330-classification_flu_outcome-multi-sglander_files/figure-latex/unnamed-chunk-27-1} \end{center}

\hypertarget{combined-results-of-predicting-validation-test-samples}{%
\subsection{Combined results of predicting validation test samples}\label{combined-results-of-predicting-validation-test-samples}}

To compare the predictions from all models, I summed up the prediction probabilities for Death and Recovery from all models and calculated the log2 of the ratio between the summed probabilities for Recovery by the summed probabilities for Death. All cases with a log2 ratio bigger than 1.5 were defined as Recover, cases with a log2 ratio below -1.5 were defined as Death, and the remaining cases were defined as uncertain.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}
  \DataTypeTok{randomForest =} \KeywordTok{predict}\NormalTok{(model_rf, }\DataTypeTok{newdata =}\NormalTok{ val_test_data[, }\DecValTok{-1}\NormalTok{], }\DataTypeTok{type=}\StringTok{"prob"}\NormalTok{),}
  \DataTypeTok{glmnet =} \KeywordTok{predict}\NormalTok{(model_glmnet, }\DataTypeTok{newdata =}\NormalTok{ val_test_data[, }\DecValTok{-1}\NormalTok{], }\DataTypeTok{type=}\StringTok{"prob"}\NormalTok{),}
  \DataTypeTok{kknn =} \KeywordTok{predict}\NormalTok{(model_kknn, }\DataTypeTok{newdata =}\NormalTok{ val_test_data[, }\DecValTok{-1}\NormalTok{], }\DataTypeTok{type=}\StringTok{"prob"}\NormalTok{),}
  \DataTypeTok{pda =} \KeywordTok{predict}\NormalTok{(model_pda, }\DataTypeTok{newdata =}\NormalTok{ val_test_data[, }\DecValTok{-1}\NormalTok{], }\DataTypeTok{type=}\StringTok{"prob"}\NormalTok{),}
  \DataTypeTok{slda =} \KeywordTok{predict}\NormalTok{(model_slda, }\DataTypeTok{newdata =}\NormalTok{ val_test_data[, }\DecValTok{-1}\NormalTok{], }\DataTypeTok{type=}\StringTok{"prob"}\NormalTok{),}
  \DataTypeTok{pam =} \KeywordTok{predict}\NormalTok{(model_pam, }\DataTypeTok{newdata =}\NormalTok{ val_test_data[, }\DecValTok{-1}\NormalTok{], }\DataTypeTok{type=}\StringTok{"prob"}\NormalTok{),}
  \DataTypeTok{C5.0Tree =} \KeywordTok{predict}\NormalTok{(model_C5}\FloatTok{.0}\NormalTok{Tree, }\DataTypeTok{newdata =}\NormalTok{ val_test_data[, }\DecValTok{-1}\NormalTok{], }\DataTypeTok{type=}\StringTok{"prob"}\NormalTok{),}
  \DataTypeTok{pls =} \KeywordTok{predict}\NormalTok{(model_pls, }\DataTypeTok{newdata =}\NormalTok{ val_test_data[, }\DecValTok{-1}\NormalTok{], }\DataTypeTok{type=}\StringTok{"prob"}\NormalTok{))}

\NormalTok{results}\OperatorTok{$}\NormalTok{sum_Death <-}\StringTok{ }\KeywordTok{rowSums}\NormalTok{(results[, }\KeywordTok{grep}\NormalTok{(}\StringTok{"Death"}\NormalTok{, }\KeywordTok{colnames}\NormalTok{(results))])}
\NormalTok{results}\OperatorTok{$}\NormalTok{sum_Recover <-}\StringTok{ }\KeywordTok{rowSums}\NormalTok{(results[, }\KeywordTok{grep}\NormalTok{(}\StringTok{"Recover"}\NormalTok{, }\KeywordTok{colnames}\NormalTok{(results))])}
\NormalTok{results}\OperatorTok{$}\NormalTok{log2_ratio <-}\StringTok{ }\KeywordTok{log2}\NormalTok{(results}\OperatorTok{$}\NormalTok{sum_Recover}\OperatorTok{/}\NormalTok{results}\OperatorTok{$}\NormalTok{sum_Death)}
\NormalTok{results}\OperatorTok{$}\NormalTok{true_outcome <-}\StringTok{ }\NormalTok{val_test_data}\OperatorTok{$}\NormalTok{outcome}
\NormalTok{results}\OperatorTok{$}\NormalTok{pred_outcome <-}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(results}\OperatorTok{$}\NormalTok{log2_ratio }\OperatorTok{>}\StringTok{ }\FloatTok{1.5}\NormalTok{, }\StringTok{"Recover"}\NormalTok{, }
              \KeywordTok{ifelse}\NormalTok{(results}\OperatorTok{$}\NormalTok{log2_ratio }\OperatorTok{<}\StringTok{ }\FloatTok{-1.5}\NormalTok{, }\StringTok{"Death"}\NormalTok{, }\StringTok{"uncertain"}\NormalTok{))}
\NormalTok{results}\OperatorTok{$}\NormalTok{prediction <-}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(results}\OperatorTok{$}\NormalTok{pred_outcome }\OperatorTok{==}\StringTok{ }\NormalTok{results}\OperatorTok{$}\NormalTok{true_outcome, }\StringTok{"CORRECT"}\NormalTok{, }
              \KeywordTok{ifelse}\NormalTok{(results}\OperatorTok{$}\NormalTok{pred_outcome }\OperatorTok{==}\StringTok{ "uncertain"}\NormalTok{, }\StringTok{"uncertain"}\NormalTok{, }\StringTok{"wrong"}\NormalTok{))}
\NormalTok{results[, }\OperatorTok{-}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{16}\NormalTok{)]}
\CommentTok{#>          sum_Death sum_Recover log2_ratio true_outcome pred_outcome prediction}
\CommentTok{#> case_10      4.237        3.76    -0.1709        Death    uncertain  uncertain}
\CommentTok{#> case_11      5.181        2.82    -0.8778        Death    uncertain  uncertain}
\CommentTok{#> case_116     2.412        5.59     1.2123      Recover    uncertain  uncertain}
\CommentTok{#> case_12      5.219        2.78    -0.9085        Death    uncertain  uncertain}
\CommentTok{#> case_121     2.356        5.64     1.2606        Death    uncertain  uncertain}
\CommentTok{#> case_127     0.694        7.31     3.3972      Recover      Recover    CORRECT}
\CommentTok{#> case_131     0.685        7.31     3.4164      Recover      Recover    CORRECT}
\CommentTok{#> case_133     0.649        7.35     3.5024      Recover      Recover    CORRECT}
\CommentTok{#> case_135     2.027        5.97     1.5589        Death      Recover      wrong}
\CommentTok{#> case_2       2.161        5.84     1.4337        Death    uncertain  uncertain}
\CommentTok{#> case_20      3.144        4.86     0.6272      Recover    uncertain  uncertain}
\CommentTok{#> case_30      4.493        3.51    -0.3576      Recover    uncertain  uncertain}
\CommentTok{#> case_45      2.594        5.41     1.0590        Death    uncertain  uncertain}
\CommentTok{#> case_5       3.019        4.98     0.7227      Recover    uncertain  uncertain}
\CommentTok{#> case_55      3.925        4.08     0.0543      Recover    uncertain  uncertain}
\CommentTok{#> case_59      1.894        6.11     1.6886      Recover      Recover    CORRECT}
\CommentTok{#> case_72      2.545        5.46     1.1002      Recover    uncertain  uncertain}
\CommentTok{#> case_74      2.339        5.66     1.2748      Recover    uncertain  uncertain}
\CommentTok{#> case_77      0.845        7.15     3.0819      Recover      Recover    CORRECT}
\CommentTok{#> case_8       2.237        5.76     1.3650        Death    uncertain  uncertain}
\CommentTok{#> case_89      1.712        6.29     1.8772      Recover      Recover    CORRECT}
\CommentTok{#> case_97      2.959        5.04     0.7687      Recover    uncertain  uncertain}
\CommentTok{#> case_98      4.798        3.20    -0.5833        Death    uncertain  uncertain}
\end{Highlighting}
\end{Shaded}

All predictions based on all models were either correct or uncertain.

\hypertarget{predicting-unknown-outcomes}{%
\section{Predicting unknown outcomes}\label{predicting-unknown-outcomes}}

The above models will now be used to predict the outcome of cases with unknown fate.

\begin{Shaded}
\begin{Highlighting}[]

\NormalTok{train_control <-}\StringTok{ }\KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"repeatedcv"}\NormalTok{, }
                              \DataTypeTok{number =} \DecValTok{10}\NormalTok{, }
                              \DataTypeTok{repeats =} \DecValTok{10}\NormalTok{, }
                              \DataTypeTok{verboseIter =} \OtherTok{FALSE}\NormalTok{)}

\KeywordTok{set.seed}\NormalTok{(}\DecValTok{27}\NormalTok{)}
\NormalTok{model_rf <-}\StringTok{ }\NormalTok{caret}\OperatorTok{::}\KeywordTok{train}\NormalTok{(outcome }\OperatorTok{~}\StringTok{ }\NormalTok{.,}
                             \DataTypeTok{data =}\NormalTok{ train_data,}
                             \DataTypeTok{method =} \StringTok{"rf"}\NormalTok{,}
                             \DataTypeTok{preProcess =} \OtherTok{NULL}\NormalTok{,}
                             \DataTypeTok{trControl =}\NormalTok{ train_control)}
\NormalTok{model_glmnet <-}\StringTok{ }\NormalTok{caret}\OperatorTok{::}\KeywordTok{train}\NormalTok{(outcome }\OperatorTok{~}\StringTok{ }\NormalTok{.,}
                             \DataTypeTok{data =}\NormalTok{ train_data,}
                             \DataTypeTok{method =} \StringTok{"glmnet"}\NormalTok{,}
                             \DataTypeTok{preProcess =} \OtherTok{NULL}\NormalTok{,}
                             \DataTypeTok{trControl =}\NormalTok{ train_control)}
\NormalTok{model_kknn <-}\StringTok{ }\NormalTok{caret}\OperatorTok{::}\KeywordTok{train}\NormalTok{(outcome }\OperatorTok{~}\StringTok{ }\NormalTok{.,}
                             \DataTypeTok{data =}\NormalTok{ train_data,}
                             \DataTypeTok{method =} \StringTok{"kknn"}\NormalTok{,}
                             \DataTypeTok{preProcess =} \OtherTok{NULL}\NormalTok{,}
                             \DataTypeTok{trControl =}\NormalTok{ train_control)}
\NormalTok{model_pda <-}\StringTok{ }\NormalTok{caret}\OperatorTok{::}\KeywordTok{train}\NormalTok{(outcome }\OperatorTok{~}\StringTok{ }\NormalTok{.,}
                             \DataTypeTok{data =}\NormalTok{ train_data,}
                             \DataTypeTok{method =} \StringTok{"pda"}\NormalTok{,}
                             \DataTypeTok{preProcess =} \OtherTok{NULL}\NormalTok{,}
                             \DataTypeTok{trControl =}\NormalTok{ train_control)}
\CommentTok{#> Warning: predictions failed for Fold01.Rep01: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold02.Rep01: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold03.Rep01: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold04.Rep01: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold05.Rep01: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold06.Rep01: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold07.Rep01: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold08.Rep01: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold09.Rep01: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold10.Rep01: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold01.Rep02: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold02.Rep02: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold03.Rep02: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold04.Rep02: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold05.Rep02: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold06.Rep02: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold07.Rep02: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold08.Rep02: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold09.Rep02: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold10.Rep02: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold01.Rep03: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold02.Rep03: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold03.Rep03: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold04.Rep03: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold05.Rep03: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold06.Rep03: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold07.Rep03: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold08.Rep03: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold09.Rep03: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold10.Rep03: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold01.Rep04: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold02.Rep04: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold03.Rep04: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold04.Rep04: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold05.Rep04: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold06.Rep04: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold07.Rep04: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold08.Rep04: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold09.Rep04: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold10.Rep04: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold01.Rep05: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold02.Rep05: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold03.Rep05: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold04.Rep05: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold05.Rep05: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold06.Rep05: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold07.Rep05: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold08.Rep05: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold09.Rep05: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold10.Rep05: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold01.Rep06: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold02.Rep06: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold03.Rep06: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold04.Rep06: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold05.Rep06: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold06.Rep06: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold07.Rep06: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold08.Rep06: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold09.Rep06: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold10.Rep06: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold01.Rep07: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold02.Rep07: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold03.Rep07: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold04.Rep07: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold05.Rep07: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold06.Rep07: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold07.Rep07: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold08.Rep07: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold09.Rep07: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold10.Rep07: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold01.Rep08: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold02.Rep08: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold03.Rep08: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold04.Rep08: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold05.Rep08: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold06.Rep08: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold07.Rep08: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold08.Rep08: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold09.Rep08: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold10.Rep08: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold01.Rep09: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold02.Rep09: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold03.Rep09: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold04.Rep09: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold05.Rep09: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold06.Rep09: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold07.Rep09: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold08.Rep09: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold09.Rep09: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold10.Rep09: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold01.Rep10: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold02.Rep10: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold03.Rep10: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold04.Rep10: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold05.Rep10: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold06.Rep10: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold07.Rep10: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold08.Rep10: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold09.Rep10: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning: predictions failed for Fold10.Rep10: lambda=0e+00 Error in mindist[l] <- ndist[l] : }
\CommentTok{#>   NAs are not allowed in subscripted assignments}
\CommentTok{#> Warning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, : There were missing values in resampled performance measures.}
\CommentTok{#> Warning in train.default(x, y, weights = w, ...): missing values found in aggregated results}
\NormalTok{model_slda <-}\StringTok{ }\NormalTok{caret}\OperatorTok{::}\KeywordTok{train}\NormalTok{(outcome }\OperatorTok{~}\StringTok{ }\NormalTok{.,}
                             \DataTypeTok{data =}\NormalTok{ train_data,}
                             \DataTypeTok{method =} \StringTok{"slda"}\NormalTok{,}
                             \DataTypeTok{preProcess =} \OtherTok{NULL}\NormalTok{,}
                             \DataTypeTok{trControl =}\NormalTok{ train_control)}
\NormalTok{model_pam <-}\StringTok{ }\NormalTok{caret}\OperatorTok{::}\KeywordTok{train}\NormalTok{(outcome }\OperatorTok{~}\StringTok{ }\NormalTok{.,}
                             \DataTypeTok{data =}\NormalTok{ train_data,}
                             \DataTypeTok{method =} \StringTok{"pam"}\NormalTok{,}
                             \DataTypeTok{preProcess =} \OtherTok{NULL}\NormalTok{,}
                             \DataTypeTok{trControl =}\NormalTok{ train_control)}
\CommentTok{#> 12345678910111213141516171819202122232425262728293011111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111}
\NormalTok{model_C5}\FloatTok{.0}\NormalTok{Tree <-}\StringTok{ }\NormalTok{caret}\OperatorTok{::}\KeywordTok{train}\NormalTok{(outcome }\OperatorTok{~}\StringTok{ }\NormalTok{.,}
                             \DataTypeTok{data =}\NormalTok{ train_data,}
                             \DataTypeTok{method =} \StringTok{"C5.0Tree"}\NormalTok{,}
                             \DataTypeTok{preProcess =} \OtherTok{NULL}\NormalTok{,}
                             \DataTypeTok{trControl =}\NormalTok{ train_control)}
\NormalTok{model_pls <-}\StringTok{ }\NormalTok{caret}\OperatorTok{::}\KeywordTok{train}\NormalTok{(outcome }\OperatorTok{~}\StringTok{ }\NormalTok{.,}
                             \DataTypeTok{data =}\NormalTok{ train_data,}
                             \DataTypeTok{method =} \StringTok{"pls"}\NormalTok{,}
                             \DataTypeTok{preProcess =} \OtherTok{NULL}\NormalTok{,}
                             \DataTypeTok{trControl =}\NormalTok{ train_control)}

\NormalTok{models <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}\DataTypeTok{rf =}\NormalTok{ model_rf, }
               \DataTypeTok{glmnet   =}\NormalTok{ model_glmnet, }
               \DataTypeTok{kknn     =}\NormalTok{ model_kknn, }
               \DataTypeTok{pda      =}\NormalTok{ model_pda, }
               \DataTypeTok{slda     =}\NormalTok{ model_slda,}
               \DataTypeTok{pam      =}\NormalTok{ model_pam, }
               \DataTypeTok{C5.0Tree =}\NormalTok{ model_C5}\FloatTok{.0}\NormalTok{Tree, }
               \DataTypeTok{pls      =}\NormalTok{ model_pls)}

\CommentTok{# Resample the models}
\NormalTok{resample_results <-}\StringTok{ }\KeywordTok{resamples}\NormalTok{(models)}

\KeywordTok{bwplot}\NormalTok{(resample_results , }\DataTypeTok{metric =} \KeywordTok{c}\NormalTok{(}\StringTok{"Kappa"}\NormalTok{,}\StringTok{"Accuracy"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{algo-comparison_330-classification_flu_outcome-multi-sglander_files/figure-latex/model-train-all-run-1} \end{center}

Here again, the accuracy is similar in all models.

\hypertarget{final-results}{%
\subsection{Final results}\label{final-results}}

The final results are calculated as described above.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}
  \DataTypeTok{randomForest =} \KeywordTok{predict}\NormalTok{(model_rf, }\DataTypeTok{newdata =}\NormalTok{ test_data, }\DataTypeTok{type=}\StringTok{"prob"}\NormalTok{),}
  \DataTypeTok{glmnet =} \KeywordTok{predict}\NormalTok{(model_glmnet, }\DataTypeTok{newdata =}\NormalTok{ test_data, }\DataTypeTok{type=}\StringTok{"prob"}\NormalTok{),}
  \DataTypeTok{kknn =} \KeywordTok{predict}\NormalTok{(model_kknn, }\DataTypeTok{newdata =}\NormalTok{ test_data, }\DataTypeTok{type=}\StringTok{"prob"}\NormalTok{),}
  \DataTypeTok{pda =} \KeywordTok{predict}\NormalTok{(model_pda, }\DataTypeTok{newdata =}\NormalTok{ test_data, }\DataTypeTok{type=}\StringTok{"prob"}\NormalTok{),}
  \DataTypeTok{slda =} \KeywordTok{predict}\NormalTok{(model_slda, }\DataTypeTok{newdata =}\NormalTok{ test_data, }\DataTypeTok{type=}\StringTok{"prob"}\NormalTok{),}
  \DataTypeTok{pam =} \KeywordTok{predict}\NormalTok{(model_pam, }\DataTypeTok{newdata =}\NormalTok{ test_data, }\DataTypeTok{type=}\StringTok{"prob"}\NormalTok{),}
  \DataTypeTok{C5.0Tree =} \KeywordTok{predict}\NormalTok{(model_C5}\FloatTok{.0}\NormalTok{Tree, }\DataTypeTok{newdata =}\NormalTok{ test_data, }\DataTypeTok{type=}\StringTok{"prob"}\NormalTok{),}
  \DataTypeTok{pls =} \KeywordTok{predict}\NormalTok{(model_pls, }\DataTypeTok{newdata =}\NormalTok{ test_data, }\DataTypeTok{type=}\StringTok{"prob"}\NormalTok{))}

\NormalTok{results}\OperatorTok{$}\NormalTok{sum_Death <-}\StringTok{ }\KeywordTok{rowSums}\NormalTok{(results[, }\KeywordTok{grep}\NormalTok{(}\StringTok{"Death"}\NormalTok{, }\KeywordTok{colnames}\NormalTok{(results))])}
\NormalTok{results}\OperatorTok{$}\NormalTok{sum_Recover <-}\StringTok{ }\KeywordTok{rowSums}\NormalTok{(results[, }\KeywordTok{grep}\NormalTok{(}\StringTok{"Recover"}\NormalTok{, }\KeywordTok{colnames}\NormalTok{(results))])}
\NormalTok{results}\OperatorTok{$}\NormalTok{log2_ratio <-}\StringTok{ }\KeywordTok{log2}\NormalTok{(results}\OperatorTok{$}\NormalTok{sum_Recover}\OperatorTok{/}\NormalTok{results}\OperatorTok{$}\NormalTok{sum_Death)}
\NormalTok{results}\OperatorTok{$}\NormalTok{predicted_outcome <-}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(results}\OperatorTok{$}\NormalTok{log2_ratio }\OperatorTok{>}\StringTok{ }\FloatTok{1.5}\NormalTok{, }\StringTok{"Recover"}\NormalTok{, }
                                    \KeywordTok{ifelse}\NormalTok{(results}\OperatorTok{$}\NormalTok{log2_ratio }\OperatorTok{<}\StringTok{ }\FloatTok{-1.5}\NormalTok{, }\StringTok{"Death"}\NormalTok{, }
                                           \StringTok{"uncertain"}\NormalTok{))}
\NormalTok{results[, }\OperatorTok{-}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{16}\NormalTok{)]}
\CommentTok{#>          sum_Death sum_Recover log2_ratio predicted_outcome}
\CommentTok{#> case_100     1.854        6.15     1.7287           Recover}
\CommentTok{#> case_101     5.432        2.57    -1.0807         uncertain}
\CommentTok{#> case_102     2.806        5.19     0.8887         uncertain}
\CommentTok{#> case_103     2.342        5.66     1.2729         uncertain}
\CommentTok{#> case_104     1.744        6.26     1.8432           Recover}
\CommentTok{#> case_105     0.955        7.04     2.8828           Recover}
\CommentTok{#> case_108     4.489        3.51    -0.3543         uncertain}
\CommentTok{#> case_109     4.515        3.48    -0.3736         uncertain}
\CommentTok{#> case_110     2.411        5.59     1.2132         uncertain}
\CommentTok{#> case_112     2.632        5.37     1.0281         uncertain}
\CommentTok{#> case_113     2.198        5.80     1.4004         uncertain}
\CommentTok{#> case_114     3.339        4.66     0.4810         uncertain}
\CommentTok{#> case_115     1.112        6.89     2.6307           Recover}
\CommentTok{#> case_118     2.778        5.22     0.9109         uncertain}
\CommentTok{#> case_120     2.213        5.79     1.3868         uncertain}
\CommentTok{#> case_122     3.235        4.77     0.5590         uncertain}
\CommentTok{#> case_126     3.186        4.81     0.5952         uncertain}
\CommentTok{#> case_130     2.300        5.70     1.3091         uncertain}
\CommentTok{#> case_132     4.473        3.53    -0.3427         uncertain}
\CommentTok{#> case_136     3.281        4.72     0.5243         uncertain}
\CommentTok{#> case_15      2.270        5.73     1.3355         uncertain}
\CommentTok{#> case_16      2.820        5.18     0.8772         uncertain}
\CommentTok{#> case_22      4.779        3.22    -0.5689         uncertain}
\CommentTok{#> case_28      2.862        5.14     0.8445         uncertain}
\CommentTok{#> case_31      2.412        5.59     1.2117         uncertain}
\CommentTok{#> case_32      2.591        5.41     1.0616         uncertain}
\CommentTok{#> case_38      2.060        5.94     1.5280           Recover}
\CommentTok{#> case_39      4.749        3.25    -0.5466         uncertain}
\CommentTok{#> case_4       5.342        2.66    -1.0074         uncertain}
\CommentTok{#> case_40      6.550        1.45    -2.1750             Death}
\CommentTok{#> case_41      4.611        3.39    -0.4441         uncertain}
\CommentTok{#> case_42      5.570        2.43    -1.1966         uncertain}
\CommentTok{#> case_47      2.563        5.44     1.0850         uncertain}
\CommentTok{#> case_48      4.850        3.15    -0.6224         uncertain}
\CommentTok{#> case_52      4.709        3.29    -0.5173         uncertain}
\CommentTok{#> case_54      2.718        5.28     0.9586         uncertain}
\CommentTok{#> case_56      6.394        1.61    -1.9933             Death}
\CommentTok{#> case_62      6.048        1.95    -1.6319             Death}
\CommentTok{#> case_63      2.337        5.66     1.2766         uncertain}
\CommentTok{#> case_66      2.176        5.82     1.4205         uncertain}
\CommentTok{#> case_67      1.893        6.11     1.6895           Recover}
\CommentTok{#> case_68      3.907        4.09     0.0672         uncertain}
\CommentTok{#> case_69      4.465        3.53    -0.3370         uncertain}
\CommentTok{#> case_70      3.885        4.12     0.0833         uncertain}
\CommentTok{#> case_71      2.524        5.48     1.1172         uncertain}
\CommentTok{#> case_80      2.759        5.24     0.9261         uncertain}
\CommentTok{#> case_84      3.661        4.34     0.2448         uncertain}
\CommentTok{#> case_85      4.921        3.08    -0.6762         uncertain}
\CommentTok{#> case_86      3.563        4.44     0.3164         uncertain}
\CommentTok{#> case_88      0.566        7.43     3.7160           Recover}
\CommentTok{#> case_9       5.981        2.02    -1.5665             Death}
\CommentTok{#> case_90      3.570        4.43     0.3116         uncertain}
\CommentTok{#> case_92      4.664        3.34    -0.4835         uncertain}
\CommentTok{#> case_93      2.056        5.94     1.5319           Recover}
\CommentTok{#> case_95      4.495        3.50    -0.3589         uncertain}
\CommentTok{#> case_96      1.313        6.69     2.3482           Recover}
\CommentTok{#> case_99      4.799        3.20    -0.5839         uncertain}
\KeywordTok{write.table}\NormalTok{(results, }\StringTok{"results_prediction_unknown_outcome_ML_part1.txt"}\NormalTok{, }
            \DataTypeTok{col.names =}\NormalTok{ T, }\DataTypeTok{sep =} \StringTok{"}\CharTok{\textbackslash{}t}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(predicted_outcome }\OperatorTok{==}\StringTok{ "Recover"}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{16}\NormalTok{))}
\CommentTok{#>   sum_Death sum_Recover log2_ratio predicted_outcome}
\CommentTok{#> 1     1.854        6.15       1.73           Recover}
\CommentTok{#> 2     1.744        6.26       1.84           Recover}
\CommentTok{#> 3     0.955        7.04       2.88           Recover}
\CommentTok{#> 4     1.112        6.89       2.63           Recover}
\CommentTok{#> 5     2.060        5.94       1.53           Recover}
\CommentTok{#> 6     1.893        6.11       1.69           Recover}
\CommentTok{#> 7     0.566        7.43       3.72           Recover}
\CommentTok{#> 8     2.056        5.94       1.53           Recover}
\CommentTok{#> 9     1.313        6.69       2.35           Recover}
\end{Highlighting}
\end{Shaded}

\hypertarget{predicted-outcome}{%
\subsection{Predicted outcome}\label{predicted-outcome}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(predicted_outcome) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarize}\NormalTok{(}\DataTypeTok{n =} \KeywordTok{n}\NormalTok{())}
\CommentTok{#> # A tibble: 3 x 2}
\CommentTok{#>   predicted_outcome     n}
\CommentTok{#>   <chr>             <int>}
\CommentTok{#> 1 Death                 4}
\CommentTok{#> 2 Recover               9}
\CommentTok{#> 3 uncertain            44}
\end{Highlighting}
\end{Shaded}

From 57 cases, 14 were defined as Recover, 3 as Death and 40 as uncertain.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results_combined <-}\StringTok{ }\KeywordTok{merge}\NormalTok{(results[, }\OperatorTok{-}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{16}\NormalTok{)],}
\NormalTok{                          fluH7N9_china_}\DecValTok{2013}\NormalTok{[}\KeywordTok{which}\NormalTok{(fluH7N9_china_}\DecValTok{2013}\OperatorTok{$}\NormalTok{case_ID }
                                                   \OperatorTok{%in%}\StringTok{ }\KeywordTok{rownames}\NormalTok{(results)), ], }
                          \DataTypeTok{by.x =} \StringTok{"row.names"}\NormalTok{, }\DataTypeTok{by.y =} \StringTok{"case_ID"}\NormalTok{)}
\CommentTok{# results_combined <- results_combined[, -c(2, 3, 8, 9)]}
\NormalTok{results_combined <-}\StringTok{ }\NormalTok{results_combined[, }\OperatorTok{-}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{9}\NormalTok{, }\DecValTok{10}\NormalTok{)]}
\NormalTok{results_combined}
\CommentTok{#>    log2_ratio predicted_outcome case_id date_of_onset date_of_hospitalisation gender age  province}
\CommentTok{#> 1      1.7287           Recover     100    2013-04-16                    <NA>      m  58  Zhejiang}
\CommentTok{#> 2     -1.0807         uncertain     101    2013-04-13                    <NA>      f  79  Zhejiang}
\CommentTok{#> 3      0.8887         uncertain     102    2013-04-12                    <NA>      m  81  Zhejiang}
\CommentTok{#> 4      1.2729         uncertain     103    2013-04-13              2013-04-19      m  68   Jiangsu}
\CommentTok{#> 5      1.8432           Recover     104    2013-04-16                    <NA>      f  54  Zhejiang}
\CommentTok{#> 6      2.8828           Recover     105    2013-04-14                    <NA>      m  32  Zhejiang}
\CommentTok{#> 7     -0.3543         uncertain     108    2013-04-15                    <NA>      m  84  Zhejiang}
\CommentTok{#> 8     -0.3736         uncertain     109    2013-04-15                    <NA>      m  62  Zhejiang}
\CommentTok{#> 9      1.2132         uncertain     110    2013-04-12              2013-04-16      m  53    Taiwan}
\CommentTok{#> 10     1.0281         uncertain     112    2013-04-17                    <NA>      m  69   Jiangxi}
\CommentTok{#> 11     1.4004         uncertain     113    2013-04-15                    <NA>      f  60  Zhejiang}
\CommentTok{#> 12     0.4810         uncertain     114    2013-04-18                    <NA>      f  50  Zhejiang}
\CommentTok{#> 13     2.6307           Recover     115    2013-04-17                    <NA>      m  38  Zhejiang}
\CommentTok{#> 14     0.9109         uncertain     118    2013-04-17                    <NA>      m  49   Jiangsu}
\CommentTok{#> 15     1.3868         uncertain     120    2013-03-08                    <NA>      m  60   Jiangsu}
\CommentTok{#> 16     0.5590         uncertain     122    2013-04-18                    <NA>      m  38  Zhejiang}
\CommentTok{#> 17     0.5952         uncertain     126    2013-04-17              2013-04-27      m  80    Fujian}
\CommentTok{#> 18     1.3091         uncertain     130    2013-04-29              2013-04-30      m  69    Fujian}
\CommentTok{#> 19    -0.3427         uncertain     132    2013-05-03              2013-05-03      f  79   Jiangxi}
\CommentTok{#> 20     0.5243         uncertain     136    2013-07-27              2013-07-28      f  51 Guangdong}
\CommentTok{#> 21     1.3355         uncertain      15    2013-03-20                    <NA>      f  61   Jiangsu}
\CommentTok{#> 22     0.8772         uncertain      16    2013-03-21                    <NA>      m  79   Jiangsu}
\CommentTok{#> 23    -0.5689         uncertain      22    2013-03-28              2013-04-01      m  85   Jiangsu}
\CommentTok{#> 24     0.8445         uncertain      28    2013-03-29                    <NA>      m  79  Zhejiang}
\CommentTok{#> 25     1.2117         uncertain      31    2013-03-29                    <NA>      m  70   Jiangsu}
\CommentTok{#> 26     1.0616         uncertain      32    2013-04-02                    <NA>      m  74   Jiangsu}
\CommentTok{#> 27     1.5280           Recover      38    2013-04-03                    <NA>      m  56   Jiangsu}
\CommentTok{#> 28    -0.5466         uncertain      39    2013-04-08              2013-04-08      m  66  Zhejiang}
\CommentTok{#> 29    -1.0074         uncertain       4    2013-03-19              2013-03-27      f  45   Jiangsu}
\CommentTok{#> 30    -2.1750             Death      40    2013-04-06              2013-04-11      m  74  Zhejiang}
\CommentTok{#> 31    -0.4441         uncertain      41    2013-04-06              2013-04-12      f  54  Zhejiang}
\CommentTok{#> 32    -1.1966         uncertain      42    2013-04-03              2013-04-10      m  53  Shanghai}
\CommentTok{#> 33     1.0850         uncertain      47    2013-04-01                    <NA>      m  72   Jiangsu}
\CommentTok{#> 34    -0.6224         uncertain      48    2013-04-03              2013-04-09      m  65  Zhejiang}
\CommentTok{#> 35    -0.5173         uncertain      52    2013-04-06                    <NA>      f  64  Zhejiang}
\CommentTok{#> 36     0.9586         uncertain      54    2013-04-06                    <NA>      m  75  Zhejiang}
\CommentTok{#> 37    -1.9933             Death      56    2013-04-05              2013-04-11      m  73  Shanghai}
\CommentTok{#> 38    -1.6319             Death      62    2013-04-03                    <NA>      f  68  Zhejiang}
\CommentTok{#> 39     1.2766         uncertain      63    2013-04-10                    <NA>      m  60     Anhui}
\CommentTok{#> 40     1.4205         uncertain      66          <NA>                    <NA>      m  72   Jiangsu}
\CommentTok{#> 41     1.6895           Recover      67    2013-04-12                    <NA>      m  56  Zhejiang}
\CommentTok{#> 42     0.0672         uncertain      68    2013-04-10                    <NA>      m  57  Zhejiang}
\CommentTok{#> 43    -0.3370         uncertain      69    2013-04-10                    <NA>      m  62  Zhejiang}
\CommentTok{#> 44     0.0833         uncertain      70    2013-04-11                    <NA>      f  58  Zhejiang}
\CommentTok{#> 45     1.1172         uncertain      71    2013-04-10                    <NA>      f  72  Zhejiang}
\CommentTok{#> 46     0.9261         uncertain      80    2013-04-08                    <NA>      m  74  Zhejiang}
\CommentTok{#> 47     0.2448         uncertain      84          <NA>                    <NA>      f  26   Jiangsu}
\CommentTok{#> 48    -0.6762         uncertain      85    2013-04-09              2013-04-16      f  80  Shanghai}
\CommentTok{#> 49     0.3164         uncertain      86    2013-04-13                    <NA>      f  54  Zhejiang}
\CommentTok{#> 50     3.7160           Recover      88          <NA>                    <NA>      m   4   Beijing}
\CommentTok{#> 51    -1.5665             Death       9    2013-03-25              2013-03-25      m  67  Zhejiang}
\CommentTok{#> 52     0.3116         uncertain      90    2013-04-12              2013-04-15      m  43  Zhejiang}
\CommentTok{#> 53    -0.4835         uncertain      92    2013-04-10              2013-04-17      f  66  Zhejiang}
\CommentTok{#> 54     1.5319           Recover      93    2013-04-11                    <NA>      m  56  Zhejiang}
\CommentTok{#> 55    -0.3589         uncertain      95    2013-03-30                    <NA>      m  37  Zhejiang}
\CommentTok{#> 56     2.3482           Recover      96    2013-04-07                    <NA>      m  43   Jiangsu}
\CommentTok{#> 57    -0.5839         uncertain      99    2013-04-12                    <NA>      f  68  Zhejiang}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# tidy dataframe for plotting}
\NormalTok{results_combined_gather <-}\StringTok{ }\NormalTok{results_combined }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{gather}\NormalTok{(group_dates, date, date_of_onset}\OperatorTok{:}\NormalTok{date_of_hospitalisation)}

\NormalTok{results_combined_gather}\OperatorTok{$}\NormalTok{group_dates <-}\StringTok{ }\KeywordTok{factor}\NormalTok{(results_combined_gather}\OperatorTok{$}\NormalTok{group_dates, }
                          \DataTypeTok{levels =} \KeywordTok{c}\NormalTok{(}\StringTok{"date_of_onset"}\NormalTok{, }\StringTok{"date_of_hospitalisation"}\NormalTok{))}

\NormalTok{results_combined_gather}\OperatorTok{$}\NormalTok{group_dates <-}\StringTok{ }\KeywordTok{mapvalues}\NormalTok{(results_combined_gather}\OperatorTok{$}\NormalTok{group_dates, }
                            \DataTypeTok{from =} \KeywordTok{c}\NormalTok{(}\StringTok{"date_of_onset"}\NormalTok{, }\StringTok{"date_of_hospitalisation"}\NormalTok{), }
                              \DataTypeTok{to =} \KeywordTok{c}\NormalTok{(}\StringTok{"Date of onset"}\NormalTok{, }\StringTok{"Date of hospitalisation"}\NormalTok{))}

\NormalTok{results_combined_gather}\OperatorTok{$}\NormalTok{gender <-}\StringTok{ }\KeywordTok{mapvalues}\NormalTok{(results_combined_gather}\OperatorTok{$}\NormalTok{gender, }
                                            \DataTypeTok{from =} \KeywordTok{c}\NormalTok{(}\StringTok{"f"}\NormalTok{, }\StringTok{"m"}\NormalTok{), }
                                             \DataTypeTok{to =} \KeywordTok{c}\NormalTok{(}\StringTok{"Female"}\NormalTok{, }\StringTok{"Male"}\NormalTok{))}
\KeywordTok{levels}\NormalTok{(results_combined_gather}\OperatorTok{$}\NormalTok{gender) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\KeywordTok{levels}\NormalTok{(results_combined_gather}\OperatorTok{$}\NormalTok{gender), }
                                            \StringTok{"unknown"}\NormalTok{)}
\NormalTok{results_combined_gather}\OperatorTok{$}\NormalTok{gender[}\KeywordTok{is.na}\NormalTok{(results_combined_gather}\OperatorTok{$}\NormalTok{gender)] <-}\StringTok{ "unknown"}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ results_combined_gather, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ date, }\DataTypeTok{y =}\NormalTok{ log2_ratio, }
                                           \DataTypeTok{color =}\NormalTok{ predicted_outcome)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_jitter}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{size =} \KeywordTok{as.numeric}\NormalTok{(age)), }\DataTypeTok{alpha =} \FloatTok{0.3}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_rug}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{facet_grid}\NormalTok{(gender }\OperatorTok{~}\StringTok{ }\NormalTok{group_dates) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}
    \DataTypeTok{color =} \StringTok{"Predicted outcome"}\NormalTok{,}
    \DataTypeTok{size =} \StringTok{"Age"}\NormalTok{,}
    \DataTypeTok{x =} \StringTok{"Date in 2013"}\NormalTok{,}
    \DataTypeTok{y =} \StringTok{"log2 ratio of prediction Recover vs Death"}\NormalTok{,}
    \DataTypeTok{title =} \StringTok{"2013 Influenza A H7N9 cases in China"}\NormalTok{,}
    \DataTypeTok{subtitle =} \StringTok{"Predicted outcome"}\NormalTok{,}
    \DataTypeTok{caption =} \StringTok{""}
\NormalTok{  ) }\OperatorTok{+}
\StringTok{  }\KeywordTok{my_theme}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_shape_manual}\NormalTok{(}\DataTypeTok{values =} \KeywordTok{c}\NormalTok{(}\DecValTok{15}\NormalTok{, }\DecValTok{16}\NormalTok{, }\DecValTok{17}\NormalTok{)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_color_brewer}\NormalTok{(}\DataTypeTok{palette=}\StringTok{"Set1"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_fill_brewer}\NormalTok{(}\DataTypeTok{palette=}\StringTok{"Set1"}\NormalTok{)}
\CommentTok{#> Warning: Removed 42 rows containing missing values (geom_point).}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{algo-comparison_330-classification_flu_outcome-multi-sglander_files/figure-latex/unnamed-chunk-32-1} \end{center}

The comparison of date of onset, data of hospitalisation, gender and age with predicted outcome shows that predicted deaths were associated with older age than predicted recoveries. Date of onset does not show an obvious bias in either direction.

\hypertarget{conclusions}{%
\section{Conclusions}\label{conclusions}}

This dataset posed a couple of difficulties to begin with, like unequal distribution of data points across variables and missing data. This makes the modeling inherently prone to flaws. However, real life data isn't perfect either, so I went ahead and tested the modeling success anyway.

By accounting for uncertain classification with low predictions probability, the validation data could be classified accurately. However, for a more accurate model, these few cases don't give enough information to reliably predict the outcome. More cases, more information (i.e.~more features) and fewer missing data would improve the modeling outcome.

Also, this example is only applicable for this specific case of flu. In order to be able to draw more general conclusions about flu outcome, other cases and additional information, for example on medical parameters like preexisting medical conditions, disase parameters, demographic information, etc. would be necessary.

All in all, this dataset served as a nice example of the possibilities (and pitfalls) of machine learning applications and showcases a basic workflow for building prediction models with R.

For a comparison of feature selection methods see here.

If you see any mistakes or have tips and tricks for improvement, please don't hesitate to let me know! Thanks. :-)

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sessionInfo}\NormalTok{()}
\CommentTok{#> R version 3.6.0 (2019-04-26)}
\CommentTok{#> Platform: x86_64-pc-linux-gnu (64-bit)}
\CommentTok{#> Running under: Ubuntu 18.04.3 LTS}
\CommentTok{#> }
\CommentTok{#> Matrix products: default}
\CommentTok{#> BLAS/LAPACK: /usr/lib/x86_64-linux-gnu/libopenblasp-r0.2.20.so}
\CommentTok{#> }
\CommentTok{#> locale:}
\CommentTok{#>  [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C               LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8     LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8    LC_PAPER=en_US.UTF-8       LC_NAME=C                  LC_ADDRESS=C               LC_TELEPHONE=C             LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       }
\CommentTok{#> }
\CommentTok{#> attached base packages:}
\CommentTok{#> [1] grid      stats     graphics  grDevices utils     datasets  methods   base     }
\CommentTok{#> }
\CommentTok{#> other attached packages:}
\CommentTok{#>  [1] RColorBrewer_1.1-2 rpart.plot_3.0.7   rattle_5.2.0       rpart_4.1-15       caret_6.0-84       mice_3.4.0         lattice_0.20-38    dplyr_0.8.0.1      gridExtra_2.3      ggplot2_3.1.1      plyr_1.8.4         tidyr_0.8.3        outbreaks_1.5.0    logging_0.9-107   }
\CommentTok{#> }
\CommentTok{#> loaded via a namespace (and not attached):}
\CommentTok{#>  [1] nlme_3.1-139        lubridate_1.7.4     rprojroot_1.3-2     C50_0.1.2           tools_3.6.0         backports_1.1.4     utf8_1.1.4          R6_2.4.0            lazyeval_0.2.2      colorspace_1.4-1    jomo_2.6-7          nnet_7.3-12         withr_2.1.2         tidyselect_0.2.5    compiler_3.6.0      mda_0.4-10          cli_1.1.0           glmnet_2.0-16       Cubist_0.2.2        labeling_0.3        bookdown_0.10       scales_1.0.0        mvtnorm_1.0-10      randomForest_4.6-14 stringr_1.4.0       digest_0.6.18       minqa_1.2.4         rmarkdown_1.12      pkgconfig_2.0.2     htmltools_0.3.6     lme4_1.1-21         rlang_0.3.4         rstudioapi_0.10     generics_0.0.2      ModelMetrics_1.2.2  magrittr_1.5        Formula_1.2-3       Matrix_1.2-17       Rcpp_1.0.1          munsell_0.5.0       fansi_0.4.0         partykit_1.2-3      stringi_1.4.3       yaml_2.2.0          inum_1.0-1          MASS_7.3-51.4       pamr_1.56.1         recipes_0.1.5       parallel_3.6.0     }
\CommentTok{#> [50] pls_2.7-1           mitml_0.3-7         crayon_1.3.4        splines_3.6.0       zeallot_0.1.0       knitr_1.22          pillar_1.4.0        igraph_1.2.4.1      boot_1.3-22         reshape2_1.4.3      codetools_0.2-16    stats4_3.6.0        pan_1.6             glue_1.3.1          evaluate_0.13       data.table_1.12.2   vctrs_0.1.0         nloptr_1.2.1        foreach_1.4.4       gtable_0.3.0        purrr_0.3.2         assertthat_0.2.1    xfun_0.6            gower_0.2.0         prodlim_2018.04.18  libcoin_1.0-4       broom_0.5.2         e1071_1.7-1         class_7.3-15        survival_2.44-1.1   timeDate_3043.102   tibble_2.1.1        iterators_1.0.10    kknn_1.3.1          cluster_2.0.9       lava_1.6.5          ipred_0.9-9}
\end{Highlighting}
\end{Shaded}

\hypertarget{part-classification}{%
\part{Classification}\label{part-classification}}

\hypertarget{a-gentle-introduction-to-support-vector-machines-using-r}{%
\chapter{A gentle introduction to support vector machines using R}\label{a-gentle-introduction-to-support-vector-machines-using-r}}

\hypertarget{introduction-6}{%
\section{Introduction}\label{introduction-6}}

Source: \url{https://eight2late.wordpress.com/2017/02/07/a-gentle-introduction-to-support-vector-machines-using-r/}

Most machine learning algorithms involve minimising an error measure of some kind (this measure is often called an objective function or loss function). For example, the error measure in linear regression problems is the famous mean squared error -- i.e.~the averaged sum of the squared differences between the predicted and actual values. Like the mean squared error, most objective functions depend on all points in the training dataset. In this post, I describe the support vector machine (SVM) approach which focuses instead on finding the optimal separation boundary between datapoints that have different classifications. I'll elaborate on what this means in the next section.

Here's the plan in brief. I'll begin with the rationale behind SVMs using a simple case of a binary (two class) dataset with a simple separation boundary (I'll clarify what ``simple'' means in a minute). Following that, I'll describe how this can be generalised to datasets with more complex boundaries. Finally, I'll work through a couple of examples in R, illustrating the principles behind SVMs. In line with the general philosophy of my ``Gentle Introduction to Data Science Using R'' series, the focus is on developing an intuitive understanding of the algorithm along with a practical demonstration of its use through a toy example.

\hypertarget{the-rationale}{%
\section{The rationale}\label{the-rationale}}

The basic idea behind SVMs is best illustrated by considering a simple case: a set of data points that belong to one of two classes, red and blue, as illustrated in figure 1 below. To make things simpler still, I have assumed that the boundary separating the two classes is a straight line, represented by the solid green line in the diagram. In the technical literature, such datasets are called linearly separable.

\begin{center}\includegraphics[width=0.7\linewidth]{/home/datascience/repos/machine-learning-rsuite/import/assets/svm-fig-1} \end{center}

In the linearly separable case, there is usually a fair amount of freedom in the way a separating line can be drawn. Figure 2 illustrates this point: the two broken green lines are also valid separation boundaries. Indeed, because there is a non-zero distance between the two closest points between categories, there are an infinite number of possible separation lines. This, quite naturally, raises the question as to whether it is possible to choose a separation boundary that is optimal.

\begin{center}\includegraphics[width=0.7\linewidth]{/home/datascience/repos/machine-learning-rsuite/import/assets/svm-fig-2} \end{center}

The short answer is, yes there is. One way to do this is to select a boundary line that maximises the margin, i.e.~the distance between the separation boundary and the points that are closest to it. Such an optimal boundary is illustrated by the black brace in Figure 3. The really cool thing about this criterion is that the location of the separation boundary depends only on the points that are closest to it. This means, unlike other classification methods, the classifier does not depend on any other points in dataset. The directed lines between the boundary and the closest points on either side are called support vectors (these are the solid black lines in figure 3). A direct implication of this is that the fewer the support vectors, the better the generalizability of the boundary.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{/home/datascience/repos/machine-learning-rsuite/import/assets/svm-fig-3} 

}

\caption{Figure 3}\label{fig:unnamed-chunk-4}
\end{figure}

Although the above sounds great, it is of limited practical value because real data sets are seldom (if ever) linearly separable.

So, what can we do when dealing with real (i.e.~non linearly separable) data sets?

A simple approach to tackle small deviations from linear separability is to allow a small number of points (those that are close to the boundary) to be misclassified. The number of possible misclassifications is governed by a free parameter C, which is called the cost. The cost is essentially the penalty associated with making an error: the higher the value of C, the less likely it is that the algorithm will misclassify a point.

This approach -- which is called soft margin classification -- is illustrated in Figure 4. Note the points on the wrong side of the separation boundary. We will demonstrate soft margin SVMs in the next section. (Note: At the risk of belabouring the obvious, the purely linearly separable case discussed in the previous para is simply is a special case of the soft margin classifier.)

\begin{center}\includegraphics[width=0.7\linewidth]{/home/datascience/repos/machine-learning-rsuite/import/assets/svm-fig-4} \end{center}

Real life situations are much more complex and cannot be dealt with using soft margin classifiers. For example, as shown in Figure 5, one could have widely separated clusters of points that belong to the same classes. Such situations, which require the use of multiple (and nonlinear) boundaries, can sometimes be dealt with using a clever approach called the kernel trick.

\begin{center}\includegraphics[width=0.7\linewidth]{/home/datascience/repos/machine-learning-rsuite/import/assets/svm-fig-5} \end{center}

\hypertarget{the-kernel-trick}{%
\section{The kernel trick}\label{the-kernel-trick}}

Recall that in the linearly separable (or soft margin) case, the SVM algorithm works by finding a separation boundary that maximises the margin, which is the distance between the boundary and the points closest to it. The distance here is the usual straight line distance between the boundary and the closest point(s). This is called the Euclidean distance in honour of the great geometer of antiquity. The point to note is that this process results in a separation boundary that is a straight line, which as Figure 5 illustrates, does not always work. In fact in most cases it won't.

So what can we do? To answer this question, we have to take a bit of a detour\ldots{}

What if we were able to generalize the notion of distance in a way that generates nonlinear separation boundaries? It turns out that this is possible. To see how, one has to first understand how the notion of distance can be generalized.

The key properties that any measure of distance must satisfy are:

\begin{verbatim}
Non-negativity  a distance cannot be negative, a point that needs no further explanation I reckon 
Symmetry  that is, the distance between point A and point B is the same as the distance between point B and point A.
Identity the distance between a point and itself is zero.
Triangle inequality  that is the sum of distances between point A and B and points B and C must be less than or equal to the distance between A and C (equality holds only if all three points lie along the same line).
\end{verbatim}

Any mathematical object that displays the above properties is akin to a distance. Such generalized distances are called metrics and the mathematical space in which they live is called a metric space. Metrics are defined using special mathematical functions designed to satisfy the above conditions. These functions are known as kernels.

The essence of the kernel trick lies in mapping the classification problem to a metric space in which the problem is rendered separable via a separation boundary that is simple in the new space, but complex -- as it has to be -- in the original one. Generally, the transformed space has a higher dimensionality, with each of the dimensions being (possibly complex) combinations of the original problem variables. However, this is not necessarily a problem because in practice one doesn't actually mess around with transformations, one just tries different kernels (the transformation being implicit in the kernel) and sees which one does the job. The check is simple: we simply test the predictions resulting from using different kernels against a held out subset of the data (as one would for any machine learning algorithm).

It turns out that a particular function -- called the radial basis function kernel (RBF kernel) -- is very effective in many cases. The RBF kernel is essentially a Gaussian (or Normal) function with the Euclidean distance between pairs of points as the variable (see equation 1 below). The basic rationale behind the RBF kernel is that it creates separation boundaries that it tends to classify points close together (in the Euclidean sense) in the original space in the same way. This is reflected in the fact that the kernel decays (i.e.~drops off to zero) as the Euclidean distance between points increases.

The rate at which a kernel decays is governed by the parameter \(\gamma\) -- the higher the value of \(\gamma\), the more rapid the decay. This serves to illustrate that the RBF kernel is extremely flexible\ldots{}.but the flexibility comes at a price -- the danger of overfitting for large values of \(\gamma\) . One should choose appropriate values of C and \(\gamma\) so as to ensure that the resulting kernel represents the best possible balance between flexibility and accuracy. We'll discuss how this is done in practice later in this article.

Finally, though it is probably obvious, it is worth mentioning that the separation boundaries for arbitrary kernels are also defined through support vectors as in Figure 3. To reiterate a point made earlier, this means that a solution that has fewer support vectors is likely to be more robust than one with many. Why? Because the data points defining support vectors are ones that are most sensitive to noise- therefore the fewer, the better.

There are many other types of kernels, each with their own pros and cons. However, I'll leave these for adventurous readers to explore by themselves. Finally, for a much more detailed\ldots{}.and dare I say, better\ldots{} explanation of the kernel trick, I highly recommend this article by Eric Kim.

\hypertarget{support-vector-machines-in-r}{%
\section{Support vector machines in R}\label{support-vector-machines-in-r}}

In this demo we'll use the svm interface that is implemented in the \texttt{e1071} R package. This interface provides R programmers access to the comprehensive \texttt{libsvm} library written by Chang and Lin. I'll use two toy datasets: the famous iris dataset available with the base R package and the sonar dataset from the mlbench package. I won't describe details of the datasets as they are discussed at length in the documentation that I have linked to. However, it is worth mentioning the reasons why I chose these datasets:

As mentioned earlier, no real life dataset is linearly separable, but the iris dataset is almost so. Consequently, it is a good illustration of using linear SVMs. Although one almost never uses these in practice, I have illustrated their use primarily for pedagogical reasons.
The sonar dataset is a good illustration of the benefits of using RBF kernels in cases where the dataset is hard to visualise (60 variables in this case!). In general, one would almost always use RBF (or other nonlinear) kernels in practice.

With that said, let's get right to it. I assume you have R and RStudio installed. For instructions on how to do this, have a look at the first article in this series. The processing preliminaries -- loading libraries, data and creating training and test datasets are much the same as in my previous articles so I won't dwell on these here. For completeness, however, I'll list all the code so you can run it directly in R or R studio (a complete listing of the code can be found here):

\hypertarget{svm-on-iris-dataset}{%
\section{\texorpdfstring{SVM on \texttt{iris} dataset}{SVM on iris dataset}}\label{svm-on-iris-dataset}}

\hypertarget{training-and-test-datasets}{%
\subsection{Training and test datasets}\label{training-and-test-datasets}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#load required library}
\KeywordTok{library}\NormalTok{(e1071)}

\CommentTok{#load built-in iris dataset}
\KeywordTok{data}\NormalTok{(iris)}

\CommentTok{#set seed to ensure reproducible results}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{42}\NormalTok{)}

\CommentTok{#split into training and test sets}
\NormalTok{iris[, }\StringTok{"train"}\NormalTok{] <-}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(}\KeywordTok{runif}\NormalTok{(}\KeywordTok{nrow}\NormalTok{(iris)) }\OperatorTok{<}\StringTok{ }\FloatTok{0.8}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)}

\CommentTok{#separate training and test sets}
\NormalTok{trainset <-}\StringTok{ }\NormalTok{iris[iris}\OperatorTok{$}\NormalTok{train }\OperatorTok{==}\StringTok{ }\DecValTok{1}\NormalTok{,]}
\NormalTok{testset <-}\StringTok{ }\NormalTok{iris[iris}\OperatorTok{$}\NormalTok{train }\OperatorTok{==}\StringTok{ }\DecValTok{0}\NormalTok{,]}

\CommentTok{#get column index of train flag}
\NormalTok{trainColNum <-}\StringTok{ }\KeywordTok{grep}\NormalTok{(}\StringTok{"train"}\NormalTok{, }\KeywordTok{names}\NormalTok{(trainset))}

\CommentTok{#remove train flag column from train and test sets}
\NormalTok{trainset <-}\StringTok{ }\NormalTok{trainset[,}\OperatorTok{-}\NormalTok{trainColNum]}
\NormalTok{testset <-}\StringTok{ }\NormalTok{testset[,}\OperatorTok{-}\NormalTok{trainColNum]}

\KeywordTok{dim}\NormalTok{(trainset)}
\CommentTok{#> [1] 115   5}
\KeywordTok{dim}\NormalTok{(testset)}
\CommentTok{#> [1] 35  5}
\end{Highlighting}
\end{Shaded}

\hypertarget{build-the-svm-model}{%
\subsection{Build the SVM model}\label{build-the-svm-model}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#get column index of predicted variable in dataset}
\NormalTok{typeColNum <-}\StringTok{ }\KeywordTok{grep}\NormalTok{(}\StringTok{"Species"}\NormalTok{, }\KeywordTok{names}\NormalTok{(iris))}

\CommentTok{#build model  linear kernel and C-classification (soft margin) with default cost (C=1)}
\NormalTok{svm_model <-}\StringTok{ }\KeywordTok{svm}\NormalTok{(Species}\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ trainset, }
                 \DataTypeTok{method =} \StringTok{"C-classification"}\NormalTok{, }
                 \DataTypeTok{kernel =} \StringTok{"linear"}\NormalTok{)}
\NormalTok{svm_model}
\CommentTok{#> }
\CommentTok{#> Call:}
\CommentTok{#> svm(formula = Species ~ ., data = trainset, method = "C-classification", }
\CommentTok{#>     kernel = "linear")}
\CommentTok{#> }
\CommentTok{#> }
\CommentTok{#> Parameters:}
\CommentTok{#>    SVM-Type:  C-classification }
\CommentTok{#>  SVM-Kernel:  linear }
\CommentTok{#>        cost:  1 }
\CommentTok{#>       gamma:  0.25 }
\CommentTok{#> }
\CommentTok{#> Number of Support Vectors:  24}
\end{Highlighting}
\end{Shaded}

The output from the SVM model show that there are 24 support vectors. If desired, these can be examined using the SV variable in the model -- i.e via svm\_model\$SV.

\hypertarget{support-vectors}{%
\subsection{Support Vectors}\label{support-vectors}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# support vectors}
\NormalTok{svm_model}\OperatorTok{$}\NormalTok{SV}
\CommentTok{#>     Sepal.Length Sepal.Width Petal.Length Petal.Width}
\CommentTok{#> 19       -0.2564      1.7668       -1.323      -1.305}
\CommentTok{#> 42       -1.7006     -1.7045       -1.559      -1.305}
\CommentTok{#> 45       -0.9785      1.7668       -1.205      -1.171}
\CommentTok{#> 53        1.1878      0.1469        0.568       0.309}
\CommentTok{#> 55        0.7064     -0.5474        0.390       0.309}
\CommentTok{#> 57        0.4657      0.6097        0.450       0.443}
\CommentTok{#> 58       -1.2192     -1.4730       -0.378      -0.364}
\CommentTok{#> 69        0.3453     -1.9359        0.331       0.309}
\CommentTok{#> 71       -0.0157      0.3783        0.509       0.712}
\CommentTok{#> 73        0.4657     -1.2416        0.568       0.309}
\CommentTok{#> 78        0.9471     -0.0845        0.627       0.578}
\CommentTok{#> 84        0.1046     -0.7788        0.686       0.443}
\CommentTok{#> 85       -0.6174     -0.0845        0.331       0.309}
\CommentTok{#> 86        0.1046      0.8412        0.331       0.443}
\CommentTok{#> 99       -0.9785     -1.2416       -0.555      -0.229}
\CommentTok{#> 107      -1.2192     -1.2416        0.331       0.578}
\CommentTok{#> 111       0.7064      0.3783        0.686       0.981}
\CommentTok{#> 117       0.7064     -0.0845        0.922       0.712}
\CommentTok{#> 124       0.4657     -0.7788        0.568       0.712}
\CommentTok{#> 130       1.5488     -0.0845        1.099       0.443}
\CommentTok{#> 138       0.5860      0.1469        0.922       0.712}
\CommentTok{#> 139       0.1046     -0.0845        0.509       0.712}
\CommentTok{#> 147       0.4657     -1.2416        0.627       0.847}
\CommentTok{#> 150      -0.0157     -0.0845        0.686       0.712}
\end{Highlighting}
\end{Shaded}

The test prediction accuracy indicates that the linear performs quite well on this dataset, confirming that it is indeed near linearly separable. To check performance by class, one can create a confusion matrix as described in my post on random forests. I'll leave this as an exercise for you. Another point is that we have used a soft-margin classification scheme with a cost C=1. You can experiment with this by explicitly changing the value of C. Again, I'll leave this for you an exercise.

\hypertarget{predictions-on-training-model}{%
\subsection{Predictions on training model}\label{predictions-on-training-model}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# training set predictions}
\NormalTok{pred_train <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(svm_model, trainset)}
\KeywordTok{mean}\NormalTok{(pred_train }\OperatorTok{==}\StringTok{ }\NormalTok{trainset}\OperatorTok{$}\NormalTok{Species)}
\CommentTok{#> [1] 0.983}
\CommentTok{# [1] 0.9826087}
\end{Highlighting}
\end{Shaded}

\hypertarget{predictions-on-test-model}{%
\subsection{Predictions on test model}\label{predictions-on-test-model}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# test set predictions}
\NormalTok{pred_test <-}\KeywordTok{predict}\NormalTok{(svm_model, testset)}
\KeywordTok{mean}\NormalTok{(pred_test }\OperatorTok{==}\StringTok{ }\NormalTok{testset}\OperatorTok{$}\NormalTok{Species)}
\CommentTok{#> [1] 0.914}
\CommentTok{# [1] 0.9142857}
\end{Highlighting}
\end{Shaded}

\hypertarget{confusion-matrix-and-accuracy}{%
\subsection{Confusion matrix and Accuracy}\label{confusion-matrix-and-accuracy}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# confusion matrix}
\NormalTok{cm <-}\StringTok{ }\KeywordTok{table}\NormalTok{(pred_test, testset}\OperatorTok{$}\NormalTok{Species)}
\NormalTok{cm}
\CommentTok{#>             }
\CommentTok{#> pred_test    setosa versicolor virginica}
\CommentTok{#>   setosa         18          0         0}
\CommentTok{#>   versicolor      0          5         3}
\CommentTok{#>   virginica       0          0         9}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# accuracy}
\KeywordTok{sum}\NormalTok{(}\KeywordTok{diag}\NormalTok{(cm)) }\OperatorTok{/}\StringTok{ }\KeywordTok{sum}\NormalTok{(cm)}
\CommentTok{#> [1] 0.914}
\end{Highlighting}
\end{Shaded}

\hypertarget{svm-with-radial-basis-function-kernel.-linear}{%
\section{SVM with Radial Basis Function kernel. Linear}\label{svm-with-radial-basis-function-kernel.-linear}}

\hypertarget{training-and-test-sets}{%
\subsection{Training and test sets}\label{training-and-test-sets}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#load required library (assuming e1071 is already loaded)}
\KeywordTok{library}\NormalTok{(mlbench)}

\CommentTok{#load Sonar dataset}
\KeywordTok{data}\NormalTok{(Sonar)}
\CommentTok{#set seed to ensure reproducible results}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{42}\NormalTok{)}
\CommentTok{#split into training and test sets}
\NormalTok{Sonar[, }\StringTok{"train"}\NormalTok{] <-}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(}\KeywordTok{runif}\NormalTok{(}\KeywordTok{nrow}\NormalTok{(Sonar))}\OperatorTok{<}\FloatTok{0.8}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{)}

\CommentTok{#separate training and test sets}
\NormalTok{trainset <-}\StringTok{ }\NormalTok{Sonar[Sonar}\OperatorTok{$}\NormalTok{train}\OperatorTok{==}\DecValTok{1}\NormalTok{,]}
\NormalTok{testset <-}\StringTok{ }\NormalTok{Sonar[Sonar}\OperatorTok{$}\NormalTok{train}\OperatorTok{==}\DecValTok{0}\NormalTok{,]}

\CommentTok{#get column index of train flag}
\NormalTok{trainColNum <-}\StringTok{ }\KeywordTok{grep}\NormalTok{(}\StringTok{"train"}\NormalTok{,}\KeywordTok{names}\NormalTok{(trainset))}
\CommentTok{#remove train flag column from train and test sets}
\NormalTok{trainset <-}\StringTok{ }\NormalTok{trainset[,}\OperatorTok{-}\NormalTok{trainColNum]}
\NormalTok{testset <-}\StringTok{ }\NormalTok{testset[,}\OperatorTok{-}\NormalTok{trainColNum]}

\CommentTok{#get column index of predicted variable in dataset}
\NormalTok{typeColNum <-}\StringTok{ }\KeywordTok{grep}\NormalTok{(}\StringTok{"Class"}\NormalTok{,}\KeywordTok{names}\NormalTok{(Sonar))}
\end{Highlighting}
\end{Shaded}

\hypertarget{predictions-on-training-model-1}{%
\subsection{Predictions on Training model}\label{predictions-on-training-model-1}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#build model  linear kernel and C-classification with default cost (C=1)}
\NormalTok{svm_model <-}\StringTok{ }\KeywordTok{svm}\NormalTok{(Class}\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data=}\NormalTok{trainset, }
                 \DataTypeTok{method=}\StringTok{"C-classification"}\NormalTok{, }
                 \DataTypeTok{kernel=}\StringTok{"linear"}\NormalTok{)}

\CommentTok{#training set predictions}
\NormalTok{pred_train <-}\KeywordTok{predict}\NormalTok{(svm_model,trainset)}
\KeywordTok{mean}\NormalTok{(pred_train}\OperatorTok{==}\NormalTok{trainset}\OperatorTok{$}\NormalTok{Class)}
\CommentTok{#> [1] 0.97}
\end{Highlighting}
\end{Shaded}

\hypertarget{predictions-on-test-model-1}{%
\subsection{Predictions on test model}\label{predictions-on-test-model-1}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#test set predictions}
\NormalTok{pred_test <-}\KeywordTok{predict}\NormalTok{(svm_model,testset)}
\KeywordTok{mean}\NormalTok{(pred_test}\OperatorTok{==}\NormalTok{testset}\OperatorTok{$}\NormalTok{Class)}
\CommentTok{#> [1] 0.605}
\end{Highlighting}
\end{Shaded}

I'll leave you to examine the contents of the model. The important point to note here is that the performance of the model with the test set is quite dismal compared to the previous case. This simply indicates that the linear kernel is not appropriate here. Let's take a look at what happens if we use the RBF kernel with default values for the parameters:

\hypertarget{svm-with-radial-basis-function-kernel.-non-linear}{%
\section{SVM with Radial Basis Function kernel. Non-linear}\label{svm-with-radial-basis-function-kernel.-non-linear}}

\hypertarget{predictions-on-training-model-2}{%
\subsection{Predictions on training model}\label{predictions-on-training-model-2}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#build model: radial kernel, default params}
\NormalTok{svm_model <-}\StringTok{ }\KeywordTok{svm}\NormalTok{(Class}\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data=}\NormalTok{trainset, }
                 \DataTypeTok{method=}\StringTok{"C-classification"}\NormalTok{, }
                 \DataTypeTok{kernel=}\StringTok{"radial"}\NormalTok{)}
\CommentTok{# print params}
\NormalTok{svm_model}\OperatorTok{$}\NormalTok{cost}
\CommentTok{#> [1] 1}
\NormalTok{svm_model}\OperatorTok{$}\NormalTok{gamma}
\CommentTok{#> [1] 0.0167}

\CommentTok{#training set predictions}
\NormalTok{pred_train <-}\KeywordTok{predict}\NormalTok{(svm_model,trainset)}
\KeywordTok{mean}\NormalTok{(pred_train}\OperatorTok{==}\NormalTok{trainset}\OperatorTok{$}\NormalTok{Class)}
\CommentTok{#> [1] 0.988}
\end{Highlighting}
\end{Shaded}

\hypertarget{predictions-on-test-model-2}{%
\subsection{Predictions on test model}\label{predictions-on-test-model-2}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#test set predictions}
\NormalTok{pred_test <-}\KeywordTok{predict}\NormalTok{(svm_model,testset)}
\KeywordTok{mean}\NormalTok{(pred_test}\OperatorTok{==}\NormalTok{testset}\OperatorTok{$}\NormalTok{Class)}
\CommentTok{#> [1] 0.767}
\end{Highlighting}
\end{Shaded}

That's a pretty decent improvement from the linear kernel. Let's see if we can do better by doing some parameter tuning. To do this we first invoke tune.svm and use the parameters it gives us in the call to svm:

\hypertarget{tuning-of-parameters}{%
\subsection{Tuning of parameters}\label{tuning-of-parameters}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# find optimal parameters in a specified range}
\NormalTok{tune_out <-}\StringTok{ }\KeywordTok{tune.svm}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ trainset[,}\OperatorTok{-}\NormalTok{typeColNum], }
                     \DataTypeTok{y =}\NormalTok{ trainset[, typeColNum], }
                     \DataTypeTok{gamma =} \DecValTok{10}\OperatorTok{^}\NormalTok{(}\OperatorTok{-}\DecValTok{3}\OperatorTok{:}\DecValTok{3}\NormalTok{), }
                     \DataTypeTok{cost =} \KeywordTok{c}\NormalTok{(}\FloatTok{0.01}\NormalTok{, }\FloatTok{0.1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{100}\NormalTok{, }\DecValTok{1000}\NormalTok{), }
                     \DataTypeTok{kernel =} \StringTok{"radial"}\NormalTok{)}

\CommentTok{#print best values of cost and gamma}
\NormalTok{tune_out}\OperatorTok{$}\NormalTok{best.parameters}\OperatorTok{$}\NormalTok{cost}
\CommentTok{#> [1] 10}
\NormalTok{tune_out}\OperatorTok{$}\NormalTok{best.parameters}\OperatorTok{$}\NormalTok{gamma}
\CommentTok{#> [1] 0.01}

\CommentTok{#build model}
\NormalTok{svm_model <-}\StringTok{ }\KeywordTok{svm}\NormalTok{(Class}\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ trainset, }
                 \DataTypeTok{method =} \StringTok{"C-classification"}\NormalTok{, }
                 \DataTypeTok{kernel =} \StringTok{"radial"}\NormalTok{, }
                 \DataTypeTok{cost =}\NormalTok{ tune_out}\OperatorTok{$}\NormalTok{best.parameters}\OperatorTok{$}\NormalTok{cost, }
                 \DataTypeTok{gamma =}\NormalTok{ tune_out}\OperatorTok{$}\NormalTok{best.parameters}\OperatorTok{$}\NormalTok{gamma)}
\end{Highlighting}
\end{Shaded}

\hypertarget{prediction-on-training-model-with-new-parameters}{%
\subsection{Prediction on training model with new parameters}\label{prediction-on-training-model-with-new-parameters}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# training set predictions}
\NormalTok{pred_train <-}\KeywordTok{predict}\NormalTok{(svm_model,trainset)}
\KeywordTok{mean}\NormalTok{(pred_train}\OperatorTok{==}\NormalTok{trainset}\OperatorTok{$}\NormalTok{Class)}
\CommentTok{#> [1] 1}
\end{Highlighting}
\end{Shaded}

\hypertarget{prediction-on-test-model-with-new-parameters}{%
\subsection{Prediction on test model with new parameters}\label{prediction-on-test-model-with-new-parameters}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# test set predictions}
\NormalTok{pred_test <-}\KeywordTok{predict}\NormalTok{(svm_model,testset)}
\KeywordTok{mean}\NormalTok{(pred_test}\OperatorTok{==}\NormalTok{testset}\OperatorTok{$}\NormalTok{Class)}
\CommentTok{#> [1] 0.814}
\end{Highlighting}
\end{Shaded}

Which is fairly decent improvement on the un-optimised case.

\hypertarget{wrapping-up}{%
\section{Wrapping up}\label{wrapping-up}}

This bring us to the end of this introductory exploration of SVMs in R. To recap, the distinguishing feature of SVMs in contrast to most other techniques is that they attempt to construct optimal separation boundaries between different categories.

SVMs are quite versatile and have been applied to a wide variety of domains ranging from chemistry to pattern recognition. They are best used in binary classification scenarios. This brings up a question as to where SVMs are to be preferred to other binary classification techniques such as logistic regression. The honest response is, ``it depends'' -- but here are some points to keep in mind when choosing between the two. A general point to keep in mind is that SVM algorithms tend to be expensive both in terms of memory and computation, issues that can start to hurt as the size of the dataset increases.

Given all the above caveats and considerations, the best way to figure out whether an SVM approach will work for your problem may be to do what most machine learning practitioners do: try it out!

\hypertarget{classification-with-svm.-social-network-dataset}{%
\chapter{Classification with SVM. Social Network dataset}\label{classification-with-svm.-social-network-dataset}}

\hypertarget{introduction-7}{%
\section{Introduction}\label{introduction-7}}

\textbf{Source}: \url{https://www.geeksforgeeks.org/classifying-data-using-support-vector-machinessvms-in-r/}

\hypertarget{data-operations}{%
\section{Data Operations}\label{data-operations}}

\hypertarget{load-libraries}{%
\subsection{Load libraries}\label{load-libraries}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load packages}
\KeywordTok{library}\NormalTok{(dplyr)}
\KeywordTok{library}\NormalTok{(caTools) }
\KeywordTok{library}\NormalTok{(e1071) }
\KeywordTok{library}\NormalTok{(ElemStatLearn) }
\end{Highlighting}
\end{Shaded}

\hypertarget{importing-dataset}{%
\subsection{Importing dataset}\label{importing-dataset}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Importing the dataset }
\NormalTok{dataset =}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\KeywordTok{file.path}\NormalTok{(data_raw_dir, }\StringTok{'Social_Network_Ads.csv'}\NormalTok{)) }
\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{glimpse}\NormalTok{(dataset)}
\CommentTok{#> Observations: 400}
\CommentTok{#> Variables: 5}
\CommentTok{#> $ User.ID         <int> 15624510, 15810944, 15668575, 15603246, 158040...}
\CommentTok{#> $ Gender          <fct> Male, Male, Female, Female, Male, Male, Female...}
\CommentTok{#> $ Age             <int> 19, 35, 26, 27, 19, 27, 27, 32, 25, 35, 26, 26...}
\CommentTok{#> $ EstimatedSalary <int> 19000, 20000, 43000, 57000, 76000, 58000, 8400...}
\CommentTok{#> $ Purchased       <int> 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0...}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tibble}\OperatorTok{::}\KeywordTok{as_tibble}\NormalTok{(dataset)}
\CommentTok{#> # A tibble: 400 x 5}
\CommentTok{#>    User.ID Gender   Age EstimatedSalary Purchased}
\CommentTok{#>      <int> <fct>  <int>           <int>     <int>}
\CommentTok{#> 1 15624510 Male      19           19000         0}
\CommentTok{#> 2 15810944 Male      35           20000         0}
\CommentTok{#> 3 15668575 Female    26           43000         0}
\CommentTok{#> 4 15603246 Female    27           57000         0}
\CommentTok{#> 5 15804002 Male      19           76000         0}
\CommentTok{#> 6 15728773 Male      27           58000         0}
\CommentTok{#> # ... with 394 more rows}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Taking columns 3-5 }
\NormalTok{dataset =}\StringTok{ }\NormalTok{dataset[}\DecValTok{3}\OperatorTok{:}\DecValTok{5}\NormalTok{]}
\NormalTok{tibble}\OperatorTok{::}\KeywordTok{as_tibble}\NormalTok{(dataset)}
\CommentTok{#> # A tibble: 400 x 3}
\CommentTok{#>     Age EstimatedSalary Purchased}
\CommentTok{#>   <int>           <int>     <int>}
\CommentTok{#> 1    19           19000         0}
\CommentTok{#> 2    35           20000         0}
\CommentTok{#> 3    26           43000         0}
\CommentTok{#> 4    27           57000         0}
\CommentTok{#> 5    19           76000         0}
\CommentTok{#> 6    27           58000         0}
\CommentTok{#> # ... with 394 more rows}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Encoding the target feature as factor }
\NormalTok{dataset}\OperatorTok{$}\NormalTok{Purchased =}\StringTok{ }\KeywordTok{factor}\NormalTok{(dataset}\OperatorTok{$}\NormalTok{Purchased, }\DataTypeTok{levels =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)) }
\KeywordTok{str}\NormalTok{(dataset)}
\CommentTok{#> 'data.frame':    400 obs. of  3 variables:}
\CommentTok{#>  $ Age            : int  19 35 26 27 19 27 27 32 25 35 ...}
\CommentTok{#>  $ EstimatedSalary: int  19000 20000 43000 57000 76000 58000 84000 150000 33000 65000 ...}
\CommentTok{#>  $ Purchased      : Factor w/ 2 levels "0","1": 1 1 1 1 1 1 1 2 1 1 ...}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Splitting the dataset into the Training set and Test set }
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{) }
\NormalTok{split =}\StringTok{ }\KeywordTok{sample.split}\NormalTok{(dataset}\OperatorTok{$}\NormalTok{Purchased, }\DataTypeTok{SplitRatio =} \FloatTok{0.75}\NormalTok{) }
  
\NormalTok{training_set =}\StringTok{ }\KeywordTok{subset}\NormalTok{(dataset, split }\OperatorTok{==}\StringTok{ }\OtherTok{TRUE}\NormalTok{) }
\NormalTok{test_set =}\StringTok{ }\KeywordTok{subset}\NormalTok{(dataset, split }\OperatorTok{==}\StringTok{ }\OtherTok{FALSE}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{dim}\NormalTok{(training_set)}
\CommentTok{#> [1] 300   3}
\KeywordTok{dim}\NormalTok{(test_set)}
\CommentTok{#> [1] 100   3}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Feature Scaling }
\NormalTok{training_set[}\OperatorTok{-}\DecValTok{3}\NormalTok{] =}\StringTok{ }\KeywordTok{scale}\NormalTok{(training_set[}\OperatorTok{-}\DecValTok{3}\NormalTok{]) }
\NormalTok{test_set[}\OperatorTok{-}\DecValTok{3}\NormalTok{] =}\StringTok{ }\KeywordTok{scale}\NormalTok{(test_set[}\OperatorTok{-}\DecValTok{3}\NormalTok{]) }
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Fitting SVM to the Training set }
\NormalTok{classifier =}\StringTok{ }\KeywordTok{svm}\NormalTok{(}\DataTypeTok{formula =}\NormalTok{ Purchased }\OperatorTok{~}\StringTok{ }\NormalTok{., }
                 \DataTypeTok{data =}\NormalTok{ training_set, }
                 \DataTypeTok{type =} \StringTok{'C-classification'}\NormalTok{, }
                 \DataTypeTok{kernel =} \StringTok{'linear'}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{classifier}
\CommentTok{#> }
\CommentTok{#> Call:}
\CommentTok{#> svm(formula = Purchased ~ ., data = training_set, type = "C-classification", }
\CommentTok{#>     kernel = "linear")}
\CommentTok{#> }
\CommentTok{#> }
\CommentTok{#> Parameters:}
\CommentTok{#>    SVM-Type:  C-classification }
\CommentTok{#>  SVM-Kernel:  linear }
\CommentTok{#>        cost:  1 }
\CommentTok{#>       gamma:  0.5 }
\CommentTok{#> }
\CommentTok{#> Number of Support Vectors:  116}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(classifier)}
\CommentTok{#> }
\CommentTok{#> Call:}
\CommentTok{#> svm(formula = Purchased ~ ., data = training_set, type = "C-classification", }
\CommentTok{#>     kernel = "linear")}
\CommentTok{#> }
\CommentTok{#> }
\CommentTok{#> Parameters:}
\CommentTok{#>    SVM-Type:  C-classification }
\CommentTok{#>  SVM-Kernel:  linear }
\CommentTok{#>        cost:  1 }
\CommentTok{#>       gamma:  0.5 }
\CommentTok{#> }
\CommentTok{#> Number of Support Vectors:  116}
\CommentTok{#> }
\CommentTok{#>  ( 58 58 )}
\CommentTok{#> }
\CommentTok{#> }
\CommentTok{#> Number of Classes:  2 }
\CommentTok{#> }
\CommentTok{#> Levels: }
\CommentTok{#>  0 1}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Predicting the Test set results }
\NormalTok{y_pred =}\StringTok{ }\KeywordTok{predict}\NormalTok{(classifier, }\DataTypeTok{newdata =}\NormalTok{ test_set[}\OperatorTok{-}\DecValTok{3}\NormalTok{]) }
\NormalTok{y_pred}
\CommentTok{#>   2   4   5   9  12  18  19  20  22  29  32  34  35  38  45  46  48  52 }
\CommentTok{#>   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 }
\CommentTok{#>  66  69  74  75  82  84  85  86  87  89 103 104 107 108 109 117 124 126 }
\CommentTok{#>   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0 }
\CommentTok{#> 127 131 134 139 148 154 156 159 162 163 170 175 176 193 199 200 208 213 }
\CommentTok{#>   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   1 }
\CommentTok{#> 224 226 228 229 230 234 236 237 239 241 255 264 265 266 273 274 281 286 }
\CommentTok{#>   1   0   1   0   1   1   1   0   1   1   1   0   1   1   1   1   1   0 }
\CommentTok{#> 292 299 302 305 307 310 316 324 326 332 339 341 343 347 353 363 364 367 }
\CommentTok{#>   1   1   1   0   1   0   0   0   0   1   0   1   0   1   1   0   1   1 }
\CommentTok{#> 368 369 372 373 380 383 389 392 395 400 }
\CommentTok{#>   1   0   1   0   1   1   0   0   0   0 }
\CommentTok{#> Levels: 0 1}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Making the Confusion Matrix }
\NormalTok{cm =}\StringTok{ }\KeywordTok{table}\NormalTok{(test_set[, }\DecValTok{3}\NormalTok{], y_pred) }
\NormalTok{cm}
\CommentTok{#>    y_pred}
\CommentTok{#>      0  1}
\CommentTok{#>   0 57  7}
\CommentTok{#>   1 13 23}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{xtable}\OperatorTok{::}\KeywordTok{xtable}\NormalTok{(cm)}
\CommentTok{#> % latex table generated in R 3.6.0 by xtable 1.8-4 package}
\CommentTok{#> % Fri Sep 20 14:33:21 2019}
\CommentTok{#> \textbackslash{}begin\{table\}[ht]}
\CommentTok{#> \textbackslash{}centering}
\CommentTok{#> \textbackslash{}begin\{tabular\}\{rrr\}}
\CommentTok{#>   \textbackslash{}hline}
\CommentTok{#>  & 0 & 1 \textbackslash{}\textbackslash{} }
\CommentTok{#>   \textbackslash{}hline}
\CommentTok{#> 0 &  57 &   7 \textbackslash{}\textbackslash{} }
\CommentTok{#>   1 &  13 &  23 \textbackslash{}\textbackslash{} }
\CommentTok{#>    \textbackslash{}hline}
\CommentTok{#> \textbackslash{}end\{tabular\}}
\CommentTok{#> \textbackslash{}end\{table\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# installing library ElemStatLearn }
\CommentTok{# library(ElemStatLearn) }
  
\CommentTok{# Plotting the training data set results }
\NormalTok{set =}\StringTok{ }\NormalTok{training_set }
\NormalTok{X1 =}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\KeywordTok{min}\NormalTok{(set[, }\DecValTok{1}\NormalTok{]) }\OperatorTok{-}\StringTok{ }\DecValTok{1}\NormalTok{, }\KeywordTok{max}\NormalTok{(set[, }\DecValTok{1}\NormalTok{]) }\OperatorTok{+}\StringTok{ }\DecValTok{1}\NormalTok{, }\DataTypeTok{by =} \FloatTok{0.01}\NormalTok{) }
\NormalTok{X2 =}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\KeywordTok{min}\NormalTok{(set[, }\DecValTok{2}\NormalTok{]) }\OperatorTok{-}\StringTok{ }\DecValTok{1}\NormalTok{, }\KeywordTok{max}\NormalTok{(set[, }\DecValTok{2}\NormalTok{]) }\OperatorTok{+}\StringTok{ }\DecValTok{1}\NormalTok{, }\DataTypeTok{by =} \FloatTok{0.01}\NormalTok{) }
  
\NormalTok{grid_set =}\StringTok{ }\KeywordTok{expand.grid}\NormalTok{(X1, X2) }
\KeywordTok{colnames}\NormalTok{(grid_set) =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{'Age'}\NormalTok{, }\StringTok{'EstimatedSalary'}\NormalTok{) }
\NormalTok{y_grid =}\StringTok{ }\KeywordTok{predict}\NormalTok{(classifier, }\DataTypeTok{newdata =}\NormalTok{ grid_set) }
  
\KeywordTok{plot}\NormalTok{(set[, }\DecValTok{-3}\NormalTok{], }
     \DataTypeTok{main =} \StringTok{'SVM (Training set)'}\NormalTok{, }
     \DataTypeTok{xlab =} \StringTok{'Age'}\NormalTok{, }\DataTypeTok{ylab =} \StringTok{'Estimated Salary'}\NormalTok{, }
     \DataTypeTok{xlim =} \KeywordTok{range}\NormalTok{(X1), }\DataTypeTok{ylim =} \KeywordTok{range}\NormalTok{(X2)) }
  
\KeywordTok{contour}\NormalTok{(X1, X2, }\KeywordTok{matrix}\NormalTok{(}\KeywordTok{as.numeric}\NormalTok{(y_grid), }\KeywordTok{length}\NormalTok{(X1), }\KeywordTok{length}\NormalTok{(X2)), }\DataTypeTok{add =} \OtherTok{TRUE}\NormalTok{) }
  
\KeywordTok{points}\NormalTok{(grid_set, }\DataTypeTok{pch =} \StringTok{'.'}\NormalTok{, }\DataTypeTok{col =} \KeywordTok{ifelse}\NormalTok{(y_grid }\OperatorTok{==}\StringTok{ }\DecValTok{1}\NormalTok{, }\StringTok{'coral1'}\NormalTok{, }\StringTok{'aquamarine'}\NormalTok{)) }
  
\KeywordTok{points}\NormalTok{(set, }\DataTypeTok{pch =} \DecValTok{21}\NormalTok{, }\DataTypeTok{bg =} \KeywordTok{ifelse}\NormalTok{(set[, }\DecValTok{3}\NormalTok{] }\OperatorTok{==}\StringTok{ }\DecValTok{1}\NormalTok{, }\StringTok{'green4'}\NormalTok{, }\StringTok{'red3'}\NormalTok{)) }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_112-social_networks-SVM_files/figure-latex/plot_training_set-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{set =}\StringTok{ }\NormalTok{test_set }
\NormalTok{X1 =}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\KeywordTok{min}\NormalTok{(set[, }\DecValTok{1}\NormalTok{]) }\OperatorTok{-}\StringTok{ }\DecValTok{1}\NormalTok{, }\KeywordTok{max}\NormalTok{(set[, }\DecValTok{1}\NormalTok{]) }\OperatorTok{+}\StringTok{ }\DecValTok{1}\NormalTok{, }\DataTypeTok{by =} \FloatTok{0.01}\NormalTok{) }
\NormalTok{X2 =}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\KeywordTok{min}\NormalTok{(set[, }\DecValTok{2}\NormalTok{]) }\OperatorTok{-}\StringTok{ }\DecValTok{1}\NormalTok{, }\KeywordTok{max}\NormalTok{(set[, }\DecValTok{2}\NormalTok{]) }\OperatorTok{+}\StringTok{ }\DecValTok{1}\NormalTok{, }\DataTypeTok{by =} \FloatTok{0.01}\NormalTok{) }
  
\NormalTok{grid_set =}\StringTok{ }\KeywordTok{expand.grid}\NormalTok{(X1, X2) }
\KeywordTok{colnames}\NormalTok{(grid_set) =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{'Age'}\NormalTok{, }\StringTok{'EstimatedSalary'}\NormalTok{) }
\NormalTok{y_grid =}\StringTok{ }\KeywordTok{predict}\NormalTok{(classifier, }\DataTypeTok{newdata =}\NormalTok{ grid_set) }
  
\KeywordTok{plot}\NormalTok{(set[, }\DecValTok{-3}\NormalTok{], }\DataTypeTok{main =} \StringTok{'SVM (Test set)'}\NormalTok{, }
     \DataTypeTok{xlab =} \StringTok{'Age'}\NormalTok{, }\DataTypeTok{ylab =} \StringTok{'Estimated Salary'}\NormalTok{, }
     \DataTypeTok{xlim =} \KeywordTok{range}\NormalTok{(X1), }\DataTypeTok{ylim =} \KeywordTok{range}\NormalTok{(X2)) }
  
\KeywordTok{contour}\NormalTok{(X1, X2, }\KeywordTok{matrix}\NormalTok{(}\KeywordTok{as.numeric}\NormalTok{(y_grid), }\KeywordTok{length}\NormalTok{(X1), }\KeywordTok{length}\NormalTok{(X2)), }\DataTypeTok{add =} \OtherTok{TRUE}\NormalTok{) }
  
\KeywordTok{points}\NormalTok{(grid_set, }\DataTypeTok{pch =} \StringTok{'.'}\NormalTok{, }\DataTypeTok{col =} \KeywordTok{ifelse}\NormalTok{(y_grid }\OperatorTok{==}\StringTok{ }\DecValTok{1}\NormalTok{, }\StringTok{'coral1'}\NormalTok{, }\StringTok{'aquamarine'}\NormalTok{)) }
  
\KeywordTok{points}\NormalTok{(set, }\DataTypeTok{pch =} \DecValTok{21}\NormalTok{, }\DataTypeTok{bg =} \KeywordTok{ifelse}\NormalTok{(set[, }\DecValTok{3}\NormalTok{] }\OperatorTok{==}\StringTok{ }\DecValTok{1}\NormalTok{, }\StringTok{'green4'}\NormalTok{, }\StringTok{'red3'}\NormalTok{)) }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_112-social_networks-SVM_files/figure-latex/unnamed-chunk-16-1} \end{center}

\hypertarget{broad-view-of-svm}{%
\chapter{Broad view of SVM}\label{broad-view-of-svm}}

\hypertarget{introduction-8}{%
\section{Introduction}\label{introduction-8}}

Source: \url{http://uc-r.github.io/svm}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# set pseudorandom number generator}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{10}\NormalTok{)}

\CommentTok{# Attach Packages}
\KeywordTok{library}\NormalTok{(tidyverse)    }\CommentTok{# data manipulation and visualization}
\CommentTok{#> Registered S3 methods overwritten by 'ggplot2':}
\CommentTok{#>   method         from }
\CommentTok{#>   [.quosures     rlang}
\CommentTok{#>   c.quosures     rlang}
\CommentTok{#>   print.quosures rlang}
\CommentTok{#> Registered S3 method overwritten by 'rvest':}
\CommentTok{#>   method            from}
\CommentTok{#>   read_xml.response xml2}
\CommentTok{#> -- Attaching packages --------------------------------- tidyverse 1.2.1 --}
\CommentTok{#> v ggplot2 3.1.1       v purrr   0.3.2  }
\CommentTok{#> v tibble  2.1.1       v dplyr   0.8.0.1}
\CommentTok{#> v tidyr   0.8.3       v stringr 1.4.0  }
\CommentTok{#> v readr   1.3.1       v forcats 0.4.0}
\CommentTok{#> -- Conflicts ------------------------------------ tidyverse_conflicts() --}
\CommentTok{#> x dplyr::filter() masks stats::filter()}
\CommentTok{#> x dplyr::lag()    masks stats::lag()}
\KeywordTok{library}\NormalTok{(kernlab)      }\CommentTok{# SVM methodology}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'kernlab'}
\CommentTok{#> The following object is masked from 'package:purrr':}
\CommentTok{#> }
\CommentTok{#>     cross}
\CommentTok{#> The following object is masked from 'package:ggplot2':}
\CommentTok{#> }
\CommentTok{#>     alpha}
\KeywordTok{library}\NormalTok{(e1071)        }\CommentTok{# SVM methodology}
\KeywordTok{library}\NormalTok{(ISLR)         }\CommentTok{# contains example data set "Khan"}
\KeywordTok{library}\NormalTok{(RColorBrewer) }\CommentTok{# customized coloring of plots}
\end{Highlighting}
\end{Shaded}

The data sets used in the tutorial (with the exception of Khan) will be generated using built-in R commands. The Support Vector Machine methodology is sound for any number of dimensions, but becomes difficult to visualize for more than 2. As previously mentioned, SVMs are robust for any number of classes, but we will stick to no more than 3 for the duration of this tutorial.

\hypertarget{maximal-margin-classifier}{%
\section{Maximal Margin Classifier}\label{maximal-margin-classifier}}

If the classes are separable by a linear boundary, we can use a Maximal Margin Classifier to find the classification boundary. To visualize an example of separated data, we generate 40 random observations and assign them to two classes. Upon visual inspection, we can see that infinitely many lines exist that split the two classes.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Construct sample data set - completely separated}
\NormalTok{x <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\KeywordTok{rnorm}\NormalTok{(}\DecValTok{20}\OperatorTok{*}\DecValTok{2}\NormalTok{), }\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{)}
\NormalTok{y <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\KeywordTok{rep}\NormalTok{(}\OperatorTok{-}\DecValTok{1}\NormalTok{,}\DecValTok{10}\NormalTok{), }\KeywordTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{10}\NormalTok{))}
\NormalTok{x[y}\OperatorTok{==}\DecValTok{1}\NormalTok{,] <-}\StringTok{ }\NormalTok{x[y}\OperatorTok{==}\DecValTok{1}\NormalTok{,] }\OperatorTok{+}\StringTok{ }\DecValTok{3}\OperatorTok{/}\DecValTok{2}
\NormalTok{dat <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{x=}\NormalTok{x, }\DataTypeTok{y=}\KeywordTok{as.factor}\NormalTok{(y))}

\CommentTok{# Plot data}
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ dat, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ x}\FloatTok{.2}\NormalTok{, }\DataTypeTok{y =}\NormalTok{ x}\FloatTok{.1}\NormalTok{, }\DataTypeTok{color =}\NormalTok{ y, }\DataTypeTok{shape =}\NormalTok{ y)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{size =} \DecValTok{2}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_color_manual}\NormalTok{(}\DataTypeTok{values=}\KeywordTok{c}\NormalTok{(}\StringTok{"#000000"}\NormalTok{, }\StringTok{"#FF0000"}\NormalTok{)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{legend.position =} \StringTok{"none"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_113-broad_view-SVM_files/figure-latex/unnamed-chunk-3-1} \end{center}

The goal of the maximal margin classifier is to identify the linear boundary that maximizes the total distance between the line and the closest point in each class. We can use the svm() function in the e1071 package to find this boundary.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Fit Support Vector Machine model to data set}
\NormalTok{svmfit <-}\StringTok{ }\KeywordTok{svm}\NormalTok{(y}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data =}\NormalTok{ dat, }\DataTypeTok{kernel =} \StringTok{"linear"}\NormalTok{, }\DataTypeTok{scale =} \OtherTok{FALSE}\NormalTok{)}
\CommentTok{# Plot Results}
\KeywordTok{plot}\NormalTok{(svmfit, dat)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_113-broad_view-SVM_files/figure-latex/unnamed-chunk-4-1} \end{center}

In the plot, points that are represented by an ``X'' are the support vectors, or the points that directly affect the classification line. The points marked with an ``o'' are the other points, which don't affect the calculation of the line. This principle will lay the foundation for support vector machines. The same plot can be generated using the kernlab package, with the following results:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# fit model and produce plot}
\NormalTok{kernfit <-}\StringTok{ }\KeywordTok{ksvm}\NormalTok{(x, y, }\DataTypeTok{type =} \StringTok{"C-svc"}\NormalTok{, }\DataTypeTok{kernel =} \StringTok{'vanilladot'}\NormalTok{)}
\CommentTok{#>  Setting default kernel parameters}
\KeywordTok{plot}\NormalTok{(kernfit, }\DataTypeTok{data =}\NormalTok{ x)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_113-broad_view-SVM_files/figure-latex/unnamed-chunk-5-1} \end{center}

\texttt{kernlab} shows a little more detail than e1071, showing a color gradient that indicates how confidently a new point would be classified based on its features. Just as in the first plot, the support vectors are marked, in this case as filled-in points, while the classes are denoted by different shapes.

\hypertarget{support-vector-classifiers}{%
\section{Support Vector Classifiers}\label{support-vector-classifiers}}

As convenient as the maximal marginal classifier is to understand, most real data sets will not be fully separable by a linear boundary. To handle such data, we must use modified methodology. We simulate a new data set where the classes are more mixed.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Construct sample data set - not completely separated}
\NormalTok{x <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\KeywordTok{rnorm}\NormalTok{(}\DecValTok{20}\OperatorTok{*}\DecValTok{2}\NormalTok{), }\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{)}
\NormalTok{y <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\KeywordTok{rep}\NormalTok{(}\OperatorTok{-}\DecValTok{1}\NormalTok{,}\DecValTok{10}\NormalTok{), }\KeywordTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{10}\NormalTok{))}
\NormalTok{x[y}\OperatorTok{==}\DecValTok{1}\NormalTok{,] <-}\StringTok{ }\NormalTok{x[y}\OperatorTok{==}\DecValTok{1}\NormalTok{,] }\OperatorTok{+}\StringTok{ }\DecValTok{1}
\NormalTok{dat <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{x=}\NormalTok{x, }\DataTypeTok{y=}\KeywordTok{as.factor}\NormalTok{(y))}

\CommentTok{# Plot data set}
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ dat, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ x}\FloatTok{.2}\NormalTok{, }\DataTypeTok{y =}\NormalTok{ x}\FloatTok{.1}\NormalTok{, }\DataTypeTok{color =}\NormalTok{ y, }\DataTypeTok{shape =}\NormalTok{ y)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{size =} \DecValTok{2}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_color_manual}\NormalTok{(}\DataTypeTok{values=}\KeywordTok{c}\NormalTok{(}\StringTok{"#000000"}\NormalTok{, }\StringTok{"#FF0000"}\NormalTok{)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{legend.position =} \StringTok{"none"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_113-broad_view-SVM_files/figure-latex/unnamed-chunk-6-1} \end{center}

Whether the data is separable or not, the \texttt{svm()} command syntax is the same. In the case of data that is not linearly separable, however, the cost = argument takes on real importance. This quantifies the penalty associated with having an observation on the wrong side of the classification boundary. We can plot the fit in the same way as the completely separable case. We first use \texttt{e1071}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Fit Support Vector Machine model to data set}
\NormalTok{svmfit <-}\StringTok{ }\KeywordTok{svm}\NormalTok{(y}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data =}\NormalTok{ dat, }\DataTypeTok{kernel =} \StringTok{"linear"}\NormalTok{, }\DataTypeTok{cost =} \DecValTok{10}\NormalTok{)}
\CommentTok{# Plot Results}
\KeywordTok{plot}\NormalTok{(svmfit, dat)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_113-broad_view-SVM_files/figure-latex/unnamed-chunk-7-1} \end{center}

By upping the cost of misclassification from 10 to 100, you can see the difference in the classification line. We repeat the process of plotting the SVM using the \texttt{kernlab} package:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Fit Support Vector Machine model to data set}
\NormalTok{kernfit <-}\StringTok{ }\KeywordTok{ksvm}\NormalTok{(x,y, }\DataTypeTok{type =} \StringTok{"C-svc"}\NormalTok{, }\DataTypeTok{kernel =} \StringTok{'vanilladot'}\NormalTok{, }\DataTypeTok{C =} \DecValTok{100}\NormalTok{)}
\CommentTok{#>  Setting default kernel parameters}
\CommentTok{# Plot results}
\KeywordTok{plot}\NormalTok{(kernfit, }\DataTypeTok{data =}\NormalTok{ x)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_113-broad_view-SVM_files/figure-latex/unnamed-chunk-8-1} \end{center}

But how do we decide how costly these misclassifications actually are? Instead of specifying a cost up front, we can use the tune() function from e1071 to test various costs and identify which value produces the best fitting model.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# find optimal cost of misclassification}
\NormalTok{tune.out <-}\StringTok{ }\KeywordTok{tune}\NormalTok{(svm, y}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data =}\NormalTok{ dat, }\DataTypeTok{kernel =} \StringTok{"linear"}\NormalTok{,}
                 \DataTypeTok{ranges =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{cost =} \KeywordTok{c}\NormalTok{(}\FloatTok{0.001}\NormalTok{, }\FloatTok{0.01}\NormalTok{, }\FloatTok{0.1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{100}\NormalTok{)))}
\CommentTok{# extract the best model}
\NormalTok{(bestmod <-}\StringTok{ }\NormalTok{tune.out}\OperatorTok{$}\NormalTok{best.model)}
\CommentTok{#> }
\CommentTok{#> Call:}
\CommentTok{#> best.tune(method = svm, train.x = y ~ ., data = dat, ranges = list(cost = c(0.001, }
\CommentTok{#>     0.01, 0.1, 1, 5, 10, 100)), kernel = "linear")}
\CommentTok{#> }
\CommentTok{#> }
\CommentTok{#> Parameters:}
\CommentTok{#>    SVM-Type:  C-classification }
\CommentTok{#>  SVM-Kernel:  linear }
\CommentTok{#>        cost:  0.1 }
\CommentTok{#>       gamma:  0.5 }
\CommentTok{#> }
\CommentTok{#> Number of Support Vectors:  16}
\end{Highlighting}
\end{Shaded}

For our data set, the optimal cost (from amongst the choices we provided) is calculated to be 0.1, which doesn't penalize the model much for misclassified observations. Once this model has been identified, we can construct a table of predicted classes against true classes using the predict() command as follows:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Create a table of misclassified observations}
\NormalTok{ypred <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(bestmod, dat)}
\NormalTok{(misclass <-}\StringTok{ }\KeywordTok{table}\NormalTok{(}\DataTypeTok{predict =}\NormalTok{ ypred, }\DataTypeTok{truth =}\NormalTok{ dat}\OperatorTok{$}\NormalTok{y))}
\CommentTok{#>        truth}
\CommentTok{#> predict -1 1}
\CommentTok{#>      -1  9 3}
\CommentTok{#>      1   1 7}
\end{Highlighting}
\end{Shaded}

Using this support vector classifier, 80\% of the observations were correctly classified, which matches what we see in the plot. If we wanted to test our classifier more rigorously, we could split our data into training and testing sets and then see how our SVC performed with the observations not used to construct the model. We will use this training-testing method later in this tutorial to validate our SVMs.

\hypertarget{support-vector-machines}{%
\section{Support Vector Machines}\label{support-vector-machines}}

Support Vector Classifiers are a subset of the group of classification structures known as Support Vector Machines. Support Vector Machines can construct classification boundaries that are nonlinear in shape. The options for classification structures using the svm() command from the e1071 package are linear, polynomial, radial, and sigmoid. To demonstrate a nonlinear classification boundary, we will construct a new data set.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# construct larger random data set}
\NormalTok{x <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\KeywordTok{rnorm}\NormalTok{(}\DecValTok{200}\OperatorTok{*}\DecValTok{2}\NormalTok{), }\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{)}
\NormalTok{x[}\DecValTok{1}\OperatorTok{:}\DecValTok{100}\NormalTok{,] <-}\StringTok{ }\NormalTok{x[}\DecValTok{1}\OperatorTok{:}\DecValTok{100}\NormalTok{,] }\OperatorTok{+}\StringTok{ }\FloatTok{2.5}
\NormalTok{x[}\DecValTok{101}\OperatorTok{:}\DecValTok{150}\NormalTok{,] <-}\StringTok{ }\NormalTok{x[}\DecValTok{101}\OperatorTok{:}\DecValTok{150}\NormalTok{,] }\OperatorTok{-}\StringTok{ }\FloatTok{2.5}
\NormalTok{y <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\KeywordTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{150}\NormalTok{), }\KeywordTok{rep}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{50}\NormalTok{))}
\NormalTok{dat <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{x=}\NormalTok{x,}\DataTypeTok{y=}\KeywordTok{as.factor}\NormalTok{(y))}

\CommentTok{# Plot data}
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ dat, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ x}\FloatTok{.2}\NormalTok{, }\DataTypeTok{y =}\NormalTok{ x}\FloatTok{.1}\NormalTok{, }\DataTypeTok{color =}\NormalTok{ y, }\DataTypeTok{shape =}\NormalTok{ y)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{size =} \DecValTok{2}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_color_manual}\NormalTok{(}\DataTypeTok{values=}\KeywordTok{c}\NormalTok{(}\StringTok{"#000000"}\NormalTok{, }\StringTok{"#FF0000"}\NormalTok{)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{legend.position =} \StringTok{"none"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_113-broad_view-SVM_files/figure-latex/unnamed-chunk-11-1} \end{center}

Notice that the data is not linearly separable, and furthermore, isn't all clustered together in a single group. There are two sections of class 1 observations with a cluster of class 2 observations in between. To demonstrate the power of SVMs, we'll take 100 random observations from the set and use them to construct our boundary. We set kernel = ``radial'' based on the shape of our data and plot the results.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# set pseudorandom number generator}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\CommentTok{# sample training data and fit model}
\NormalTok{train <-}\StringTok{ }\NormalTok{base}\OperatorTok{::}\KeywordTok{sample}\NormalTok{(}\DecValTok{200}\NormalTok{,}\DecValTok{100}\NormalTok{, }\DataTypeTok{replace =} \OtherTok{FALSE}\NormalTok{)}
\NormalTok{svmfit <-}\StringTok{ }\KeywordTok{svm}\NormalTok{(y}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data =}\NormalTok{ dat[train,], }\DataTypeTok{kernel =} \StringTok{"radial"}\NormalTok{, }\DataTypeTok{gamma =} \DecValTok{1}\NormalTok{, }\DataTypeTok{cost =} \DecValTok{1}\NormalTok{)}
\CommentTok{# plot classifier}
\KeywordTok{plot}\NormalTok{(svmfit, dat)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_113-broad_view-SVM_files/figure-latex/unnamed-chunk-12-1} \end{center}

The same procedure can be run using the kernlab package, which has far more kernel options than the corresponding function in e1071. In addition to the four choices in e1071, this package allows use of a hyperbolic tangent, Laplacian, Bessel, Spline, String, or ANOVA RBF kernel. To fit this data, we set the cost to be the same as it was before, 1.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Fit radial-based SVM in kernlab}
\NormalTok{kernfit <-}\StringTok{ }\KeywordTok{ksvm}\NormalTok{(x[train,],y[train], }\DataTypeTok{type =} \StringTok{"C-svc"}\NormalTok{, }\DataTypeTok{kernel =} \StringTok{'rbfdot'}\NormalTok{, }\DataTypeTok{C =} \DecValTok{1}\NormalTok{, }\DataTypeTok{scaled =} \KeywordTok{c}\NormalTok{())}
\CommentTok{# Plot training data}
\KeywordTok{plot}\NormalTok{(kernfit, }\DataTypeTok{data =}\NormalTok{ x[train,])}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_113-broad_view-SVM_files/figure-latex/unnamed-chunk-13-1} \end{center}

We see that, at least visually, the SVM does a reasonable job of separating the two classes. To fit the model, we used \texttt{cost\ =\ 1}, but as mentioned previously, it isn't usually obvious which cost will produce the optimal classification boundary. We can use the tune() command to try several different values of cost as well as several different values of \(\gamma\), a scaling parameter used to fit nonlinear boundaries.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# tune model to find optimal cost, gamma values}
\NormalTok{tune.out <-}\StringTok{ }\KeywordTok{tune}\NormalTok{(svm, y}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data =}\NormalTok{ dat[train,], }\DataTypeTok{kernel =} \StringTok{"radial"}\NormalTok{,}
                 \DataTypeTok{ranges =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{cost =} \KeywordTok{c}\NormalTok{(}\FloatTok{0.1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{10}\NormalTok{,}\DecValTok{100}\NormalTok{,}\DecValTok{1000}\NormalTok{),}
                 \DataTypeTok{gamma =} \KeywordTok{c}\NormalTok{(}\FloatTok{0.5}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{)))}
\CommentTok{# show best model}
\NormalTok{tune.out}\OperatorTok{$}\NormalTok{best.model}
\CommentTok{#> }
\CommentTok{#> Call:}
\CommentTok{#> best.tune(method = svm, train.x = y ~ ., data = dat[train, ], }
\CommentTok{#>     ranges = list(cost = c(0.1, 1, 10, 100, 1000), gamma = c(0.5, }
\CommentTok{#>         1, 2, 3, 4)), kernel = "radial")}
\CommentTok{#> }
\CommentTok{#> }
\CommentTok{#> Parameters:}
\CommentTok{#>    SVM-Type:  C-classification }
\CommentTok{#>  SVM-Kernel:  radial }
\CommentTok{#>        cost:  1 }
\CommentTok{#>       gamma:  0.5 }
\CommentTok{#> }
\CommentTok{#> Number of Support Vectors:  30}
\end{Highlighting}
\end{Shaded}

The model that reduces the error the most in the training data uses a cost of 1 and \(\gamma\)
value of 0.5. We can now see how well the SVM performs by predicting the class of the 100 testing observations:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# validate model performance}
\NormalTok{(valid <-}\StringTok{ }\KeywordTok{table}\NormalTok{(}\DataTypeTok{true =}\NormalTok{ dat[}\OperatorTok{-}\NormalTok{train,}\StringTok{"y"}\NormalTok{], }\DataTypeTok{pred =} \KeywordTok{predict}\NormalTok{(tune.out}\OperatorTok{$}\NormalTok{best.model,}
                                             \DataTypeTok{newx =}\NormalTok{ dat[}\OperatorTok{-}\NormalTok{train,])))}
\CommentTok{#>     pred}
\CommentTok{#> true  1  2}
\CommentTok{#>    1 55 28}
\CommentTok{#>    2 12  5}
\CommentTok{##     pred}
\CommentTok{## true  1  2}
\CommentTok{##    1 58 19}
\CommentTok{##    2 16  7}
\end{Highlighting}
\end{Shaded}

Our best-fitting model produces 65\% accuracy in identifying classes. For such a complicated shape of observations, this performed reasonably well. We can challenge this method further by adding additional classes of observations.

\hypertarget{svms-for-multiple-classes}{%
\section{SVMs for Multiple Classes}\label{svms-for-multiple-classes}}

The procedure does not change for data sets that involve more than two classes of observations. We construct our data set the same way as we have previously, only now specifying three classes instead of two:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# construct data set}
\NormalTok{x <-}\StringTok{ }\KeywordTok{rbind}\NormalTok{(x, }\KeywordTok{matrix}\NormalTok{(}\KeywordTok{rnorm}\NormalTok{(}\DecValTok{50}\OperatorTok{*}\DecValTok{2}\NormalTok{), }\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{))}
\NormalTok{y <-}\StringTok{ }\KeywordTok{c}\NormalTok{(y, }\KeywordTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{50}\NormalTok{))}
\NormalTok{x[y}\OperatorTok{==}\DecValTok{0}\NormalTok{,}\DecValTok{2}\NormalTok{] <-}\StringTok{ }\NormalTok{x[y}\OperatorTok{==}\DecValTok{0}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OperatorTok{+}\StringTok{ }\FloatTok{2.5}
\NormalTok{dat <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{x=}\NormalTok{x, }\DataTypeTok{y=}\KeywordTok{as.factor}\NormalTok{(y))}
\CommentTok{# plot data set}
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ dat, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ x}\FloatTok{.2}\NormalTok{, }\DataTypeTok{y =}\NormalTok{ x}\FloatTok{.1}\NormalTok{, }\DataTypeTok{color =}\NormalTok{ y, }\DataTypeTok{shape =}\NormalTok{ y)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{size =} \DecValTok{2}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_color_manual}\NormalTok{(}\DataTypeTok{values=}\KeywordTok{c}\NormalTok{(}\StringTok{"#000000"}\NormalTok{,}\StringTok{"#FF0000"}\NormalTok{,}\StringTok{"#00BA00"}\NormalTok{)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{legend.position =} \StringTok{"none"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_113-broad_view-SVM_files/figure-latex/unnamed-chunk-16-1} \end{center}

The commands don't change for the e1071 package. We specify a cost and tuning parameter
\(\gamma\)
and fit a support vector machine. The results and interpretation are similar to two-class classification.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# fit model}
\NormalTok{svmfit <-}\StringTok{ }\KeywordTok{svm}\NormalTok{(y}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data =}\NormalTok{ dat, }\DataTypeTok{kernel =} \StringTok{"radial"}\NormalTok{, }\DataTypeTok{cost =} \DecValTok{10}\NormalTok{, }\DataTypeTok{gamma =} \DecValTok{1}\NormalTok{)}
\CommentTok{# plot results}
\KeywordTok{plot}\NormalTok{(svmfit, dat)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_113-broad_view-SVM_files/figure-latex/unnamed-chunk-17-1} \end{center}

We can check to see how well our model fit the data by using the \texttt{predict()} command, as follows:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#construct table}
\NormalTok{ypred <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(svmfit, dat)}
\NormalTok{(misclass <-}\StringTok{ }\KeywordTok{table}\NormalTok{(}\DataTypeTok{predict =}\NormalTok{ ypred, }\DataTypeTok{truth =}\NormalTok{ dat}\OperatorTok{$}\NormalTok{y))}
\CommentTok{#>        truth}
\CommentTok{#> predict   0   1   2}
\CommentTok{#>       0  38   2   5}
\CommentTok{#>       1   7 145   2}
\CommentTok{#>       2   5   3  43}
\CommentTok{##        truth}
\CommentTok{## predict   0   1   2}
\CommentTok{##       0  38   2   4}
\CommentTok{##       1   8 143   4}
\CommentTok{##       2   4   5  42}
\end{Highlighting}
\end{Shaded}

As shown in the resulting table, 89\% of our training observations were correctly classified. However, since we didn't break our data into training and testing sets, we didn't truly validate our results.

The kernlab package, on the other hand, can fit more than 2 classes, but cannot plot the results. To visualize the results of the ksvm function, we take the steps listed below to create a grid of points, predict the value of each point, and plot the results:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# fit and plot}
\NormalTok{kernfit <-}\StringTok{ }\KeywordTok{ksvm}\NormalTok{(}\KeywordTok{as.matrix}\NormalTok{(dat[,}\DecValTok{2}\OperatorTok{:}\DecValTok{1}\NormalTok{]),dat}\OperatorTok{$}\NormalTok{y, }\DataTypeTok{type =} \StringTok{"C-svc"}\NormalTok{, }\DataTypeTok{kernel =} \StringTok{'rbfdot'}\NormalTok{, }
                \DataTypeTok{C =} \DecValTok{100}\NormalTok{, }\DataTypeTok{scaled =} \KeywordTok{c}\NormalTok{())}

\CommentTok{# Create a fine grid of the feature space}
\NormalTok{x}\FloatTok{.1}\NormalTok{ <-}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\DataTypeTok{from =} \KeywordTok{min}\NormalTok{(dat}\OperatorTok{$}\NormalTok{x}\FloatTok{.1}\NormalTok{), }\DataTypeTok{to =} \KeywordTok{max}\NormalTok{(dat}\OperatorTok{$}\NormalTok{x}\FloatTok{.1}\NormalTok{), }\DataTypeTok{length =} \DecValTok{100}\NormalTok{)}
\NormalTok{x}\FloatTok{.2}\NormalTok{ <-}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\DataTypeTok{from =} \KeywordTok{min}\NormalTok{(dat}\OperatorTok{$}\NormalTok{x}\FloatTok{.2}\NormalTok{), }\DataTypeTok{to =} \KeywordTok{max}\NormalTok{(dat}\OperatorTok{$}\NormalTok{x}\FloatTok{.2}\NormalTok{), }\DataTypeTok{length =} \DecValTok{100}\NormalTok{)}
\NormalTok{x.grid <-}\StringTok{ }\KeywordTok{expand.grid}\NormalTok{(x}\FloatTok{.2}\NormalTok{, x}\FloatTok{.1}\NormalTok{)}

\CommentTok{# Get class predictions over grid}
\NormalTok{pred <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(kernfit, }\DataTypeTok{newdata =}\NormalTok{ x.grid)}

\CommentTok{# Plot the results}
\NormalTok{cols <-}\StringTok{ }\KeywordTok{brewer.pal}\NormalTok{(}\DecValTok{3}\NormalTok{, }\StringTok{"Set1"}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(x.grid, }\DataTypeTok{pch =} \DecValTok{19}\NormalTok{, }\DataTypeTok{col =} \KeywordTok{adjustcolor}\NormalTok{(cols[pred], }\DataTypeTok{alpha.f =} \FloatTok{0.05}\NormalTok{))}

\NormalTok{classes <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(pred, }\DataTypeTok{nrow =} \DecValTok{100}\NormalTok{, }\DataTypeTok{ncol =} \DecValTok{100}\NormalTok{)}
\KeywordTok{contour}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ x}\FloatTok{.2}\NormalTok{, }\DataTypeTok{y =}\NormalTok{ x}\FloatTok{.1}\NormalTok{, }\DataTypeTok{z =}\NormalTok{ classes, }\DataTypeTok{levels =} \DecValTok{1}\OperatorTok{:}\DecValTok{3}\NormalTok{, }\DataTypeTok{labels =} \StringTok{""}\NormalTok{, }\DataTypeTok{add =} \OtherTok{TRUE}\NormalTok{)}

\KeywordTok{points}\NormalTok{(dat[, }\DecValTok{2}\OperatorTok{:}\DecValTok{1}\NormalTok{], }\DataTypeTok{pch =} \DecValTok{19}\NormalTok{, }\DataTypeTok{col =}\NormalTok{ cols[}\KeywordTok{predict}\NormalTok{(kernfit)])}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_113-broad_view-SVM_files/figure-latex/unnamed-chunk-19-1} \end{center}

\hypertarget{application}{%
\section{Application}\label{application}}

The Khan data set contains data on 83 tissue samples with 2308 gene expression measurements on each sample. These were split into 63 training observations and 20 testing observations, and there are four distinct classes in the set. It would be impossible to visualize such data, so we choose the simplest classifier (linear) to construct our model. We will use the svm command from \texttt{e1071} to conduct our analysis.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# fit model}
\NormalTok{dat <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Khan}\OperatorTok{$}\NormalTok{xtrain, }\DataTypeTok{y=}\KeywordTok{as.factor}\NormalTok{(Khan}\OperatorTok{$}\NormalTok{ytrain))}
\NormalTok{(out <-}\StringTok{ }\KeywordTok{svm}\NormalTok{(y}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data =}\NormalTok{ dat, }\DataTypeTok{kernel =} \StringTok{"linear"}\NormalTok{, }\DataTypeTok{cost=}\DecValTok{10}\NormalTok{))}
\CommentTok{#> }
\CommentTok{#> Call:}
\CommentTok{#> svm(formula = y ~ ., data = dat, kernel = "linear", cost = 10)}
\CommentTok{#> }
\CommentTok{#> }
\CommentTok{#> Parameters:}
\CommentTok{#>    SVM-Type:  C-classification }
\CommentTok{#>  SVM-Kernel:  linear }
\CommentTok{#>        cost:  10 }
\CommentTok{#>       gamma:  0.000433 }
\CommentTok{#> }
\CommentTok{#> Number of Support Vectors:  58}
\end{Highlighting}
\end{Shaded}

First of all, we can check how well our model did at classifying the training observations. This is usually high, but again, doesn't validate the model. If the model doesn't do a very good job of classifying the training set, it could be a red flag. In our case, all 63 training observations were correctly classified.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# check model performance on training set}
\KeywordTok{table}\NormalTok{(out}\OperatorTok{$}\NormalTok{fitted, dat}\OperatorTok{$}\NormalTok{y)}
\CommentTok{#>    }
\CommentTok{#>      1  2  3  4}
\CommentTok{#>   1  8  0  0  0}
\CommentTok{#>   2  0 23  0  0}
\CommentTok{#>   3  0  0 12  0}
\CommentTok{#>   4  0  0  0 20}
\end{Highlighting}
\end{Shaded}

To perform validation, we can check how the model performs on the testing set:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# validate model performance}
\NormalTok{dat.te <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{x=}\NormalTok{Khan}\OperatorTok{$}\NormalTok{xtest, }\DataTypeTok{y=}\KeywordTok{as.factor}\NormalTok{(Khan}\OperatorTok{$}\NormalTok{ytest))}
\NormalTok{pred.te <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(out, }\DataTypeTok{newdata=}\NormalTok{dat.te)}
\KeywordTok{table}\NormalTok{(pred.te, dat.te}\OperatorTok{$}\NormalTok{y)}
\CommentTok{#>        }
\CommentTok{#> pred.te 1 2 3 4}
\CommentTok{#>       1 3 0 0 0}
\CommentTok{#>       2 0 6 2 0}
\CommentTok{#>       3 0 0 4 0}
\CommentTok{#>       4 0 0 0 5}
\end{Highlighting}
\end{Shaded}

The model correctly identifies 18 of the 20 testing observations. SVMs and the boundaries they impose are more difficult to interpret at higher dimensions, but these results seem to suggest that our model is a good classifier for the gene data.

\hypertarget{sonar-standalone-model-with-random-forest}{%
\chapter{Sonar Standalone Model with Random Forest}\label{sonar-standalone-model-with-random-forest}}

\textbf{Classification problem}

\hypertarget{introduction-9}{%
\section{Introduction}\label{introduction-9}}

\begin{itemize}
\tightlist
\item
  \texttt{mtry}: Number of variables randomly sampled as candidates at each split.
\item
  \texttt{ntree}: Number of trees to grow.
\end{itemize}

\hypertarget{load-libraries-1}{%
\section{Load libraries}\label{load-libraries-1}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load packages}
\KeywordTok{library}\NormalTok{(caret)}
\KeywordTok{library}\NormalTok{(mlbench)}
\KeywordTok{library}\NormalTok{(randomForest)}
\KeywordTok{library}\NormalTok{(tictoc)}

\CommentTok{# load dataset}
\KeywordTok{data}\NormalTok{(Sonar)}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{7}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{explore-data}{%
\section{Explore data}\label{explore-data}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{glimpse}\NormalTok{(Sonar)}
\CommentTok{#> Observations: 208}
\CommentTok{#> Variables: 61}
\CommentTok{#> $ V1    <dbl> 0.0200, 0.0453, 0.0262, 0.0100, 0.0762, 0.0286, 0.0317, ...}
\CommentTok{#> $ V2    <dbl> 0.0371, 0.0523, 0.0582, 0.0171, 0.0666, 0.0453, 0.0956, ...}
\CommentTok{#> $ V3    <dbl> 0.0428, 0.0843, 0.1099, 0.0623, 0.0481, 0.0277, 0.1321, ...}
\CommentTok{#> $ V4    <dbl> 0.0207, 0.0689, 0.1083, 0.0205, 0.0394, 0.0174, 0.1408, ...}
\CommentTok{#> $ V5    <dbl> 0.0954, 0.1183, 0.0974, 0.0205, 0.0590, 0.0384, 0.1674, ...}
\CommentTok{#> $ V6    <dbl> 0.0986, 0.2583, 0.2280, 0.0368, 0.0649, 0.0990, 0.1710, ...}
\CommentTok{#> $ V7    <dbl> 0.1539, 0.2156, 0.2431, 0.1098, 0.1209, 0.1201, 0.0731, ...}
\CommentTok{#> $ V8    <dbl> 0.1601, 0.3481, 0.3771, 0.1276, 0.2467, 0.1833, 0.1401, ...}
\CommentTok{#> $ V9    <dbl> 0.3109, 0.3337, 0.5598, 0.0598, 0.3564, 0.2105, 0.2083, ...}
\CommentTok{#> $ V10   <dbl> 0.2111, 0.2872, 0.6194, 0.1264, 0.4459, 0.3039, 0.3513, ...}
\CommentTok{#> $ V11   <dbl> 0.1609, 0.4918, 0.6333, 0.0881, 0.4152, 0.2988, 0.1786, ...}
\CommentTok{#> $ V12   <dbl> 0.1582, 0.6552, 0.7060, 0.1992, 0.3952, 0.4250, 0.0658, ...}
\CommentTok{#> $ V13   <dbl> 0.2238, 0.6919, 0.5544, 0.0184, 0.4256, 0.6343, 0.0513, ...}
\CommentTok{#> $ V14   <dbl> 0.0645, 0.7797, 0.5320, 0.2261, 0.4135, 0.8198, 0.3752, ...}
\CommentTok{#> $ V15   <dbl> 0.0660, 0.7464, 0.6479, 0.1729, 0.4528, 1.0000, 0.5419, ...}
\CommentTok{#> $ V16   <dbl> 0.2273, 0.9444, 0.6931, 0.2131, 0.5326, 0.9988, 0.5440, ...}
\CommentTok{#> $ V17   <dbl> 0.3100, 1.0000, 0.6759, 0.0693, 0.7306, 0.9508, 0.5150, ...}
\CommentTok{#> $ V18   <dbl> 0.300, 0.887, 0.755, 0.228, 0.619, 0.902, 0.426, 0.120, ...}
\CommentTok{#> $ V19   <dbl> 0.508, 0.802, 0.893, 0.406, 0.203, 0.723, 0.202, 0.668, ...}
\CommentTok{#> $ V20   <dbl> 0.4797, 0.7818, 0.8619, 0.3973, 0.4636, 0.5122, 0.4233, ...}
\CommentTok{#> $ V21   <dbl> 0.578, 0.521, 0.797, 0.274, 0.415, 0.207, 0.772, 0.783, ...}
\CommentTok{#> $ V22   <dbl> 0.507, 0.405, 0.674, 0.369, 0.429, 0.399, 0.974, 0.535, ...}
\CommentTok{#> $ V23   <dbl> 0.433, 0.396, 0.429, 0.556, 0.573, 0.589, 0.939, 0.681, ...}
\CommentTok{#> $ V24   <dbl> 0.555, 0.391, 0.365, 0.485, 0.540, 0.287, 0.556, 0.917, ...}
\CommentTok{#> $ V25   <dbl> 0.671, 0.325, 0.533, 0.314, 0.316, 0.204, 0.527, 0.761, ...}
\CommentTok{#> $ V26   <dbl> 0.641, 0.320, 0.241, 0.533, 0.229, 0.578, 0.683, 0.822, ...}
\CommentTok{#> $ V27   <dbl> 0.7104, 0.3271, 0.5070, 0.5256, 0.6995, 0.5389, 0.5713, ...}
\CommentTok{#> $ V28   <dbl> 0.8080, 0.2767, 0.8533, 0.2520, 1.0000, 0.3750, 0.5429, ...}
\CommentTok{#> $ V29   <dbl> 0.6791, 0.4423, 0.6036, 0.2090, 0.7262, 0.3411, 0.2177, ...}
\CommentTok{#> $ V30   <dbl> 0.3857, 0.2028, 0.8514, 0.3559, 0.4724, 0.5067, 0.2149, ...}
\CommentTok{#> $ V31   <dbl> 0.131, 0.379, 0.851, 0.626, 0.510, 0.558, 0.581, 0.132, ...}
\CommentTok{#> $ V32   <dbl> 0.2604, 0.2947, 0.5045, 0.7340, 0.5459, 0.4778, 0.6323, ...}
\CommentTok{#> $ V33   <dbl> 0.512, 0.198, 0.186, 0.612, 0.288, 0.330, 0.296, 0.099, ...}
\CommentTok{#> $ V34   <dbl> 0.7547, 0.2341, 0.2709, 0.3497, 0.0981, 0.2198, 0.1873, ...}
\CommentTok{#> $ V35   <dbl> 0.8537, 0.1306, 0.4232, 0.3953, 0.1951, 0.1407, 0.2969, ...}
\CommentTok{#> $ V36   <dbl> 0.851, 0.418, 0.304, 0.301, 0.418, 0.286, 0.516, 0.105, ...}
\CommentTok{#> $ V37   <dbl> 0.669, 0.384, 0.612, 0.541, 0.460, 0.381, 0.615, 0.192, ...}
\CommentTok{#> $ V38   <dbl> 0.6097, 0.1057, 0.6756, 0.8814, 0.3217, 0.4158, 0.4283, ...}
\CommentTok{#> $ V39   <dbl> 0.4943, 0.1840, 0.5375, 0.9857, 0.2828, 0.4054, 0.5479, ...}
\CommentTok{#> $ V40   <dbl> 0.2744, 0.1970, 0.4719, 0.9167, 0.2430, 0.3296, 0.6133, ...}
\CommentTok{#> $ V41   <dbl> 0.0510, 0.1674, 0.4647, 0.6121, 0.1979, 0.2707, 0.5017, ...}
\CommentTok{#> $ V42   <dbl> 0.2834, 0.0583, 0.2587, 0.5006, 0.2444, 0.2650, 0.2377, ...}
\CommentTok{#> $ V43   <dbl> 0.2825, 0.1401, 0.2129, 0.3210, 0.1847, 0.0723, 0.1957, ...}
\CommentTok{#> $ V44   <dbl> 0.4256, 0.1628, 0.2222, 0.3202, 0.0841, 0.1238, 0.1749, ...}
\CommentTok{#> $ V45   <dbl> 0.2641, 0.0621, 0.2111, 0.4295, 0.0692, 0.1192, 0.1304, ...}
\CommentTok{#> $ V46   <dbl> 0.1386, 0.0203, 0.0176, 0.3654, 0.0528, 0.1089, 0.0597, ...}
\CommentTok{#> $ V47   <dbl> 0.1051, 0.0530, 0.1348, 0.2655, 0.0357, 0.0623, 0.1124, ...}
\CommentTok{#> $ V48   <dbl> 0.1343, 0.0742, 0.0744, 0.1576, 0.0085, 0.0494, 0.1047, ...}
\CommentTok{#> $ V49   <dbl> 0.0383, 0.0409, 0.0130, 0.0681, 0.0230, 0.0264, 0.0507, ...}
\CommentTok{#> $ V50   <dbl> 0.0324, 0.0061, 0.0106, 0.0294, 0.0046, 0.0081, 0.0159, ...}
\CommentTok{#> $ V51   <dbl> 0.0232, 0.0125, 0.0033, 0.0241, 0.0156, 0.0104, 0.0195, ...}
\CommentTok{#> $ V52   <dbl> 0.0027, 0.0084, 0.0232, 0.0121, 0.0031, 0.0045, 0.0201, ...}
\CommentTok{#> $ V53   <dbl> 0.0065, 0.0089, 0.0166, 0.0036, 0.0054, 0.0014, 0.0248, ...}
\CommentTok{#> $ V54   <dbl> 0.0159, 0.0048, 0.0095, 0.0150, 0.0105, 0.0038, 0.0131, ...}
\CommentTok{#> $ V55   <dbl> 0.0072, 0.0094, 0.0180, 0.0085, 0.0110, 0.0013, 0.0070, ...}
\CommentTok{#> $ V56   <dbl> 0.0167, 0.0191, 0.0244, 0.0073, 0.0015, 0.0089, 0.0138, ...}
\CommentTok{#> $ V57   <dbl> 0.0180, 0.0140, 0.0316, 0.0050, 0.0072, 0.0057, 0.0092, ...}
\CommentTok{#> $ V58   <dbl> 0.0084, 0.0049, 0.0164, 0.0044, 0.0048, 0.0027, 0.0143, ...}
\CommentTok{#> $ V59   <dbl> 0.0090, 0.0052, 0.0095, 0.0040, 0.0107, 0.0051, 0.0036, ...}
\CommentTok{#> $ V60   <dbl> 0.0032, 0.0044, 0.0078, 0.0117, 0.0094, 0.0062, 0.0103, ...}
\CommentTok{#> $ Class <fct> R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R,...}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tibble}\OperatorTok{::}\KeywordTok{as_tibble}\NormalTok{(Sonar)}
\CommentTok{#> # A tibble: 208 x 61}
\CommentTok{#>       V1     V2     V3     V4     V5     V6    V7    V8     V9   V10    V11}
\CommentTok{#>    <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl> <dbl> <dbl>  <dbl> <dbl>  <dbl>}
\CommentTok{#> 1 0.02   0.0371 0.0428 0.0207 0.0954 0.0986 0.154 0.160 0.311  0.211 0.161 }
\CommentTok{#> 2 0.0453 0.0523 0.0843 0.0689 0.118  0.258  0.216 0.348 0.334  0.287 0.492 }
\CommentTok{#> 3 0.0262 0.0582 0.110  0.108  0.0974 0.228  0.243 0.377 0.560  0.619 0.633 }
\CommentTok{#> 4 0.01   0.0171 0.0623 0.0205 0.0205 0.0368 0.110 0.128 0.0598 0.126 0.0881}
\CommentTok{#> 5 0.0762 0.0666 0.0481 0.0394 0.059  0.0649 0.121 0.247 0.356  0.446 0.415 }
\CommentTok{#> 6 0.0286 0.0453 0.0277 0.0174 0.0384 0.099  0.120 0.183 0.210  0.304 0.299 }
\CommentTok{#> # ... with 202 more rows, and 50 more variables: V12 <dbl>, V13 <dbl>,}
\CommentTok{#> #   V14 <dbl>, V15 <dbl>, V16 <dbl>, V17 <dbl>, V18 <dbl>, V19 <dbl>,}
\CommentTok{#> #   V20 <dbl>, V21 <dbl>, V22 <dbl>, V23 <dbl>, V24 <dbl>, V25 <dbl>,}
\CommentTok{#> #   V26 <dbl>, V27 <dbl>, V28 <dbl>, V29 <dbl>, V30 <dbl>, V31 <dbl>,}
\CommentTok{#> #   V32 <dbl>, V33 <dbl>, V34 <dbl>, V35 <dbl>, V36 <dbl>, V37 <dbl>,}
\CommentTok{#> #   V38 <dbl>, V39 <dbl>, V40 <dbl>, V41 <dbl>, V42 <dbl>, V43 <dbl>,}
\CommentTok{#> #   V44 <dbl>, V45 <dbl>, V46 <dbl>, V47 <dbl>, V48 <dbl>, V49 <dbl>,}
\CommentTok{#> #   V50 <dbl>, V51 <dbl>, V52 <dbl>, V53 <dbl>, V54 <dbl>, V55 <dbl>,}
\CommentTok{#> #   V56 <dbl>, V57 <dbl>, V58 <dbl>, V59 <dbl>, V60 <dbl>, Class <fct>}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# create 80%/20% for training and validation datasets}
\NormalTok{validationIndex <-}\StringTok{ }\KeywordTok{createDataPartition}\NormalTok{(Sonar}\OperatorTok{$}\NormalTok{Class, }\DataTypeTok{p=}\FloatTok{0.80}\NormalTok{, }\DataTypeTok{list=}\OtherTok{FALSE}\NormalTok{)}
\NormalTok{validation <-}\StringTok{ }\NormalTok{Sonar[}\OperatorTok{-}\NormalTok{validationIndex,]}
\NormalTok{training   <-}\StringTok{ }\NormalTok{Sonar[validationIndex,]}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{tic}\NormalTok{()}
\CommentTok{# train a model and summarize model}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{7}\NormalTok{)}
\NormalTok{trainControl <-}\StringTok{ }\KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method=}\StringTok{"repeatedcv"}\NormalTok{, }\DataTypeTok{number=}\DecValTok{10}\NormalTok{, }\DataTypeTok{repeats=}\DecValTok{3}\NormalTok{)}
\NormalTok{fit.rf <-}\StringTok{ }\KeywordTok{train}\NormalTok{(Class}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{training, }
                \DataTypeTok{method =} \StringTok{"rf"}\NormalTok{, }
                \DataTypeTok{metric =} \StringTok{"Accuracy"}\NormalTok{, }
                \DataTypeTok{trControl =}\NormalTok{ trainControl, }
                \DataTypeTok{ntree =} \DecValTok{2000}\NormalTok{)}
\KeywordTok{toc}\NormalTok{()}
\CommentTok{#> 61.175 sec elapsed}
\KeywordTok{print}\NormalTok{(fit.rf)}
\CommentTok{#> Random Forest }
\CommentTok{#> }
\CommentTok{#> 167 samples}
\CommentTok{#>  60 predictor}
\CommentTok{#>   2 classes: 'M', 'R' }
\CommentTok{#> }
\CommentTok{#> No pre-processing}
\CommentTok{#> Resampling: Cross-Validated (10 fold, repeated 3 times) }
\CommentTok{#> Summary of sample sizes: 150, 150, 150, 151, 151, 150, ... }
\CommentTok{#> Resampling results across tuning parameters:}
\CommentTok{#> }
\CommentTok{#>   mtry  Accuracy  Kappa}
\CommentTok{#>    2    0.845     0.682}
\CommentTok{#>   31    0.828     0.651}
\CommentTok{#>   60    0.808     0.611}
\CommentTok{#> }
\CommentTok{#> Accuracy was used to select the optimal model using the largest value.}
\CommentTok{#> The final value used for the model was mtry = 2.}
\KeywordTok{print}\NormalTok{(fit.rf}\OperatorTok{$}\NormalTok{finalModel)}
\CommentTok{#> }
\CommentTok{#> Call:}
\CommentTok{#>  randomForest(x = x, y = y, ntree = 2000, mtry = param$mtry) }
\CommentTok{#>                Type of random forest: classification}
\CommentTok{#>                      Number of trees: 2000}
\CommentTok{#> No. of variables tried at each split: 2}
\CommentTok{#> }
\CommentTok{#>         OOB estimate of  error rate: 14.4%}
\CommentTok{#> Confusion matrix:}
\CommentTok{#>    M  R class.error}
\CommentTok{#> M 84  5      0.0562}
\CommentTok{#> R 19 59      0.2436}
\end{Highlighting}
\end{Shaded}

\begin{quote}
Accuracy: 85.26\% at mtry=2
\end{quote}

\hypertarget{apply-tuning-parameters-for-final-model}{%
\section{Apply tuning parameters for final model}\label{apply-tuning-parameters-for-final-model}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# create standalone model using all training data}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{7}\NormalTok{)}
\NormalTok{finalModel <-}\StringTok{ }\KeywordTok{randomForest}\NormalTok{(Class}\OperatorTok{~}\NormalTok{., training, }\DataTypeTok{mtry=}\DecValTok{2}\NormalTok{, }\DataTypeTok{ntree=}\DecValTok{2000}\NormalTok{)}

\CommentTok{# make a predictions on "new data" using the final model}
\NormalTok{finalPredictions <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(finalModel, validation[,}\DecValTok{1}\OperatorTok{:}\DecValTok{60}\NormalTok{])}
\KeywordTok{confusionMatrix}\NormalTok{(finalPredictions, validation}\OperatorTok{$}\NormalTok{Class)}
\CommentTok{#> Confusion Matrix and Statistics}
\CommentTok{#> }
\CommentTok{#>           Reference}
\CommentTok{#> Prediction  M  R}
\CommentTok{#>          M 20  4}
\CommentTok{#>          R  2 15}
\CommentTok{#>                                         }
\CommentTok{#>                Accuracy : 0.854         }
\CommentTok{#>                  95% CI : (0.708, 0.944)}
\CommentTok{#>     No Information Rate : 0.537         }
\CommentTok{#>     P-Value [Acc > NIR] : 1.88e-05      }
\CommentTok{#>                                         }
\CommentTok{#>                   Kappa : 0.704         }
\CommentTok{#>                                         }
\CommentTok{#>  Mcnemar's Test P-Value : 0.683         }
\CommentTok{#>                                         }
\CommentTok{#>             Sensitivity : 0.909         }
\CommentTok{#>             Specificity : 0.789         }
\CommentTok{#>          Pos Pred Value : 0.833         }
\CommentTok{#>          Neg Pred Value : 0.882         }
\CommentTok{#>              Prevalence : 0.537         }
\CommentTok{#>          Detection Rate : 0.488         }
\CommentTok{#>    Detection Prevalence : 0.585         }
\CommentTok{#>       Balanced Accuracy : 0.849         }
\CommentTok{#>                                         }
\CommentTok{#>        'Positive' Class : M             }
\CommentTok{#> }
\end{Highlighting}
\end{Shaded}

\begin{quote}
Accuracy: 82.93\%
\end{quote}

\hypertarget{save-model}{%
\section{Save model}\label{save-model}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# save the model to disk}
\KeywordTok{saveRDS}\NormalTok{(finalModel, }\KeywordTok{file.path}\NormalTok{(model_out_dir, }\StringTok{"sonar-finalModel.rds"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\hypertarget{use-the-saved-model}{%
\section{Use the saved model}\label{use-the-saved-model}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load the model}
\NormalTok{superModel <-}\StringTok{ }\KeywordTok{readRDS}\NormalTok{(}\KeywordTok{file.path}\NormalTok{(model_out_dir, }\StringTok{"sonar-finalModel.rds"}\NormalTok{))}
\KeywordTok{print}\NormalTok{(superModel)}
\CommentTok{#> }
\CommentTok{#> Call:}
\CommentTok{#>  randomForest(formula = Class ~ ., data = training, mtry = 2,      ntree = 2000) }
\CommentTok{#>                Type of random forest: classification}
\CommentTok{#>                      Number of trees: 2000}
\CommentTok{#> No. of variables tried at each split: 2}
\CommentTok{#> }
\CommentTok{#>         OOB estimate of  error rate: 16.2%}
\CommentTok{#> Confusion matrix:}
\CommentTok{#>    M  R class.error}
\CommentTok{#> M 81  8      0.0899}
\CommentTok{#> R 19 59      0.2436}
\end{Highlighting}
\end{Shaded}

\hypertarget{make-prediction-with-new-data}{%
\section{Make prediction with new data}\label{make-prediction-with-new-data}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# make a predictions on "new data" using the final model}
\NormalTok{finalPredictions <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(superModel, validation[,}\DecValTok{1}\OperatorTok{:}\DecValTok{60}\NormalTok{])}
\KeywordTok{confusionMatrix}\NormalTok{(finalPredictions, validation}\OperatorTok{$}\NormalTok{Class)}
\CommentTok{#> Confusion Matrix and Statistics}
\CommentTok{#> }
\CommentTok{#>           Reference}
\CommentTok{#> Prediction  M  R}
\CommentTok{#>          M 20  4}
\CommentTok{#>          R  2 15}
\CommentTok{#>                                         }
\CommentTok{#>                Accuracy : 0.854         }
\CommentTok{#>                  95% CI : (0.708, 0.944)}
\CommentTok{#>     No Information Rate : 0.537         }
\CommentTok{#>     P-Value [Acc > NIR] : 1.88e-05      }
\CommentTok{#>                                         }
\CommentTok{#>                   Kappa : 0.704         }
\CommentTok{#>                                         }
\CommentTok{#>  Mcnemar's Test P-Value : 0.683         }
\CommentTok{#>                                         }
\CommentTok{#>             Sensitivity : 0.909         }
\CommentTok{#>             Specificity : 0.789         }
\CommentTok{#>          Pos Pred Value : 0.833         }
\CommentTok{#>          Neg Pred Value : 0.882         }
\CommentTok{#>              Prevalence : 0.537         }
\CommentTok{#>          Detection Rate : 0.488         }
\CommentTok{#>    Detection Prevalence : 0.585         }
\CommentTok{#>       Balanced Accuracy : 0.849         }
\CommentTok{#>                                         }
\CommentTok{#>        'Positive' Class : M             }
\CommentTok{#> }
\end{Highlighting}
\end{Shaded}

\hypertarget{glass-classification}{%
\chapter{Glass classification}\label{glass-classification}}

\url{https://cran.r-project.org/web/packages/e1071/vignettes/svmdoc.pdf}

In this example, we use the glass data from the UCI Repository of Machine
Learning Databases for classification. The task is to predict the type of a glass
on basis of its chemical analysis. We start by splitting the data into a train and
test set:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(caret)}
\CommentTok{#> Loading required package: lattice}
\CommentTok{#> Loading required package: ggplot2}
\CommentTok{#> Registered S3 methods overwritten by 'ggplot2':}
\CommentTok{#>   method         from }
\CommentTok{#>   [.quosures     rlang}
\CommentTok{#>   c.quosures     rlang}
\CommentTok{#>   print.quosures rlang}
\KeywordTok{library}\NormalTok{(e1071)}
\KeywordTok{library}\NormalTok{(rpart)}

\KeywordTok{data}\NormalTok{(Glass, }\DataTypeTok{package=}\StringTok{"mlbench"}\NormalTok{)}
\KeywordTok{str}\NormalTok{(Glass)}
\CommentTok{#> 'data.frame':    214 obs. of  10 variables:}
\CommentTok{#>  $ RI  : num  1.52 1.52 1.52 1.52 1.52 ...}
\CommentTok{#>  $ Na  : num  13.6 13.9 13.5 13.2 13.3 ...}
\CommentTok{#>  $ Mg  : num  4.49 3.6 3.55 3.69 3.62 3.61 3.6 3.61 3.58 3.6 ...}
\CommentTok{#>  $ Al  : num  1.1 1.36 1.54 1.29 1.24 1.62 1.14 1.05 1.37 1.36 ...}
\CommentTok{#>  $ Si  : num  71.8 72.7 73 72.6 73.1 ...}
\CommentTok{#>  $ K   : num  0.06 0.48 0.39 0.57 0.55 0.64 0.58 0.57 0.56 0.57 ...}
\CommentTok{#>  $ Ca  : num  8.75 7.83 7.78 8.22 8.07 8.07 8.17 8.24 8.3 8.4 ...}
\CommentTok{#>  $ Ba  : num  0 0 0 0 0 0 0 0 0 0 ...}
\CommentTok{#>  $ Fe  : num  0 0 0 0 0 0.26 0 0 0 0.11 ...}
\CommentTok{#>  $ Type: Factor w/ 6 levels "1","2","3","5",..: 1 1 1 1 1 1 1 1 1 1 ...}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## split data into a train and test set}
\NormalTok{index <-}\StringTok{ }\DecValTok{1}\OperatorTok{:}\KeywordTok{nrow}\NormalTok{(Glass)}
\NormalTok{testindex <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(index, }\KeywordTok{trunc}\NormalTok{(}\KeywordTok{length}\NormalTok{(index)}\OperatorTok{/}\DecValTok{3}\NormalTok{))}
\NormalTok{testset  <-}\StringTok{ }\NormalTok{Glass[testindex,]}
\NormalTok{trainset <-}\StringTok{ }\NormalTok{Glass[}\OperatorTok{-}\NormalTok{testindex,]}
\end{Highlighting}
\end{Shaded}

Both for the SVM and the partitioning tree (via \texttt{rpart()}), we fit the model and
try to predict the test set values:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## svm}
\NormalTok{svm.model <-}\StringTok{ }\KeywordTok{svm}\NormalTok{(Type }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ trainset, }\DataTypeTok{cost =} \DecValTok{100}\NormalTok{, }\DataTypeTok{gamma =} \DecValTok{1}\NormalTok{)}
\NormalTok{svm.pred  <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(svm.model, testset[,}\OperatorTok{-}\DecValTok{10}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

(The dependent variable, Type, has column number 10. cost is a general penalizing
parameter for C-classification and \texttt{gamma} is the radial basis function-specific
\texttt{kernel} parameter.)

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## rpart}
\NormalTok{rpart.model <-}\StringTok{ }\KeywordTok{rpart}\NormalTok{(Type }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ trainset)}
\NormalTok{rpart.pred <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(rpart.model, testset[,}\OperatorTok{-}\DecValTok{10}\NormalTok{], }\DataTypeTok{type =} \StringTok{"class"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

A cross-tabulation of the true versus the predicted values yields:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## compute svm confusion matrix}
\KeywordTok{table}\NormalTok{(}\DataTypeTok{pred =}\NormalTok{ svm.pred, }\DataTypeTok{true =}\NormalTok{ testset[,}\DecValTok{10}\NormalTok{])}
\CommentTok{#>     true}
\CommentTok{#> pred  1  2  3  5  6  7}
\CommentTok{#>    1 20  3  3  0  0  0}
\CommentTok{#>    2  6 13  5  4  2  4}
\CommentTok{#>    3  2  1  0  0  0  0}
\CommentTok{#>    5  0  0  0  1  0  0}
\CommentTok{#>    6  0  0  0  0  0  0}
\CommentTok{#>    7  0  0  0  0  0  7}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## compute rpart confusion matrix}
\KeywordTok{table}\NormalTok{(}\DataTypeTok{pred =}\NormalTok{ rpart.pred, }\DataTypeTok{true =}\NormalTok{ testset[,}\DecValTok{10}\NormalTok{])}
\CommentTok{#>     true}
\CommentTok{#> pred  1  2  3  5  6  7}
\CommentTok{#>    1 22  0  3  0  0  0}
\CommentTok{#>    2  5 12  4  0  0  0}
\CommentTok{#>    3  0  2  1  0  0  0}
\CommentTok{#>    5  0  2  0  5  2  1}
\CommentTok{#>    6  0  0  0  0  0  0}
\CommentTok{#>    7  1  1  0  0  0 10}
\end{Highlighting}
\end{Shaded}

\hypertarget{comparison-test-sets}{%
\subsection{Comparison test sets}\label{comparison-test-sets}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{confusionMatrix}\NormalTok{(svm.pred, testset}\OperatorTok{$}\NormalTok{Type)}
\CommentTok{#> Confusion Matrix and Statistics}
\CommentTok{#> }
\CommentTok{#>           Reference}
\CommentTok{#> Prediction  1  2  3  5  6  7}
\CommentTok{#>          1 20  3  3  0  0  0}
\CommentTok{#>          2  6 13  5  4  2  4}
\CommentTok{#>          3  2  1  0  0  0  0}
\CommentTok{#>          5  0  0  0  1  0  0}
\CommentTok{#>          6  0  0  0  0  0  0}
\CommentTok{#>          7  0  0  0  0  0  7}
\CommentTok{#> }
\CommentTok{#> Overall Statistics}
\CommentTok{#>                                         }
\CommentTok{#>                Accuracy : 0.577         }
\CommentTok{#>                  95% CI : (0.454, 0.694)}
\CommentTok{#>     No Information Rate : 0.394         }
\CommentTok{#>     P-Value [Acc > NIR] : 0.00137       }
\CommentTok{#>                                         }
\CommentTok{#>                   Kappa : 0.413         }
\CommentTok{#>                                         }
\CommentTok{#>  Mcnemar's Test P-Value : NA            }
\CommentTok{#> }
\CommentTok{#> Statistics by Class:}
\CommentTok{#> }
\CommentTok{#>                      Class: 1 Class: 2 Class: 3 Class: 5 Class: 6 Class: 7}
\CommentTok{#> Sensitivity             0.714    0.765   0.0000   0.2000   0.0000   0.6364}
\CommentTok{#> Specificity             0.860    0.611   0.9524   1.0000   1.0000   1.0000}
\CommentTok{#> Pos Pred Value          0.769    0.382   0.0000   1.0000      NaN   1.0000}
\CommentTok{#> Neg Pred Value          0.822    0.892   0.8824   0.9429   0.9718   0.9375}
\CommentTok{#> Prevalence              0.394    0.239   0.1127   0.0704   0.0282   0.1549}
\CommentTok{#> Detection Rate          0.282    0.183   0.0000   0.0141   0.0000   0.0986}
\CommentTok{#> Detection Prevalence    0.366    0.479   0.0423   0.0141   0.0000   0.0986}
\CommentTok{#> Balanced Accuracy       0.787    0.688   0.4762   0.6000   0.5000   0.8182}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{confusionMatrix}\NormalTok{(rpart.pred, testset}\OperatorTok{$}\NormalTok{Type)}
\CommentTok{#> Confusion Matrix and Statistics}
\CommentTok{#> }
\CommentTok{#>           Reference}
\CommentTok{#> Prediction  1  2  3  5  6  7}
\CommentTok{#>          1 22  0  3  0  0  0}
\CommentTok{#>          2  5 12  4  0  0  0}
\CommentTok{#>          3  0  2  1  0  0  0}
\CommentTok{#>          5  0  2  0  5  2  1}
\CommentTok{#>          6  0  0  0  0  0  0}
\CommentTok{#>          7  1  1  0  0  0 10}
\CommentTok{#> }
\CommentTok{#> Overall Statistics}
\CommentTok{#>                                         }
\CommentTok{#>                Accuracy : 0.704         }
\CommentTok{#>                  95% CI : (0.584, 0.807)}
\CommentTok{#>     No Information Rate : 0.394         }
\CommentTok{#>     P-Value [Acc > NIR] : 1.23e-07      }
\CommentTok{#>                                         }
\CommentTok{#>                   Kappa : 0.605         }
\CommentTok{#>                                         }
\CommentTok{#>  Mcnemar's Test P-Value : NA            }
\CommentTok{#> }
\CommentTok{#> Statistics by Class:}
\CommentTok{#> }
\CommentTok{#>                      Class: 1 Class: 2 Class: 3 Class: 5 Class: 6 Class: 7}
\CommentTok{#> Sensitivity             0.786    0.706   0.1250   1.0000   0.0000    0.909}
\CommentTok{#> Specificity             0.930    0.833   0.9683   0.9242   1.0000    0.967}
\CommentTok{#> Pos Pred Value          0.880    0.571   0.3333   0.5000      NaN    0.833}
\CommentTok{#> Neg Pred Value          0.870    0.900   0.8971   1.0000   0.9718    0.983}
\CommentTok{#> Prevalence              0.394    0.239   0.1127   0.0704   0.0282    0.155}
\CommentTok{#> Detection Rate          0.310    0.169   0.0141   0.0704   0.0000    0.141}
\CommentTok{#> Detection Prevalence    0.352    0.296   0.0423   0.1408   0.0000    0.169}
\CommentTok{#> Balanced Accuracy       0.858    0.770   0.5466   0.9621   0.5000    0.938}
\end{Highlighting}
\end{Shaded}

\hypertarget{comparison-with-resamples}{%
\subsection{Comparison with resamples}\label{comparison-with-resamples}}

Finally, we compare the performance of the two methods by computing the
respective accuracy rates and the kappa indices (as computed by \texttt{classAgreement()}
also contained in package \texttt{e1071}). In Table 1, we summarize the results
of 10 replications---Support Vector Machines show better results.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1234567}\NormalTok{)}

\CommentTok{# SVM}
\NormalTok{fit.svm <-}\StringTok{ }\KeywordTok{train}\NormalTok{(Type }\OperatorTok{~}\NormalTok{., }\DataTypeTok{data =}\NormalTok{ trainset, }
                 \DataTypeTok{method =} \StringTok{"svmRadial"}\NormalTok{)}

\CommentTok{# Random Forest}
\NormalTok{fit.rpart <-}\StringTok{ }\KeywordTok{train}\NormalTok{(Type }\OperatorTok{~}\NormalTok{., }\DataTypeTok{data =}\NormalTok{ trainset, }
                \DataTypeTok{method=}\StringTok{"rpart"}\NormalTok{)}

\CommentTok{# collect resamples}
\NormalTok{results <-}\StringTok{ }\KeywordTok{resamples}\NormalTok{(}\KeywordTok{list}\NormalTok{(}\DataTypeTok{svm =}\NormalTok{ fit.svm, }
                          \DataTypeTok{rpart  =}\NormalTok{ fit.rpart))}

\KeywordTok{summary}\NormalTok{(results)}
\CommentTok{#> }
\CommentTok{#> Call:}
\CommentTok{#> summary.resamples(object = results)}
\CommentTok{#> }
\CommentTok{#> Models: svm, rpart }
\CommentTok{#> Number of resamples: 25 }
\CommentTok{#> }
\CommentTok{#> Accuracy }
\CommentTok{#>        Min. 1st Qu. Median  Mean 3rd Qu.  Max. NA's}
\CommentTok{#> svm   0.510   0.565  0.600 0.599   0.625 0.704    0}
\CommentTok{#> rpart 0.462   0.519  0.554 0.558   0.600 0.660    0}
\CommentTok{#> }
\CommentTok{#> Kappa }
\CommentTok{#>        Min. 1st Qu. Median  Mean 3rd Qu.  Max. NA's}
\CommentTok{#> svm   0.267   0.376  0.406 0.410   0.446 0.559    0}
\CommentTok{#> rpart 0.135   0.299  0.358 0.363   0.443 0.545    0}
\end{Highlighting}
\end{Shaded}

\hypertarget{ozone-svm}{%
\chapter{Ozone SVM}\label{ozone-svm}}

\url{https://cran.r-project.org/web/packages/e1071/vignettes/svmdoc.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(e1071)}
\KeywordTok{library}\NormalTok{(rpart)}

\KeywordTok{data}\NormalTok{(Ozone, }\DataTypeTok{package=}\StringTok{"mlbench"}\NormalTok{)}
\CommentTok{## split data into a train and test set}
\NormalTok{index <-}\StringTok{ }\DecValTok{1}\OperatorTok{:}\KeywordTok{nrow}\NormalTok{(Ozone)}
\NormalTok{testindex <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(index, }\KeywordTok{trunc}\NormalTok{(}\KeywordTok{length}\NormalTok{(index)}\OperatorTok{/}\DecValTok{3}\NormalTok{))}
\NormalTok{testset <-}\StringTok{ }\KeywordTok{na.omit}\NormalTok{(Ozone[testindex,}\OperatorTok{-}\DecValTok{3}\NormalTok{])}
\NormalTok{trainset <-}\StringTok{ }\KeywordTok{na.omit}\NormalTok{(Ozone[}\OperatorTok{-}\NormalTok{testindex,}\OperatorTok{-}\DecValTok{3}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## svm}
\NormalTok{svm.model <-}\StringTok{ }\KeywordTok{svm}\NormalTok{(V4 }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ trainset, }\DataTypeTok{cost =} \DecValTok{1000}\NormalTok{, }\DataTypeTok{gamma =} \FloatTok{0.0001}\NormalTok{)}
\NormalTok{svm.pred  <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(svm.model, testset[,}\OperatorTok{-}\DecValTok{3}\NormalTok{])}
\KeywordTok{crossprod}\NormalTok{(svm.pred }\OperatorTok{-}\StringTok{ }\NormalTok{testset[,}\DecValTok{3}\NormalTok{]) }\OperatorTok{/}\StringTok{ }\KeywordTok{length}\NormalTok{(testindex)}
\CommentTok{#>      [,1]}
\CommentTok{#> [1,] 10.7}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## rpart}
\NormalTok{rpart.model <-}\StringTok{ }\KeywordTok{rpart}\NormalTok{(V4 }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ trainset)}
\NormalTok{rpart.pred  <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(rpart.model, testset[,}\OperatorTok{-}\DecValTok{3}\NormalTok{])}
\KeywordTok{crossprod}\NormalTok{(rpart.pred }\OperatorTok{-}\StringTok{ }\NormalTok{testset[,}\DecValTok{3}\NormalTok{]) }\OperatorTok{/}\StringTok{ }\KeywordTok{length}\NormalTok{(testindex)}
\CommentTok{#>      [,1]}
\CommentTok{#> [1,] 11.9}
\end{Highlighting}
\end{Shaded}

\hypertarget{a-gentle-introduction-to-support-vector-machines-using-r-1}{%
\chapter{A gentle introduction to support vector machines using R}\label{a-gentle-introduction-to-support-vector-machines-using-r-1}}

\url{https://eight2late.wordpress.com/2017/02/07/a-gentle-introduction-to-support-vector-machines-using-r/}

\hypertarget{support-vector-machines-in-r-1}{%
\section{Support vector machines in R}\label{support-vector-machines-in-r-1}}

In this demo we'll use the svm interface that is implemented in the \texttt{e1071} R package. This interface provides R programmers access to the comprehensive \texttt{libsvm} library written by Chang and Lin. I'll use two toy datasets: the famous iris dataset available with the base R package and the sonar dataset from the mlbench package. I won't describe details of the datasets as they are discussed at length in the documentation that I have linked to. However, it is worth mentioning the reasons why I chose these datasets:

As mentioned earlier, no real life dataset is linearly separable, but the iris dataset is almost so. Consequently, it is a good illustration of using linear SVMs. Although one almost never uses these in practice, I have illustrated their use primarily for pedagogical reasons.
The sonar dataset is a good illustration of the benefits of using RBF kernels in cases where the dataset is hard to visualise (60 variables in this case!). In general, one would almost always use RBF (or other nonlinear) kernels in practice.

With that said, let's get right to it. I assume you have R and RStudio installed. For instructions on how to do this, have a look at the first article in this series. The processing preliminaries -- loading libraries, data and creating training and test datasets are much the same as in my previous articles so I won't dwell on these here. For completeness, however, I'll list all the code so you can run it directly in R or R studio (a complete listing of the code can be found here):

\hypertarget{svm-on-iris-dataset-1}{%
\section{\texorpdfstring{SVM on \texttt{iris} dataset}{SVM on iris dataset}}\label{svm-on-iris-dataset-1}}

\hypertarget{training-and-test-datasets-1}{%
\subsection{Training and test datasets}\label{training-and-test-datasets-1}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#load required library}
\KeywordTok{library}\NormalTok{(e1071)}

\CommentTok{#load built-in iris dataset}
\KeywordTok{data}\NormalTok{(iris)}

\CommentTok{#set seed to ensure reproducible results}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{42}\NormalTok{)}

\CommentTok{#split into training and test sets}
\NormalTok{iris[, }\StringTok{"train"}\NormalTok{] <-}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(}\KeywordTok{runif}\NormalTok{(}\KeywordTok{nrow}\NormalTok{(iris)) }\OperatorTok{<}\StringTok{ }\FloatTok{0.8}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)}

\CommentTok{#separate training and test sets}
\NormalTok{trainset <-}\StringTok{ }\NormalTok{iris[iris}\OperatorTok{$}\NormalTok{train }\OperatorTok{==}\StringTok{ }\DecValTok{1}\NormalTok{,]}
\NormalTok{testset <-}\StringTok{ }\NormalTok{iris[iris}\OperatorTok{$}\NormalTok{train }\OperatorTok{==}\StringTok{ }\DecValTok{0}\NormalTok{,]}

\CommentTok{#get column index of train flag}
\NormalTok{trainColNum <-}\StringTok{ }\KeywordTok{grep}\NormalTok{(}\StringTok{"train"}\NormalTok{, }\KeywordTok{names}\NormalTok{(trainset))}

\CommentTok{#remove train flag column from train and test sets}
\NormalTok{trainset <-}\StringTok{ }\NormalTok{trainset[,}\OperatorTok{-}\NormalTok{trainColNum]}
\NormalTok{testset <-}\StringTok{ }\NormalTok{testset[,}\OperatorTok{-}\NormalTok{trainColNum]}

\KeywordTok{dim}\NormalTok{(trainset)}
\CommentTok{#> [1] 115   5}
\KeywordTok{dim}\NormalTok{(testset)}
\CommentTok{#> [1] 35  5}
\end{Highlighting}
\end{Shaded}

\hypertarget{build-the-svm-model-1}{%
\subsection{Build the SVM model}\label{build-the-svm-model-1}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#get column index of predicted variable in dataset}
\NormalTok{typeColNum <-}\StringTok{ }\KeywordTok{grep}\NormalTok{(}\StringTok{"Species"}\NormalTok{, }\KeywordTok{names}\NormalTok{(iris))}

\CommentTok{#build model  linear kernel and C-classification (soft margin) with default cost (C=1)}
\NormalTok{svm_model <-}\StringTok{ }\KeywordTok{svm}\NormalTok{(Species}\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ trainset, }
                 \DataTypeTok{method =} \StringTok{"C-classification"}\NormalTok{, }
                 \DataTypeTok{kernel =} \StringTok{"linear"}\NormalTok{)}
\NormalTok{svm_model}
\CommentTok{#> }
\CommentTok{#> Call:}
\CommentTok{#> svm(formula = Species ~ ., data = trainset, method = "C-classification", }
\CommentTok{#>     kernel = "linear")}
\CommentTok{#> }
\CommentTok{#> }
\CommentTok{#> Parameters:}
\CommentTok{#>    SVM-Type:  C-classification }
\CommentTok{#>  SVM-Kernel:  linear }
\CommentTok{#>        cost:  1 }
\CommentTok{#>       gamma:  0.25 }
\CommentTok{#> }
\CommentTok{#> Number of Support Vectors:  24}
\end{Highlighting}
\end{Shaded}

The output from the SVM model show that there are 24 support vectors. If desired, these can be examined using the SV variable in the model -- i.e via svm\_model\$SV.

\hypertarget{support-vectors-1}{%
\subsection{Support Vectors}\label{support-vectors-1}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# support vectors}
\NormalTok{svm_model}\OperatorTok{$}\NormalTok{SV}
\CommentTok{#>     Sepal.Length Sepal.Width Petal.Length Petal.Width}
\CommentTok{#> 19       -0.2564      1.7668       -1.323      -1.305}
\CommentTok{#> 42       -1.7006     -1.7045       -1.559      -1.305}
\CommentTok{#> 45       -0.9785      1.7668       -1.205      -1.171}
\CommentTok{#> 53        1.1878      0.1469        0.568       0.309}
\CommentTok{#> 55        0.7064     -0.5474        0.390       0.309}
\CommentTok{#> 57        0.4657      0.6097        0.450       0.443}
\CommentTok{#> 58       -1.2192     -1.4730       -0.378      -0.364}
\CommentTok{#> 69        0.3453     -1.9359        0.331       0.309}
\CommentTok{#> 71       -0.0157      0.3783        0.509       0.712}
\CommentTok{#> 73        0.4657     -1.2416        0.568       0.309}
\CommentTok{#> 78        0.9471     -0.0845        0.627       0.578}
\CommentTok{#> 84        0.1046     -0.7788        0.686       0.443}
\CommentTok{#> 85       -0.6174     -0.0845        0.331       0.309}
\CommentTok{#> 86        0.1046      0.8412        0.331       0.443}
\CommentTok{#> 99       -0.9785     -1.2416       -0.555      -0.229}
\CommentTok{#> 107      -1.2192     -1.2416        0.331       0.578}
\CommentTok{#> 111       0.7064      0.3783        0.686       0.981}
\CommentTok{#> 117       0.7064     -0.0845        0.922       0.712}
\CommentTok{#> 124       0.4657     -0.7788        0.568       0.712}
\CommentTok{#> 130       1.5488     -0.0845        1.099       0.443}
\CommentTok{#> 138       0.5860      0.1469        0.922       0.712}
\CommentTok{#> 139       0.1046     -0.0845        0.509       0.712}
\CommentTok{#> 147       0.4657     -1.2416        0.627       0.847}
\CommentTok{#> 150      -0.0157     -0.0845        0.686       0.712}
\end{Highlighting}
\end{Shaded}

The test prediction accuracy indicates that the linear performs quite well on this dataset, confirming that it is indeed near linearly separable. To check performance by class, one can create a confusion matrix as described in my post on random forests. I'll leave this as an exercise for you. Another point is that we have used a soft-margin classification scheme with a cost C=1. You can experiment with this by explicitly changing the value of C. Again, I'll leave this for you an exercise.

\hypertarget{predictions-on-training-model-3}{%
\subsection{Predictions on training model}\label{predictions-on-training-model-3}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# training set predictions}
\NormalTok{pred_train <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(svm_model, trainset)}
\KeywordTok{mean}\NormalTok{(pred_train }\OperatorTok{==}\StringTok{ }\NormalTok{trainset}\OperatorTok{$}\NormalTok{Species)}
\CommentTok{#> [1] 0.983}
\CommentTok{# [1] 0.9826087}
\end{Highlighting}
\end{Shaded}

\hypertarget{predictions-on-test-model-3}{%
\subsection{Predictions on test model}\label{predictions-on-test-model-3}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# test set predictions}
\NormalTok{pred_test <-}\KeywordTok{predict}\NormalTok{(svm_model, testset)}
\KeywordTok{mean}\NormalTok{(pred_test }\OperatorTok{==}\StringTok{ }\NormalTok{testset}\OperatorTok{$}\NormalTok{Species)}
\CommentTok{#> [1] 0.914}
\CommentTok{# [1] 0.9142857}
\end{Highlighting}
\end{Shaded}

\hypertarget{confusion-matrix-and-accuracy-1}{%
\subsection{Confusion matrix and Accuracy}\label{confusion-matrix-and-accuracy-1}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# confusion matrix}
\NormalTok{cm <-}\StringTok{ }\KeywordTok{table}\NormalTok{(pred_test, testset}\OperatorTok{$}\NormalTok{Species)}
\NormalTok{cm}
\CommentTok{#>             }
\CommentTok{#> pred_test    setosa versicolor virginica}
\CommentTok{#>   setosa         18          0         0}
\CommentTok{#>   versicolor      0          5         3}
\CommentTok{#>   virginica       0          0         9}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# accuracy}
\KeywordTok{sum}\NormalTok{(}\KeywordTok{diag}\NormalTok{(cm)) }\OperatorTok{/}\StringTok{ }\KeywordTok{sum}\NormalTok{(cm)}
\CommentTok{#> [1] 0.914}
\end{Highlighting}
\end{Shaded}

\hypertarget{svm-with-radial-basis-function-kernel.-linear-1}{%
\section{SVM with Radial Basis Function kernel. Linear}\label{svm-with-radial-basis-function-kernel.-linear-1}}

\hypertarget{training-and-test-sets-1}{%
\subsection{Training and test sets}\label{training-and-test-sets-1}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#load required library (assuming e1071 is already loaded)}
\KeywordTok{library}\NormalTok{(mlbench)}

\CommentTok{#load Sonar dataset}
\KeywordTok{data}\NormalTok{(Sonar)}
\CommentTok{#set seed to ensure reproducible results}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{42}\NormalTok{)}
\CommentTok{#split into training and test sets}
\NormalTok{Sonar[, }\StringTok{"train"}\NormalTok{] <-}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(}\KeywordTok{runif}\NormalTok{(}\KeywordTok{nrow}\NormalTok{(Sonar))}\OperatorTok{<}\FloatTok{0.8}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{)}

\CommentTok{#separate training and test sets}
\NormalTok{trainset <-}\StringTok{ }\NormalTok{Sonar[Sonar}\OperatorTok{$}\NormalTok{train}\OperatorTok{==}\DecValTok{1}\NormalTok{,]}
\NormalTok{testset <-}\StringTok{ }\NormalTok{Sonar[Sonar}\OperatorTok{$}\NormalTok{train}\OperatorTok{==}\DecValTok{0}\NormalTok{,]}

\CommentTok{#get column index of train flag}
\NormalTok{trainColNum <-}\StringTok{ }\KeywordTok{grep}\NormalTok{(}\StringTok{"train"}\NormalTok{,}\KeywordTok{names}\NormalTok{(trainset))}
\CommentTok{#remove train flag column from train and test sets}
\NormalTok{trainset <-}\StringTok{ }\NormalTok{trainset[,}\OperatorTok{-}\NormalTok{trainColNum]}
\NormalTok{testset <-}\StringTok{ }\NormalTok{testset[,}\OperatorTok{-}\NormalTok{trainColNum]}

\CommentTok{#get column index of predicted variable in dataset}
\NormalTok{typeColNum <-}\StringTok{ }\KeywordTok{grep}\NormalTok{(}\StringTok{"Class"}\NormalTok{,}\KeywordTok{names}\NormalTok{(Sonar))}
\end{Highlighting}
\end{Shaded}

\hypertarget{predictions-on-training-model-4}{%
\subsection{Predictions on Training model}\label{predictions-on-training-model-4}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#build model  linear kernel and C-classification with default cost (C=1)}
\NormalTok{svm_model <-}\StringTok{ }\KeywordTok{svm}\NormalTok{(Class}\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data=}\NormalTok{trainset, }
                 \DataTypeTok{method=}\StringTok{"C-classification"}\NormalTok{, }
                 \DataTypeTok{kernel=}\StringTok{"linear"}\NormalTok{)}

\CommentTok{#training set predictions}
\NormalTok{pred_train <-}\KeywordTok{predict}\NormalTok{(svm_model,trainset)}
\KeywordTok{mean}\NormalTok{(pred_train}\OperatorTok{==}\NormalTok{trainset}\OperatorTok{$}\NormalTok{Class)}
\CommentTok{#> [1] 0.97}
\end{Highlighting}
\end{Shaded}

\hypertarget{predictions-on-test-model-4}{%
\subsection{Predictions on test model}\label{predictions-on-test-model-4}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#test set predictions}
\NormalTok{pred_test <-}\KeywordTok{predict}\NormalTok{(svm_model,testset)}
\KeywordTok{mean}\NormalTok{(pred_test}\OperatorTok{==}\NormalTok{testset}\OperatorTok{$}\NormalTok{Class)}
\CommentTok{#> [1] 0.605}
\end{Highlighting}
\end{Shaded}

I'll leave you to examine the contents of the model. The important point to note here is that the performance of the model with the test set is quite dismal compared to the previous case. This simply indicates that the linear kernel is not appropriate here. Let's take a look at what happens if we use the RBF kernel with default values for the parameters:

\hypertarget{svm-with-radial-basis-function-kernel.-non-linear-1}{%
\section{SVM with Radial Basis Function kernel. Non-linear}\label{svm-with-radial-basis-function-kernel.-non-linear-1}}

\hypertarget{predictions-on-training-model-5}{%
\subsection{Predictions on training model}\label{predictions-on-training-model-5}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#build model: radial kernel, default params}
\NormalTok{svm_model <-}\StringTok{ }\KeywordTok{svm}\NormalTok{(Class}\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data=}\NormalTok{trainset, }
                 \DataTypeTok{method=}\StringTok{"C-classification"}\NormalTok{, }
                 \DataTypeTok{kernel=}\StringTok{"radial"}\NormalTok{)}
\CommentTok{# print params}
\NormalTok{svm_model}\OperatorTok{$}\NormalTok{cost}
\CommentTok{#> [1] 1}
\NormalTok{svm_model}\OperatorTok{$}\NormalTok{gamma}
\CommentTok{#> [1] 0.0167}

\CommentTok{#training set predictions}
\NormalTok{pred_train <-}\KeywordTok{predict}\NormalTok{(svm_model,trainset)}
\KeywordTok{mean}\NormalTok{(pred_train}\OperatorTok{==}\NormalTok{trainset}\OperatorTok{$}\NormalTok{Class)}
\CommentTok{#> [1] 0.988}
\end{Highlighting}
\end{Shaded}

\hypertarget{predictions-on-test-model-5}{%
\subsection{Predictions on test model}\label{predictions-on-test-model-5}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#test set predictions}
\NormalTok{pred_test <-}\KeywordTok{predict}\NormalTok{(svm_model,testset)}
\KeywordTok{mean}\NormalTok{(pred_test}\OperatorTok{==}\NormalTok{testset}\OperatorTok{$}\NormalTok{Class)}
\CommentTok{#> [1] 0.767}
\end{Highlighting}
\end{Shaded}

That's a pretty decent improvement from the linear kernel. Let's see if we can do better by doing some parameter tuning. To do this we first invoke tune.svm and use the parameters it gives us in the call to svm:

\hypertarget{tuning-of-parameters-1}{%
\subsection{Tuning of parameters}\label{tuning-of-parameters-1}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# find optimal parameters in a specified range}
\NormalTok{tune_out <-}\StringTok{ }\KeywordTok{tune.svm}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ trainset[,}\OperatorTok{-}\NormalTok{typeColNum], }
                     \DataTypeTok{y =}\NormalTok{ trainset[, typeColNum], }
                     \DataTypeTok{gamma =} \DecValTok{10}\OperatorTok{^}\NormalTok{(}\OperatorTok{-}\DecValTok{3}\OperatorTok{:}\DecValTok{3}\NormalTok{), }
                     \DataTypeTok{cost =} \KeywordTok{c}\NormalTok{(}\FloatTok{0.01}\NormalTok{, }\FloatTok{0.1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{100}\NormalTok{, }\DecValTok{1000}\NormalTok{), }
                     \DataTypeTok{kernel =} \StringTok{"radial"}\NormalTok{)}

\CommentTok{#print best values of cost and gamma}
\NormalTok{tune_out}\OperatorTok{$}\NormalTok{best.parameters}\OperatorTok{$}\NormalTok{cost}
\CommentTok{#> [1] 10}
\NormalTok{tune_out}\OperatorTok{$}\NormalTok{best.parameters}\OperatorTok{$}\NormalTok{gamma}
\CommentTok{#> [1] 0.01}

\CommentTok{#build model}
\NormalTok{svm_model <-}\StringTok{ }\KeywordTok{svm}\NormalTok{(Class}\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ trainset, }
                 \DataTypeTok{method =} \StringTok{"C-classification"}\NormalTok{, }
                 \DataTypeTok{kernel =} \StringTok{"radial"}\NormalTok{, }
                 \DataTypeTok{cost =}\NormalTok{ tune_out}\OperatorTok{$}\NormalTok{best.parameters}\OperatorTok{$}\NormalTok{cost, }
                 \DataTypeTok{gamma =}\NormalTok{ tune_out}\OperatorTok{$}\NormalTok{best.parameters}\OperatorTok{$}\NormalTok{gamma)}
\end{Highlighting}
\end{Shaded}

\hypertarget{prediction-on-training-model-with-new-parameters-1}{%
\subsection{Prediction on training model with new parameters}\label{prediction-on-training-model-with-new-parameters-1}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# training set predictions}
\NormalTok{pred_train <-}\KeywordTok{predict}\NormalTok{(svm_model,trainset)}
\KeywordTok{mean}\NormalTok{(pred_train}\OperatorTok{==}\NormalTok{trainset}\OperatorTok{$}\NormalTok{Class)}
\CommentTok{#> [1] 1}
\end{Highlighting}
\end{Shaded}

\hypertarget{prediction-on-test-model-with-new-parameters-1}{%
\subsection{Prediction on test model with new parameters}\label{prediction-on-test-model-with-new-parameters-1}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# test set predictions}
\NormalTok{pred_test <-}\KeywordTok{predict}\NormalTok{(svm_model,testset)}
\KeywordTok{mean}\NormalTok{(pred_test}\OperatorTok{==}\NormalTok{testset}\OperatorTok{$}\NormalTok{Class)}
\CommentTok{#> [1] 0.814}
\end{Highlighting}
\end{Shaded}

Which is fairly decent improvement on the un-optimised case.

\hypertarget{wrapping-up-1}{%
\section{Wrapping up}\label{wrapping-up-1}}

This bring us to the end of this introductory exploration of SVMs in R. To recap, the distinguishing feature of SVMs in contrast to most other techniques is that they attempt to construct optimal separation boundaries between different categories.

SVMs are quite versatile and have been applied to a wide variety of domains ranging from chemistry to pattern recognition. They are best used in binary classification scenarios. This brings up a question as to where SVMs are to be preferred to other binary classification techniques such as logistic regression. The honest response is, ``it depends'' -- but here are some points to keep in mind when choosing between the two. A general point to keep in mind is that SVM algorithms tend to be expensive both in terms of memory and computation, issues that can start to hurt as the size of the dataset increases.

Given all the above caveats and considerations, the best way to figure out whether an SVM approach will work for your problem may be to do what most machine learning practitioners do: try it out!

\hypertarget{sms-spam.-naive-bayes.-classification}{%
\chapter{SMS spam. Naive Bayes. Classification}\label{sms-spam.-naive-bayes.-classification}}

Dataset: \url{https://github.com/stedy/Machine-Learning-with-R-datasets/blob/master/sms_spam.csv}

Instructions: Machine Learning with R. Page 104.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tictoc)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sms_raw <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\KeywordTok{file.path}\NormalTok{(data_raw_dir, }\StringTok{"sms_spam.csv"}\NormalTok{), }\DataTypeTok{stringsAsFactors =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str}\NormalTok{(sms_raw)}
\CommentTok{#> 'data.frame':    5574 obs. of  2 variables:}
\CommentTok{#>  $ type: chr  "ham" "ham" "spam" "ham" ...}
\CommentTok{#>  $ text: chr  "Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat..." "Ok lar... Joking wif u oni..." "Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question("| __truncated__ "U dun say so early hor... U c already then say..." ...}
\end{Highlighting}
\end{Shaded}

\hypertarget{convert-type-to-a-factor}{%
\subsection{convert type to a factor}\label{convert-type-to-a-factor}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sms_raw}\OperatorTok{$}\NormalTok{type <-}\StringTok{ }\KeywordTok{factor}\NormalTok{(sms_raw}\OperatorTok{$}\NormalTok{type)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str}\NormalTok{(sms_raw}\OperatorTok{$}\NormalTok{type)}
\CommentTok{#>  Factor w/ 2 levels "ham","spam": 1 1 2 1 1 2 1 1 2 2 ...}
\end{Highlighting}
\end{Shaded}

How many email of type ham or spam:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{table}\NormalTok{(sms_raw}\OperatorTok{$}\NormalTok{type)}
\CommentTok{#> }
\CommentTok{#>  ham spam }
\CommentTok{#> 4827  747}
\end{Highlighting}
\end{Shaded}

Create the corpus:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tm)}
\CommentTok{#> Loading required package: NLP}

\NormalTok{sms_corpus <-}\StringTok{ }\KeywordTok{VCorpus}\NormalTok{(}\KeywordTok{VectorSource}\NormalTok{(sms_raw}\OperatorTok{$}\NormalTok{text))}
\KeywordTok{print}\NormalTok{(sms_corpus)}
\CommentTok{#> <<VCorpus>>}
\CommentTok{#> Metadata:  corpus specific: 0, document level (indexed): 0}
\CommentTok{#> Content:  documents: 5574}
\end{Highlighting}
\end{Shaded}

Let's see a couple of documents:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{inspect}\NormalTok{(sms_corpus[}\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{])}
\CommentTok{#> <<VCorpus>>}
\CommentTok{#> Metadata:  corpus specific: 0, document level (indexed): 0}
\CommentTok{#> Content:  documents: 2}
\CommentTok{#> }
\CommentTok{#> [[1]]}
\CommentTok{#> <<PlainTextDocument>>}
\CommentTok{#> Metadata:  7}
\CommentTok{#> Content:  chars: 111}
\CommentTok{#> }
\CommentTok{#> [[2]]}
\CommentTok{#> <<PlainTextDocument>>}
\CommentTok{#> Metadata:  7}
\CommentTok{#> Content:  chars: 29}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# show some text}
\KeywordTok{as.character}\NormalTok{(sms_corpus[[}\DecValTok{1}\NormalTok{]])}
\CommentTok{#> [1] "Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat..."}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# show three documents}
\KeywordTok{lapply}\NormalTok{(sms_corpus[}\DecValTok{1}\OperatorTok{:}\DecValTok{3}\NormalTok{], as.character)}
\CommentTok{#> $`1`}
\CommentTok{#> [1] "Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat..."}
\CommentTok{#> }
\CommentTok{#> $`2`}
\CommentTok{#> [1] "Ok lar... Joking wif u oni..."}
\CommentTok{#> }
\CommentTok{#> $`3`}
\CommentTok{#> [1] "Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's"}
\end{Highlighting}
\end{Shaded}

\hypertarget{some-conversion}{%
\section{Some conversion}\label{some-conversion}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# convert to lowercase}
\NormalTok{sms_corpus_clean <-}\StringTok{ }\KeywordTok{tm_map}\NormalTok{(sms_corpus, }\KeywordTok{content_transformer}\NormalTok{(tolower))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{as.character}\NormalTok{(sms_corpus[[}\DecValTok{1}\NormalTok{]])}
\CommentTok{#> [1] "Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat..."}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# converted to lowercase}
\KeywordTok{as.character}\NormalTok{(sms_corpus_clean[[}\DecValTok{1}\NormalTok{]])}
\CommentTok{#> [1] "go until jurong point, crazy.. available only in bugis n great world la e buffet... cine there got amore wat..."}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# remove numbers}
\NormalTok{sms_corpus_clean <-}\StringTok{ }\KeywordTok{tm_map}\NormalTok{(sms_corpus_clean, removeNumbers)}
\end{Highlighting}
\end{Shaded}

What transformations are available

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# what transformations are available}
\KeywordTok{getTransformations}\NormalTok{()}
\CommentTok{#> [1] "removeNumbers"     "removePunctuation" "removeWords"      }
\CommentTok{#> [4] "stemDocument"      "stripWhitespace"}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# remove stop words}
\NormalTok{sms_corpus_clean <-}\StringTok{ }\KeywordTok{tm_map}\NormalTok{(sms_corpus_clean, removeWords, }\KeywordTok{stopwords}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# remove punctuation}
\NormalTok{sms_corpus_clean <-}\StringTok{ }\KeywordTok{tm_map}\NormalTok{(sms_corpus_clean, removePunctuation)}
\end{Highlighting}
\end{Shaded}

Stemming:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(SnowballC)}
\KeywordTok{wordStem}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"learn"}\NormalTok{, }\StringTok{"learned"}\NormalTok{, }\StringTok{"learning"}\NormalTok{, }\StringTok{"learns"}\NormalTok{))}
\CommentTok{#> [1] "learn" "learn" "learn" "learn"}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# stemming corpus}
\NormalTok{sms_corpus_clean <-}\StringTok{ }\KeywordTok{tm_map}\NormalTok{(sms_corpus_clean, stemDocument)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# remove white spaces}
\NormalTok{sms_corpus_clean <-}\StringTok{ }\KeywordTok{tm_map}\NormalTok{(sms_corpus_clean, stripWhitespace)}
\end{Highlighting}
\end{Shaded}

Show what we've got so far

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# show what we've got so far}
\KeywordTok{lapply}\NormalTok{(sms_corpus[}\DecValTok{1}\OperatorTok{:}\DecValTok{3}\NormalTok{], as.character)}
\CommentTok{#> $`1`}
\CommentTok{#> [1] "Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat..."}
\CommentTok{#> }
\CommentTok{#> $`2`}
\CommentTok{#> [1] "Ok lar... Joking wif u oni..."}
\CommentTok{#> }
\CommentTok{#> $`3`}
\CommentTok{#> [1] "Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's"}

\KeywordTok{lapply}\NormalTok{(sms_corpus_clean[}\DecValTok{1}\OperatorTok{:}\DecValTok{3}\NormalTok{], as.character)}
\CommentTok{#> $`1`}
\CommentTok{#> [1] "go jurong point crazi avail bugi n great world la e buffet cine got amor wat"}
\CommentTok{#> }
\CommentTok{#> $`2`}
\CommentTok{#> [1] "ok lar joke wif u oni"}
\CommentTok{#> }
\CommentTok{#> $`3`}
\CommentTok{#> [1] "free entri wkli comp win fa cup final tkts st may text fa receiv entri questionstd txt ratetc appli s"}
\end{Highlighting}
\end{Shaded}

\hypertarget{convert-to-document-term-matrix-dtm}{%
\section{Convert to Document Term Matrix (dtm}\label{convert-to-document-term-matrix-dtm}}

)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sms_dtm <-}\StringTok{ }\KeywordTok{DocumentTermMatrix}\NormalTok{(sms_corpus_clean)}
\NormalTok{sms_dtm}
\CommentTok{#> <<DocumentTermMatrix (documents: 5574, terms: 6592)>>}
\CommentTok{#> Non-/sparse entries: 42608/36701200}
\CommentTok{#> Sparsity           : 100%}
\CommentTok{#> Maximal term length: 40}
\CommentTok{#> Weighting          : term frequency (tf)}
\end{Highlighting}
\end{Shaded}

\hypertarget{split-in-training-and-test-datasets}{%
\section{split in training and test datasets}\label{split-in-training-and-test-datasets}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sms_dtm_train <-}\StringTok{ }\NormalTok{sms_dtm[}\DecValTok{1}\OperatorTok{:}\DecValTok{4169}\NormalTok{, ]}
\NormalTok{sms_dtm_test  <-}\StringTok{ }\NormalTok{sms_dtm[}\DecValTok{4170}\OperatorTok{:}\DecValTok{5559}\NormalTok{, ]}
\end{Highlighting}
\end{Shaded}

\hypertarget{separate-the-labels}{%
\subsection{separate the labels}\label{separate-the-labels}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sms_train_labels <-}\StringTok{ }\NormalTok{sms_raw[}\DecValTok{1}\OperatorTok{:}\DecValTok{4169}\NormalTok{, ]}\OperatorTok{$}\NormalTok{type}
\NormalTok{sms_test_labels  <-}\StringTok{ }\NormalTok{sms_raw[}\DecValTok{4170}\OperatorTok{:}\DecValTok{5559}\NormalTok{, ]}\OperatorTok{$}\NormalTok{type}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{prop.table}\NormalTok{(}\KeywordTok{table}\NormalTok{(sms_train_labels))}
\CommentTok{#> sms_train_labels}
\CommentTok{#>   ham  spam }
\CommentTok{#> 0.865 0.135}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{prop.table}\NormalTok{(}\KeywordTok{table}\NormalTok{(sms_test_labels))}
\CommentTok{#> sms_test_labels}
\CommentTok{#>  ham spam }
\CommentTok{#> 0.87 0.13}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# convert dtm to matrix}
\NormalTok{sms_mat_train <-}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{(}\KeywordTok{t}\NormalTok{(sms_dtm_train))}
\NormalTok{dtm.rs <-}\StringTok{ }\KeywordTok{sort}\NormalTok{(}\KeywordTok{rowSums}\NormalTok{(sms_mat_train), }\DataTypeTok{decreasing=}\OtherTok{TRUE}\NormalTok{)}

\CommentTok{# dataframe with word-frequency}
\NormalTok{dtm.df <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{word =} \KeywordTok{names}\NormalTok{(dtm.rs), }\DataTypeTok{freq =} \KeywordTok{as.integer}\NormalTok{(dtm.rs),}
                     \DataTypeTok{stringsAsFactors =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{plot-wordcloud}{%
\section{plot wordcloud}\label{plot-wordcloud}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(wordcloud)}
\CommentTok{#> Loading required package: RColorBrewer}
\KeywordTok{wordcloud}\NormalTok{(sms_corpus_clean, }\DataTypeTok{min.freq =} \DecValTok{50}\NormalTok{, }\DataTypeTok{random.order =} \OtherTok{FALSE}\NormalTok{)}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): tone could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): also could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): look could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): start could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): smile could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): urgent could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): use could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): someth could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): place could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): gud could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): guy could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): custom could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): next could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): person could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): someon could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): tonight could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): went could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): around could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): talk could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): chat could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): money could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): collect could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): mani could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): per could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): soon could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): gonna could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): plan could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): alway could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): nice could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): check could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): dun could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): special could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): told could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): box could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): lot could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): shop could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): hello could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): hour could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): mean could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): month could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): guarante could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): peopl could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): happen could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): thk could not be fit on page. It will not be plotted.}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_225-sms_spam-tm-nb_files/figure-latex/unnamed-chunk-29-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spam <-}\StringTok{ }\KeywordTok{subset}\NormalTok{(sms_raw, type }\OperatorTok{==}\StringTok{ "spam"}\NormalTok{)}
\NormalTok{ham  <-}\StringTok{ }\KeywordTok{subset}\NormalTok{(sms_raw, type }\OperatorTok{==}\StringTok{ "ham"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Words related to \textbf{spam}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{wordcloud}\NormalTok{(spam}\OperatorTok{$}\NormalTok{text, }\DataTypeTok{max.words =} \DecValTok{40}\NormalTok{, }\DataTypeTok{scale =} \KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\FloatTok{0.5}\NormalTok{))}
\CommentTok{#> Warning in tm_map.SimpleCorpus(corpus, tm::removePunctuation):}
\CommentTok{#> transformation drops documents}
\CommentTok{#> Warning in tm_map.SimpleCorpus(corpus, function(x) tm::removeWords(x,}
\CommentTok{#> tm::stopwords())): transformation drops documents}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_225-sms_spam-tm-nb_files/figure-latex/unnamed-chunk-31-1} \end{center}

Words related to \textbf{ham}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{wordcloud}\NormalTok{(ham}\OperatorTok{$}\NormalTok{text, }\DataTypeTok{max.words =} \DecValTok{40}\NormalTok{, }\DataTypeTok{scale =} \KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\FloatTok{0.5}\NormalTok{))}
\CommentTok{#> Warning in tm_map.SimpleCorpus(corpus, tm::removePunctuation):}
\CommentTok{#> transformation drops documents}
\CommentTok{#> Warning in tm_map.SimpleCorpus(corpus, function(x) tm::removeWords(x,}
\CommentTok{#> tm::stopwords())): transformation drops documents}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_225-sms_spam-tm-nb_files/figure-latex/unnamed-chunk-32-1} \end{center}

\hypertarget{limit-frequent-words}{%
\section{Limit Frequent words}\label{limit-frequent-words}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# words that appear at least in 5 messages}
\NormalTok{sms_freq_words <-}\StringTok{ }\KeywordTok{findFreqTerms}\NormalTok{(sms_dtm_train, }\DecValTok{6}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str}\NormalTok{(sms_freq_words)}
\CommentTok{#>  chr [1:997] "abiola" "abl" "abt" "accept" "access" "account" "across" ...}
\end{Highlighting}
\end{Shaded}

\hypertarget{get-only-frequent-words}{%
\subsection{get only frequent words}\label{get-only-frequent-words}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sms_dtm_freq_train<-}\StringTok{ }\NormalTok{sms_dtm_train[ , sms_freq_words]}
\NormalTok{sms_dtm_freq_test <-}\StringTok{ }\NormalTok{sms_dtm_test[ , sms_freq_words]}
\end{Highlighting}
\end{Shaded}

\hypertarget{function-to-change-value-to-yesno}{%
\subsection{function to change value to Yes/No}\label{function-to-change-value-to-yesno}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{convert_counts <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x) \{}
\NormalTok{    x <-}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(x }\OperatorTok{>}\StringTok{ }\DecValTok{0}\NormalTok{, }\StringTok{"Yes"}\NormalTok{, }\StringTok{"No"}\NormalTok{)}
\NormalTok{  \}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# change from number to Yes/No}
\CommentTok{# also the result returns a matrix}
\NormalTok{sms_train <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(sms_dtm_freq_train, }\DataTypeTok{MARGIN =} \DecValTok{2}\NormalTok{,}
\NormalTok{                                       convert_counts)}
\NormalTok{sms_test  <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(sms_dtm_freq_test, }\DataTypeTok{MARGIN =} \DecValTok{2}\NormalTok{,}
\NormalTok{                                      convert_counts)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# matrix of}
\CommentTok{# 4169 documents as rows}
\CommentTok{# 1159 terms as columns}
\KeywordTok{dim}\NormalTok{(sms_train)}
\CommentTok{#> [1] 4169  997}
\KeywordTok{length}\NormalTok{(sms_train_labels)}
\CommentTok{#> [1] 4169}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# this is how the matrix looks}
\NormalTok{sms_train[}\DecValTok{1}\OperatorTok{:}\DecValTok{10}\NormalTok{, }\DecValTok{10}\OperatorTok{:}\DecValTok{15}\NormalTok{]}
\CommentTok{#>     Terms}
\CommentTok{#> Docs add  address admir advanc aft  afternoon}
\CommentTok{#>   1  "No" "No"    "No"  "No"   "No" "No"     }
\CommentTok{#>   2  "No" "No"    "No"  "No"   "No" "No"     }
\CommentTok{#>   3  "No" "No"    "No"  "No"   "No" "No"     }
\CommentTok{#>   4  "No" "No"    "No"  "No"   "No" "No"     }
\CommentTok{#>   5  "No" "No"    "No"  "No"   "No" "No"     }
\CommentTok{#>   6  "No" "No"    "No"  "No"   "No" "No"     }
\CommentTok{#>   7  "No" "No"    "No"  "No"   "No" "No"     }
\CommentTok{#>   8  "No" "No"    "No"  "No"   "No" "No"     }
\CommentTok{#>   9  "No" "No"    "No"  "No"   "No" "No"     }
\CommentTok{#>   10 "No" "No"    "No"  "No"   "No" "No"}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(e1071)}
\NormalTok{sms_classifier <-}\StringTok{ }\KeywordTok{naiveBayes}\NormalTok{(sms_train, sms_train_labels)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{tic}\NormalTok{()}
\NormalTok{sms_test_pred <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(sms_classifier, sms_test)}
\KeywordTok{toc}\NormalTok{()}
\CommentTok{#> 20.665 sec elapsed}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(gmodels)}
\KeywordTok{CrossTable}\NormalTok{(sms_test_pred, sms_test_labels,}
    \DataTypeTok{prop.chisq =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{prop.t =} \OtherTok{FALSE}\NormalTok{,}
    \DataTypeTok{dnn =} \KeywordTok{c}\NormalTok{(}\StringTok{'predicted'}\NormalTok{, }\StringTok{'actual'}\NormalTok{))}
\CommentTok{#> }
\CommentTok{#>  }
\CommentTok{#>    Cell Contents}
\CommentTok{#> |-------------------------|}
\CommentTok{#> |                       N |}
\CommentTok{#> |           N / Row Total |}
\CommentTok{#> |           N / Col Total |}
\CommentTok{#> |-------------------------|}
\CommentTok{#> }
\CommentTok{#>  }
\CommentTok{#> Total Observations in Table:  1390 }
\CommentTok{#> }
\CommentTok{#>  }
\CommentTok{#>              | actual }
\CommentTok{#>    predicted |       ham |      spam | Row Total | }
\CommentTok{#> -------------|-----------|-----------|-----------|}
\CommentTok{#>          ham |      1202 |        21 |      1223 | }
\CommentTok{#>              |     0.983 |     0.017 |     0.880 | }
\CommentTok{#>              |     0.994 |     0.116 |           | }
\CommentTok{#> -------------|-----------|-----------|-----------|}
\CommentTok{#>         spam |         7 |       160 |       167 | }
\CommentTok{#>              |     0.042 |     0.958 |     0.120 | }
\CommentTok{#>              |     0.006 |     0.884 |           | }
\CommentTok{#> -------------|-----------|-----------|-----------|}
\CommentTok{#> Column Total |      1209 |       181 |      1390 | }
\CommentTok{#>              |     0.870 |     0.130 |           | }
\CommentTok{#> -------------|-----------|-----------|-----------|}
\CommentTok{#> }
\CommentTok{#> }
\end{Highlighting}
\end{Shaded}

\begin{quote}
Misclassified:
20+9 (frequency = 5)
25+7 (freq=4)
23+7 (freq=3)
25+8 (freq=2)
21+7 (freq=6)
\end{quote}

\begin{quote}
Decreasing the minimum word frequency doesn't make the model better.
\end{quote}

\hypertarget{improve-model-performance}{%
\section{Improve model performance}\label{improve-model-performance}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sms_classifier2 <-}\StringTok{ }\KeywordTok{naiveBayes}\NormalTok{(sms_train, sms_train_labels, }
                              \DataTypeTok{laplace =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{tic}\NormalTok{()}
\NormalTok{sms_test_pred2 <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(sms_classifier2, sms_test)}
\KeywordTok{toc}\NormalTok{()}
\CommentTok{#> 20.166 sec elapsed}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{CrossTable}\NormalTok{(sms_test_pred2, sms_test_labels,}
    \DataTypeTok{prop.chisq =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{prop.t =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{prop.r =} \OtherTok{FALSE}\NormalTok{,}
    \DataTypeTok{dnn =} \KeywordTok{c}\NormalTok{(}\StringTok{'predicted'}\NormalTok{, }\StringTok{'actual'}\NormalTok{))}
\CommentTok{#> }
\CommentTok{#>  }
\CommentTok{#>    Cell Contents}
\CommentTok{#> |-------------------------|}
\CommentTok{#> |                       N |}
\CommentTok{#> |           N / Col Total |}
\CommentTok{#> |-------------------------|}
\CommentTok{#> }
\CommentTok{#>  }
\CommentTok{#> Total Observations in Table:  1390 }
\CommentTok{#> }
\CommentTok{#>  }
\CommentTok{#>              | actual }
\CommentTok{#>    predicted |       ham |      spam | Row Total | }
\CommentTok{#> -------------|-----------|-----------|-----------|}
\CommentTok{#>          ham |      1203 |        28 |      1231 | }
\CommentTok{#>              |     0.995 |     0.155 |           | }
\CommentTok{#> -------------|-----------|-----------|-----------|}
\CommentTok{#>         spam |         6 |       153 |       159 | }
\CommentTok{#>              |     0.005 |     0.845 |           | }
\CommentTok{#> -------------|-----------|-----------|-----------|}
\CommentTok{#> Column Total |      1209 |       181 |      1390 | }
\CommentTok{#>              |     0.870 |     0.130 |           | }
\CommentTok{#> -------------|-----------|-----------|-----------|}
\CommentTok{#> }
\CommentTok{#> }
\end{Highlighting}
\end{Shaded}

\begin{quote}
Misclassified: 28+7
\end{quote}

\hypertarget{classification-tree-vehicle-example}{%
\chapter{Classification Tree: Vehicle example}\label{classification-tree-vehicle-example}}

\begin{itemize}
\tightlist
\item
  Dataset: Vehicle (mlbench)
\item
  Instructions: book ``Applied Predictive Modeling Techniques'', Lewis, N.D.
\end{itemize}

\hypertarget{load-packages}{%
\section{Load packages}\label{load-packages}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tree)}
\KeywordTok{library}\NormalTok{(mlbench)}

\KeywordTok{data}\NormalTok{(Vehicle)}
\KeywordTok{str}\NormalTok{(Vehicle)}
\CommentTok{#> 'data.frame':    846 obs. of  19 variables:}
\CommentTok{#>  $ Comp        : num  95 91 104 93 85 107 97 90 86 93 ...}
\CommentTok{#>  $ Circ        : num  48 41 50 41 44 57 43 43 34 44 ...}
\CommentTok{#>  $ D.Circ      : num  83 84 106 82 70 106 73 66 62 98 ...}
\CommentTok{#>  $ Rad.Ra      : num  178 141 209 159 205 172 173 157 140 197 ...}
\CommentTok{#>  $ Pr.Axis.Ra  : num  72 57 66 63 103 50 65 65 61 62 ...}
\CommentTok{#>  $ Max.L.Ra    : num  10 9 10 9 52 6 6 9 7 11 ...}
\CommentTok{#>  $ Scat.Ra     : num  162 149 207 144 149 255 153 137 122 183 ...}
\CommentTok{#>  $ Elong       : num  42 45 32 46 45 26 42 48 54 36 ...}
\CommentTok{#>  $ Pr.Axis.Rect: num  20 19 23 19 19 28 19 18 17 22 ...}
\CommentTok{#>  $ Max.L.Rect  : num  159 143 158 143 144 169 143 146 127 146 ...}
\CommentTok{#>  $ Sc.Var.Maxis: num  176 170 223 160 241 280 176 162 141 202 ...}
\CommentTok{#>  $ Sc.Var.maxis: num  379 330 635 309 325 957 361 281 223 505 ...}
\CommentTok{#>  $ Ra.Gyr      : num  184 158 220 127 188 264 172 164 112 152 ...}
\CommentTok{#>  $ Skew.Maxis  : num  70 72 73 63 127 85 66 67 64 64 ...}
\CommentTok{#>  $ Skew.maxis  : num  6 9 14 6 9 5 13 3 2 4 ...}
\CommentTok{#>  $ Kurt.maxis  : num  16 14 9 10 11 9 1 3 14 14 ...}
\CommentTok{#>  $ Kurt.Maxis  : num  187 189 188 199 180 181 200 193 200 195 ...}
\CommentTok{#>  $ Holl.Ra     : num  197 199 196 207 183 183 204 202 208 204 ...}
\CommentTok{#>  $ Class       : Factor w/ 4 levels "bus","opel","saab",..: 4 4 3 4 1 1 1 4 4 3 ...}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(Vehicle[}\DecValTok{1}\NormalTok{])}
\CommentTok{#>       Comp      }
\CommentTok{#>  Min.   : 73.0  }
\CommentTok{#>  1st Qu.: 87.0  }
\CommentTok{#>  Median : 93.0  }
\CommentTok{#>  Mean   : 93.7  }
\CommentTok{#>  3rd Qu.:100.0  }
\CommentTok{#>  Max.   :119.0}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(Vehicle[}\DecValTok{2}\NormalTok{])}
\CommentTok{#>       Circ     }
\CommentTok{#>  Min.   :33.0  }
\CommentTok{#>  1st Qu.:40.0  }
\CommentTok{#>  Median :44.0  }
\CommentTok{#>  Mean   :44.9  }
\CommentTok{#>  3rd Qu.:49.0  }
\CommentTok{#>  Max.   :59.0}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{attributes}\NormalTok{(Vehicle}\OperatorTok{$}\NormalTok{Class)}
\CommentTok{#> $levels}
\CommentTok{#> [1] "bus"  "opel" "saab" "van" }
\CommentTok{#> }
\CommentTok{#> $class}
\CommentTok{#> [1] "factor"}
\end{Highlighting}
\end{Shaded}

\hypertarget{prepare-data}{%
\section{Prepare data}\label{prepare-data}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{107}\NormalTok{)}
\NormalTok{N =}\StringTok{ }\KeywordTok{nrow}\NormalTok{(Vehicle)}
\NormalTok{train <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\NormalTok{N, }\DecValTok{500}\NormalTok{, }\OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# training and test sets}
\NormalTok{trainset <-}\StringTok{ }\NormalTok{Vehicle[train,]}
\NormalTok{testset  <-}\StringTok{ }\NormalTok{Vehicle[}\OperatorTok{-}\NormalTok{train,]}
\end{Highlighting}
\end{Shaded}

\hypertarget{estimate-the-decision-tree}{%
\section{Estimate the decision tree}\label{estimate-the-decision-tree}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit <-}\StringTok{ }\KeywordTok{tree}\NormalTok{(Class }\OperatorTok{~}\NormalTok{., }\DataTypeTok{data =}\NormalTok{ trainset, }\DataTypeTok{split =} \StringTok{"deviance"}\NormalTok{)}
\NormalTok{fit}
\CommentTok{#> node), split, n, deviance, yval, (yprob)}
\CommentTok{#>       * denotes terminal node}
\CommentTok{#> }
\CommentTok{#>   1) root 500 1000 opel ( 0 0 0 0 )  }
\CommentTok{#>     2) Elong < 41.5 215  500 saab ( 0 0 0 0 )  }
\CommentTok{#>       4) Max.L.Ra < 7.5 51   50 bus ( 1 0 0 0 )  }
\CommentTok{#>         8) Comp < 93.5 12   20 bus ( 0 0 0 0 )  }
\CommentTok{#>          16) Pr.Axis.Ra < 67.5 7    8 saab ( 0 0 1 0 ) *}
\CommentTok{#>          17) Pr.Axis.Ra > 67.5 5    0 bus ( 1 0 0 0 ) *}
\CommentTok{#>         9) Comp > 93.5 39    9 bus ( 1 0 0 0 ) *}
\CommentTok{#>       5) Max.L.Ra > 7.5 164  200 opel ( 0 1 0 0 )  }
\CommentTok{#>        10) Sc.Var.maxis < 723 149  200 saab ( 0 0 1 0 )  }
\CommentTok{#>          20) Comp < 109.5 137  200 opel ( 0 1 0 0 ) *}
\CommentTok{#>          21) Comp > 109.5 12    0 saab ( 0 0 1 0 ) *}
\CommentTok{#>        11) Sc.Var.maxis > 723 15    7 opel ( 0 1 0 0 ) *}
\CommentTok{#>     3) Elong > 41.5 285  700 van ( 0 0 0 0 )  }
\CommentTok{#>       6) Sc.Var.maxis < 305.5 116  200 van ( 0 0 0 1 )  }
\CommentTok{#>        12) Max.L.Rect < 128.5 40   90 saab ( 0 0 0 0 )  }
\CommentTok{#>          24) Scat.Ra < 120.5 15   30 van ( 0 0 0 1 ) *}
\CommentTok{#>          25) Scat.Ra > 120.5 25   30 saab ( 0 0 1 0 ) *}
\CommentTok{#>        13) Max.L.Rect > 128.5 76   90 van ( 0 0 0 1 )  }
\CommentTok{#>          26) Max.L.Rect < 138.5 38   60 van ( 0 0 0 1 )  }
\CommentTok{#>            52) Circ < 37.5 17   10 van ( 0 0 0 1 ) *}
\CommentTok{#>            53) Circ > 37.5 21   40 opel ( 0 0 0 0 ) *}
\CommentTok{#>          27) Max.L.Rect > 138.5 38   20 van ( 0 0 0 1 ) *}
\CommentTok{#>       7) Sc.Var.maxis > 305.5 169  400 bus ( 0 0 0 0 )  }
\CommentTok{#>        14) Max.L.Ra < 8.5 116  200 bus ( 1 0 0 0 )  }
\CommentTok{#>          28) D.Circ < 76.5 97  100 bus ( 1 0 0 0 )  }
\CommentTok{#>            56) Skew.maxis < 10.5 87   70 bus ( 1 0 0 0 )  }
\CommentTok{#>             112) Max.L.Rect < 134.5 12   20 bus ( 0 0 0 0 ) *}
\CommentTok{#>             113) Max.L.Rect > 134.5 75   20 bus ( 1 0 0 0 ) *}
\CommentTok{#>            57) Skew.maxis > 10.5 10   20 opel ( 0 0 0 0 ) *}
\CommentTok{#>          29) D.Circ > 76.5 19   30 opel ( 0 1 0 0 ) *}
\CommentTok{#>        15) Max.L.Ra > 8.5 53   20 van ( 0 0 0 1 ) *}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# fit <- tree(Class ~., data = Vehicle[train,], split ="deviance")}
\CommentTok{# fit}
\end{Highlighting}
\end{Shaded}

\begin{quote}
We use deviance as the splitting criteria, a common alternative is to use
split=``gini''.
\end{quote}

\begin{quote}
At each branch of the tree (after root) we see in order:
1. The branch number (e.g.~in this case 1,2,14 and 15);
2. the split (e.g.~Elong \textless{} 41.5);
3. the number of samples going along that split (e.g.~229);
4. the deviance associated with that split (e.g.~489.1);
5. the predicted class (e.g.~opel);
6. the associated probabilities (e.g. ( 0.222707 0.410480 0.366812 0.000000
);
7. and for a terminal node (or leaf), the symbol "*".
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(fit)}
\CommentTok{#> }
\CommentTok{#> Classification tree:}
\CommentTok{#> tree(formula = Class ~ ., data = trainset, split = "deviance")}
\CommentTok{#> Variables actually used in tree construction:}
\CommentTok{#>  [1] "Elong"        "Max.L.Ra"     "Comp"         "Pr.Axis.Ra"  }
\CommentTok{#>  [5] "Sc.Var.maxis" "Max.L.Rect"   "Scat.Ra"      "Circ"        }
\CommentTok{#>  [9] "D.Circ"       "Skew.maxis"  }
\CommentTok{#> Number of terminal nodes:  16 }
\CommentTok{#> Residual mean deviance:  0.943 = 456 / 484 }
\CommentTok{#> Misclassification error rate: 0.252 = 126 / 500}
\end{Highlighting}
\end{Shaded}

\begin{quote}
Notice that summary(fit) shows:
1. The type of tree, in this case a Classification tree;
2. the formula used to fit the tree;
3. the variables used to fit the tree;
4. the number of terminal nodes in this case 15;
5. the residual mean deviance - 0.9381;
6. the misclassification error rate 0.232 or 23.2\%.
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(fit); }\KeywordTok{text}\NormalTok{(fit)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_226-vehicle-mlbench-tree_files/figure-latex/unnamed-chunk-7-1} \end{center}

\hypertarget{assess-model}{%
\section{Assess model}\label{assess-model}}

Unfortunately, classification trees have a tendency to overfit the data. One
approach to reduce this risk is to use cross-validation. For each hold out
sample we fit the model and note at what level the tree gives the best results
(using deviance or the misclassification rate). Then we hold out a different
sample and repeat. This can be carried out using the \texttt{cv.tree()} function.
We use a leave-one-out cross-validation using the misclassification rate and
deviance (\texttt{FUN=prune.misclass}, followed by \texttt{FUN=prune.tree}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitM.cv <-}\StringTok{ }\KeywordTok{cv.tree}\NormalTok{(fit, }\DataTypeTok{K=}\DecValTok{346}\NormalTok{, }\DataTypeTok{FUN =}\NormalTok{ prune.misclass)}
\NormalTok{fitP.cv <-}\StringTok{ }\KeywordTok{cv.tree}\NormalTok{(fit, }\DataTypeTok{K=}\DecValTok{346}\NormalTok{, }\DataTypeTok{FUN =}\NormalTok{ prune.tree)}
\end{Highlighting}
\end{Shaded}

The results are plotted out side by side in Figure 1.2. The jagged lines
shows where the minimum deviance / misclassification occurred with the
cross-validated tree. Since the cross validated misclassification and deviance
both reach their minimum close to the number of branches in the original
fitted tree there is little to be gained from pruning this tree

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\KeywordTok{plot}\NormalTok{(fitM.cv)}
\KeywordTok{plot}\NormalTok{(fitP.cv)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_226-vehicle-mlbench-tree_files/figure-latex/plot-fit-1} \end{center}

\hypertarget{make-predictions-1}{%
\section{Make predictions}\label{make-predictions-1}}

We use the validation data set and the fitted decision tree to predict vehicle
classes; then we display the confusion matrix and calculate the error rate of
the fitted tree. Overall, the model has an error rate of 32\%.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{testLabels <-}\StringTok{ }\NormalTok{Vehicle}\OperatorTok{$}\NormalTok{Class[}\OperatorTok{-}\NormalTok{train]}
\NormalTok{testLabels}
\CommentTok{#>   [1] van  bus  bus  van  van  bus  bus  saab opel bus  van  saab van  saab}
\CommentTok{#>  [15] saab van  saab opel van  saab saab saab bus  bus  saab opel bus  opel}
\CommentTok{#>  [29] bus  opel van  opel opel saab saab bus  bus  bus  van  van  saab opel}
\CommentTok{#>  [43] bus  opel van  opel saab bus  van  bus  opel van  saab bus  opel bus }
\CommentTok{#>  [57] opel opel van  bus  van  saab opel bus  van  saab opel opel saab saab}
\CommentTok{#>  [71] saab opel bus  van  bus  opel bus  saab bus  bus  bus  opel opel van }
\CommentTok{#>  [85] saab bus  bus  bus  van  saab opel van  van  bus  bus  opel bus  opel}
\CommentTok{#>  [99] saab opel bus  opel bus  saab van  van  saab saab bus  van  opel van }
\CommentTok{#> [113] saab opel saab saab van  van  van  van  bus  bus  opel bus  bus  van }
\CommentTok{#> [127] saab bus  opel bus  bus  bus  bus  opel van  saab saab bus  opel van }
\CommentTok{#> [141] bus  saab bus  van  bus  opel van  saab opel saab opel van  saab van }
\CommentTok{#> [155] saab opel bus  van  bus  saab saab opel opel bus  bus  opel van  van }
\CommentTok{#> [169] bus  van  van  saab bus  saab opel saab opel bus  bus  bus  saab bus }
\CommentTok{#> [183] opel opel saab saab saab van  van  opel opel van  van  opel bus  saab}
\CommentTok{#> [197] bus  van  opel opel bus  bus  bus  opel saab opel van  bus  opel opel}
\CommentTok{#> [211] saab opel bus  opel opel opel van  opel van  saab saab van  saab saab}
\CommentTok{#> [225] saab saab van  van  van  saab bus  van  van  bus  saab opel saab saab}
\CommentTok{#> [239] opel saab saab saab saab van  saab opel bus  saab bus  opel opel opel}
\CommentTok{#> [253] saab bus  van  opel saab opel bus  bus  saab van  opel bus  saab van }
\CommentTok{#> [267] opel saab saab saab saab van  opel bus  bus  bus  opel saab saab saab}
\CommentTok{#> [281] van  saab bus  opel saab van  opel bus  saab saab opel opel van  saab}
\CommentTok{#> [295] bus  opel bus  van  van  opel bus  bus  saab bus  van  saab bus  van }
\CommentTok{#> [309] saab van  opel bus  bus  opel saab opel bus  bus  saab van  saab saab}
\CommentTok{#> [323] bus  opel opel opel bus  saab bus  van  bus  van  saab opel saab van }
\CommentTok{#> [337] opel opel van  bus  saab saab van  saab opel saab}
\CommentTok{#> Levels: bus opel saab van}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Confusion Matrix}
\NormalTok{pred <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(fit, }\DataTypeTok{newdata =}\NormalTok{ testset)}
\CommentTok{# find column whih has the maximum of all rows }
\NormalTok{pred.class <-}\StringTok{ }\KeywordTok{colnames}\NormalTok{(pred)[}\KeywordTok{max.col}\NormalTok{(pred, }\DataTypeTok{ties.method =} \KeywordTok{c}\NormalTok{(}\StringTok{"random"}\NormalTok{))]}
\NormalTok{cm <-}\StringTok{ }\KeywordTok{table}\NormalTok{(testLabels, pred.class, }
      \DataTypeTok{dnn =} \KeywordTok{c}\NormalTok{(}\StringTok{"Observed Class"}\NormalTok{, }\StringTok{"Predicted Class"}\NormalTok{))}
\NormalTok{cm}
\CommentTok{#>               Predicted Class}
\CommentTok{#> Observed Class bus opel saab van}
\CommentTok{#>           bus   85    1    1   5}
\CommentTok{#>           opel   3   70   10   2}
\CommentTok{#>           saab   7   67   14   7}
\CommentTok{#>           van    1    4    5  64}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Sensitivity}
\KeywordTok{sum}\NormalTok{(}\KeywordTok{diag}\NormalTok{(cm)) }\OperatorTok{/}\StringTok{ }\KeywordTok{sum}\NormalTok{(cm)}
\CommentTok{#> [1] 0.673}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# pred <- predict(fit, newdata = Vehicle[-train,])}
\CommentTok{# pred.class <- colnames(pred)[max.col(pred, ties.method = c("random"))]}
\CommentTok{# table(Vehicle$Class[-train], pred.class, }
\CommentTok{#       dnn = c("Observed Class", "Predicted Class"))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{error_rate =}\StringTok{ }\NormalTok{(}\DecValTok{1} \OperatorTok{-}\StringTok{ }\KeywordTok{sum}\NormalTok{(pred.class }\OperatorTok{==}\StringTok{ }\NormalTok{testset) }\OperatorTok{/}\StringTok{ }\KeywordTok{nrow}\NormalTok{(testset))}
\KeywordTok{round}\NormalTok{(error_rate, }\DecValTok{3}\NormalTok{)}
\CommentTok{#> [1] 0.327}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# error_rate = (1 - sum(pred.class == Vehicle$Class[-train])/346)}
\CommentTok{# round(error_rate,3)}
\end{Highlighting}
\end{Shaded}

\hypertarget{bike-sharing-demand}{%
\chapter{Bike sharing demand}\label{bike-sharing-demand}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#loading the required libraries}
\KeywordTok{library}\NormalTok{(rpart)}
\KeywordTok{library}\NormalTok{(rattle)}
\CommentTok{#> Rattle: A free graphical interface for data science with R.}
\CommentTok{#> Version 5.2.0 Copyright (c) 2006-2018 Togaware Pty Ltd.}
\CommentTok{#> Type 'rattle()' to shake, rattle, and roll your data.}
\KeywordTok{library}\NormalTok{(rpart.plot)}
\KeywordTok{library}\NormalTok{(RColorBrewer)}
\KeywordTok{library}\NormalTok{(randomForest)}
\CommentTok{#> randomForest 4.6-14}
\CommentTok{#> Type rfNews() to see new features/changes/bug fixes.}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'randomForest'}
\CommentTok{#> The following object is masked from 'package:rattle':}
\CommentTok{#> }
\CommentTok{#>     importance}
\KeywordTok{library}\NormalTok{(corrplot)}
\CommentTok{#> corrplot 0.84 loaded}
\KeywordTok{library}\NormalTok{(dplyr)}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'dplyr'}
\CommentTok{#> The following object is masked from 'package:randomForest':}
\CommentTok{#> }
\CommentTok{#>     combine}
\CommentTok{#> The following objects are masked from 'package:stats':}
\CommentTok{#> }
\CommentTok{#>     filter, lag}
\CommentTok{#> The following objects are masked from 'package:base':}
\CommentTok{#> }
\CommentTok{#>     intersect, setdiff, setequal, union}
\KeywordTok{library}\NormalTok{(tictoc)}
\end{Highlighting}
\end{Shaded}

Source: \url{https://www.analyticsvidhya.com/blog/2015/06/solution-kaggle-competition-bike-sharing-demand/}

\hypertarget{hypothesis-generation}{%
\section{Hypothesis Generation}\label{hypothesis-generation}}

Before exploring the data to understand the relationship between variables, I'd recommend you to focus on hypothesis generation first. Now, this might sound counter-intuitive for solving a data science problem, but if there is one thing I have learnt over years, it is this. Before exploring data, you should spend some time thinking about the business problem, gaining the domain knowledge and may be gaining first hand experience of the problem (only if I could travel to North America!)

How does it help? This practice usually helps you form better features later on, which are not biased by the data available in the dataset. At this stage, you are expected to posses structured thinking i.e.~a thinking process which takes into consideration all the possible aspects of a particular problem.

Here are some of the hypothesis which I thought could influence the demand of bikes:

\begin{itemize}
\item
  \textbf{Hourly trend}: There must be high demand during office timings. Early morning and late evening can have different trend (cyclist) and low demand during 10:00 pm to 4:00 am.
\item
  \textbf{Daily Trend}: Registered users demand more bike on weekdays as compared to weekend or holiday.
\item
  \textbf{Rain}: The demand of bikes will be lower on a rainy day as compared to a sunny day. Similarly, higher humidity will cause to lower the demand and vice versa.
\item
  \textbf{Temperature}: Would high or low temperature encourage or disencourage bike riding?
\item
  \textbf{Pollution}: If the pollution level in a city starts soaring, people may start using Bike (it may be influenced by government / company policies or increased awareness).
\item
  \textbf{Time}: Total demand should have higher contribution of registered user as compared to casual because registered user base would increase over time.
\item
  \textbf{Traffic}: It can be positively correlated with Bike demand. Higher traffic may force people to use bike as compared to other road transport medium like car, taxi etc
\end{itemize}

\hypertarget{understanding-the-data-set}{%
\section{Understanding the Data Set}\label{understanding-the-data-set}}

The dataset shows hourly rental data for two years (2011 and 2012). The training data set is for the \textbf{first 19 days of each month}. The test dataset is from \textbf{20th day to month's end}. We are required to predict the total count of bikes rented during each hour covered by the test set.

In the training data set, they have separately given bike demand by registered, casual users and sum of both is given as count.

Training data set has 12 variables (see below) and Test has 9 (excluding registered, casual and count).

\hypertarget{independent-variables}{%
\subsection{Independent variables}\label{independent-variables}}

\begin{verbatim}
datetime:   date and hour in "mm/dd/yyyy hh:mm" format
season:     Four categories-> 1 = spring, 2 = summer, 3 = fall, 4 = winter
holiday:    whether the day is a holiday or not (1/0)
workingday: whether the day is neither a weekend nor holiday (1/0)
weather:    Four Categories of weather
            1-> Clear, Few clouds, Partly cloudy, Partly cloudy
            2-> Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist
            3-> Light Snow and Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds
            4-> Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog
temp:       hourly temperature in Celsius
atemp:      "feels like" temperature in Celsius
humidity:   relative humidity
windspeed:  wind speed
\end{verbatim}

\hypertarget{dependent-variables}{%
\subsection{Dependent variables}\label{dependent-variables}}

\begin{verbatim}
registered: number of registered user
casual:     number of non-registered user
count:      number of total rentals (registered + casual)
\end{verbatim}

\hypertarget{importing-the-dataset-and-data-exploration}{%
\section{Importing the dataset and Data Exploration}\label{importing-the-dataset-and-data-exploration}}

For this solution, I have used R (R Studio 0.99.442) in Windows Environment.

Below are the steps to import and perform data exploration. If you are new to this concept, you can refer this guide on Data Exploration in R

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Import Train and Test Data Set
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# https://www.kaggle.com/c/bike-sharing-demand/data}
\NormalTok{train =}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\KeywordTok{file.path}\NormalTok{(data_raw_dir, }\StringTok{"bike_train.csv"}\NormalTok{))}
\NormalTok{test =}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\KeywordTok{file.path}\NormalTok{(data_raw_dir, }\StringTok{"bike_test.csv"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{glimpse}\NormalTok{(train)}
\CommentTok{#> Observations: 10,886}
\CommentTok{#> Variables: 12}
\CommentTok{#> $ datetime   <fct> 2011-01-01 00:00:00, 2011-01-01 01:00:00, 2011-01-0...}
\CommentTok{#> $ season     <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...}
\CommentTok{#> $ holiday    <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...}
\CommentTok{#> $ workingday <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...}
\CommentTok{#> $ weather    <int> 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, ...}
\CommentTok{#> $ temp       <dbl> 9.84, 9.02, 9.02, 9.84, 9.84, 9.84, 9.02, 8.20, 9.8...}
\CommentTok{#> $ atemp      <dbl> 14.4, 13.6, 13.6, 14.4, 14.4, 12.9, 13.6, 12.9, 14....}
\CommentTok{#> $ humidity   <int> 81, 80, 80, 75, 75, 75, 80, 86, 75, 76, 76, 81, 77,...}
\CommentTok{#> $ windspeed  <dbl> 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 17, 19, 19, 20, 19, 2...}
\CommentTok{#> $ casual     <int> 3, 8, 5, 3, 0, 0, 2, 1, 1, 8, 12, 26, 29, 47, 35, 4...}
\CommentTok{#> $ registered <int> 13, 32, 27, 10, 1, 1, 0, 2, 7, 6, 24, 30, 55, 47, 7...}
\CommentTok{#> $ count      <int> 16, 40, 32, 13, 1, 1, 2, 3, 8, 14, 36, 56, 84, 94, ...}
\KeywordTok{glimpse}\NormalTok{(test)}
\CommentTok{#> Observations: 6,493}
\CommentTok{#> Variables: 9}
\CommentTok{#> $ datetime   <fct> 2011-01-20 00:00:00, 2011-01-20 01:00:00, 2011-01-2...}
\CommentTok{#> $ season     <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...}
\CommentTok{#> $ holiday    <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...}
\CommentTok{#> $ workingday <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...}
\CommentTok{#> $ weather    <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 2, 2, 2, 2, 2, ...}
\CommentTok{#> $ temp       <dbl> 10.66, 10.66, 10.66, 10.66, 10.66, 9.84, 9.02, 9.02...}
\CommentTok{#> $ atemp      <dbl> 11.4, 13.6, 13.6, 12.9, 12.9, 11.4, 10.6, 10.6, 10....}
\CommentTok{#> $ humidity   <int> 56, 56, 56, 56, 56, 60, 60, 55, 55, 52, 48, 45, 42,...}
\CommentTok{#> $ windspeed  <dbl> 26, 0, 0, 11, 11, 15, 15, 15, 19, 15, 20, 11, 0, 7,...}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Combine both Train and Test Data set (to understand the distribution of independent variable together).
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# add variables to test dataset before merging}
\NormalTok{test}\OperatorTok{$}\NormalTok{registered=}\DecValTok{0}
\NormalTok{test}\OperatorTok{$}\NormalTok{casual=}\DecValTok{0}
\NormalTok{test}\OperatorTok{$}\NormalTok{count=}\DecValTok{0}

\NormalTok{data =}\StringTok{ }\KeywordTok{rbind}\NormalTok{(train,test)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Variable Type Identification
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str}\NormalTok{(data)}
\CommentTok{#> 'data.frame':    17379 obs. of  12 variables:}
\CommentTok{#>  $ datetime  : Factor w/ 17379 levels "2011-01-01 00:00:00",..: 1 2 3 4 5 6 7 8 9 10 ...}
\CommentTok{#>  $ season    : int  1 1 1 1 1 1 1 1 1 1 ...}
\CommentTok{#>  $ holiday   : int  0 0 0 0 0 0 0 0 0 0 ...}
\CommentTok{#>  $ workingday: int  0 0 0 0 0 0 0 0 0 0 ...}
\CommentTok{#>  $ weather   : int  1 1 1 1 1 2 1 1 1 1 ...}
\CommentTok{#>  $ temp      : num  9.84 9.02 9.02 9.84 9.84 ...}
\CommentTok{#>  $ atemp     : num  14.4 13.6 13.6 14.4 14.4 ...}
\CommentTok{#>  $ humidity  : int  81 80 80 75 75 75 80 86 75 76 ...}
\CommentTok{#>  $ windspeed : num  0 0 0 0 0 ...}
\CommentTok{#>  $ casual    : num  3 8 5 3 0 0 2 1 1 8 ...}
\CommentTok{#>  $ registered: num  13 32 27 10 1 1 0 2 7 6 ...}
\CommentTok{#>  $ count     : num  16 40 32 13 1 1 2 3 8 14 ...}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Find missing values in the dataset if any
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{table}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(data))}
\CommentTok{#> }
\CommentTok{#>  FALSE }
\CommentTok{#> 208548}
\end{Highlighting}
\end{Shaded}

\begin{quote}
No NAs in the dataset.
\end{quote}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Understand the distribution of numerical variables and generate a frequency table for numeric variables. Analyze the distribution.
\end{enumerate}

\hypertarget{histograms}{%
\subsection{histograms}\label{histograms}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# histograms each attribute}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{4}\NormalTok{))}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{2}\OperatorTok{:}\DecValTok{9}\NormalTok{) \{}
    \KeywordTok{hist}\NormalTok{(data[,i], }\DataTypeTok{main =} \KeywordTok{names}\NormalTok{(data)[i])}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_227-bike_sharing_demand-rf_files/figure-latex/unnamed-chunk-8-1} \end{center}

\hypertarget{density-plots}{%
\subsection{density plots}\label{density-plots}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# density plot for each attribute}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{4}\NormalTok{))}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{2}\OperatorTok{:}\DecValTok{9}\NormalTok{) \{}
    \KeywordTok{plot}\NormalTok{(}\KeywordTok{density}\NormalTok{(data[,i]), }\DataTypeTok{main=}\KeywordTok{names}\NormalTok{(data)[i])}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_227-bike_sharing_demand-rf_files/figure-latex/unnamed-chunk-9-1} \end{center}

\hypertarget{boxplots}{%
\subsection{boxplots}\label{boxplots}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# boxplots for each attribute}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{4}\NormalTok{))}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{2}\OperatorTok{:}\DecValTok{9}\NormalTok{) \{}
    \KeywordTok{boxplot}\NormalTok{(data[,i], }\DataTypeTok{main=}\KeywordTok{names}\NormalTok{(data)[i])}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_227-bike_sharing_demand-rf_files/figure-latex/unnamed-chunk-10-1} \end{center}

\hypertarget{unique-values-of-discrete-variables}{%
\subsection{Unique values of discrete variables}\label{unique-values-of-discrete-variables}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# the discrete variables in this case are integers}
\NormalTok{ints <-}\StringTok{ }\KeywordTok{unlist}\NormalTok{(}\KeywordTok{lapply}\NormalTok{(data, is.integer))}
\KeywordTok{names}\NormalTok{(data)[ints]}
\CommentTok{#> [1] "season"     "holiday"    "workingday" "weather"    "humidity"}
\end{Highlighting}
\end{Shaded}

\begin{quote}
Humidity should not be an integer or discrete variable; it is a continuous or numeric variable.
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# convert humidity to numeric}
\NormalTok{data}\OperatorTok{$}\NormalTok{humidity <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(data}\OperatorTok{$}\NormalTok{humidity)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# list unique values of integer variables}
\NormalTok{ints <-}\StringTok{ }\KeywordTok{unlist}\NormalTok{(}\KeywordTok{lapply}\NormalTok{(data, is.integer))}
\NormalTok{int_vars <-}\StringTok{ }\KeywordTok{names}\NormalTok{(data)[ints]}

\KeywordTok{sapply}\NormalTok{(int_vars, }\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{unique}\NormalTok{(data[x]))}
\CommentTok{#> $season.season}
\CommentTok{#> [1] 1 2 3 4}
\CommentTok{#> }
\CommentTok{#> $holiday.holiday}
\CommentTok{#> [1] 0 1}
\CommentTok{#> }
\CommentTok{#> $workingday.workingday}
\CommentTok{#> [1] 0 1}
\CommentTok{#> }
\CommentTok{#> $weather.weather}
\CommentTok{#> [1] 1 2 3 4}
\end{Highlighting}
\end{Shaded}

\hypertarget{inferences}{%
\subsection{Inferences}\label{inferences}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The variables \texttt{season}, \texttt{holiday}, \texttt{workingday} and \texttt{weather} are discrete (integer).
\item
  Activity is even through all seasons.
\item
  Most of the activitity happens during non-holidays.
\item
  Activity doubles during the working days.
\item
  Activity happens mostly during clear (1) weather.
\item
  temp, atemp and humidity are continuous variables (numeric).
\end{enumerate}

\hypertarget{hypothesis-testing-using-multivariate-analysis}{%
\section{Hypothesis Testing (using multivariate analysis)}\label{hypothesis-testing-using-multivariate-analysis}}

Till now, we have got a fair understanding of the data set. Now, let's test the hypothesis which we had generated earlier. Here I have added some additional hypothesis from the dataset. Let's test them one by one:

\hypertarget{hourly-trend}{%
\subsection{Hourly trend}\label{hourly-trend}}

\begin{quote}
\emph{There must be high demand during office timings. Early morning and late evening can have different trend (cyclist) and low demand during 10:00 pm to 4:00 am.}
\end{quote}

We don't have the variable `hour' with us. But we can extract it using the datetime column.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(data}\OperatorTok{$}\NormalTok{datetime)}
\CommentTok{#> [1] 2011-01-01 00:00:00 2011-01-01 01:00:00 2011-01-01 02:00:00}
\CommentTok{#> [4] 2011-01-01 03:00:00 2011-01-01 04:00:00 2011-01-01 05:00:00}
\CommentTok{#> 17379 Levels: 2011-01-01 00:00:00 2011-01-01 01:00:00 ... 2012-12-31 23:00:00}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{(data}\OperatorTok{$}\NormalTok{datetime)}
\CommentTok{#> [1] "factor"}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# show hour and day from the variable datetime}
\KeywordTok{head}\NormalTok{(}\KeywordTok{substr}\NormalTok{(data}\OperatorTok{$}\NormalTok{datetime, }\DecValTok{12}\NormalTok{, }\DecValTok{13}\NormalTok{))  }\CommentTok{# hour}
\CommentTok{#> [1] "00" "01" "02" "03" "04" "05"}
\KeywordTok{head}\NormalTok{(}\KeywordTok{substr}\NormalTok{(data}\OperatorTok{$}\NormalTok{datetime, }\DecValTok{9}\NormalTok{, }\DecValTok{10}\NormalTok{))   }\CommentTok{# day}
\CommentTok{#> [1] "01" "01" "01" "01" "01" "01"}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# extracting hour}
\NormalTok{data}\OperatorTok{$}\NormalTok{hour =}\StringTok{ }\KeywordTok{substr}\NormalTok{(data}\OperatorTok{$}\NormalTok{datetime,}\DecValTok{12}\NormalTok{,}\DecValTok{13}\NormalTok{)}
\NormalTok{data}\OperatorTok{$}\NormalTok{hour =}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(data}\OperatorTok{$}\NormalTok{hour)}
\KeywordTok{head}\NormalTok{(data}\OperatorTok{$}\NormalTok{hour)}
\CommentTok{#> [1] 00 01 02 03 04 05}
\CommentTok{#> 24 Levels: 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 ... 23}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{### dividing again in train and test}
\CommentTok{# the train dataset is for the first 19 days}
\NormalTok{train =}\StringTok{ }\NormalTok{data[}\KeywordTok{as.integer}\NormalTok{(}\KeywordTok{substr}\NormalTok{(data}\OperatorTok{$}\NormalTok{datetime, }\DecValTok{9}\NormalTok{, }\DecValTok{10}\NormalTok{)) }\OperatorTok{<}\StringTok{ }\DecValTok{20}\NormalTok{,]}

\CommentTok{# the test dataset is from day 20 to the end of the month}
\NormalTok{test =}\StringTok{ }\NormalTok{data[}\KeywordTok{as.integer}\NormalTok{(}\KeywordTok{substr}\NormalTok{(data}\OperatorTok{$}\NormalTok{datetime, }\DecValTok{9}\NormalTok{, }\DecValTok{10}\NormalTok{)) }\OperatorTok{>}\StringTok{ }\DecValTok{19}\NormalTok{,]}
\end{Highlighting}
\end{Shaded}

\hypertarget{boxplot-count-vs-hour-in-training-set}{%
\subsection{boxplot count vs hour in training set}\label{boxplot-count-vs-hour-in-training-set}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{boxplot}\NormalTok{(train}\OperatorTok{$}\NormalTok{count }\OperatorTok{~}\StringTok{ }\NormalTok{train}\OperatorTok{$}\NormalTok{hour, }\DataTypeTok{xlab=}\StringTok{"hour"}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{"count of users"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_227-bike_sharing_demand-rf_files/figure-latex/unnamed-chunk-19-1} \end{center}

\begin{quote}
Rides increase from 6 am to 6pm, during office hours.
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# casual users}
\NormalTok{casual <-}\StringTok{ }\NormalTok{data[data}\OperatorTok{$}\NormalTok{casual }\OperatorTok{>}\StringTok{ }\DecValTok{0}\NormalTok{, ]}
\NormalTok{registered <-}\StringTok{ }\NormalTok{data[data}\OperatorTok{$}\NormalTok{registered }\OperatorTok{>}\StringTok{ }\DecValTok{0}\NormalTok{, ]}

\KeywordTok{dim}\NormalTok{(casual)}
\CommentTok{#> [1] 9900   13}
\KeywordTok{dim}\NormalTok{(registered)}
\CommentTok{#> [1] 10871    13}
\end{Highlighting}
\end{Shaded}

\hypertarget{boxplot-hourly-casual-vs-registered-users-in-the-training-set}{%
\subsection{Boxplot hourly: casual vs registered users in the training set}\label{boxplot-hourly-casual-vs-registered-users-in-the-training-set}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# by hour: casual vs registered users}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\KeywordTok{boxplot}\NormalTok{(train}\OperatorTok{$}\NormalTok{casual }\OperatorTok{~}\StringTok{ }\NormalTok{train}\OperatorTok{$}\NormalTok{hour, }\DataTypeTok{xlab=}\StringTok{"hour"}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{"casual users"}\NormalTok{)}
\KeywordTok{boxplot}\NormalTok{(train}\OperatorTok{$}\NormalTok{registered }\OperatorTok{~}\StringTok{ }\NormalTok{train}\OperatorTok{$}\NormalTok{hour, }\DataTypeTok{xlab=}\StringTok{"hour"}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{"registered users"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_227-bike_sharing_demand-rf_files/figure-latex/unnamed-chunk-21-1} \end{center}

\begin{quote}
Casual and Registered users have different distributions. Casual users tend to rent more during office hours.
\end{quote}

\hypertarget{outliers-in-the-training-set}{%
\subsection{outliers in the training set}\label{outliers-in-the-training-set}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\KeywordTok{boxplot}\NormalTok{(train}\OperatorTok{$}\NormalTok{count }\OperatorTok{~}\StringTok{ }\NormalTok{train}\OperatorTok{$}\NormalTok{hour, }\DataTypeTok{xlab=}\StringTok{"hour"}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{"count of users"}\NormalTok{)}
\KeywordTok{boxplot}\NormalTok{(}\KeywordTok{log}\NormalTok{(train}\OperatorTok{$}\NormalTok{count) }\OperatorTok{~}\StringTok{ }\NormalTok{train}\OperatorTok{$}\NormalTok{hour,}\DataTypeTok{xlab=}\StringTok{"hour"}\NormalTok{,}\DataTypeTok{ylab=}\StringTok{"log(count)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_227-bike_sharing_demand-rf_files/figure-latex/unnamed-chunk-22-1} \end{center}

\hypertarget{daily-trend}{%
\subsection{Daily trend}\label{daily-trend}}

\begin{quote}
\emph{Registered users demand more bike on weekdays as compared to weekend or holiday.}
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# extracting days of week}
\NormalTok{date <-}\StringTok{ }\KeywordTok{substr}\NormalTok{(data}\OperatorTok{$}\NormalTok{datetime, }\DecValTok{1}\NormalTok{, }\DecValTok{10}\NormalTok{)}
\NormalTok{days <-}\StringTok{ }\KeywordTok{weekdays}\NormalTok{(}\KeywordTok{as.Date}\NormalTok{(date))}
\NormalTok{data}\OperatorTok{$}\NormalTok{day <-}\StringTok{ }\NormalTok{days}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# split the dataset again at day 20 of the month, before and after}
\NormalTok{train =}\StringTok{ }\NormalTok{data[}\KeywordTok{as.integer}\NormalTok{(}\KeywordTok{substr}\NormalTok{(data}\OperatorTok{$}\NormalTok{datetime,}\DecValTok{9}\NormalTok{,}\DecValTok{10}\NormalTok{)) }\OperatorTok{<}\StringTok{ }\DecValTok{20}\NormalTok{,]}
\NormalTok{test  =}\StringTok{ }\NormalTok{data[}\KeywordTok{as.integer}\NormalTok{(}\KeywordTok{substr}\NormalTok{(data}\OperatorTok{$}\NormalTok{datetime,}\DecValTok{9}\NormalTok{,}\DecValTok{10}\NormalTok{)) }\OperatorTok{>}\StringTok{ }\DecValTok{19}\NormalTok{,]}
\end{Highlighting}
\end{Shaded}

\hypertarget{boxplot-daily-trend-casual-vs-registered-users-training-set}{%
\subsection{Boxplot daily trend: casual vs registered users, training set}\label{boxplot-daily-trend-casual-vs-registered-users-training-set}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# creating boxplots for rentals with different variables to see the variation}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\KeywordTok{boxplot}\NormalTok{(train}\OperatorTok{$}\NormalTok{casual }\OperatorTok{~}\StringTok{ }\NormalTok{train}\OperatorTok{$}\NormalTok{day, }\DataTypeTok{xlab=}\StringTok{"day"}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{"casual users"}\NormalTok{)}
\KeywordTok{boxplot}\NormalTok{(train}\OperatorTok{$}\NormalTok{registered }\OperatorTok{~}\StringTok{ }\NormalTok{train}\OperatorTok{$}\NormalTok{day, }\DataTypeTok{xlab=}\StringTok{"day"}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{"registered users"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_227-bike_sharing_demand-rf_files/figure-latex/unnamed-chunk-25-1} \end{center}

\begin{quote}
Demand of casual users increases during the weekend, contrary of registered users.
\end{quote}

\hypertarget{rain}{%
\subsection{Rain}\label{rain}}

\begin{quote}
\emph{The demand of bikes will be lower on a rainy day as compared to a sunny day. Similarly, higher humidity will cause to lower the demand and vice versa.}
\end{quote}

We use the variable weather (1 to 4) to analyze riding under rain conditions.

\hypertarget{boxplot-of-rain-effect-on-bike-riding-training-set}{%
\subsubsection{Boxplot of rain effect on bike riding, training set}\label{boxplot-of-rain-effect-on-bike-riding-training-set}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\KeywordTok{boxplot}\NormalTok{(train}\OperatorTok{$}\NormalTok{casual }\OperatorTok{~}\StringTok{ }\NormalTok{train}\OperatorTok{$}\NormalTok{weather, }\DataTypeTok{xlab=}\StringTok{"day"}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{"casual users"}\NormalTok{)}
\KeywordTok{boxplot}\NormalTok{(train}\OperatorTok{$}\NormalTok{registered }\OperatorTok{~}\StringTok{ }\NormalTok{train}\OperatorTok{$}\NormalTok{weather, }\DataTypeTok{xlab=}\StringTok{"day"}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{"registered users"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_227-bike_sharing_demand-rf_files/figure-latex/unnamed-chunk-26-1} \end{center}

\begin{quote}
Registered used tend to ride even with rain.
\end{quote}

\hypertarget{temperature}{%
\subsection{Temperature}\label{temperature}}

\begin{quote}
\emph{Would high or low temperature encourage or disencourage bike riding?}
\end{quote}

\hypertarget{boxplot-of-temperature-effect-training-set}{%
\subsubsection{boxplot of temperature effect, training set}\label{boxplot-of-temperature-effect-training-set}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\KeywordTok{boxplot}\NormalTok{(train}\OperatorTok{$}\NormalTok{casual }\OperatorTok{~}\StringTok{ }\NormalTok{train}\OperatorTok{$}\NormalTok{temp, }\DataTypeTok{xlab=}\StringTok{"temp"}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{"casual users"}\NormalTok{)}
\KeywordTok{boxplot}\NormalTok{(train}\OperatorTok{$}\NormalTok{registered }\OperatorTok{~}\StringTok{ }\NormalTok{train}\OperatorTok{$}\NormalTok{temp, }\DataTypeTok{xlab=}\StringTok{"temp"}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{"registered users"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_227-bike_sharing_demand-rf_files/figure-latex/unnamed-chunk-27-1} \end{center}

\begin{quote}
Casual users tend to ride with milder temperatures while registered users ride even at low temperatures.
\end{quote}

\hypertarget{correlation}{%
\subsection{Correlation}\label{correlation}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sub =}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(train}\OperatorTok{$}\NormalTok{registered, train}\OperatorTok{$}\NormalTok{casual, train}\OperatorTok{$}\NormalTok{count, train}\OperatorTok{$}\NormalTok{temp,}
\NormalTok{                 train}\OperatorTok{$}\NormalTok{humidity, train}\OperatorTok{$}\NormalTok{atemp, train}\OperatorTok{$}\NormalTok{windspeed)}
\KeywordTok{cor}\NormalTok{(sub)}
\CommentTok{#>                  train.registered train.casual train.count train.temp}
\CommentTok{#> train.registered           1.0000       0.4972       0.971     0.3186}
\CommentTok{#> train.casual               0.4972       1.0000       0.690     0.4671}
\CommentTok{#> train.count                0.9709       0.6904       1.000     0.3945}
\CommentTok{#> train.temp                 0.3186       0.4671       0.394     1.0000}
\CommentTok{#> train.humidity            -0.2655      -0.3482      -0.317    -0.0649}
\CommentTok{#> train.atemp                0.3146       0.4621       0.390     0.9849}
\CommentTok{#> train.windspeed            0.0911       0.0923       0.101    -0.0179}
\CommentTok{#>                  train.humidity train.atemp train.windspeed}
\CommentTok{#> train.registered        -0.2655      0.3146          0.0911}
\CommentTok{#> train.casual            -0.3482      0.4621          0.0923}
\CommentTok{#> train.count             -0.3174      0.3898          0.1014}
\CommentTok{#> train.temp              -0.0649      0.9849         -0.0179}
\CommentTok{#> train.humidity           1.0000     -0.0435         -0.3186}
\CommentTok{#> train.atemp             -0.0435      1.0000         -0.0575}
\CommentTok{#> train.windspeed         -0.3186     -0.0575          1.0000}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# do not show the diagonal}
\KeywordTok{corrplot}\NormalTok{(}\KeywordTok{cor}\NormalTok{(sub), }\DataTypeTok{diag =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_227-bike_sharing_demand-rf_files/figure-latex/unnamed-chunk-29-1} \end{center}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  correlation between \texttt{casual} and atemp, temp.
\item
  Strong correlation between temp and atemp.
\end{enumerate}

\hypertarget{activity-by-year}{%
\subsection{Activity by year}\label{activity-by-year}}

\hypertarget{year-extraction}{%
\subsubsection{Year extraction}\label{year-extraction}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# extracting year}
\NormalTok{data}\OperatorTok{$}\NormalTok{year =}\StringTok{ }\KeywordTok{substr}\NormalTok{(data}\OperatorTok{$}\NormalTok{datetime, }\DecValTok{1}\NormalTok{, }\DecValTok{4}\NormalTok{)}
\NormalTok{data}\OperatorTok{$}\NormalTok{year =}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(data}\OperatorTok{$}\NormalTok{year)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# ignore the division of data again and again, this could have been done together also}
\NormalTok{train =}\StringTok{ }\NormalTok{data[}\KeywordTok{as.integer}\NormalTok{(}\KeywordTok{substr}\NormalTok{(data}\OperatorTok{$}\NormalTok{datetime,}\DecValTok{9}\NormalTok{,}\DecValTok{10}\NormalTok{)) }\OperatorTok{<}\StringTok{ }\DecValTok{20}\NormalTok{,]}
\NormalTok{test =}\StringTok{ }\NormalTok{data[}\KeywordTok{as.integer}\NormalTok{(}\KeywordTok{substr}\NormalTok{(data}\OperatorTok{$}\NormalTok{datetime,}\DecValTok{9}\NormalTok{,}\DecValTok{10}\NormalTok{)) }\OperatorTok{>}\StringTok{ }\DecValTok{19}\NormalTok{,]}
\end{Highlighting}
\end{Shaded}

\hypertarget{trend-by-year-training-set}{%
\subsubsection{Trend by year, training set}\label{trend-by-year-training-set}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\CommentTok{# again some boxplots with different variables}
\CommentTok{# these boxplots give important information about the dependent variable with respect to the independent variables}
\KeywordTok{boxplot}\NormalTok{(train}\OperatorTok{$}\NormalTok{casual }\OperatorTok{~}\StringTok{ }\NormalTok{train}\OperatorTok{$}\NormalTok{year, }\DataTypeTok{xlab=}\StringTok{"year"}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{"casual users"}\NormalTok{)}
\KeywordTok{boxplot}\NormalTok{(train}\OperatorTok{$}\NormalTok{registered }\OperatorTok{~}\StringTok{ }\NormalTok{train}\OperatorTok{$}\NormalTok{year, }\DataTypeTok{xlab=}\StringTok{"year"}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{"registered users"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_227-bike_sharing_demand-rf_files/figure-latex/unnamed-chunk-32-1} \end{center}

\begin{quote}
Activity increased in 2012.
\end{quote}

\hypertarget{trend-by-windspeed-training-set}{%
\subsubsection{trend by windspeed, training set}\label{trend-by-windspeed-training-set}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\KeywordTok{boxplot}\NormalTok{(train}\OperatorTok{$}\NormalTok{casual }\OperatorTok{~}\StringTok{ }\NormalTok{train}\OperatorTok{$}\NormalTok{windspeed, }\DataTypeTok{xlab=}\StringTok{"windspeed"}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{"casual users"}\NormalTok{)}
\KeywordTok{boxplot}\NormalTok{(train}\OperatorTok{$}\NormalTok{registered }\OperatorTok{~}\StringTok{ }\NormalTok{train}\OperatorTok{$}\NormalTok{windspeed, }\DataTypeTok{xlab=}\StringTok{"windspeed"}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{"registered users"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_227-bike_sharing_demand-rf_files/figure-latex/unnamed-chunk-33-1} \end{center}

\begin{quote}
Casual users ride even with stron winds.
\end{quote}

\hypertarget{trend-by-humidity-training-set}{%
\subsubsection{trend by humidity, training set}\label{trend-by-humidity-training-set}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\KeywordTok{boxplot}\NormalTok{(train}\OperatorTok{$}\NormalTok{casual }\OperatorTok{~}\StringTok{ }\NormalTok{train}\OperatorTok{$}\NormalTok{humidity, }\DataTypeTok{xlab=}\StringTok{"humidity"}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{"casual users"}\NormalTok{)}
\KeywordTok{boxplot}\NormalTok{(train}\OperatorTok{$}\NormalTok{registered }\OperatorTok{~}\StringTok{ }\NormalTok{train}\OperatorTok{$}\NormalTok{humidity, }\DataTypeTok{xlab=}\StringTok{"humidity"}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{"registered users"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_227-bike_sharing_demand-rf_files/figure-latex/unnamed-chunk-34-1} \end{center}

\begin{quote}
Casual users prefer not to ride with humid weather.
\end{quote}

\hypertarget{feature-engineering-1}{%
\section{Feature Engineering}\label{feature-engineering-1}}

\hypertarget{prepare-data-1}{%
\subsection{Prepare data}\label{prepare-data-1}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# factoring some variables from integer}
\NormalTok{data}\OperatorTok{$}\NormalTok{season     <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(data}\OperatorTok{$}\NormalTok{season)}
\NormalTok{data}\OperatorTok{$}\NormalTok{weather    <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(data}\OperatorTok{$}\NormalTok{weather)}
\NormalTok{data}\OperatorTok{$}\NormalTok{holiday    <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(data}\OperatorTok{$}\NormalTok{holiday)}
\NormalTok{data}\OperatorTok{$}\NormalTok{workingday <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(data}\OperatorTok{$}\NormalTok{workingday)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# new column}
\NormalTok{data}\OperatorTok{$}\NormalTok{hour <-}\StringTok{ }\KeywordTok{as.integer}\NormalTok{(data}\OperatorTok{$}\NormalTok{hour)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# created this variable to divide a day into parts, but did not finally use it}
\NormalTok{data}\OperatorTok{$}\NormalTok{day_part <-}\StringTok{ }\DecValTok{0}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# split in training and test sets again}
\NormalTok{train <-}\StringTok{ }\NormalTok{data[}\KeywordTok{as.integer}\NormalTok{(}\KeywordTok{substr}\NormalTok{(data}\OperatorTok{$}\NormalTok{datetime, }\DecValTok{9}\NormalTok{, }\DecValTok{10}\NormalTok{)) }\OperatorTok{<}\StringTok{ }\DecValTok{20}\NormalTok{,]}
\NormalTok{test  <-}\StringTok{ }\NormalTok{data[}\KeywordTok{as.integer}\NormalTok{(}\KeywordTok{substr}\NormalTok{(data}\OperatorTok{$}\NormalTok{datetime, }\DecValTok{9}\NormalTok{, }\DecValTok{10}\NormalTok{)) }\OperatorTok{>}\StringTok{ }\DecValTok{19}\NormalTok{,]}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# combine the sets}
\NormalTok{data <-}\StringTok{ }\KeywordTok{rbind}\NormalTok{(train, test)}
\end{Highlighting}
\end{Shaded}

\hypertarget{build-hour-bins}{%
\subsection{Build hour bins}\label{build-hour-bins}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# for registered users}
\NormalTok{d =}\StringTok{ }\KeywordTok{rpart}\NormalTok{(registered }\OperatorTok{~}\StringTok{ }\NormalTok{hour, }\DataTypeTok{data =}\NormalTok{ train)}
\KeywordTok{fancyRpartPlot}\NormalTok{(d)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_227-bike_sharing_demand-rf_files/figure-latex/model-rpart-registered-hour-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# for casual users}
\NormalTok{d =}\StringTok{ }\KeywordTok{rpart}\NormalTok{(casual }\OperatorTok{~}\StringTok{ }\NormalTok{hour, }\DataTypeTok{data =}\NormalTok{ train)}
\KeywordTok{fancyRpartPlot}\NormalTok{(d)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_227-bike_sharing_demand-rf_files/figure-latex/model-rpart-casual-hour-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Assign the timings according to tree}
\CommentTok{# fill the hour bins}
\NormalTok{data =}\StringTok{ }\KeywordTok{rbind}\NormalTok{(train,test)}

\CommentTok{# create hour buckets for registered users}
\CommentTok{# 0,1,2,3,4,5,6,7 < 7.5}
\CommentTok{# 22,23,24 >=22}
\CommentTok{# 10,11,12,13,14,15,16,17: h>=9.5 & h<18}
\CommentTok{# h<9.5 & h<8.5 : 8}
\CommentTok{# h<9.5 & h>=8.5 : 9}
\CommentTok{# h>=20: 20,21}
\CommentTok{# h < 20: 18,19}

\NormalTok{data}\OperatorTok{$}\NormalTok{dp_reg =}\StringTok{ }\DecValTok{0}
\NormalTok{data}\OperatorTok{$}\NormalTok{dp_reg[data}\OperatorTok{$}\NormalTok{hour }\OperatorTok{<}\StringTok{ }\DecValTok{8}\NormalTok{] =}\StringTok{ }\DecValTok{1}
\NormalTok{data}\OperatorTok{$}\NormalTok{dp_reg[data}\OperatorTok{$}\NormalTok{hour }\OperatorTok{>=}\StringTok{ }\DecValTok{22}\NormalTok{] =}\StringTok{ }\DecValTok{2}
\NormalTok{data}\OperatorTok{$}\NormalTok{dp_reg[data}\OperatorTok{$}\NormalTok{hour }\OperatorTok{>}\StringTok{ }\DecValTok{9} \OperatorTok{&}\StringTok{ }\NormalTok{data}\OperatorTok{$}\NormalTok{hour }\OperatorTok{<}\StringTok{ }\DecValTok{18}\NormalTok{] =}\StringTok{ }\DecValTok{3}
\NormalTok{data}\OperatorTok{$}\NormalTok{dp_reg[data}\OperatorTok{$}\NormalTok{hour }\OperatorTok{==}\StringTok{ }\DecValTok{8}\NormalTok{] =}\StringTok{ }\DecValTok{4}
\NormalTok{data}\OperatorTok{$}\NormalTok{dp_reg[data}\OperatorTok{$}\NormalTok{hour }\OperatorTok{==}\StringTok{ }\DecValTok{9}\NormalTok{] =}\StringTok{ }\DecValTok{5}
\NormalTok{data}\OperatorTok{$}\NormalTok{dp_reg[data}\OperatorTok{$}\NormalTok{hour }\OperatorTok{==}\StringTok{ }\DecValTok{20} \OperatorTok{|}\StringTok{ }\NormalTok{data}\OperatorTok{$}\NormalTok{hour }\OperatorTok{==}\StringTok{ }\DecValTok{21}\NormalTok{] =}\StringTok{ }\DecValTok{6}
\NormalTok{data}\OperatorTok{$}\NormalTok{dp_reg[data}\OperatorTok{$}\NormalTok{hour }\OperatorTok{==}\StringTok{ }\DecValTok{19} \OperatorTok{|}\StringTok{ }\NormalTok{data}\OperatorTok{$}\NormalTok{hour }\OperatorTok{==}\StringTok{ }\DecValTok{18}\NormalTok{] =}\StringTok{ }\DecValTok{7}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# casual users}
\CommentTok{# h<11, h<8.5: 0,1,2,3,4,5,6,7,8}
\CommentTok{# h>=8.5 & h<11: 9, 10 }
\CommentTok{# h >=11 & h>=21: 21,22,23,24}
\CommentTok{# h >=11 & h<21: 11,12,13,14,15,16,17,18,19,20}
\NormalTok{data}\OperatorTok{$}\NormalTok{dp_cas =}\StringTok{ }\DecValTok{0}
\NormalTok{data}\OperatorTok{$}\NormalTok{dp_cas[data}\OperatorTok{$}\NormalTok{hour }\OperatorTok{<}\StringTok{ }\DecValTok{11} \OperatorTok{&}\StringTok{ }\NormalTok{data}\OperatorTok{$}\NormalTok{hour }\OperatorTok{>=}\StringTok{ }\DecValTok{8}\NormalTok{] =}\StringTok{ }\DecValTok{1}
\NormalTok{data}\OperatorTok{$}\NormalTok{dp_cas[data}\OperatorTok{$}\NormalTok{hour }\OperatorTok{==}\StringTok{ }\DecValTok{9} \OperatorTok{|}\StringTok{ }\NormalTok{data}\OperatorTok{$}\NormalTok{hour }\OperatorTok{==}\StringTok{ }\DecValTok{10}\NormalTok{] =}\StringTok{ }\DecValTok{2}
\NormalTok{data}\OperatorTok{$}\NormalTok{dp_cas[data}\OperatorTok{$}\NormalTok{hour }\OperatorTok{>=}\StringTok{ }\DecValTok{11} \OperatorTok{&}\StringTok{ }\NormalTok{data}\OperatorTok{$}\NormalTok{hour }\OperatorTok{<}\StringTok{ }\DecValTok{21}\NormalTok{] =}\StringTok{ }\DecValTok{3}
\NormalTok{data}\OperatorTok{$}\NormalTok{dp_cas[data}\OperatorTok{$}\NormalTok{hour }\OperatorTok{>=}\StringTok{ }\DecValTok{21}\NormalTok{] =}\StringTok{ }\DecValTok{4}
\end{Highlighting}
\end{Shaded}

\hypertarget{temperature-bins}{%
\subsection{Temperature bins}\label{temperature-bins}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# partition the data by temperature, registered users}
\NormalTok{f =}\StringTok{ }\KeywordTok{rpart}\NormalTok{(registered }\OperatorTok{~}\StringTok{ }\NormalTok{temp, }\DataTypeTok{data=}\NormalTok{train)}
\KeywordTok{fancyRpartPlot}\NormalTok{(f)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_227-bike_sharing_demand-rf_files/figure-latex/model-rpart-temperature-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# partition the data by temperature,, casual users}
\NormalTok{f=}\KeywordTok{rpart}\NormalTok{(casual }\OperatorTok{~}\StringTok{ }\NormalTok{temp, }\DataTypeTok{data=}\NormalTok{train)}
\KeywordTok{fancyRpartPlot}\NormalTok{(f)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_227-bike_sharing_demand-rf_files/figure-latex/unnamed-chunk-42-1} \end{center}

\hypertarget{assign-temperature-ranges-accoding-to-trees}{%
\subsubsection{Assign temperature ranges accoding to trees}\label{assign-temperature-ranges-accoding-to-trees}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data}\OperatorTok{$}\NormalTok{temp_reg =}\StringTok{ }\DecValTok{0}
\NormalTok{data}\OperatorTok{$}\NormalTok{temp_reg[data}\OperatorTok{$}\NormalTok{temp }\OperatorTok{<}\StringTok{ }\DecValTok{13}\NormalTok{] =}\StringTok{ }\DecValTok{1}
\NormalTok{data}\OperatorTok{$}\NormalTok{temp_reg[data}\OperatorTok{$}\NormalTok{temp }\OperatorTok{>=}\StringTok{ }\DecValTok{13} \OperatorTok{&}\StringTok{ }\NormalTok{data}\OperatorTok{$}\NormalTok{temp }\OperatorTok{<}\StringTok{ }\DecValTok{23}\NormalTok{] =}\StringTok{ }\DecValTok{2}
\NormalTok{data}\OperatorTok{$}\NormalTok{temp_reg[data}\OperatorTok{$}\NormalTok{temp }\OperatorTok{>=}\StringTok{ }\DecValTok{23} \OperatorTok{&}\StringTok{ }\NormalTok{data}\OperatorTok{$}\NormalTok{temp }\OperatorTok{<}\StringTok{ }\DecValTok{30}\NormalTok{] =}\StringTok{ }\DecValTok{3}
\NormalTok{data}\OperatorTok{$}\NormalTok{temp_reg[data}\OperatorTok{$}\NormalTok{temp }\OperatorTok{>=}\StringTok{ }\DecValTok{30}\NormalTok{] =}\StringTok{ }\DecValTok{4}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data}\OperatorTok{$}\NormalTok{temp_cas =}\StringTok{ }\DecValTok{0}
\NormalTok{data}\OperatorTok{$}\NormalTok{temp_cas[data}\OperatorTok{$}\NormalTok{temp }\OperatorTok{<}\StringTok{ }\DecValTok{15}\NormalTok{] =}\StringTok{ }\DecValTok{1}
\NormalTok{data}\OperatorTok{$}\NormalTok{temp_cas[data}\OperatorTok{$}\NormalTok{temp }\OperatorTok{>=}\StringTok{ }\DecValTok{15} \OperatorTok{&}\StringTok{ }\NormalTok{data}\OperatorTok{$}\NormalTok{temp }\OperatorTok{<}\StringTok{ }\DecValTok{23}\NormalTok{] =}\StringTok{ }\DecValTok{2}
\NormalTok{data}\OperatorTok{$}\NormalTok{temp_cas[data}\OperatorTok{$}\NormalTok{temp }\OperatorTok{>=}\StringTok{ }\DecValTok{23} \OperatorTok{&}\StringTok{ }\NormalTok{data}\OperatorTok{$}\NormalTok{temp }\OperatorTok{<}\StringTok{ }\DecValTok{30}\NormalTok{] =}\StringTok{ }\DecValTok{3}
\NormalTok{data}\OperatorTok{$}\NormalTok{temp_cas[data}\OperatorTok{$}\NormalTok{temp }\OperatorTok{>=}\StringTok{ }\DecValTok{30}\NormalTok{] =}\StringTok{ }\DecValTok{4}
\end{Highlighting}
\end{Shaded}

\hypertarget{year-bins-by-quarter}{%
\subsection{Year bins by quarter}\label{year-bins-by-quarter}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# add new variable with the month number}
\NormalTok{data}\OperatorTok{$}\NormalTok{month <-}\StringTok{ }\KeywordTok{substr}\NormalTok{(data}\OperatorTok{$}\NormalTok{datetime, }\DecValTok{6}\NormalTok{, }\DecValTok{7}\NormalTok{)}
\NormalTok{data}\OperatorTok{$}\NormalTok{month <-}\StringTok{ }\KeywordTok{as.integer}\NormalTok{(data}\OperatorTok{$}\NormalTok{month)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# bin by quarter manually}
\NormalTok{data}\OperatorTok{$}\NormalTok{year_part[data}\OperatorTok{$}\NormalTok{year}\OperatorTok{==}\StringTok{'2011'}\NormalTok{]                =}\StringTok{ }\DecValTok{1}
\NormalTok{data}\OperatorTok{$}\NormalTok{year_part[data}\OperatorTok{$}\NormalTok{year}\OperatorTok{==}\StringTok{'2011'} \OperatorTok{&}\StringTok{ }\NormalTok{data}\OperatorTok{$}\NormalTok{month}\OperatorTok{>}\DecValTok{3}\NormalTok{] =}\StringTok{ }\DecValTok{2}
\NormalTok{data}\OperatorTok{$}\NormalTok{year_part[data}\OperatorTok{$}\NormalTok{year}\OperatorTok{==}\StringTok{'2011'} \OperatorTok{&}\StringTok{ }\NormalTok{data}\OperatorTok{$}\NormalTok{month}\OperatorTok{>}\DecValTok{6}\NormalTok{] =}\StringTok{ }\DecValTok{3}
\NormalTok{data}\OperatorTok{$}\NormalTok{year_part[data}\OperatorTok{$}\NormalTok{year}\OperatorTok{==}\StringTok{'2011'} \OperatorTok{&}\StringTok{ }\NormalTok{data}\OperatorTok{$}\NormalTok{month}\OperatorTok{>}\DecValTok{9}\NormalTok{] =}\StringTok{ }\DecValTok{4}
\NormalTok{data}\OperatorTok{$}\NormalTok{year_part[data}\OperatorTok{$}\NormalTok{year}\OperatorTok{==}\StringTok{'2012'}\NormalTok{]                =}\StringTok{ }\DecValTok{5}
\NormalTok{data}\OperatorTok{$}\NormalTok{year_part[data}\OperatorTok{$}\NormalTok{year}\OperatorTok{==}\StringTok{'2012'} \OperatorTok{&}\StringTok{ }\NormalTok{data}\OperatorTok{$}\NormalTok{month}\OperatorTok{>}\DecValTok{3}\NormalTok{] =}\StringTok{ }\DecValTok{6}
\NormalTok{data}\OperatorTok{$}\NormalTok{year_part[data}\OperatorTok{$}\NormalTok{year}\OperatorTok{==}\StringTok{'2012'} \OperatorTok{&}\StringTok{ }\NormalTok{data}\OperatorTok{$}\NormalTok{month}\OperatorTok{>}\DecValTok{6}\NormalTok{] =}\StringTok{ }\DecValTok{7}
\NormalTok{data}\OperatorTok{$}\NormalTok{year_part[data}\OperatorTok{$}\NormalTok{year}\OperatorTok{==}\StringTok{'2012'} \OperatorTok{&}\StringTok{ }\NormalTok{data}\OperatorTok{$}\NormalTok{month}\OperatorTok{>}\DecValTok{9}\NormalTok{] =}\StringTok{ }\DecValTok{8}
\KeywordTok{table}\NormalTok{(data}\OperatorTok{$}\NormalTok{year_part)}
\CommentTok{#> }
\CommentTok{#>    1    2    3    4    5    6    7    8 }
\CommentTok{#> 2067 2183 2192 2203 2176 2182 2208 2168}
\end{Highlighting}
\end{Shaded}

\hypertarget{day-type}{%
\subsection{Day Type}\label{day-type}}

Created a variable having categories like ``weekday'', ``weekend'' and ``holiday''.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# creating another variable day_type which may affect our accuracy as weekends and weekdays are important in deciding rentals}
\NormalTok{data}\OperatorTok{$}\NormalTok{day_type =}\StringTok{ }\DecValTok{0}
\NormalTok{data}\OperatorTok{$}\NormalTok{day_type[data}\OperatorTok{$}\NormalTok{holiday}\OperatorTok{==}\DecValTok{0} \OperatorTok{&}\StringTok{ }\NormalTok{data}\OperatorTok{$}\NormalTok{workingday}\OperatorTok{==}\DecValTok{0}\NormalTok{] =}\StringTok{ "weekend"}
\NormalTok{data}\OperatorTok{$}\NormalTok{day_type[data}\OperatorTok{$}\NormalTok{holiday}\OperatorTok{==}\DecValTok{1}\NormalTok{]                      =}\StringTok{ "holiday"}
\NormalTok{data}\OperatorTok{$}\NormalTok{day_type[data}\OperatorTok{$}\NormalTok{holiday}\OperatorTok{==}\DecValTok{0} \OperatorTok{&}\StringTok{ }\NormalTok{data}\OperatorTok{$}\NormalTok{workingday}\OperatorTok{==}\DecValTok{1}\NormalTok{] =}\StringTok{ "working day"}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# split dataset again}
\NormalTok{train =}\StringTok{ }\NormalTok{data[}\KeywordTok{as.integer}\NormalTok{(}\KeywordTok{substr}\NormalTok{(data}\OperatorTok{$}\NormalTok{datetime,}\DecValTok{9}\NormalTok{,}\DecValTok{10}\NormalTok{)) }\OperatorTok{<}\StringTok{ }\DecValTok{20}\NormalTok{,]}
\NormalTok{test =}\StringTok{ }\NormalTok{data[}\KeywordTok{as.integer}\NormalTok{(}\KeywordTok{substr}\NormalTok{(data}\OperatorTok{$}\NormalTok{datetime,}\DecValTok{9}\NormalTok{,}\DecValTok{10}\NormalTok{)) }\OperatorTok{>}\StringTok{ }\DecValTok{19}\NormalTok{,]}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\KeywordTok{boxplot}\NormalTok{(train}\OperatorTok{$}\NormalTok{casual }\OperatorTok{~}\StringTok{ }\NormalTok{train}\OperatorTok{$}\NormalTok{dp_cas, }\DataTypeTok{xlab =} \StringTok{"day partition"}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{"casual users"}\NormalTok{)}
\KeywordTok{boxplot}\NormalTok{(train}\OperatorTok{$}\NormalTok{registered }\OperatorTok{~}\StringTok{ }\NormalTok{train}\OperatorTok{$}\NormalTok{dp_reg, }\DataTypeTok{xlab =} \StringTok{"day partition"}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{"registered users"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_227-bike_sharing_demand-rf_files/figure-latex/unnamed-chunk-49-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\KeywordTok{boxplot}\NormalTok{(train}\OperatorTok{$}\NormalTok{casual }\OperatorTok{~}\StringTok{ }\NormalTok{train}\OperatorTok{$}\NormalTok{day_type, }\DataTypeTok{xlab =} \StringTok{"day type"}\NormalTok{, }
        \DataTypeTok{ylab=}\StringTok{"casual users"}\NormalTok{, }\DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{900}\NormalTok{))}
\KeywordTok{boxplot}\NormalTok{(train}\OperatorTok{$}\NormalTok{registered }\OperatorTok{~}\StringTok{ }\NormalTok{train}\OperatorTok{$}\NormalTok{day_type, }\DataTypeTok{xlab =} \StringTok{"day type"}\NormalTok{, }
        \DataTypeTok{ylab=}\StringTok{"registered users"}\NormalTok{, }\DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{900}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_227-bike_sharing_demand-rf_files/figure-latex/unnamed-chunk-50-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\KeywordTok{boxplot}\NormalTok{(train}\OperatorTok{$}\NormalTok{casual }\OperatorTok{~}\StringTok{ }\NormalTok{train}\OperatorTok{$}\NormalTok{year_part, }\DataTypeTok{xlab =} \StringTok{"year partition, quarter"}\NormalTok{, }
        \DataTypeTok{ylab=}\StringTok{"casual users"}\NormalTok{, }\DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{900}\NormalTok{))}
\KeywordTok{boxplot}\NormalTok{(train}\OperatorTok{$}\NormalTok{registered }\OperatorTok{~}\StringTok{ }\NormalTok{train}\OperatorTok{$}\NormalTok{year_part, }\DataTypeTok{xlab =} \StringTok{"year partition, quarter"}\NormalTok{, }
        \DataTypeTok{ylab=}\StringTok{"registered users"}\NormalTok{, }\DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{900}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_227-bike_sharing_demand-rf_files/figure-latex/unnamed-chunk-51-1} \end{center}

\hypertarget{temperatures}{%
\subsection{Temperatures}\label{temperatures}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\KeywordTok{boxplot}\NormalTok{(train}\OperatorTok{$}\NormalTok{casual }\OperatorTok{~}\StringTok{ }\NormalTok{train}\OperatorTok{$}\NormalTok{temp, }\DataTypeTok{xlab =} \StringTok{"temperature"}\NormalTok{, }
        \DataTypeTok{ylab=}\StringTok{"casual users"}\NormalTok{, }\DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{900}\NormalTok{))}
\KeywordTok{boxplot}\NormalTok{(train}\OperatorTok{$}\NormalTok{registered }\OperatorTok{~}\StringTok{ }\NormalTok{train}\OperatorTok{$}\NormalTok{temp, }\DataTypeTok{xlab =} \StringTok{"temperature"}\NormalTok{, }
        \DataTypeTok{ylab=}\StringTok{"registered users"}\NormalTok{, }\DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{900}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_227-bike_sharing_demand-rf_files/figure-latex/unnamed-chunk-52-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(train}\OperatorTok{$}\NormalTok{temp, train}\OperatorTok{$}\NormalTok{count)}
\NormalTok{data <-}\StringTok{ }\KeywordTok{rbind}\NormalTok{(train, test)}
\CommentTok{# data$month <- substr(data$datetime, 6, 7)}
\CommentTok{# data$month <- as.integer(data$month)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_227-bike_sharing_demand-rf_files/figure-latex/unnamed-chunk-53-1} \end{center}

\hypertarget{imputting-missing-data-to-wind-speed}{%
\subsection{Imputting missing data to wind speed}\label{imputting-missing-data-to-wind-speed}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# dividing total data depending on windspeed to impute/predict the missing values}
\KeywordTok{table}\NormalTok{(data}\OperatorTok{$}\NormalTok{windspeed }\OperatorTok{==}\StringTok{ }\DecValTok{0}\NormalTok{)}
\CommentTok{#> }
\CommentTok{#> FALSE  TRUE }
\CommentTok{#> 15199  2180}
    \CommentTok{# FALSE  TRUE }
    \CommentTok{# 15199  2180 }

\NormalTok{k =}\StringTok{ }\NormalTok{data}\OperatorTok{$}\NormalTok{windspeed }\OperatorTok{==}\StringTok{ }\DecValTok{0}

\NormalTok{wind_}\DecValTok{0}\NormalTok{ =}\StringTok{ }\KeywordTok{subset}\NormalTok{(data, k)    }\CommentTok{# windspeed is zero}
\NormalTok{wind_}\DecValTok{1}\NormalTok{ =}\StringTok{ }\KeywordTok{subset}\NormalTok{(data, }\OperatorTok{!}\NormalTok{k)   }\CommentTok{# windspeed not zero}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{tic}\NormalTok{()}
\CommentTok{# predicting missing values in windspeed using a random forest model}
\CommentTok{# this is a different approach to impute missing values rather than }
\CommentTok{# just using the mean or median or some other statistic for imputation}

\KeywordTok{set.seed}\NormalTok{(}\DecValTok{415}\NormalTok{)}
\NormalTok{fit <-}\StringTok{ }\KeywordTok{randomForest}\NormalTok{(windspeed }\OperatorTok{~}\StringTok{ }\NormalTok{season }\OperatorTok{+}\StringTok{ }\NormalTok{weather }\OperatorTok{+}\StringTok{ }\NormalTok{humidity }\OperatorTok{+}\StringTok{ }\NormalTok{month }\OperatorTok{+}\StringTok{ }\NormalTok{temp }\OperatorTok{+}\StringTok{ }
\StringTok{                        }\NormalTok{year }\OperatorTok{+}\StringTok{ }\NormalTok{atemp, }
                    \DataTypeTok{data =}\NormalTok{ wind_}\DecValTok{1}\NormalTok{, }
                    \DataTypeTok{importance =} \OtherTok{TRUE}\NormalTok{, }
                    \DataTypeTok{ntree =} \DecValTok{250}\NormalTok{)}

\NormalTok{pred =}\StringTok{ }\KeywordTok{predict}\NormalTok{(fit, wind_}\DecValTok{0}\NormalTok{)}
\NormalTok{wind_}\DecValTok{0}\OperatorTok{$}\NormalTok{windspeed =}\StringTok{ }\NormalTok{pred       }\CommentTok{# fill with wind speed predictions}
\KeywordTok{toc}\NormalTok{()}
\CommentTok{#> 52.33 sec elapsed}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# recompose the whole dataset}
\NormalTok{data =}\StringTok{ }\KeywordTok{rbind}\NormalTok{(wind_}\DecValTok{0}\NormalTok{, wind_}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# how many zero values now?}
\KeywordTok{sum}\NormalTok{(data}\OperatorTok{$}\NormalTok{windspeed }\OperatorTok{==}\StringTok{ }\DecValTok{0}\NormalTok{)}
\CommentTok{#> [1] 0}
\end{Highlighting}
\end{Shaded}

\hypertarget{weekend-variable}{%
\subsection{Weekend variable}\label{weekend-variable}}

Created a separate variable for weekend (0/1)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data}\OperatorTok{$}\NormalTok{weekend =}\StringTok{ }\DecValTok{0}
\NormalTok{data}\OperatorTok{$}\NormalTok{weekend[data}\OperatorTok{$}\NormalTok{day}\OperatorTok{==}\StringTok{"Sunday"} \OperatorTok{|}\StringTok{ }\NormalTok{data}\OperatorTok{$}\NormalTok{day}\OperatorTok{==}\StringTok{"Saturday"}\NormalTok{ ] =}\StringTok{ }\DecValTok{1}
\end{Highlighting}
\end{Shaded}

\hypertarget{model-building}{%
\section{Model Building}\label{model-building}}

As this was our first attempt, we applied decision tree, conditional inference tree and random forest algorithms and found that random forest is performing the best. You can also go with regression, boosted regression, neural network and find which one is working well for you.

Before executing the random forest model code, I have followed following steps:

Convert discrete variables into factor (weather, season, hour, holiday, working day, month, day)

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str}\NormalTok{(data)}
\CommentTok{#> 'data.frame':    17379 obs. of  24 variables:}
\CommentTok{#>  $ datetime  : Factor w/ 17379 levels "2011-01-01 00:00:00",..: 1 2 3 4 5 7 8 9 10 65 ...}
\CommentTok{#>  $ season    : Factor w/ 4 levels "1","2","3","4": 1 1 1 1 1 1 1 1 1 1 ...}
\CommentTok{#>  $ holiday   : Factor w/ 2 levels "0","1": 1 1 1 1 1 1 1 1 1 1 ...}
\CommentTok{#>  $ workingday: Factor w/ 2 levels "0","1": 1 1 1 1 1 1 1 1 1 2 ...}
\CommentTok{#>  $ weather   : Factor w/ 4 levels "1","2","3","4": 1 1 1 1 1 1 1 1 1 1 ...}
\CommentTok{#>  $ temp      : num  9.84 9.02 9.02 9.84 9.84 ...}
\CommentTok{#>  $ atemp     : num  14.4 13.6 13.6 14.4 14.4 ...}
\CommentTok{#>  $ humidity  : num  81 80 80 75 75 80 86 75 76 47 ...}
\CommentTok{#>  $ windspeed : num  9.03 9.05 9.05 9.15 9.15 ...}
\CommentTok{#>  $ casual    : num  3 8 5 3 0 2 1 1 8 8 ...}
\CommentTok{#>  $ registered: num  13 32 27 10 1 0 2 7 6 102 ...}
\CommentTok{#>  $ count     : num  16 40 32 13 1 2 3 8 14 110 ...}
\CommentTok{#>  $ hour      : int  1 2 3 4 5 7 8 9 10 20 ...}
\CommentTok{#>  $ day       : chr  "Saturday" "Saturday" "Saturday" "Saturday" ...}
\CommentTok{#>  $ year      : Factor w/ 2 levels "2011","2012": 1 1 1 1 1 1 1 1 1 1 ...}
\CommentTok{#>  $ day_part  : num  0 0 0 0 0 0 0 0 0 0 ...}
\CommentTok{#>  $ dp_reg    : num  1 1 1 1 1 1 4 5 3 6 ...}
\CommentTok{#>  $ dp_cas    : num  0 0 0 0 0 0 1 2 2 3 ...}
\CommentTok{#>  $ temp_reg  : num  1 1 1 1 1 1 1 1 2 1 ...}
\CommentTok{#>  $ temp_cas  : num  1 1 1 1 1 1 1 1 1 1 ...}
\CommentTok{#>  $ month     : int  1 1 1 1 1 1 1 1 1 1 ...}
\CommentTok{#>  $ year_part : num  1 1 1 1 1 1 1 1 1 1 ...}
\CommentTok{#>  $ day_type  : chr  "weekend" "weekend" "weekend" "weekend" ...}
\CommentTok{#>  $ weekend   : num  1 1 1 1 1 1 1 1 1 0 ...}
\end{Highlighting}
\end{Shaded}

\hypertarget{convert-variables-to-factors}{%
\subsection{Convert variables to factors}\label{convert-variables-to-factors}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# converting all relevant categorical variables into factors to feed to our random forest model}
\NormalTok{data}\OperatorTok{$}\NormalTok{season     =}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(data}\OperatorTok{$}\NormalTok{season)}
\NormalTok{data}\OperatorTok{$}\NormalTok{holiday    =}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(data}\OperatorTok{$}\NormalTok{holiday)}
\NormalTok{data}\OperatorTok{$}\NormalTok{workingday =}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(data}\OperatorTok{$}\NormalTok{workingday)}
\NormalTok{data}\OperatorTok{$}\NormalTok{weather    =}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(data}\OperatorTok{$}\NormalTok{weather)}
\NormalTok{data}\OperatorTok{$}\NormalTok{hour       =}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(data}\OperatorTok{$}\NormalTok{hour)}
\NormalTok{data}\OperatorTok{$}\NormalTok{month      =}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(data}\OperatorTok{$}\NormalTok{month)}
\NormalTok{data}\OperatorTok{$}\NormalTok{day_part   =}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(data}\OperatorTok{$}\NormalTok{dp_cas)}
\NormalTok{data}\OperatorTok{$}\NormalTok{day_type   =}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(data}\OperatorTok{$}\NormalTok{dp_reg)}
\NormalTok{data}\OperatorTok{$}\NormalTok{day        =}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(data}\OperatorTok{$}\NormalTok{day)}
\NormalTok{data}\OperatorTok{$}\NormalTok{temp_cas   =}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(data}\OperatorTok{$}\NormalTok{temp_cas)}
\NormalTok{data}\OperatorTok{$}\NormalTok{temp_reg   =}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(data}\OperatorTok{$}\NormalTok{temp_reg)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str}\NormalTok{(data)}
\CommentTok{#> 'data.frame':    17379 obs. of  24 variables:}
\CommentTok{#>  $ datetime  : Factor w/ 17379 levels "2011-01-01 00:00:00",..: 1 2 3 4 5 7 8 9 10 65 ...}
\CommentTok{#>  $ season    : Factor w/ 4 levels "1","2","3","4": 1 1 1 1 1 1 1 1 1 1 ...}
\CommentTok{#>  $ holiday   : Factor w/ 2 levels "0","1": 1 1 1 1 1 1 1 1 1 1 ...}
\CommentTok{#>  $ workingday: Factor w/ 2 levels "0","1": 1 1 1 1 1 1 1 1 1 2 ...}
\CommentTok{#>  $ weather   : Factor w/ 4 levels "1","2","3","4": 1 1 1 1 1 1 1 1 1 1 ...}
\CommentTok{#>  $ temp      : num  9.84 9.02 9.02 9.84 9.84 ...}
\CommentTok{#>  $ atemp     : num  14.4 13.6 13.6 14.4 14.4 ...}
\CommentTok{#>  $ humidity  : num  81 80 80 75 75 80 86 75 76 47 ...}
\CommentTok{#>  $ windspeed : num  9.03 9.05 9.05 9.15 9.15 ...}
\CommentTok{#>  $ casual    : num  3 8 5 3 0 2 1 1 8 8 ...}
\CommentTok{#>  $ registered: num  13 32 27 10 1 0 2 7 6 102 ...}
\CommentTok{#>  $ count     : num  16 40 32 13 1 2 3 8 14 110 ...}
\CommentTok{#>  $ hour      : Factor w/ 24 levels "1","2","3","4",..: 1 2 3 4 5 7 8 9 10 20 ...}
\CommentTok{#>  $ day       : Factor w/ 7 levels "Friday","Monday",..: 3 3 3 3 3 3 3 3 3 2 ...}
\CommentTok{#>  $ year      : Factor w/ 2 levels "2011","2012": 1 1 1 1 1 1 1 1 1 1 ...}
\CommentTok{#>  $ day_part  : Factor w/ 5 levels "0","1","2","3",..: 1 1 1 1 1 1 2 3 3 4 ...}
\CommentTok{#>  $ dp_reg    : num  1 1 1 1 1 1 4 5 3 6 ...}
\CommentTok{#>  $ dp_cas    : num  0 0 0 0 0 0 1 2 2 3 ...}
\CommentTok{#>  $ temp_reg  : Factor w/ 4 levels "1","2","3","4": 1 1 1 1 1 1 1 1 2 1 ...}
\CommentTok{#>  $ temp_cas  : Factor w/ 4 levels "1","2","3","4": 1 1 1 1 1 1 1 1 1 1 ...}
\CommentTok{#>  $ month     : Factor w/ 12 levels "1","2","3","4",..: 1 1 1 1 1 1 1 1 1 1 ...}
\CommentTok{#>  $ year_part : num  1 1 1 1 1 1 1 1 1 1 ...}
\CommentTok{#>  $ day_type  : Factor w/ 7 levels "1","2","3","4",..: 1 1 1 1 1 1 4 5 3 6 ...}
\CommentTok{#>  $ weekend   : num  1 1 1 1 1 1 1 1 1 0 ...}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\item
  As we know that dependent variables have natural outliers so we will predict log of dependent variables.
\item
  Predict bike demand registered and casual users separately.
  \(y1 = \log(casual+1)\) and \(y2 = \log(registered+1)\), Here we have added 1 to deal with zero values in the casual and registered columns.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# separate again as train and test set}
\NormalTok{train =}\StringTok{ }\NormalTok{data[}\KeywordTok{as.integer}\NormalTok{(}\KeywordTok{substr}\NormalTok{(data}\OperatorTok{$}\NormalTok{datetime, }\DecValTok{9}\NormalTok{, }\DecValTok{10}\NormalTok{)) }\OperatorTok{<}\StringTok{ }\DecValTok{20}\NormalTok{,]}
\NormalTok{test =}\StringTok{ }\NormalTok{data[}\KeywordTok{as.integer}\NormalTok{(}\KeywordTok{substr}\NormalTok{(data}\OperatorTok{$}\NormalTok{datetime, }\DecValTok{9}\NormalTok{, }\DecValTok{10}\NormalTok{)) }\OperatorTok{>}\StringTok{ }\DecValTok{19}\NormalTok{,]}
\end{Highlighting}
\end{Shaded}

\hypertarget{log-transform}{%
\subsection{Log transform}\label{log-transform}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# log transformation for some skewed variables, }
\CommentTok{# which can be seen from their distribution}
\NormalTok{train}\OperatorTok{$}\NormalTok{reg1   =}\StringTok{ }\NormalTok{train}\OperatorTok{$}\NormalTok{registered }\OperatorTok{+}\StringTok{ }\DecValTok{1}
\NormalTok{train}\OperatorTok{$}\NormalTok{cas1   =}\StringTok{ }\NormalTok{train}\OperatorTok{$}\NormalTok{casual }\OperatorTok{+}\StringTok{ }\DecValTok{1}
\NormalTok{train}\OperatorTok{$}\NormalTok{logcas =}\StringTok{ }\KeywordTok{log}\NormalTok{(train}\OperatorTok{$}\NormalTok{cas1)}
\NormalTok{train}\OperatorTok{$}\NormalTok{logreg =}\StringTok{ }\KeywordTok{log}\NormalTok{(train}\OperatorTok{$}\NormalTok{reg1)}
\NormalTok{test}\OperatorTok{$}\NormalTok{logreg  =}\StringTok{ }\DecValTok{0}
\NormalTok{test}\OperatorTok{$}\NormalTok{logcas  =}\StringTok{ }\DecValTok{0}
\end{Highlighting}
\end{Shaded}

\hypertarget{plot-by-weather-by-season}{%
\subsubsection{Plot by weather, by season}\label{plot-by-weather-by-season}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# cartesian plot}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\KeywordTok{boxplot}\NormalTok{(train}\OperatorTok{$}\NormalTok{registered }\OperatorTok{~}\StringTok{ }\NormalTok{train}\OperatorTok{$}\NormalTok{weather, }\DataTypeTok{xlab=}\StringTok{"weather"}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{"registered users"}\NormalTok{)}
\KeywordTok{boxplot}\NormalTok{(train}\OperatorTok{$}\NormalTok{registered }\OperatorTok{~}\StringTok{ }\NormalTok{train}\OperatorTok{$}\NormalTok{season, }\DataTypeTok{xlab=}\StringTok{"season"}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{"registered users"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_227-bike_sharing_demand-rf_files/figure-latex/unnamed-chunk-63-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# semilog plot}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\KeywordTok{boxplot}\NormalTok{(train}\OperatorTok{$}\NormalTok{logreg }\OperatorTok{~}\StringTok{ }\NormalTok{train}\OperatorTok{$}\NormalTok{weather, }\DataTypeTok{xlab =} \StringTok{"weather"}\NormalTok{)}
\KeywordTok{boxplot}\NormalTok{(train}\OperatorTok{$}\NormalTok{logreg }\OperatorTok{~}\StringTok{ }\NormalTok{train}\OperatorTok{$}\NormalTok{season, }\DataTypeTok{xlab =} \StringTok{"season"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_227-bike_sharing_demand-rf_files/figure-latex/unnamed-chunk-64-1} \end{center}

\hypertarget{predicting-for-registered-and-casual-users-test-dataset}{%
\subsection{Predicting for registered and casual users, test dataset}\label{predicting-for-registered-and-casual-users-test-dataset}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{tic}\NormalTok{()}
\CommentTok{# final model building using random forest}
\CommentTok{# note that we build different models for predicting for }
\CommentTok{# registered and casual users}
\CommentTok{# this was seen as giving best result after a lot of experimentation}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{415}\NormalTok{)}
\NormalTok{fit1 <-}\StringTok{ }\KeywordTok{randomForest}\NormalTok{(logreg }\OperatorTok{~}\StringTok{ }\NormalTok{hour }\OperatorTok{+}\StringTok{ }\NormalTok{workingday }\OperatorTok{+}\StringTok{ }\NormalTok{day }\OperatorTok{+}\StringTok{ }\NormalTok{holiday }\OperatorTok{+}\StringTok{ }\NormalTok{day_type }\OperatorTok{+}
\StringTok{                         }\NormalTok{temp_reg }\OperatorTok{+}\StringTok{ }\NormalTok{humidity }\OperatorTok{+}\StringTok{ }\NormalTok{atemp }\OperatorTok{+}\StringTok{ }\NormalTok{windspeed }\OperatorTok{+}\StringTok{ }\NormalTok{season }\OperatorTok{+}\StringTok{ }
\StringTok{                         }\NormalTok{weather }\OperatorTok{+}\StringTok{ }\NormalTok{dp_reg }\OperatorTok{+}\StringTok{ }\NormalTok{weekend }\OperatorTok{+}\StringTok{ }\NormalTok{year }\OperatorTok{+}\StringTok{ }\NormalTok{year_part, }
                     \DataTypeTok{data =}\NormalTok{ train, }
                     \DataTypeTok{importance =} \OtherTok{TRUE}\NormalTok{, }
                     \DataTypeTok{ntree =} \DecValTok{250}\NormalTok{)}

\NormalTok{pred1 =}\StringTok{ }\KeywordTok{predict}\NormalTok{(fit1, test)}
\NormalTok{test}\OperatorTok{$}\NormalTok{logreg =}\StringTok{ }\NormalTok{pred1}
\KeywordTok{toc}\NormalTok{()}
\CommentTok{#> 131.372 sec elapsed}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# casual users}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{415}\NormalTok{)}
\NormalTok{fit2 <-}\StringTok{ }\KeywordTok{randomForest}\NormalTok{(logcas }\OperatorTok{~}\StringTok{ }\NormalTok{hour }\OperatorTok{+}\StringTok{ }\NormalTok{day_type }\OperatorTok{+}\StringTok{ }\NormalTok{day }\OperatorTok{+}\StringTok{ }\NormalTok{humidity }\OperatorTok{+}\StringTok{ }\NormalTok{atemp }\OperatorTok{+}\StringTok{ }
\StringTok{                         }\NormalTok{temp_cas }\OperatorTok{+}\StringTok{ }\NormalTok{windspeed }\OperatorTok{+}\StringTok{ }\NormalTok{season }\OperatorTok{+}\StringTok{ }\NormalTok{weather }\OperatorTok{+}\StringTok{ }\NormalTok{holiday }\OperatorTok{+}
\StringTok{                         }\NormalTok{workingday }\OperatorTok{+}\StringTok{ }\NormalTok{dp_cas }\OperatorTok{+}\StringTok{ }\NormalTok{weekend }\OperatorTok{+}\StringTok{ }\NormalTok{year }\OperatorTok{+}\StringTok{ }\NormalTok{year_part, }
                     \DataTypeTok{data =}\NormalTok{ train, }\DataTypeTok{importance =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{ntree =} \DecValTok{250}\NormalTok{)}

\NormalTok{pred2 =}\StringTok{ }\KeywordTok{predict}\NormalTok{(fit2, test)}
\NormalTok{test}\OperatorTok{$}\NormalTok{logcas =}\StringTok{ }\NormalTok{pred2}
\end{Highlighting}
\end{Shaded}

\hypertarget{preparing-and-exporting-results}{%
\subsection{Preparing and exporting results}\label{preparing-and-exporting-results}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# creating the final submission file}
\CommentTok{# reverse log conversion}
\NormalTok{test}\OperatorTok{$}\NormalTok{registered <-}\StringTok{ }\KeywordTok{exp}\NormalTok{(test}\OperatorTok{$}\NormalTok{logreg) }\OperatorTok{-}\StringTok{ }\DecValTok{1}
\NormalTok{test}\OperatorTok{$}\NormalTok{casual     <-}\StringTok{ }\KeywordTok{exp}\NormalTok{(test}\OperatorTok{$}\NormalTok{logcas) }\OperatorTok{-}\StringTok{ }\DecValTok{1}
\NormalTok{test}\OperatorTok{$}\NormalTok{count      <-}\StringTok{ }\NormalTok{test}\OperatorTok{$}\NormalTok{casual }\OperatorTok{+}\StringTok{ }\NormalTok{test}\OperatorTok{$}\NormalTok{registered}

\NormalTok{r <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{datetime =}\NormalTok{ test}\OperatorTok{$}\NormalTok{datetime, }
                \DataTypeTok{casual =}\NormalTok{ test}\OperatorTok{$}\NormalTok{casual, }
                \DataTypeTok{registered =}\NormalTok{ test}\OperatorTok{$}\NormalTok{registered)}

\KeywordTok{print}\NormalTok{(}\KeywordTok{sum}\NormalTok{(r}\OperatorTok{$}\NormalTok{casual))}
\CommentTok{#> [1] 205804}
\KeywordTok{print}\NormalTok{(}\KeywordTok{sum}\NormalTok{(r}\OperatorTok{$}\NormalTok{registered))}
\CommentTok{#> [1] 962834}

\NormalTok{s <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{datetime =}\NormalTok{ test}\OperatorTok{$}\NormalTok{datetime, }\DataTypeTok{count =}\NormalTok{ test}\OperatorTok{$}\NormalTok{count)}
\KeywordTok{write.csv}\NormalTok{(s, }\DataTypeTok{file =}\KeywordTok{file.path}\NormalTok{(data_out_dir, }\StringTok{"bike-submit.csv"}\NormalTok{), }\DataTypeTok{row.names =} \OtherTok{FALSE}\NormalTok{)}

\CommentTok{# sum(cas+reg) = 1168638}
\CommentTok{# month number now is correct}
\end{Highlighting}
\end{Shaded}

After following the steps mentioned above, you can score 0.38675 on Kaggle leaderboard i.e.~top 5 percentile of total participants. As you might have seen, we have not applied any extraordinary science in getting to this level. But, the real competition starts here. I would like to see, if I can improve this further by use of more features and some more advanced modeling techniques.

\hypertarget{end-notes}{%
\section{End Notes}\label{end-notes}}

In this article, we have looked at structured approach of problem solving and how this method can help you to improve performance. I would recommend you to generate hypothesis before you deep dive in the data set as this technique will not limit your thought process. You can improve your performance by applying advanced techniques (or ensemble methods) and understand your data trend better.

You can find the complete solution here : \href{https://github.com/adityashrm21/Kaggle/blob/master/Bike_Sharing_Demand.R}{GitHub Link}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# this is the older submission. months were incomplete}
\NormalTok{old <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\DataTypeTok{file =} \KeywordTok{file.path}\NormalTok{(data_raw_dir, }\StringTok{"bike-submit-old.csv"}\NormalTok{))}
\KeywordTok{sum}\NormalTok{(old}\OperatorTok{$}\NormalTok{count)}
\CommentTok{#> [1] 1164829}
\end{Highlighting}
\end{Shaded}

\hypertarget{breast-cancer-wisconsin}{%
\chapter{Breast Cancer Wisconsin}\label{breast-cancer-wisconsin}}

Source: \url{https://shiring.github.io/machine_learning/2017/01/15/rfe_ga_post}

\hypertarget{read-and-process-the-data}{%
\section{Read and process the data}\label{read-and-process-the-data}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bc_data <-}\StringTok{ }\KeywordTok{read.table}\NormalTok{(}\KeywordTok{file.path}\NormalTok{(data_raw_dir, }\StringTok{"breast-cancer-wisconsin.data"}\NormalTok{), }
                      \DataTypeTok{header =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{sep =} \StringTok{","}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# assign the column names}
\KeywordTok{colnames}\NormalTok{(bc_data) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"sample_code_number"}\NormalTok{, }\StringTok{"clump_thickness"}\NormalTok{, }
                       \StringTok{"uniformity_of_cell_size"}\NormalTok{, }\StringTok{"uniformity_of_cell_shape"}\NormalTok{,}
                       \StringTok{"marginal_adhesion"}\NormalTok{, }\StringTok{"single_epithelial_cell_size"}\NormalTok{, }
                       \StringTok{"bare_nuclei"}\NormalTok{, }\StringTok{"bland_chromatin"}\NormalTok{, }\StringTok{"normal_nucleoli"}\NormalTok{, }
                       \StringTok{"mitosis"}\NormalTok{, }\StringTok{"classes"}\NormalTok{)}

\CommentTok{# change classes from numeric to character}
\NormalTok{bc_data}\OperatorTok{$}\NormalTok{classes <-}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(bc_data}\OperatorTok{$}\NormalTok{classes }\OperatorTok{==}\StringTok{ "2"}\NormalTok{, }\StringTok{"benign"}\NormalTok{,}
                          \KeywordTok{ifelse}\NormalTok{(bc_data}\OperatorTok{$}\NormalTok{classes }\OperatorTok{==}\StringTok{ "4"}\NormalTok{, }\StringTok{"malignant"}\NormalTok{, }\OtherTok{NA}\NormalTok{))}

\CommentTok{# if query sign make NA}
\NormalTok{bc_data[bc_data }\OperatorTok{==}\StringTok{ "?"}\NormalTok{] <-}\StringTok{ }\OtherTok{NA}

\CommentTok{# how many NAs are in the data}
\KeywordTok{length}\NormalTok{(}\KeywordTok{which}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(bc_data)))}
\CommentTok{#> [1] 16}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{names}\NormalTok{(bc_data)}
\CommentTok{#>  [1] "sample_code_number"          "clump_thickness"            }
\CommentTok{#>  [3] "uniformity_of_cell_size"     "uniformity_of_cell_shape"   }
\CommentTok{#>  [5] "marginal_adhesion"           "single_epithelial_cell_size"}
\CommentTok{#>  [7] "bare_nuclei"                 "bland_chromatin"            }
\CommentTok{#>  [9] "normal_nucleoli"             "mitosis"                    }
\CommentTok{#> [11] "classes"}
\end{Highlighting}
\end{Shaded}

\hypertarget{missing-data}{%
\subsection{Missing data}\label{missing-data}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# impute missing data}
\KeywordTok{library}\NormalTok{(mice)}
\CommentTok{#> Loading required package: lattice}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'mice'}
\CommentTok{#> The following objects are masked from 'package:base':}
\CommentTok{#> }
\CommentTok{#>     cbind, rbind}

\CommentTok{# skip these columns: sample_code_number and classes}
\CommentTok{# convert to numeric}
\NormalTok{bc_data[,}\DecValTok{2}\OperatorTok{:}\DecValTok{10}\NormalTok{] <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(bc_data[, }\DecValTok{2}\OperatorTok{:}\DecValTok{10}\NormalTok{], }\DecValTok{2}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{as.character}\NormalTok{(x)))}

\CommentTok{# impute but mute}
\NormalTok{dataset_impute <-}\StringTok{ }\KeywordTok{mice}\NormalTok{(bc_data[, }\DecValTok{2}\OperatorTok{:}\DecValTok{10}\NormalTok{],  }\DataTypeTok{print =} \OtherTok{FALSE}\NormalTok{)}

\CommentTok{# bind "classes" with the rest. skip "sample_code_number"}
\NormalTok{bc_data <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(bc_data[, }\DecValTok{11}\NormalTok{, }\DataTypeTok{drop =} \OtherTok{FALSE}\NormalTok{], }
\NormalTok{                 mice}\OperatorTok{::}\KeywordTok{complete}\NormalTok{(dataset_impute, }\DataTypeTok{action =}\DecValTok{1}\NormalTok{))}

\NormalTok{bc_data}\OperatorTok{$}\NormalTok{classes <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(bc_data}\OperatorTok{$}\NormalTok{classes)}

\CommentTok{# how many benign and malignant cases are there?}
\KeywordTok{summary}\NormalTok{(bc_data}\OperatorTok{$}\NormalTok{classes)}
\CommentTok{#>    benign malignant }
\CommentTok{#>       458       241}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# confirm NAs have been removed}
\KeywordTok{length}\NormalTok{(}\KeywordTok{which}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(bc_data)))}
\CommentTok{#> [1] 0}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str}\NormalTok{(bc_data)}
\CommentTok{#> 'data.frame':    699 obs. of  10 variables:}
\CommentTok{#>  $ classes                    : Factor w/ 2 levels "benign","malignant": 1 1 1 1 1 2 1 1 1 1 ...}
\CommentTok{#>  $ clump_thickness            : num  5 5 3 6 4 8 1 2 2 4 ...}
\CommentTok{#>  $ uniformity_of_cell_size    : num  1 4 1 8 1 10 1 1 1 2 ...}
\CommentTok{#>  $ uniformity_of_cell_shape   : num  1 4 1 8 1 10 1 2 1 1 ...}
\CommentTok{#>  $ marginal_adhesion          : num  1 5 1 1 3 8 1 1 1 1 ...}
\CommentTok{#>  $ single_epithelial_cell_size: num  2 7 2 3 2 7 2 2 2 2 ...}
\CommentTok{#>  $ bare_nuclei                : num  1 10 2 4 1 10 10 1 1 1 ...}
\CommentTok{#>  $ bland_chromatin            : num  3 3 3 3 3 9 3 3 1 2 ...}
\CommentTok{#>  $ normal_nucleoli            : num  1 2 1 7 1 7 1 1 1 1 ...}
\CommentTok{#>  $ mitosis                    : num  1 1 1 1 1 1 1 1 5 1 ...}
\end{Highlighting}
\end{Shaded}

\hypertarget{principal-component-analysis-pca}{%
\section{Principal Component Analysis (PCA)}\label{principal-component-analysis-pca}}

To get an idea about the dimensionality and variance of the datasets, I am first looking at PCA plots for samples and features. The first two principal components (PCs) show the two components that explain the majority of variation in the data.

After defining my custom \texttt{ggplot2} theme, I am creating a function that performs the PCA (using the \texttt{pcaGoPromoter} package), calculates ellipses of the data points (with the \texttt{ellipse} package) and produces the plot with \texttt{ggplot2}. Some of the features in datasets 2 and 3 are not very distinct and overlap in the PCA plots, therefore I am also plotting hierarchical clustering dendrograms.

\hypertarget{theme}{%
\subsubsection{theme}\label{theme}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plotting theme}

\KeywordTok{library}\NormalTok{(ggplot2)}
\CommentTok{#> Registered S3 methods overwritten by 'ggplot2':}
\CommentTok{#>   method         from }
\CommentTok{#>   [.quosures     rlang}
\CommentTok{#>   c.quosures     rlang}
\CommentTok{#>   print.quosures rlang}

\NormalTok{my_theme <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(}\DataTypeTok{base_size =} \DecValTok{12}\NormalTok{, }\DataTypeTok{base_family =} \StringTok{"sans"}\NormalTok{)\{}
  \KeywordTok{theme_minimal}\NormalTok{(}\DataTypeTok{base_size =}\NormalTok{ base_size, }\DataTypeTok{base_family =}\NormalTok{ base_family) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}
    \DataTypeTok{axis.text =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{size =} \DecValTok{12}\NormalTok{),}
    \DataTypeTok{axis.text.x =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{angle =} \DecValTok{0}\NormalTok{, }\DataTypeTok{vjust =} \FloatTok{0.5}\NormalTok{, }\DataTypeTok{hjust =} \FloatTok{0.5}\NormalTok{),}
    \DataTypeTok{axis.title =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{size =} \DecValTok{14}\NormalTok{),}
    \DataTypeTok{panel.grid.major =} \KeywordTok{element_line}\NormalTok{(}\DataTypeTok{color =} \StringTok{"grey"}\NormalTok{),}
    \DataTypeTok{panel.grid.minor =} \KeywordTok{element_blank}\NormalTok{(),}
    \DataTypeTok{panel.background =} \KeywordTok{element_rect}\NormalTok{(}\DataTypeTok{fill =} \StringTok{"aliceblue"}\NormalTok{),}
    \DataTypeTok{strip.background =} \KeywordTok{element_rect}\NormalTok{(}\DataTypeTok{fill =} \StringTok{"navy"}\NormalTok{, }\DataTypeTok{color =} \StringTok{"navy"}\NormalTok{, }\DataTypeTok{size =} \DecValTok{1}\NormalTok{),}
    \DataTypeTok{strip.text =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{face =} \StringTok{"bold"}\NormalTok{, }\DataTypeTok{size =} \DecValTok{12}\NormalTok{, }\DataTypeTok{color =} \StringTok{"white"}\NormalTok{),}
    \DataTypeTok{legend.position =} \StringTok{"right"}\NormalTok{,}
    \DataTypeTok{legend.justification =} \StringTok{"top"}\NormalTok{, }
    \DataTypeTok{legend.background =} \KeywordTok{element_blank}\NormalTok{(),}
    \DataTypeTok{panel.border =} \KeywordTok{element_rect}\NormalTok{(}\DataTypeTok{color =} \StringTok{"grey"}\NormalTok{, }\DataTypeTok{fill =} \OtherTok{NA}\NormalTok{, }\DataTypeTok{size =} \FloatTok{0.5}\NormalTok{)}
\NormalTok{  )}
\NormalTok{\}}

\KeywordTok{theme_set}\NormalTok{(}\KeywordTok{my_theme}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\hypertarget{pca-function}{%
\subsubsection{PCA function}\label{pca-function}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# function for PCA plotting}
\KeywordTok{library}\NormalTok{(pcaGoPromoter)                  }\CommentTok{# install from BioConductor}
\CommentTok{#> Loading required package: ellipse}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'ellipse'}
\CommentTok{#> The following object is masked from 'package:graphics':}
\CommentTok{#> }
\CommentTok{#>     pairs}
\CommentTok{#> Loading required package: Biostrings}
\CommentTok{#> Loading required package: BiocGenerics}
\CommentTok{#> Loading required package: parallel}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'BiocGenerics'}
\CommentTok{#> The following objects are masked from 'package:parallel':}
\CommentTok{#> }
\CommentTok{#>     clusterApply, clusterApplyLB, clusterCall, clusterEvalQ,}
\CommentTok{#>     clusterExport, clusterMap, parApply, parCapply, parLapply,}
\CommentTok{#>     parLapplyLB, parRapply, parSapply, parSapplyLB}
\CommentTok{#> The following objects are masked from 'package:mice':}
\CommentTok{#> }
\CommentTok{#>     cbind, rbind}
\CommentTok{#> The following objects are masked from 'package:stats':}
\CommentTok{#> }
\CommentTok{#>     IQR, mad, sd, var, xtabs}
\CommentTok{#> The following objects are masked from 'package:base':}
\CommentTok{#> }
\CommentTok{#>     anyDuplicated, append, as.data.frame, basename, cbind,}
\CommentTok{#>     colnames, dirname, do.call, duplicated, eval, evalq, Filter,}
\CommentTok{#>     Find, get, grep, grepl, intersect, is.unsorted, lapply, Map,}
\CommentTok{#>     mapply, match, mget, order, paste, pmax, pmax.int, pmin,}
\CommentTok{#>     pmin.int, Position, rank, rbind, Reduce, rownames, sapply,}
\CommentTok{#>     setdiff, sort, table, tapply, union, unique, unsplit, which,}
\CommentTok{#>     which.max, which.min}
\CommentTok{#> Loading required package: S4Vectors}
\CommentTok{#> Loading required package: stats4}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'S4Vectors'}
\CommentTok{#> The following object is masked from 'package:base':}
\CommentTok{#> }
\CommentTok{#>     expand.grid}
\CommentTok{#> Loading required package: IRanges}
\CommentTok{#> Loading required package: XVector}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'Biostrings'}
\CommentTok{#> The following object is masked from 'package:base':}
\CommentTok{#> }
\CommentTok{#>     strsplit}
\KeywordTok{library}\NormalTok{(ellipse)}

\NormalTok{pca_func <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(data, groups, title, }\DataTypeTok{print_ellipse =} \OtherTok{TRUE}\NormalTok{) \{}
  
  \CommentTok{# perform pca and extract scores for all principal components: PC1:PC9}
\NormalTok{  pcaOutput <-}\StringTok{ }\KeywordTok{pca}\NormalTok{(data, }\DataTypeTok{printDropped =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{scale =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{center =} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{  pcaOutput2 <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(pcaOutput}\OperatorTok{$}\NormalTok{scores)}
  
  \CommentTok{# define groups for plotting. will group the classes}
\NormalTok{  pcaOutput2}\OperatorTok{$}\NormalTok{groups <-}\StringTok{ }\NormalTok{groups}
  
  \CommentTok{# when plotting samples calculate ellipses for plotting }
  \CommentTok{# (when plotting features, there are no replicates)}
  \ControlFlowTok{if}\NormalTok{ (print_ellipse) \{}
    \CommentTok{# group and summarize by classes: benign, malignant}
    \CommentTok{# centroids w/3 columns: groups, PC1, PC2}
\NormalTok{    centroids <-}\StringTok{ }\KeywordTok{aggregate}\NormalTok{(}\KeywordTok{cbind}\NormalTok{(PC1, PC2) }\OperatorTok{~}\StringTok{ }\NormalTok{groups, pcaOutput2, mean)}
    \CommentTok{# bind for the two groups (classes)}
    \CommentTok{# conf.rgn w/3 columns: groups, PC1, PC2}
\NormalTok{    conf.rgn  <-}\StringTok{ }\KeywordTok{do.call}\NormalTok{(rbind, }\KeywordTok{lapply}\NormalTok{(}\KeywordTok{unique}\NormalTok{(pcaOutput2}\OperatorTok{$}\NormalTok{groups), }\ControlFlowTok{function}\NormalTok{(t)}
      \KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{groups =} \KeywordTok{as.character}\NormalTok{(t),}
                 \CommentTok{# ellipse data for PC1 and PC2}
                 \KeywordTok{ellipse}\NormalTok{(}\KeywordTok{cov}\NormalTok{(pcaOutput2[pcaOutput2}\OperatorTok{$}\NormalTok{groups }\OperatorTok{==}\StringTok{ }\NormalTok{t, }\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{]),}
                       \DataTypeTok{centre =} \KeywordTok{as.matrix}\NormalTok{(centroids[centroids}\OperatorTok{$}\NormalTok{groups }\OperatorTok{==}\StringTok{ }\NormalTok{t, }\DecValTok{2}\OperatorTok{:}\DecValTok{3}\NormalTok{]),}
                       \DataTypeTok{level =} \FloatTok{0.95}\NormalTok{),}
                 \DataTypeTok{stringsAsFactors =} \OtherTok{FALSE}\NormalTok{)))}
    
\NormalTok{    plot <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ pcaOutput2, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ PC1, }\DataTypeTok{y =}\NormalTok{ PC2, }
                                          \DataTypeTok{group =}\NormalTok{ groups, }
                                          \DataTypeTok{color =}\NormalTok{ groups)) }\OperatorTok{+}\StringTok{ }
\StringTok{      }\KeywordTok{geom_polygon}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ conf.rgn, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{fill =}\NormalTok{ groups), }\DataTypeTok{alpha =} \FloatTok{0.2}\NormalTok{) }\OperatorTok{+}\StringTok{ }\CommentTok{# ellipses}
\StringTok{      }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{size =} \DecValTok{2}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{0.6}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{      }\KeywordTok{scale_color_brewer}\NormalTok{(}\DataTypeTok{palette =} \StringTok{"Set1"}\NormalTok{) }\OperatorTok{+}
\StringTok{      }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{title =}\NormalTok{ title,}
           \DataTypeTok{color =} \StringTok{""}\NormalTok{,}
           \DataTypeTok{fill =} \StringTok{""}\NormalTok{,}
           \DataTypeTok{x =} \KeywordTok{paste0}\NormalTok{(}\StringTok{"PC1: "}\NormalTok{, }\KeywordTok{round}\NormalTok{(pcaOutput}\OperatorTok{$}\NormalTok{pov[}\DecValTok{1}\NormalTok{], }\DataTypeTok{digits =} \DecValTok{2}\NormalTok{) }\OperatorTok{*}\StringTok{ }\DecValTok{100}\NormalTok{, }
                      \StringTok{"% variance"}\NormalTok{),}
           \DataTypeTok{y =} \KeywordTok{paste0}\NormalTok{(}\StringTok{"PC2: "}\NormalTok{, }\KeywordTok{round}\NormalTok{(pcaOutput}\OperatorTok{$}\NormalTok{pov[}\DecValTok{2}\NormalTok{], }\DataTypeTok{digits =} \DecValTok{2}\NormalTok{) }\OperatorTok{*}\StringTok{ }\DecValTok{100}\NormalTok{, }
                      \StringTok{"% variance"}\NormalTok{))}
    
\NormalTok{  \} }\ControlFlowTok{else}\NormalTok{ \{}
    
    \CommentTok{# if < 10 groups (e.g. the predictor classes) have colors from RColorBrewer}
    \ControlFlowTok{if}\NormalTok{ (}\KeywordTok{length}\NormalTok{(}\KeywordTok{unique}\NormalTok{(pcaOutput2}\OperatorTok{$}\NormalTok{groups)) }\OperatorTok{<=}\StringTok{ }\DecValTok{10}\NormalTok{) \{}
      
\NormalTok{      plot <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ pcaOutput2, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ PC1, }\DataTypeTok{y =}\NormalTok{ PC2, }
                                            \DataTypeTok{group =}\NormalTok{ groups, }
                                            \DataTypeTok{color =}\NormalTok{ groups)) }\OperatorTok{+}\StringTok{ }
\StringTok{        }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{size =} \DecValTok{2}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{0.6}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{        }\KeywordTok{scale_color_brewer}\NormalTok{(}\DataTypeTok{palette =} \StringTok{"Set1"}\NormalTok{) }\OperatorTok{+}
\StringTok{        }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{title =}\NormalTok{ title,}
             \DataTypeTok{color =} \StringTok{""}\NormalTok{,}
             \DataTypeTok{fill =} \StringTok{""}\NormalTok{,}
             \DataTypeTok{x =} \KeywordTok{paste0}\NormalTok{(}\StringTok{"PC1: "}\NormalTok{, }\KeywordTok{round}\NormalTok{(pcaOutput}\OperatorTok{$}\NormalTok{pov[}\DecValTok{1}\NormalTok{], }\DataTypeTok{digits =} \DecValTok{2}\NormalTok{) }\OperatorTok{*}\StringTok{ }\DecValTok{100}\NormalTok{, }
                        \StringTok{"% variance"}\NormalTok{),}
             \DataTypeTok{y =} \KeywordTok{paste0}\NormalTok{(}\StringTok{"PC2: "}\NormalTok{, }\KeywordTok{round}\NormalTok{(pcaOutput}\OperatorTok{$}\NormalTok{pov[}\DecValTok{2}\NormalTok{], }\DataTypeTok{digits =} \DecValTok{2}\NormalTok{) }\OperatorTok{*}\StringTok{ }\DecValTok{100}\NormalTok{, }
                        \StringTok{"% variance"}\NormalTok{))}
      
\NormalTok{    \} }\ControlFlowTok{else}\NormalTok{ \{}
      \CommentTok{# otherwise use the default rainbow colors}
\NormalTok{      plot <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ pcaOutput2, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ PC1, }\DataTypeTok{y =}\NormalTok{ PC2, }
                                            \DataTypeTok{group =}\NormalTok{ groups, }\DataTypeTok{color =}\NormalTok{ groups)) }\OperatorTok{+}\StringTok{ }
\StringTok{        }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{size =} \DecValTok{2}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{0.6}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{        }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{title =}\NormalTok{ title,}
             \DataTypeTok{color =} \StringTok{""}\NormalTok{,}
             \DataTypeTok{fill =} \StringTok{""}\NormalTok{,}
             \DataTypeTok{x =} \KeywordTok{paste0}\NormalTok{(}\StringTok{"PC1: "}\NormalTok{, }\KeywordTok{round}\NormalTok{(pcaOutput}\OperatorTok{$}\NormalTok{pov[}\DecValTok{1}\NormalTok{], }\DataTypeTok{digits =} \DecValTok{2}\NormalTok{) }\OperatorTok{*}\StringTok{ }\DecValTok{100}\NormalTok{, }
                        \StringTok{"% variance"}\NormalTok{),}
             \DataTypeTok{y =} \KeywordTok{paste0}\NormalTok{(}\StringTok{"PC2: "}\NormalTok{, }\KeywordTok{round}\NormalTok{(pcaOutput}\OperatorTok{$}\NormalTok{pov[}\DecValTok{2}\NormalTok{], }\DataTypeTok{digits =} \DecValTok{2}\NormalTok{) }\OperatorTok{*}\StringTok{ }\DecValTok{100}\NormalTok{, }
                        \StringTok{"% variance"}\NormalTok{))}
\NormalTok{    \}}
\NormalTok{  \}}
  
  \KeywordTok{return}\NormalTok{(plot)}
  
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(gridExtra)}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'gridExtra'}
\CommentTok{#> The following object is masked from 'package:BiocGenerics':}
\CommentTok{#> }
\CommentTok{#>     combine}
\KeywordTok{library}\NormalTok{(grid)}

\CommentTok{# plot all data. one row is a feature}
\NormalTok{p1 <-}\StringTok{ }\KeywordTok{pca_func}\NormalTok{(}\DataTypeTok{data =} \KeywordTok{t}\NormalTok{(bc_data[, }\DecValTok{2}\OperatorTok{:}\DecValTok{10}\NormalTok{]), }
               \DataTypeTok{groups =} \KeywordTok{as.character}\NormalTok{(bc_data}\OperatorTok{$}\NormalTok{classes), }
               \DataTypeTok{title =} \StringTok{"Breast cancer dataset 1: Samples"}\NormalTok{)}

\CommentTok{# plot features only. features as columns}
\NormalTok{p2 <-}\StringTok{ }\KeywordTok{pca_func}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ bc_data[, }\DecValTok{2}\OperatorTok{:}\DecValTok{10}\NormalTok{], }
               \DataTypeTok{groups =} \KeywordTok{as.character}\NormalTok{(}\KeywordTok{colnames}\NormalTok{(bc_data[, }\DecValTok{2}\OperatorTok{:}\DecValTok{10}\NormalTok{])), }
               \DataTypeTok{title =} \StringTok{"Breast cancer dataset 1: Features"}\NormalTok{, }\DataTypeTok{print_ellipse =} \OtherTok{FALSE}\NormalTok{)}

\KeywordTok{grid.arrange}\NormalTok{(p1, p2, }\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_229-breast_cancer_ga-sglander_files/figure-latex/plot-pca-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{h_}\DecValTok{1}\NormalTok{ <-}\StringTok{ }\KeywordTok{hclust}\NormalTok{(}\KeywordTok{dist}\NormalTok{(}\KeywordTok{t}\NormalTok{(bc_data[, }\DecValTok{2}\OperatorTok{:}\DecValTok{10}\NormalTok{]), }\DataTypeTok{method =} \StringTok{"euclidean"}\NormalTok{), }\DataTypeTok{method =} \StringTok{"complete"}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(h_}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_229-breast_cancer_ga-sglander_files/figure-latex/plot-dendrogram-1} \end{center}

\hypertarget{density-plots-vs-class}{%
\subsection{density plots vs class}\label{density-plots-vs-class}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# density plot showing the feature vs classes}
\KeywordTok{library}\NormalTok{(tidyr)}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'tidyr'}
\CommentTok{#> The following object is masked from 'package:S4Vectors':}
\CommentTok{#> }
\CommentTok{#>     expand}
\CommentTok{#> The following object is masked from 'package:mice':}
\CommentTok{#> }
\CommentTok{#>     complete}

\CommentTok{# gather data. from column clump_thickness to mitosis}
\NormalTok{bc_data_gather <-}\StringTok{ }\NormalTok{bc_data }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{gather}\NormalTok{(measure, value, clump_thickness}\OperatorTok{:}\NormalTok{mitosis)}

\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ bc_data_gather, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ value, }\DataTypeTok{fill =}\NormalTok{ classes, }\DataTypeTok{color =}\NormalTok{ classes)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_density}\NormalTok{(}\DataTypeTok{alpha =} \FloatTok{0.3}\NormalTok{, }\DataTypeTok{size =} \DecValTok{1}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_rug}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_fill_brewer}\NormalTok{(}\DataTypeTok{palette =} \StringTok{"Set1"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_color_brewer}\NormalTok{(}\DataTypeTok{palette =} \StringTok{"Set1"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{facet_wrap}\NormalTok{( }\OperatorTok{~}\StringTok{ }\NormalTok{measure, }\DataTypeTok{scales =} \StringTok{"free_y"}\NormalTok{, }\DataTypeTok{ncol =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_229-breast_cancer_ga-sglander_files/figure-latex/unnamed-chunk-9-1} \end{center}

\hypertarget{feature-importance-1}{%
\section{Feature importance}\label{feature-importance-1}}

To get an idea about the feature's respective importances, I'm running Random Forest models with 10 x 10 cross validation using the \texttt{caret} package. If I wanted to use feature importance to select features for modeling, I would need to perform it on the training data instead of on the complete dataset. But here, I only want to use it to get acquainted with my data. I am again defining a function that estimates the feature importance and produces a plot.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(caret)}
\CommentTok{# library(doParallel) # parallel processing}
\CommentTok{# registerDoParallel()}

\CommentTok{# prepare training scheme}
\NormalTok{control <-}\StringTok{ }\KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"repeatedcv"}\NormalTok{, }\DataTypeTok{number =} \DecValTok{10}\NormalTok{, }\DataTypeTok{repeats =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{feature_imp <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(model, title) \{}
  \CommentTok{# estimate variable importance}
\NormalTok{  importance <-}\StringTok{ }\KeywordTok{varImp}\NormalTok{(model, }\DataTypeTok{scale =} \OtherTok{TRUE}\NormalTok{)}
  \CommentTok{# prepare dataframes for plotting}
\NormalTok{  importance_df_}\DecValTok{1}\NormalTok{ <-}\StringTok{ }\NormalTok{importance}\OperatorTok{$}\NormalTok{importance}
\NormalTok{  importance_df_}\DecValTok{1}\OperatorTok{$}\NormalTok{group <-}\StringTok{ }\KeywordTok{rownames}\NormalTok{(importance_df_}\DecValTok{1}\NormalTok{)}
  
\NormalTok{  importance_df_}\DecValTok{2}\NormalTok{ <-}\StringTok{ }\NormalTok{importance_df_}\DecValTok{1}
\NormalTok{  importance_df_}\DecValTok{2}\OperatorTok{$}\NormalTok{Overall <-}\StringTok{ }\DecValTok{0}
\NormalTok{  importance_df <-}\StringTok{ }\KeywordTok{rbind}\NormalTok{(importance_df_}\DecValTok{1}\NormalTok{, importance_df_}\DecValTok{2}\NormalTok{)}
  
\NormalTok{  plot <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{() }\OperatorTok{+}
\StringTok{    }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ importance_df_}\DecValTok{1}\NormalTok{, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Overall, }
                                           \DataTypeTok{y =}\NormalTok{ group, }
                                           \DataTypeTok{color =}\NormalTok{ group), }\DataTypeTok{size =} \DecValTok{2}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{geom_path}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ importance_df, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Overall, }
                                        \DataTypeTok{y =}\NormalTok{ group, }
                                        \DataTypeTok{color =}\NormalTok{ group, }
                                        \DataTypeTok{group =}\NormalTok{ group), }\DataTypeTok{size =} \DecValTok{1}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{legend.position =} \StringTok{"none"}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{labs}\NormalTok{(}
      \DataTypeTok{x =} \StringTok{"Importance"}\NormalTok{,}
      \DataTypeTok{y =} \StringTok{""}\NormalTok{,}
      \DataTypeTok{title =}\NormalTok{ title,}
      \DataTypeTok{subtitle =} \StringTok{"Scaled feature importance"}\NormalTok{,}
      \DataTypeTok{caption =} \StringTok{"}\CharTok{\textbackslash{}n}\StringTok{Determined with Random Forest and}
\StringTok{      repeated cross validation (10 repeats, 10 times)"}
\NormalTok{    )}
  \KeywordTok{return}\NormalTok{(plot)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# train the model}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{27}\NormalTok{)}
\NormalTok{imp_}\DecValTok{1}\NormalTok{ <-}\StringTok{ }\KeywordTok{train}\NormalTok{(classes }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ bc_data, }\DataTypeTok{method =} \StringTok{"rf"}\NormalTok{, }
               \DataTypeTok{preProcess =} \KeywordTok{c}\NormalTok{(}\StringTok{"scale"}\NormalTok{, }\StringTok{"center"}\NormalTok{), }
               \DataTypeTok{trControl =}\NormalTok{ control)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p1 <-}\StringTok{ }\KeywordTok{feature_imp}\NormalTok{(imp_}\DecValTok{1}\NormalTok{, }\DataTypeTok{title =} \StringTok{"Breast cancer dataset 1"}\NormalTok{)}
\NormalTok{p1}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_229-breast_cancer_ga-sglander_files/figure-latex/plot-featureImportance-1} \end{center}

\hypertarget{feature-selection-1}{%
\section{Feature Selection}\label{feature-selection-1}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  By correlation
\item
  By Recursive Feature Elimination
\item
  By Genetic Algorithm
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{27}\NormalTok{)}
\NormalTok{bc_data_index <-}\StringTok{ }\KeywordTok{createDataPartition}\NormalTok{(bc_data}\OperatorTok{$}\NormalTok{classes, }\DataTypeTok{p =} \FloatTok{0.7}\NormalTok{, }\DataTypeTok{list =} \OtherTok{FALSE}\NormalTok{)}
\NormalTok{bc_data_train <-}\StringTok{ }\NormalTok{bc_data[bc_data_index, ]}
\NormalTok{bc_data_test  <-}\StringTok{ }\NormalTok{bc_data[}\OperatorTok{-}\NormalTok{bc_data_index, ]}
\end{Highlighting}
\end{Shaded}

\hypertarget{correlation-1}{%
\subsection{Correlation}\label{correlation-1}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(corrplot)}
\CommentTok{#> corrplot 0.84 loaded}

\CommentTok{# calculate correlation matrix}
\NormalTok{corMatMy <-}\StringTok{ }\KeywordTok{cor}\NormalTok{(bc_data_train[, }\DecValTok{-1}\NormalTok{])}
\KeywordTok{corrplot}\NormalTok{(corMatMy, }\DataTypeTok{order =} \StringTok{"hclust"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_229-breast_cancer_ga-sglander_files/figure-latex/plot-correlation-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Apply correlation filter at 0.70,}
\NormalTok{highlyCor <-}\StringTok{ }\KeywordTok{colnames}\NormalTok{(bc_data_train[, }\DecValTok{-1}\NormalTok{])[}\KeywordTok{findCorrelation}\NormalTok{(corMatMy, }
                                                           \DataTypeTok{cutoff =} \FloatTok{0.7}\NormalTok{, }
                                                           \DataTypeTok{verbose =} \OtherTok{TRUE}\NormalTok{)]}
\CommentTok{#> Compare row 2  and column  3 with corr  0.9 }
\CommentTok{#>   Means:  0.709 vs 0.595 so flagging column 2 }
\CommentTok{#> Compare row 3  and column  7 with corr  0.737 }
\CommentTok{#>   Means:  0.674 vs 0.572 so flagging column 3 }
\CommentTok{#> All correlations <= 0.7}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# which variables are flagged for removal?}
\NormalTok{highlyCor}
\CommentTok{#> [1] "uniformity_of_cell_size"  "uniformity_of_cell_shape"}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# then we remove these variables}
\NormalTok{bc_data_cor <-}\StringTok{ }\NormalTok{bc_data_train[, }\KeywordTok{which}\NormalTok{(}\OperatorTok{!}\KeywordTok{colnames}\NormalTok{(bc_data_train) }\OperatorTok{%in%}\StringTok{ }\NormalTok{highlyCor)]}
\KeywordTok{names}\NormalTok{(bc_data_cor)}
\CommentTok{#> [1] "classes"                     "clump_thickness"            }
\CommentTok{#> [3] "marginal_adhesion"           "single_epithelial_cell_size"}
\CommentTok{#> [5] "bare_nuclei"                 "bland_chromatin"            }
\CommentTok{#> [7] "normal_nucleoli"             "mitosis"}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# confirm features were removed}
\NormalTok{outersect <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x, y) \{}
  \KeywordTok{sort}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\KeywordTok{setdiff}\NormalTok{(x, y),}
         \KeywordTok{setdiff}\NormalTok{(y, x)))}
\NormalTok{\}}
 
\KeywordTok{outersect}\NormalTok{(}\KeywordTok{names}\NormalTok{(bc_data_cor), }\KeywordTok{names}\NormalTok{(bc_data_train))}
\CommentTok{#> [1] "uniformity_of_cell_shape" "uniformity_of_cell_size"}
\end{Highlighting}
\end{Shaded}

\begin{quote}
Four features removed
\end{quote}

\hypertarget{recursive-feature-elimination-rfe}{%
\subsection{Recursive Feature Elimination (RFE)}\label{recursive-feature-elimination-rfe}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# ensure the results are repeatable}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{7}\NormalTok{)}

\CommentTok{# define the control using a random forest selection function with cross validation}
\NormalTok{control <-}\StringTok{ }\KeywordTok{rfeControl}\NormalTok{(}\DataTypeTok{functions =}\NormalTok{ rfFuncs, }\DataTypeTok{method =} \StringTok{"cv"}\NormalTok{, }\DataTypeTok{number =} \DecValTok{10}\NormalTok{)}

\CommentTok{# run the RFE algorithm}
\NormalTok{results_}\DecValTok{1}\NormalTok{ <-}\StringTok{ }\KeywordTok{rfe}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ bc_data_train[, }\DecValTok{-1}\NormalTok{], }
                 \DataTypeTok{y =}\NormalTok{ bc_data_train}\OperatorTok{$}\NormalTok{classes, }
                 \DataTypeTok{sizes =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{9}\NormalTok{), }
                 \DataTypeTok{rfeControl =}\NormalTok{ control)}

\CommentTok{# chosen features}
\KeywordTok{predictors}\NormalTok{(results_}\DecValTok{1}\NormalTok{)}
\CommentTok{#> [1] "bare_nuclei"                 "clump_thickness"            }
\CommentTok{#> [3] "normal_nucleoli"             "uniformity_of_cell_size"    }
\CommentTok{#> [5] "uniformity_of_cell_shape"    "single_epithelial_cell_size"}
\CommentTok{#> [7] "bland_chromatin"             "marginal_adhesion"}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# subset the chosen features}
\NormalTok{sel_cols <-}\StringTok{ }\KeywordTok{which}\NormalTok{(}\KeywordTok{colnames}\NormalTok{(bc_data_train) }\OperatorTok{%in%}\StringTok{ }\KeywordTok{predictors}\NormalTok{(results_}\DecValTok{1}\NormalTok{))}
\NormalTok{bc_data_rfe <-}\StringTok{ }\NormalTok{bc_data_train[, }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, sel_cols)]}
\KeywordTok{names}\NormalTok{(bc_data_rfe)}
\CommentTok{#> [1] "classes"                     "clump_thickness"            }
\CommentTok{#> [3] "uniformity_of_cell_size"     "uniformity_of_cell_shape"   }
\CommentTok{#> [5] "marginal_adhesion"           "single_epithelial_cell_size"}
\CommentTok{#> [7] "bare_nuclei"                 "bland_chromatin"            }
\CommentTok{#> [9] "normal_nucleoli"}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# confirm features removed by RFE}
\KeywordTok{outersect}\NormalTok{(}\KeywordTok{names}\NormalTok{(bc_data_rfe), }\KeywordTok{names}\NormalTok{(bc_data_train))}
\CommentTok{#> [1] "mitosis"}
\end{Highlighting}
\end{Shaded}

\begin{quote}
No features removed with RFE
\end{quote}

\hypertarget{genetic-algorithm-ga}{%
\subsection{Genetic Algorithm (GA)}\label{genetic-algorithm-ga}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(dplyr)}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'dplyr'}
\CommentTok{#> The following object is masked from 'package:gridExtra':}
\CommentTok{#> }
\CommentTok{#>     combine}
\CommentTok{#> The following objects are masked from 'package:Biostrings':}
\CommentTok{#> }
\CommentTok{#>     collapse, intersect, setdiff, setequal, union}
\CommentTok{#> The following object is masked from 'package:XVector':}
\CommentTok{#> }
\CommentTok{#>     slice}
\CommentTok{#> The following objects are masked from 'package:IRanges':}
\CommentTok{#> }
\CommentTok{#>     collapse, desc, intersect, setdiff, slice, union}
\CommentTok{#> The following objects are masked from 'package:S4Vectors':}
\CommentTok{#> }
\CommentTok{#>     first, intersect, rename, setdiff, setequal, union}
\CommentTok{#> The following objects are masked from 'package:BiocGenerics':}
\CommentTok{#> }
\CommentTok{#>     combine, intersect, setdiff, union}
\CommentTok{#> The following objects are masked from 'package:stats':}
\CommentTok{#> }
\CommentTok{#>     filter, lag}
\CommentTok{#> The following objects are masked from 'package:base':}
\CommentTok{#> }
\CommentTok{#>     intersect, setdiff, setequal, union}

\NormalTok{ga_ctrl <-}\StringTok{ }\KeywordTok{gafsControl}\NormalTok{(}\DataTypeTok{functions =}\NormalTok{ rfGA, }\CommentTok{# Assess fitness with RF}
                       \DataTypeTok{method =} \StringTok{"cv"}\NormalTok{,    }\CommentTok{# 10 fold cross validation}
                       \DataTypeTok{genParallel =} \OtherTok{TRUE}\NormalTok{, }\CommentTok{# Use parallel programming}
                       \DataTypeTok{allowParallel =} \OtherTok{TRUE}\NormalTok{)}

\NormalTok{lev <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"malignant"}\NormalTok{, }\StringTok{"benign"}\NormalTok{)     }\CommentTok{# Set the levels}

\KeywordTok{set.seed}\NormalTok{(}\DecValTok{27}\NormalTok{)}
\NormalTok{model_}\DecValTok{1}\NormalTok{ <-}\StringTok{ }\KeywordTok{gafs}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ bc_data_train[, }\DecValTok{-1}\NormalTok{], }\DataTypeTok{y =}\NormalTok{ bc_data_train}\OperatorTok{$}\NormalTok{classes,}
                   \DataTypeTok{iters =} \DecValTok{10}\NormalTok{, }\CommentTok{# generations of algorithm}
                   \DataTypeTok{popSize =} \DecValTok{5}\NormalTok{, }\CommentTok{# population size for each generation}
                   \DataTypeTok{levels =}\NormalTok{ lev,}
                   \DataTypeTok{gafsControl =}\NormalTok{ ga_ctrl)}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'recipes'}
\CommentTok{#> The following object is masked from 'package:stats':}
\CommentTok{#> }
\CommentTok{#>     step}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(model_}\DecValTok{1}\NormalTok{) }\CommentTok{# Plot mean fitness (AUC) by generation}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_229-breast_cancer_ga-sglander_files/figure-latex/plot-auc-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# features}
\NormalTok{model_}\DecValTok{1}\OperatorTok{$}\NormalTok{ga}\OperatorTok{$}\NormalTok{final}
\CommentTok{#> [1] "clump_thickness"          "uniformity_of_cell_size" }
\CommentTok{#> [3] "uniformity_of_cell_shape" "marginal_adhesion"       }
\CommentTok{#> [5] "bare_nuclei"              "bland_chromatin"         }
\CommentTok{#> [7] "normal_nucleoli"}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# select features}
\NormalTok{sel_cols_ga <-}\StringTok{ }\KeywordTok{which}\NormalTok{(}\KeywordTok{colnames}\NormalTok{(bc_data_train) }\OperatorTok{%in%}\StringTok{ }\NormalTok{model_}\DecValTok{1}\OperatorTok{$}\NormalTok{ga}\OperatorTok{$}\NormalTok{final)}
\NormalTok{bc_data_ga <-}\StringTok{ }\NormalTok{bc_data_train[, }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, sel_cols_ga)]}
\KeywordTok{names}\NormalTok{(bc_data_ga)}
\CommentTok{#> [1] "classes"                  "clump_thickness"         }
\CommentTok{#> [3] "uniformity_of_cell_size"  "uniformity_of_cell_shape"}
\CommentTok{#> [5] "marginal_adhesion"        "bare_nuclei"             }
\CommentTok{#> [7] "bland_chromatin"          "normal_nucleoli"}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# features removed GA}
\KeywordTok{outersect}\NormalTok{(}\KeywordTok{names}\NormalTok{(bc_data_ga), }\KeywordTok{names}\NormalTok{(bc_data_train))}
\CommentTok{#> [1] "mitosis"                     "single_epithelial_cell_size"}
\end{Highlighting}
\end{Shaded}

\begin{quote}
Two features removed with GA.
\end{quote}

\hypertarget{model-comparison}{%
\section{Model comparison}\label{model-comparison}}

\hypertarget{using-all-features}{%
\subsection{Using all features}\label{using-all-features}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{27}\NormalTok{)}
\NormalTok{model_bc_data_all <-}\StringTok{ }\KeywordTok{train}\NormalTok{(classes }\OperatorTok{~}\StringTok{ }\NormalTok{.,}
                           \DataTypeTok{data =}\NormalTok{ bc_data_train,}
                           \DataTypeTok{method =} \StringTok{"rf"}\NormalTok{,}
                           \DataTypeTok{preProcess =} \KeywordTok{c}\NormalTok{(}\StringTok{"scale"}\NormalTok{, }\StringTok{"center"}\NormalTok{),}
                           \DataTypeTok{trControl =} \KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"repeatedcv"}\NormalTok{, }
                                                    \DataTypeTok{number =} \DecValTok{5}\NormalTok{, }\DataTypeTok{repeats =} \DecValTok{10}\NormalTok{,}
                                                    \DataTypeTok{verboseIter =} \OtherTok{FALSE}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# confusion matrix }
\NormalTok{cm_all_}\DecValTok{1}\NormalTok{ <-}\StringTok{ }\KeywordTok{confusionMatrix}\NormalTok{(}\KeywordTok{predict}\NormalTok{(model_bc_data_all, bc_data_test[, }\DecValTok{-1}\NormalTok{]), bc_data_test}\OperatorTok{$}\NormalTok{classes)}
\NormalTok{cm_all_}\DecValTok{1}
\CommentTok{#> Confusion Matrix and Statistics}
\CommentTok{#> }
\CommentTok{#>            Reference}
\CommentTok{#> Prediction  benign malignant}
\CommentTok{#>   benign       131         2}
\CommentTok{#>   malignant      6        70}
\CommentTok{#>                                         }
\CommentTok{#>                Accuracy : 0.962         }
\CommentTok{#>                  95% CI : (0.926, 0.983)}
\CommentTok{#>     No Information Rate : 0.656         }
\CommentTok{#>     P-Value [Acc > NIR] : <2e-16        }
\CommentTok{#>                                         }
\CommentTok{#>                   Kappa : 0.916         }
\CommentTok{#>                                         }
\CommentTok{#>  Mcnemar's Test P-Value : 0.289         }
\CommentTok{#>                                         }
\CommentTok{#>             Sensitivity : 0.956         }
\CommentTok{#>             Specificity : 0.972         }
\CommentTok{#>          Pos Pred Value : 0.985         }
\CommentTok{#>          Neg Pred Value : 0.921         }
\CommentTok{#>              Prevalence : 0.656         }
\CommentTok{#>          Detection Rate : 0.627         }
\CommentTok{#>    Detection Prevalence : 0.636         }
\CommentTok{#>       Balanced Accuracy : 0.964         }
\CommentTok{#>                                         }
\CommentTok{#>        'Positive' Class : benign        }
\CommentTok{#> }
\end{Highlighting}
\end{Shaded}

\hypertarget{compare-selection-methods}{%
\subsection{Compare selection methods}\label{compare-selection-methods}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# compare features selected by the three methods}
\KeywordTok{library}\NormalTok{(gplots)}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'gplots'}
\CommentTok{#> The following object is masked from 'package:IRanges':}
\CommentTok{#> }
\CommentTok{#>     space}
\CommentTok{#> The following object is masked from 'package:S4Vectors':}
\CommentTok{#> }
\CommentTok{#>     space}
\CommentTok{#> The following object is masked from 'package:stats':}
\CommentTok{#> }
\CommentTok{#>     lowess}

\NormalTok{venn_list <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}\DataTypeTok{cor =} \KeywordTok{colnames}\NormalTok{(bc_data_cor)[}\OperatorTok{-}\DecValTok{1}\NormalTok{],}
                  \DataTypeTok{rfe =} \KeywordTok{colnames}\NormalTok{(bc_data_rfe)[}\OperatorTok{-}\DecValTok{1}\NormalTok{],}
                  \DataTypeTok{ga  =} \KeywordTok{colnames}\NormalTok{(bc_data_ga)[}\OperatorTok{-}\DecValTok{1}\NormalTok{])}

\NormalTok{venn <-}\StringTok{ }\KeywordTok{venn}\NormalTok{(venn_list)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_229-breast_cancer_ga-sglander_files/figure-latex/unnamed-chunk-20-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{venn}
\CommentTok{#>     num cor rfe ga}
\CommentTok{#> 000   0   0   0  0}
\CommentTok{#> 001   0   0   0  1}
\CommentTok{#> 010   0   0   1  0}
\CommentTok{#> 011   2   0   1  1}
\CommentTok{#> 100   1   1   0  0}
\CommentTok{#> 101   0   1   0  1}
\CommentTok{#> 110   1   1   1  0}
\CommentTok{#> 111   5   1   1  1}
\CommentTok{#> attr(,"intersections")}
\CommentTok{#> attr(,"intersections")$`cor:rfe:ga`}
\CommentTok{#> [1] "clump_thickness"   "marginal_adhesion" "bare_nuclei"      }
\CommentTok{#> [4] "bland_chromatin"   "normal_nucleoli"  }
\CommentTok{#> }
\CommentTok{#> attr(,"intersections")$cor}
\CommentTok{#> [1] "mitosis"}
\CommentTok{#> }
\CommentTok{#> attr(,"intersections")$`rfe:ga`}
\CommentTok{#> [1] "uniformity_of_cell_size"  "uniformity_of_cell_shape"}
\CommentTok{#> }
\CommentTok{#> attr(,"intersections")$`cor:rfe`}
\CommentTok{#> [1] "single_epithelial_cell_size"}
\CommentTok{#> }
\CommentTok{#> attr(,"class")}
\CommentTok{#> [1] "venn"}
\end{Highlighting}
\end{Shaded}

\begin{quote}
4 out of 10 features were chosen by all three methods; the biggest overlap is seen between GA and RFE with 7 features. RFE and GA both retained 8 features for modeling, compared to only 5 based on the correlation method.
\end{quote}

\hypertarget{correlation-2}{%
\subsection{Correlation}\label{correlation-2}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# correlation}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{127}\NormalTok{)}
\NormalTok{model_bc_data_cor <-}\StringTok{ }\KeywordTok{train}\NormalTok{(classes }\OperatorTok{~}\StringTok{ }\NormalTok{.,}
                 \DataTypeTok{data =}\NormalTok{ bc_data_cor,}
                 \DataTypeTok{method =} \StringTok{"rf"}\NormalTok{,}
                 \DataTypeTok{preProcess =} \KeywordTok{c}\NormalTok{(}\StringTok{"scale"}\NormalTok{, }\StringTok{"center"}\NormalTok{),}
                 \DataTypeTok{trControl =} \KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"repeatedcv"}\NormalTok{, }\DataTypeTok{number =} \DecValTok{5}\NormalTok{, }\DataTypeTok{repeats =} \DecValTok{10}\NormalTok{, }\DataTypeTok{verboseIter =} \OtherTok{FALSE}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cm_cor_}\DecValTok{1}\NormalTok{ <-}\StringTok{ }\KeywordTok{confusionMatrix}\NormalTok{(}\KeywordTok{predict}\NormalTok{(model_bc_data_cor, bc_data_test[, }\DecValTok{-1}\NormalTok{]), bc_data_test}\OperatorTok{$}\NormalTok{classes)}
\NormalTok{cm_cor_}\DecValTok{1}
\CommentTok{#> Confusion Matrix and Statistics}
\CommentTok{#> }
\CommentTok{#>            Reference}
\CommentTok{#> Prediction  benign malignant}
\CommentTok{#>   benign       130         4}
\CommentTok{#>   malignant      7        68}
\CommentTok{#>                                         }
\CommentTok{#>                Accuracy : 0.947         }
\CommentTok{#>                  95% CI : (0.908, 0.973)}
\CommentTok{#>     No Information Rate : 0.656         }
\CommentTok{#>     P-Value [Acc > NIR] : <2e-16        }
\CommentTok{#>                                         }
\CommentTok{#>                   Kappa : 0.885         }
\CommentTok{#>                                         }
\CommentTok{#>  Mcnemar's Test P-Value : 0.546         }
\CommentTok{#>                                         }
\CommentTok{#>             Sensitivity : 0.949         }
\CommentTok{#>             Specificity : 0.944         }
\CommentTok{#>          Pos Pred Value : 0.970         }
\CommentTok{#>          Neg Pred Value : 0.907         }
\CommentTok{#>              Prevalence : 0.656         }
\CommentTok{#>          Detection Rate : 0.622         }
\CommentTok{#>    Detection Prevalence : 0.641         }
\CommentTok{#>       Balanced Accuracy : 0.947         }
\CommentTok{#>                                         }
\CommentTok{#>        'Positive' Class : benign        }
\CommentTok{#> }
\end{Highlighting}
\end{Shaded}

\hypertarget{recursive-feature-elimination}{%
\subsection{Recursive Feature Elimination}\label{recursive-feature-elimination}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{127}\NormalTok{)}
\NormalTok{model_bc_data_rfe <-}\StringTok{ }\KeywordTok{train}\NormalTok{(classes }\OperatorTok{~}\StringTok{ }\NormalTok{.,}
                           \DataTypeTok{data =}\NormalTok{ bc_data_rfe,}
                           \DataTypeTok{method =} \StringTok{"rf"}\NormalTok{,}
                           \DataTypeTok{preProcess =} \KeywordTok{c}\NormalTok{(}\StringTok{"scale"}\NormalTok{, }\StringTok{"center"}\NormalTok{),}
                           \DataTypeTok{trControl =} \KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"repeatedcv"}\NormalTok{, }
                                                    \DataTypeTok{number =} \DecValTok{5}\NormalTok{, }\DataTypeTok{repeats =} \DecValTok{10}\NormalTok{, }
                                                    \DataTypeTok{verboseIter =} \OtherTok{FALSE}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cm_rfe_}\DecValTok{1}\NormalTok{ <-}\StringTok{ }\KeywordTok{confusionMatrix}\NormalTok{(}\KeywordTok{predict}\NormalTok{(model_bc_data_rfe, bc_data_test[, }\DecValTok{-1}\NormalTok{]), bc_data_test}\OperatorTok{$}\NormalTok{classes)}
\NormalTok{cm_rfe_}\DecValTok{1}
\CommentTok{#> Confusion Matrix and Statistics}
\CommentTok{#> }
\CommentTok{#>            Reference}
\CommentTok{#> Prediction  benign malignant}
\CommentTok{#>   benign       130         3}
\CommentTok{#>   malignant      7        69}
\CommentTok{#>                                         }
\CommentTok{#>                Accuracy : 0.952         }
\CommentTok{#>                  95% CI : (0.914, 0.977)}
\CommentTok{#>     No Information Rate : 0.656         }
\CommentTok{#>     P-Value [Acc > NIR] : <2e-16        }
\CommentTok{#>                                         }
\CommentTok{#>                   Kappa : 0.895         }
\CommentTok{#>                                         }
\CommentTok{#>  Mcnemar's Test P-Value : 0.343         }
\CommentTok{#>                                         }
\CommentTok{#>             Sensitivity : 0.949         }
\CommentTok{#>             Specificity : 0.958         }
\CommentTok{#>          Pos Pred Value : 0.977         }
\CommentTok{#>          Neg Pred Value : 0.908         }
\CommentTok{#>              Prevalence : 0.656         }
\CommentTok{#>          Detection Rate : 0.622         }
\CommentTok{#>    Detection Prevalence : 0.636         }
\CommentTok{#>       Balanced Accuracy : 0.954         }
\CommentTok{#>                                         }
\CommentTok{#>        'Positive' Class : benign        }
\CommentTok{#> }
\end{Highlighting}
\end{Shaded}

\hypertarget{ga}{%
\subsection{GA}\label{ga}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{127}\NormalTok{)}
\NormalTok{model_bc_data_ga <-}\StringTok{ }\KeywordTok{train}\NormalTok{(classes }\OperatorTok{~}\StringTok{ }\NormalTok{.,}
                           \DataTypeTok{data =}\NormalTok{ bc_data_ga,}
                           \DataTypeTok{method =} \StringTok{"rf"}\NormalTok{,}
                           \DataTypeTok{preProcess =} \KeywordTok{c}\NormalTok{(}\StringTok{"scale"}\NormalTok{, }\StringTok{"center"}\NormalTok{),}
                           \DataTypeTok{trControl =} \KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"repeatedcv"}\NormalTok{, }
                                                    \DataTypeTok{number =} \DecValTok{5}\NormalTok{, }\DataTypeTok{repeats =} \DecValTok{10}\NormalTok{, }
                                                    \DataTypeTok{verboseIter =} \OtherTok{FALSE}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cm_ga_}\DecValTok{1}\NormalTok{ <-}\StringTok{ }\KeywordTok{confusionMatrix}\NormalTok{(}\KeywordTok{predict}\NormalTok{(model_bc_data_ga, bc_data_test[, }\DecValTok{-1}\NormalTok{]), bc_data_test}\OperatorTok{$}\NormalTok{classes)}
\NormalTok{cm_ga_}\DecValTok{1}
\CommentTok{#> Confusion Matrix and Statistics}
\CommentTok{#> }
\CommentTok{#>            Reference}
\CommentTok{#> Prediction  benign malignant}
\CommentTok{#>   benign       131         2}
\CommentTok{#>   malignant      6        70}
\CommentTok{#>                                         }
\CommentTok{#>                Accuracy : 0.962         }
\CommentTok{#>                  95% CI : (0.926, 0.983)}
\CommentTok{#>     No Information Rate : 0.656         }
\CommentTok{#>     P-Value [Acc > NIR] : <2e-16        }
\CommentTok{#>                                         }
\CommentTok{#>                   Kappa : 0.916         }
\CommentTok{#>                                         }
\CommentTok{#>  Mcnemar's Test P-Value : 0.289         }
\CommentTok{#>                                         }
\CommentTok{#>             Sensitivity : 0.956         }
\CommentTok{#>             Specificity : 0.972         }
\CommentTok{#>          Pos Pred Value : 0.985         }
\CommentTok{#>          Neg Pred Value : 0.921         }
\CommentTok{#>              Prevalence : 0.656         }
\CommentTok{#>          Detection Rate : 0.627         }
\CommentTok{#>    Detection Prevalence : 0.636         }
\CommentTok{#>       Balanced Accuracy : 0.964         }
\CommentTok{#>                                         }
\CommentTok{#>        'Positive' Class : benign        }
\CommentTok{#> }
\end{Highlighting}
\end{Shaded}

\hypertarget{create-comparison-tables}{%
\section{Create comparison tables}\label{create-comparison-tables}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# take "overall" variable only from Confusion Matrix}
\NormalTok{overall <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{dataset =} \DecValTok{1}\NormalTok{, }
           \DataTypeTok{model =} \KeywordTok{rep}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"all"}\NormalTok{, }\StringTok{"cor"}\NormalTok{, }\StringTok{"rfe"}\NormalTok{, }\StringTok{"ga"}\NormalTok{), }\DecValTok{1}\NormalTok{),}
           \KeywordTok{rbind}\NormalTok{(cm_all_}\DecValTok{1}\OperatorTok{$}\NormalTok{overall,}
\NormalTok{                 cm_cor_}\DecValTok{1}\OperatorTok{$}\NormalTok{overall,}
\NormalTok{                 cm_rfe_}\DecValTok{1}\OperatorTok{$}\NormalTok{overall,}
\NormalTok{                 cm_ga_}\DecValTok{1}\OperatorTok{$}\NormalTok{overall)}
\NormalTok{)}

\CommentTok{# convert to tidy data}
\KeywordTok{library}\NormalTok{(tidyr)}
\NormalTok{overall_gather <-}\StringTok{ }\NormalTok{overall[, }\DecValTok{1}\OperatorTok{:}\DecValTok{4}\NormalTok{] }\OperatorTok{%>%}\StringTok{     }\CommentTok{# take the first columns:}
\StringTok{  }\KeywordTok{gather}\NormalTok{(measure, value, Accuracy}\OperatorTok{:}\NormalTok{Kappa) }\CommentTok{# dataset, model, Accuracy and Kappa}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# take "byClass" variable only from Confusion Matrix}
\NormalTok{byClass <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{dataset =} \DecValTok{1}\NormalTok{,}
                      \DataTypeTok{model =} \KeywordTok{rep}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"all"}\NormalTok{, }\StringTok{"cor"}\NormalTok{, }\StringTok{"rfe"}\NormalTok{, }\StringTok{"ga"}\NormalTok{), }\DecValTok{1}\NormalTok{),}
                      \KeywordTok{rbind}\NormalTok{(cm_all_}\DecValTok{1}\OperatorTok{$}\NormalTok{byClass,}
\NormalTok{                      cm_cor_}\DecValTok{1}\OperatorTok{$}\NormalTok{byClass,}
\NormalTok{                      cm_rfe_}\DecValTok{1}\OperatorTok{$}\NormalTok{byClass,}
\NormalTok{                      cm_ga_}\DecValTok{1}\OperatorTok{$}\NormalTok{byClass)}
\NormalTok{)}

\CommentTok{# convert to tidy data}
\NormalTok{byClass_gather <-}\StringTok{ }\NormalTok{byClass[, }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{4}\NormalTok{, }\DecValTok{7}\NormalTok{)] }\OperatorTok{%>%}\StringTok{      }\CommentTok{# select columns: dataset, model}
\StringTok{  }\KeywordTok{gather}\NormalTok{(measure, value, Sensitivity}\OperatorTok{:}\NormalTok{Precision) }\CommentTok{# Sensitiv, Specific, Precis}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# join the two tables}
\NormalTok{overall_byClass_gather <-}\StringTok{ }\KeywordTok{rbind}\NormalTok{(overall_gather, byClass_gather)}
\NormalTok{overall_byClass_gather <-}\StringTok{ }\KeywordTok{within}\NormalTok{(}
\NormalTok{  overall_byClass_gather, model <-}\StringTok{ }\KeywordTok{factor}\NormalTok{(model, }
                                          \DataTypeTok{levels =} \KeywordTok{c}\NormalTok{(}\StringTok{"all"}\NormalTok{, }\StringTok{"cor"}\NormalTok{, }\StringTok{"rfe"}\NormalTok{, }\StringTok{"ga"}\NormalTok{)))  }
                                          \CommentTok{# convert to factor}

\KeywordTok{ggplot}\NormalTok{(overall_byClass_gather, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ model, }\DataTypeTok{y =}\NormalTok{ value, }\DataTypeTok{color =}\NormalTok{ measure, }
                                   \DataTypeTok{shape =}\NormalTok{ measure, }\DataTypeTok{group =}\NormalTok{ measure)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{size =} \DecValTok{4}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{0.8}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_path}\NormalTok{(}\DataTypeTok{alpha =} \FloatTok{0.7}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_colour_brewer}\NormalTok{(}\DataTypeTok{palette =} \StringTok{"Set1"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{facet_grid}\NormalTok{(dataset }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{scales =} \StringTok{"free_y"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}
    \DataTypeTok{x =} \StringTok{"Feature Selection method"}\NormalTok{,}
    \DataTypeTok{y =} \StringTok{"Value"}\NormalTok{,}
    \DataTypeTok{color =} \StringTok{""}\NormalTok{,}
    \DataTypeTok{shape =} \StringTok{""}\NormalTok{,}
    \DataTypeTok{title =} \StringTok{"Comparison of feature selection methods"}\NormalTok{,}
    \DataTypeTok{subtitle =} \StringTok{"in three breast cancer datasets"}\NormalTok{,}
    \DataTypeTok{caption =} \StringTok{"}\CharTok{\textbackslash{}n}\StringTok{Breast Cancer Wisconsin (Diagnostic) Data Sets: 1, 2 & 3}
\StringTok{    Street et al., 1993;}
\StringTok{    all: no feature selection}
\StringTok{    cor: features with correlation > 0.7 removed}
\StringTok{    rfe: Recursive Feature Elimination}
\StringTok{    ga: Genetic Algorithm"}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_229-breast_cancer_ga-sglander_files/figure-latex/unnamed-chunk-27-1} \end{center}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Less accurate: selection of features by correlation
\item
  More accurate: genetic algorithm
\item
  Including all features is more accurate to removing features by correlation.
\end{enumerate}

\hypertarget{notes}{%
\section{Notes}\label{notes}}

\texttt{pcaGoPromoter} is a BioConductor package. Its dependencies are \texttt{BioGenerics}, \texttt{AnnotationDbi} and \texttt{BioStrings}, which at their turn require \texttt{DBI} and \texttt{RSQLite} packages from CRAN. Install first those from CRAN, and then move to install \texttt{pcaGoPromoter}.

\hypertarget{titanic-with-naive-bayes-classifier}{%
\chapter{Titanic with Naive-Bayes Classifier}\label{titanic-with-naive-bayes-classifier}}

The Titanic dataset in R is a table for about 2200 passengers summarised according to four factors -- economic status ranging from 1st class, 2nd class, 3rd class and crew; gender which is either male or female; Age category which is either Child or Adult and whether the type of passenger survived. For each combination of Age, Gender, Class and Survived status, the table gives the number of passengers who fall into the combination. We will use the Naive Bayes Technique to classify such passengers and check how well it performs.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#Getting started with Naive Bayes}
\CommentTok{#Install the package}
\CommentTok{#install.packages(e1071)}
\CommentTok{#Loading the library}
\KeywordTok{library}\NormalTok{(e1071)}

\CommentTok{#Next load the Titanic dataset}
\KeywordTok{data}\NormalTok{(}\StringTok{"Titanic"}\NormalTok{)}
\CommentTok{#Save into a data frame and view it}
\NormalTok{Titanic_df =}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(Titanic)}
\end{Highlighting}
\end{Shaded}

We see that there are 32 observations which represent all possible combinations of Class, Sex, Age and Survived with their frequency. Since it is summarised, this table is not suitable for modelling purposes. We need to expand the table into individual rows. Let's create a repeating sequence of rows based on the frequencies in the table

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#Creating data from table}
\NormalTok{repeating_sequence=}\KeywordTok{rep.int}\NormalTok{(}\KeywordTok{seq_len}\NormalTok{(}\KeywordTok{nrow}\NormalTok{(Titanic_df)), Titanic_df}\OperatorTok{$}\NormalTok{Freq) }\CommentTok{#This will repeat each combination equal to the frequency of each combination}

\CommentTok{# Create the dataset by row repetition created}
\NormalTok{Titanic_dataset=Titanic_df[repeating_sequence,]}

\CommentTok{# We no longer need the frequency, drop the feature}
\NormalTok{Titanic_dataset}\OperatorTok{$}\NormalTok{Freq=}\OtherTok{NULL}
\end{Highlighting}
\end{Shaded}

The data is now ready for Naive Bayes to process. Let's fit the model

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Fitting the Naive Bayes model}
\NormalTok{Naive_Bayes_Model=}\KeywordTok{naiveBayes}\NormalTok{(Survived }\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{Titanic_dataset)}

\CommentTok{# What does the model say? Print the model summary}
\NormalTok{Naive_Bayes_Model}
\CommentTok{#> }
\CommentTok{#> Naive Bayes Classifier for Discrete Predictors}
\CommentTok{#> }
\CommentTok{#> Call:}
\CommentTok{#> naiveBayes.default(x = X, y = Y, laplace = laplace)}
\CommentTok{#> }
\CommentTok{#> A-priori probabilities:}
\CommentTok{#> Y}
\CommentTok{#>    No   Yes }
\CommentTok{#> 0.677 0.323 }
\CommentTok{#> }
\CommentTok{#> Conditional probabilities:}
\CommentTok{#>      Class}
\CommentTok{#> Y        1st    2nd    3rd   Crew}
\CommentTok{#>   No  0.0819 0.1121 0.3544 0.4517}
\CommentTok{#>   Yes 0.2855 0.1660 0.2504 0.2982}
\CommentTok{#> }
\CommentTok{#>      Sex}
\CommentTok{#> Y       Male Female}
\CommentTok{#>   No  0.9154 0.0846}
\CommentTok{#>   Yes 0.5162 0.4838}
\CommentTok{#> }
\CommentTok{#>      Age}
\CommentTok{#> Y      Child  Adult}
\CommentTok{#>   No  0.0349 0.9651}
\CommentTok{#>   Yes 0.0802 0.9198}
\end{Highlighting}
\end{Shaded}

The model creates the conditional probability for each feature separately. We also have the a-priori probabilities which indicates the distribution of our data. Let's calculate how we perform on the data.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Prediction on the dataset}
\NormalTok{NB_Predictions=}\KeywordTok{predict}\NormalTok{(Naive_Bayes_Model,Titanic_dataset)}
\CommentTok{# Confusion matrix to check accuracy}
\KeywordTok{table}\NormalTok{(NB_Predictions,Titanic_dataset}\OperatorTok{$}\NormalTok{Survived)}
\CommentTok{#>               }
\CommentTok{#> NB_Predictions   No  Yes}
\CommentTok{#>            No  1364  362}
\CommentTok{#>            Yes  126  349}
\end{Highlighting}
\end{Shaded}

We have the results! We are able to classify 1364 out of 1490 ``No'' cases correctly and 349 out of 711 ``Yes'' cases correctly. This means the ability of Naive Bayes algorithm to predict ``No'' cases is about 91.5\% but it falls down to only 49\% of the ``Yes'' cases resulting in an overall accuracy of 77.8\%

\hypertarget{can-we-do-any-better}{%
\chapter{Can we Do any Better?}\label{can-we-do-any-better}}

Naive Bayes is a parametric algorithm which implies that you cannot perform differently in different runs as long as the data remains the same. We will, however, learn another implementation of Naive Bayes algorithm using the `mlr' package. Assuming the same session is going on for the readers, I will install and load the package and start fitting a model

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Getting started with Naive Bayes in mlr}
\CommentTok{# install.packages(mlr)}
\CommentTok{# Loading the library}
\KeywordTok{library}\NormalTok{(mlr)}
\CommentTok{#> Loading required package: ParamHelpers}
\CommentTok{#> Registered S3 methods overwritten by 'ggplot2':}
\CommentTok{#>   method         from }
\CommentTok{#>   [.quosures     rlang}
\CommentTok{#>   c.quosures     rlang}
\CommentTok{#>   print.quosures rlang}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'mlr'}
\CommentTok{#> The following object is masked from 'package:e1071':}
\CommentTok{#> }
\CommentTok{#>     impute}
\end{Highlighting}
\end{Shaded}

The mlr package consists of a lot of models and works by creating tasks and learners which are then trained. Let's create a classification task using the titanic dataset and fit a model with the naive bayes algorithm.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Create a classification task for learning on Titanic Dataset and specify the target feature}
\NormalTok{task =}\StringTok{ }\KeywordTok{makeClassifTask}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ Titanic_dataset, }\DataTypeTok{target =} \StringTok{"Survived"}\NormalTok{)}

\CommentTok{# Initialize the Naive Bayes classifier}
\NormalTok{selected_model =}\StringTok{ }\KeywordTok{makeLearner}\NormalTok{(}\StringTok{"classif.naiveBayes"}\NormalTok{)}

\CommentTok{# Train the model}
\NormalTok{NB_mlr =}\StringTok{ }\KeywordTok{train}\NormalTok{(selected_model, task)}
\end{Highlighting}
\end{Shaded}

The summary of the model which was printed in e3071 package is stored in learner model. Let's print it and compare

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Read the model learned  }
\NormalTok{NB_mlr}\OperatorTok{$}\NormalTok{learner.model}
\CommentTok{#> }
\CommentTok{#> Naive Bayes Classifier for Discrete Predictors}
\CommentTok{#> }
\CommentTok{#> Call:}
\CommentTok{#> naiveBayes.default(x = X, y = Y, laplace = laplace)}
\CommentTok{#> }
\CommentTok{#> A-priori probabilities:}
\CommentTok{#> Y}
\CommentTok{#>    No   Yes }
\CommentTok{#> 0.677 0.323 }
\CommentTok{#> }
\CommentTok{#> Conditional probabilities:}
\CommentTok{#>      Class}
\CommentTok{#> Y        1st    2nd    3rd   Crew}
\CommentTok{#>   No  0.0819 0.1121 0.3544 0.4517}
\CommentTok{#>   Yes 0.2855 0.1660 0.2504 0.2982}
\CommentTok{#> }
\CommentTok{#>      Sex}
\CommentTok{#> Y       Male Female}
\CommentTok{#>   No  0.9154 0.0846}
\CommentTok{#>   Yes 0.5162 0.4838}
\CommentTok{#> }
\CommentTok{#>      Age}
\CommentTok{#> Y      Child  Adult}
\CommentTok{#>   No  0.0349 0.9651}
\CommentTok{#>   Yes 0.0802 0.9198}
\end{Highlighting}
\end{Shaded}

The a-priori probabilities and the conditional probabilities for the model are similar to the one calculated by e3071 package as was expected. This means that our predictions will also be the same.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Predict on the dataset without passing the target feature}
\NormalTok{predictions_mlr =}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(}\KeywordTok{predict}\NormalTok{(NB_mlr, }\DataTypeTok{newdata =}\NormalTok{ Titanic_dataset[,}\DecValTok{1}\OperatorTok{:}\DecValTok{3}\NormalTok{]))}

\CommentTok{## Confusion matrix to check accuracy}
\KeywordTok{table}\NormalTok{(predictions_mlr[,}\DecValTok{1}\NormalTok{],Titanic_dataset}\OperatorTok{$}\NormalTok{Survived)}
\CommentTok{#>      }
\CommentTok{#>         No  Yes}
\CommentTok{#>   No  1364  362}
\CommentTok{#>   Yes  126  349}
\end{Highlighting}
\end{Shaded}

As we see, the predictions are exactly same. The only way to improve is to have more features or more data. Perhaps, if we have more features such as the exact age, size of family, number of parents in the ship and siblings then we may arrive at a better model using Naive Bayes. In essence, Naive Bayes has an advantage of a strong foundation build and is very robust. I know of the `caret' package which also consists of Naive Bayes function but it will also give us the same predictions and probability.

\hypertarget{building-a-naive-bayes-classifier-in-r}{%
\chapter{Building a Naive Bayes Classifier in R}\label{building-a-naive-bayes-classifier-in-r}}

\url{https://www.machinelearningplus.com/predictive-modeling/how-naive-bayes-algorithm-works-with-example-and-full-code/}

\hypertarget{building-a-naive-bayes-classifier-in-r-1}{%
\section{8. Building a Naive Bayes Classifier in R}\label{building-a-naive-bayes-classifier-in-r-1}}

Understanding Naive Bayes was the (slightly) tricky part. Implementing it is fairly straightforward.

In R, Naive Bayes classifier is implemented in packages such as \texttt{e1071}, \texttt{klaR} and \texttt{bnlearn}. In Python, it is implemented in \texttt{scikit-learn}.

For sake of demonstration, let's use the standard iris dataset to predict the Species of flower using 4 different features: Sepal.Length, Sepal.Width, Petal.Length, Petal.Width

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Import Data}
\NormalTok{training <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{'https://raw.githubusercontent.com/selva86/datasets/master/iris_train.csv'}\NormalTok{)}
\NormalTok{test <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{'https://raw.githubusercontent.com/selva86/datasets/master/iris_test.csv'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The training data is now contained in training and test data in test dataframe. Lets load the klaR package and build the naive bayes model.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Using klaR for Naive Bayes}
\KeywordTok{library}\NormalTok{(klaR)}
\CommentTok{#> Loading required package: MASS}
\NormalTok{nb_mod <-}\StringTok{ }\KeywordTok{NaiveBayes}\NormalTok{(Species }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data=}\NormalTok{training)}
\NormalTok{pred <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(nb_mod, test)}
\end{Highlighting}
\end{Shaded}

Lets see the confusion matrix.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Confusion Matrix}
\NormalTok{tab <-}\StringTok{ }\KeywordTok{table}\NormalTok{(pred}\OperatorTok{$}\NormalTok{class, test}\OperatorTok{$}\NormalTok{Species)}
\NormalTok{caret}\OperatorTok{::}\KeywordTok{confusionMatrix}\NormalTok{(tab)  }
\CommentTok{#> Registered S3 methods overwritten by 'ggplot2':}
\CommentTok{#>   method         from }
\CommentTok{#>   [.quosures     rlang}
\CommentTok{#>   c.quosures     rlang}
\CommentTok{#>   print.quosures rlang}
\CommentTok{#> Confusion Matrix and Statistics}
\CommentTok{#> }
\CommentTok{#>             }
\CommentTok{#>              setosa versicolor virginica}
\CommentTok{#>   setosa         15          0         0}
\CommentTok{#>   versicolor      0         11         0}
\CommentTok{#>   virginica       0          4        15}
\CommentTok{#> }
\CommentTok{#> Overall Statistics}
\CommentTok{#>                                         }
\CommentTok{#>                Accuracy : 0.911         }
\CommentTok{#>                  95% CI : (0.788, 0.975)}
\CommentTok{#>     No Information Rate : 0.333         }
\CommentTok{#>     P-Value [Acc > NIR] : 8.47e-16      }
\CommentTok{#>                                         }
\CommentTok{#>                   Kappa : 0.867         }
\CommentTok{#>                                         }
\CommentTok{#>  Mcnemar's Test P-Value : NA            }
\CommentTok{#> }
\CommentTok{#> Statistics by Class:}
\CommentTok{#> }
\CommentTok{#>                      Class: setosa Class: versicolor Class: virginica}
\CommentTok{#> Sensitivity                  1.000             0.733            1.000}
\CommentTok{#> Specificity                  1.000             1.000            0.867}
\CommentTok{#> Pos Pred Value               1.000             1.000            0.789}
\CommentTok{#> Neg Pred Value               1.000             0.882            1.000}
\CommentTok{#> Prevalence                   0.333             0.333            0.333}
\CommentTok{#> Detection Rate               0.333             0.244            0.333}
\CommentTok{#> Detection Prevalence         0.333             0.244            0.422}
\CommentTok{#> Balanced Accuracy            1.000             0.867            0.933}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Plot density of each feature using nb_mod}
\NormalTok{opar =}\StringTok{ }\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{), }\DataTypeTok{mar=}\KeywordTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{))}
\KeywordTok{plot}\NormalTok{(nb_mod, }\DataTypeTok{main=}\StringTok{""}\NormalTok{)  }
\KeywordTok{par}\NormalTok{(opar)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_252b-iris-naive_bayes_files/figure-latex/unnamed-chunk-5-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Plot the Confusion Matrix}
\KeywordTok{library}\NormalTok{(ggplot2)}
\NormalTok{test}\OperatorTok{$}\NormalTok{pred <-}\StringTok{ }\NormalTok{pred}\OperatorTok{$}\NormalTok{class}
\KeywordTok{ggplot}\NormalTok{(test, }\KeywordTok{aes}\NormalTok{(Species, pred, }\DataTypeTok{color =}\NormalTok{ Species)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_jitter}\NormalTok{(}\DataTypeTok{width =} \FloatTok{0.2}\NormalTok{, }\DataTypeTok{height =} \FloatTok{0.1}\NormalTok{, }\DataTypeTok{size=}\DecValTok{2}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{title=}\StringTok{"Confusion Matrix"}\NormalTok{, }
       \DataTypeTok{subtitle=}\StringTok{"Predicted vs. Observed from Iris dataset"}\NormalTok{, }
       \DataTypeTok{y=}\StringTok{"Predicted"}\NormalTok{, }
       \DataTypeTok{x=}\StringTok{"Truth"}\NormalTok{,}
       \DataTypeTok{caption=}\StringTok{"machinelearningplus.com"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_252b-iris-naive_bayes_files/figure-latex/unnamed-chunk-6-1} \end{center}

\hypertarget{part-feature-engineering}{%
\part{Feature Engineering}\label{part-feature-engineering}}

\hypertarget{employee-attrition.-employee-attrition-dataset.-lime-package}{%
\chapter{\texorpdfstring{Employee attrition. Employee-Attrition dataset. \emph{LIME} package}{Employee attrition. Employee-Attrition dataset. LIME package}}\label{employee-attrition.-employee-attrition-dataset.-lime-package}}

Article: \url{https://www.business-science.io/business/2017/09/18/hr_employee_attrition.html}
Data: \url{https://www.ibm.com/communities/analytics/watson-analytics-blog/hr-employee-attrition/}

\hypertarget{introduction-10}{%
\section{Introduction}\label{introduction-10}}

\hypertarget{employee-attrition-a-major-problem}{%
\subsection{Employee attrition: a major problem}\label{employee-attrition-a-major-problem}}

Bill Gates was once quoted as saying,

\begin{quote}
``You take away our top 20 employees and we {[}Microsoft{]} become a mediocre company''.
\end{quote}

His statement cuts to the core of a major problem: employee attrition. An organization is only as good as its employees, and these people are the true source of its competitive advantage.

Organizations face huge costs resulting from employee turnover. Some costs are tangible such as training expenses and the time it takes from when an employee starts to when they become a productive member. However, the most important costs are intangible. Consider what's lost when a productive employee quits: new product ideas, great project management, or customer relationships.

With advances in machine learning and data science, its possible to not only predict employee attrition but to understand the key variables that influence turnover. We'll take a look at two cutting edge techniques:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Machine Learning with \texttt{h2o.automl()} from the h2o package: This function takes automated machine learning to the next level by testing a number of advanced algorithms such as random forests, ensemble methods, and deep learning along with more traditional algorithms such as logistic regression. The main takeaway is that we can now easily achieve predictive performance that is in the same ball park (and in some cases even better than) commercial algorithms and ML/AI software.
\item
  Feature Importance with the \texttt{lime} package: The problem with advanced machine learning algorithms such as deep learning is that it's near impossible to understand the algorithm because of its complexity. This has all changed with the lime package. The major advancement with lime is that, by recursively analyzing the models locally, it can extract feature importance that repeats globally. What this means to us is that lime has opened the door to understanding the ML models regardless of complexity. Now the best (and typically very complex) models can also be investigated and potentially understood as to what variables or features make the model tick.
\end{enumerate}

\hypertarget{employee-attrition-machine-learning-analysis}{%
\subsection{Employee attrition: machine learning analysis}\label{employee-attrition-machine-learning-analysis}}

With these new automated ML tools combined with tools to uncover critical variables, we now have capabilities for both extreme predictive accuracy and understandability, which was previously impossible! We'll investigate an HR Analytic example of employee attrition that was evaluated by IBM Watson.

\hypertarget{where-we-got-the-data}{%
\subsection{Where we got the data}\label{where-we-got-the-data}}

The example comes from IBM Watson Analytics website. You can download the data and read the analysis here:

Get data used in this post here.
Read IBM Watson Analytics article here.
To summarize, the article makes a usage case for IBM Watson as an automated ML platform. The article shows that using Watson, the analyst was able to detect features that led to increased probability of attrition.

\hypertarget{automated-machine-learning-what-we-did-with-the-data}{%
\subsection{Automated machine learning (what we did with the data)}\label{automated-machine-learning-what-we-did-with-the-data}}

In this example we'll show how we can use the combination of H2O for developing a complex model with high predictive accuracy on unseen data and then how we can use LIME to understand important features related to employee attrition.

\hypertarget{load-packages-1}{%
\subsection{Load packages}\label{load-packages-1}}

Load the following packages.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Load the following packages}
\KeywordTok{library}\NormalTok{(tidyquant)  }\CommentTok{# Loads tidyverse and several other pkgs }
\KeywordTok{library}\NormalTok{(readxl)     }\CommentTok{# Super simple excel reader}
\KeywordTok{library}\NormalTok{(h2o)        }\CommentTok{# Professional grade ML pkg}
\KeywordTok{library}\NormalTok{(lime)       }\CommentTok{# Explain complex black-box ML models}
\end{Highlighting}
\end{Shaded}

\hypertarget{load-data}{%
\subsection{Load data}\label{load-data}}

Download the data here. You can load the data using read\_excel(), pointing the path to your local file.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Read excel data}
\NormalTok{hr_data_raw <-}\StringTok{ }\KeywordTok{read_excel}\NormalTok{(}\DataTypeTok{path =} \KeywordTok{file.path}\NormalTok{(data_raw_dir,}
                                           \StringTok{"WA_Fn-UseC_-HR-Employee-Attrition.xlsx"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Let's check out the raw data. It's 1470 rows (observations) by 35 columns (features). The ``Attrition'' column is our target. We'll use all other columns as features to our model.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# View first 10 rows}
\NormalTok{hr_data_raw[}\DecValTok{1}\OperatorTok{:}\DecValTok{10}\NormalTok{,] }\OperatorTok{%>%}
\StringTok{    }\NormalTok{knitr}\OperatorTok{::}\KeywordTok{kable}\NormalTok{(}\DataTypeTok{caption =} \StringTok{"First 10 rows"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}[t]

\caption{\label{tab:unnamed-chunk-2}First 10 rows}
\centering
\begin{tabular}{r|l|l|r|l|r|r|l|r|r|r|l|r|r|r|l|r|l|r|r|r|l|l|r|r|r|r|r|r|r|r|r|r|r|r}
\hline
Age & Attrition & BusinessTravel & DailyRate & Department & DistanceFromHome & Education & EducationField & EmployeeCount & EmployeeNumber & EnvironmentSatisfaction & Gender & HourlyRate & JobInvolvement & JobLevel & JobRole & JobSatisfaction & MaritalStatus & MonthlyIncome & MonthlyRate & NumCompaniesWorked & Over18 & OverTime & PercentSalaryHike & PerformanceRating & RelationshipSatisfaction & StandardHours & StockOptionLevel & TotalWorkingYears & TrainingTimesLastYear & WorkLifeBalance & YearsAtCompany & YearsInCurrentRole & YearsSinceLastPromotion & YearsWithCurrManager\\
\hline
41 & Yes & Travel\_Rarely & 1102 & Sales & 1 & 2 & Life Sciences & 1 & 1 & 2 & Female & 94 & 3 & 2 & Sales Executive & 4 & Single & 5993 & 19479 & 8 & Y & Yes & 11 & 3 & 1 & 80 & 0 & 8 & 0 & 1 & 6 & 4 & 0 & 5\\
\hline
49 & No & Travel\_Frequently & 279 & Research \& Development & 8 & 1 & Life Sciences & 1 & 2 & 3 & Male & 61 & 2 & 2 & Research Scientist & 2 & Married & 5130 & 24907 & 1 & Y & No & 23 & 4 & 4 & 80 & 1 & 10 & 3 & 3 & 10 & 7 & 1 & 7\\
\hline
37 & Yes & Travel\_Rarely & 1373 & Research \& Development & 2 & 2 & Other & 1 & 4 & 4 & Male & 92 & 2 & 1 & Laboratory Technician & 3 & Single & 2090 & 2396 & 6 & Y & Yes & 15 & 3 & 2 & 80 & 0 & 7 & 3 & 3 & 0 & 0 & 0 & 0\\
\hline
33 & No & Travel\_Frequently & 1392 & Research \& Development & 3 & 4 & Life Sciences & 1 & 5 & 4 & Female & 56 & 3 & 1 & Research Scientist & 3 & Married & 2909 & 23159 & 1 & Y & Yes & 11 & 3 & 3 & 80 & 0 & 8 & 3 & 3 & 8 & 7 & 3 & 0\\
\hline
27 & No & Travel\_Rarely & 591 & Research \& Development & 2 & 1 & Medical & 1 & 7 & 1 & Male & 40 & 3 & 1 & Laboratory Technician & 2 & Married & 3468 & 16632 & 9 & Y & No & 12 & 3 & 4 & 80 & 1 & 6 & 3 & 3 & 2 & 2 & 2 & 2\\
\hline
32 & No & Travel\_Frequently & 1005 & Research \& Development & 2 & 2 & Life Sciences & 1 & 8 & 4 & Male & 79 & 3 & 1 & Laboratory Technician & 4 & Single & 3068 & 11864 & 0 & Y & No & 13 & 3 & 3 & 80 & 0 & 8 & 2 & 2 & 7 & 7 & 3 & 6\\
\hline
59 & No & Travel\_Rarely & 1324 & Research \& Development & 3 & 3 & Medical & 1 & 10 & 3 & Female & 81 & 4 & 1 & Laboratory Technician & 1 & Married & 2670 & 9964 & 4 & Y & Yes & 20 & 4 & 1 & 80 & 3 & 12 & 3 & 2 & 1 & 0 & 0 & 0\\
\hline
30 & No & Travel\_Rarely & 1358 & Research \& Development & 24 & 1 & Life Sciences & 1 & 11 & 4 & Male & 67 & 3 & 1 & Laboratory Technician & 3 & Divorced & 2693 & 13335 & 1 & Y & No & 22 & 4 & 2 & 80 & 1 & 1 & 2 & 3 & 1 & 0 & 0 & 0\\
\hline
38 & No & Travel\_Frequently & 216 & Research \& Development & 23 & 3 & Life Sciences & 1 & 12 & 4 & Male & 44 & 2 & 3 & Manufacturing Director & 3 & Single & 9526 & 8787 & 0 & Y & No & 21 & 4 & 2 & 80 & 0 & 10 & 2 & 3 & 9 & 7 & 1 & 8\\
\hline
36 & No & Travel\_Rarely & 1299 & Research \& Development & 27 & 3 & Medical & 1 & 13 & 3 & Male & 94 & 3 & 2 & Healthcare Representative & 3 & Married & 5237 & 16577 & 6 & Y & No & 13 & 3 & 2 & 80 & 2 & 17 & 3 & 2 & 7 & 7 & 7 & 7\\
\hline
\end{tabular}
\end{table}

The only pre-processing we'll do in this example is change all character data types to factors. This is needed for H2O. We could make a number of other numeric data that is actually categorical factors, but this tends to increase modeling time and can have little improvement on model performance.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{hr_data <-}\StringTok{ }\NormalTok{hr_data_raw }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{mutate_if}\NormalTok{(is.character, as.factor) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{select}\NormalTok{(Attrition, }\KeywordTok{everything}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

Let's take a glimpse at the processed dataset. We can see all of the columns. Note our target (``Attrition'') is the first column.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{glimpse}\NormalTok{(hr_data)}
\CommentTok{#> Observations: 1,470}
\CommentTok{#> Variables: 35}
\CommentTok{#> $ Attrition                <fct> Yes, No, Yes, No, No, No, No, No, No,...}
\CommentTok{#> $ Age                      <dbl> 41, 49, 37, 33, 27, 32, 59, 30, 38, 3...}
\CommentTok{#> $ BusinessTravel           <fct> Travel_Rarely, Travel_Frequently, Tra...}
\CommentTok{#> $ DailyRate                <dbl> 1102, 279, 1373, 1392, 591, 1005, 132...}
\CommentTok{#> $ Department               <fct> Sales, Research & Development, Resear...}
\CommentTok{#> $ DistanceFromHome         <dbl> 1, 8, 2, 3, 2, 2, 3, 24, 23, 27, 16, ...}
\CommentTok{#> $ Education                <dbl> 2, 1, 2, 4, 1, 2, 3, 1, 3, 3, 3, 2, 1...}
\CommentTok{#> $ EducationField           <fct> Life Sciences, Life Sciences, Other, ...}
\CommentTok{#> $ EmployeeCount            <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1...}
\CommentTok{#> $ EmployeeNumber           <dbl> 1, 2, 4, 5, 7, 8, 10, 11, 12, 13, 14,...}
\CommentTok{#> $ EnvironmentSatisfaction  <dbl> 2, 3, 4, 4, 1, 4, 3, 4, 4, 3, 1, 4, 1...}
\CommentTok{#> $ Gender                   <fct> Female, Male, Male, Female, Male, Mal...}
\CommentTok{#> $ HourlyRate               <dbl> 94, 61, 92, 56, 40, 79, 81, 67, 44, 9...}
\CommentTok{#> $ JobInvolvement           <dbl> 3, 2, 2, 3, 3, 3, 4, 3, 2, 3, 4, 2, 3...}
\CommentTok{#> $ JobLevel                 <dbl> 2, 2, 1, 1, 1, 1, 1, 1, 3, 2, 1, 2, 1...}
\CommentTok{#> $ JobRole                  <fct> Sales Executive, Research Scientist, ...}
\CommentTok{#> $ JobSatisfaction          <dbl> 4, 2, 3, 3, 2, 4, 1, 3, 3, 3, 2, 3, 3...}
\CommentTok{#> $ MaritalStatus            <fct> Single, Married, Single, Married, Mar...}
\CommentTok{#> $ MonthlyIncome            <dbl> 5993, 5130, 2090, 2909, 3468, 3068, 2...}
\CommentTok{#> $ MonthlyRate              <dbl> 19479, 24907, 2396, 23159, 16632, 118...}
\CommentTok{#> $ NumCompaniesWorked       <dbl> 8, 1, 6, 1, 9, 0, 4, 1, 0, 6, 0, 0, 1...}
\CommentTok{#> $ Over18                   <fct> Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y...}
\CommentTok{#> $ OverTime                 <fct> Yes, No, Yes, Yes, No, No, Yes, No, N...}
\CommentTok{#> $ PercentSalaryHike        <dbl> 11, 23, 15, 11, 12, 13, 20, 22, 21, 1...}
\CommentTok{#> $ PerformanceRating        <dbl> 3, 4, 3, 3, 3, 3, 4, 4, 4, 3, 3, 3, 3...}
\CommentTok{#> $ RelationshipSatisfaction <dbl> 1, 4, 2, 3, 4, 3, 1, 2, 2, 2, 3, 4, 4...}
\CommentTok{#> $ StandardHours            <dbl> 80, 80, 80, 80, 80, 80, 80, 80, 80, 8...}
\CommentTok{#> $ StockOptionLevel         <dbl> 0, 1, 0, 0, 1, 0, 3, 1, 0, 2, 1, 0, 1...}
\CommentTok{#> $ TotalWorkingYears        <dbl> 8, 10, 7, 8, 6, 8, 12, 1, 10, 17, 6, ...}
\CommentTok{#> $ TrainingTimesLastYear    <dbl> 0, 3, 3, 3, 3, 2, 3, 2, 2, 3, 5, 3, 1...}
\CommentTok{#> $ WorkLifeBalance          <dbl> 1, 3, 3, 3, 3, 2, 2, 3, 3, 2, 3, 3, 2...}
\CommentTok{#> $ YearsAtCompany           <dbl> 6, 10, 0, 8, 2, 7, 1, 1, 9, 7, 5, 9, ...}
\CommentTok{#> $ YearsInCurrentRole       <dbl> 4, 7, 0, 7, 2, 7, 0, 0, 7, 7, 4, 5, 2...}
\CommentTok{#> $ YearsSinceLastPromotion  <dbl> 0, 1, 0, 3, 2, 3, 0, 0, 1, 7, 0, 0, 4...}
\CommentTok{#> $ YearsWithCurrManager     <dbl> 5, 7, 0, 0, 2, 6, 0, 0, 8, 7, 3, 8, 3...}
\end{Highlighting}
\end{Shaded}

\hypertarget{modeling-employee-attrition}{%
\section{Modeling Employee attrition}\label{modeling-employee-attrition}}

We are going to use the \texttt{h2o.automl()} function from the H2O platform to model employee attrition.

\hypertarget{machine-learning-with-h2o}{%
\subsection{\texorpdfstring{Machine Learning with \texttt{h2o}}{Machine Learning with h2o}}\label{machine-learning-with-h2o}}

First, we need to initialize the \emph{Java Virtual Machine (JVM)} that H2O uses locally.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Initialize H2O JVM}
\KeywordTok{h2o.init}\NormalTok{()}
\CommentTok{#> }
\CommentTok{#> H2O is not running yet, starting it now...}
\CommentTok{#> }
\CommentTok{#> Note:  In case of errors look at the following log files:}
\CommentTok{#>     /tmp/RtmpLPDVN2/h2o_datascience_started_from_r.out}
\CommentTok{#>     /tmp/RtmpLPDVN2/h2o_datascience_started_from_r.err}
\CommentTok{#> }
\CommentTok{#> }
\CommentTok{#> Starting H2O JVM and connecting: . Connection successful!}
\CommentTok{#> }
\CommentTok{#> R is connected to the H2O cluster: }
\CommentTok{#>     H2O cluster uptime:         1 seconds 130 milliseconds }
\CommentTok{#>     H2O cluster timezone:       America/Chicago }
\CommentTok{#>     H2O data parsing timezone:  UTC }
\CommentTok{#>     H2O cluster version:        3.22.1.1 }
\CommentTok{#>     H2O cluster version age:    8 months and 23 days !!! }
\CommentTok{#>     H2O cluster name:           H2O_started_from_R_datascience_mwl453 }
\CommentTok{#>     H2O cluster total nodes:    1 }
\CommentTok{#>     H2O cluster total memory:   6.96 GB }
\CommentTok{#>     H2O cluster total cores:    8 }
\CommentTok{#>     H2O cluster allowed cores:  8 }
\CommentTok{#>     H2O cluster healthy:        TRUE }
\CommentTok{#>     H2O Connection ip:          localhost }
\CommentTok{#>     H2O Connection port:        54321 }
\CommentTok{#>     H2O Connection proxy:       NA }
\CommentTok{#>     H2O Internal Security:      FALSE }
\CommentTok{#>     H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4 }
\CommentTok{#>     R Version:                  R version 3.6.0 (2019-04-26)}
\KeywordTok{h2o.no_progress}\NormalTok{() }\CommentTok{# Turn off output of progress bars}
\end{Highlighting}
\end{Shaded}

Next, we change our data to an h2o object that the package can interpret. We also split the data into training, validation, and test sets. Our preference is to use 70\%, 15\%, 15\%, respectively.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Split data into Train/Validation/Test Sets}
\NormalTok{hr_data_h2o <-}\StringTok{ }\KeywordTok{as.h2o}\NormalTok{(hr_data)}

\NormalTok{split_h2o <-}\StringTok{ }\KeywordTok{h2o.splitFrame}\NormalTok{(hr_data_h2o, }\KeywordTok{c}\NormalTok{(}\FloatTok{0.7}\NormalTok{, }\FloatTok{0.15}\NormalTok{), }\DataTypeTok{seed =} \DecValTok{1234}\NormalTok{ )}
\NormalTok{train_h2o <-}\StringTok{ }\KeywordTok{h2o.assign}\NormalTok{(split_h2o[[}\DecValTok{1}\NormalTok{]], }\StringTok{"train"}\NormalTok{ ) }\CommentTok{# 70%}
\NormalTok{valid_h2o <-}\StringTok{ }\KeywordTok{h2o.assign}\NormalTok{(split_h2o[[}\DecValTok{2}\NormalTok{]], }\StringTok{"valid"}\NormalTok{ ) }\CommentTok{# 15%}
\NormalTok{test_h2o  <-}\StringTok{ }\KeywordTok{h2o.assign}\NormalTok{(split_h2o[[}\DecValTok{3}\NormalTok{]], }\StringTok{"test"}\NormalTok{ )  }\CommentTok{# 15%}
\end{Highlighting}
\end{Shaded}

\hypertarget{model}{%
\section{Model}\label{model}}

Now we are ready to model. We'll set the target and feature names. The target is what we aim to predict (in our case ``Attrition''). The features (every other column) are what we will use to model the prediction.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Set names for h2o}
\NormalTok{y <-}\StringTok{ "Attrition"}
\NormalTok{x <-}\StringTok{ }\KeywordTok{setdiff}\NormalTok{(}\KeywordTok{names}\NormalTok{(train_h2o), y)}
\end{Highlighting}
\end{Shaded}

Now the fun begins. We run the h2o.automl() setting the arguments it needs to run models against. For more information, see the h2o.automl documentation.

\begin{itemize}
\tightlist
\item
  \texttt{x\ =\ x}: The names of our feature columns.
\item
  \texttt{y\ =\ y}: The name of our target column.
\item
  \texttt{training\_frame} = train\_h2o: Our training set consisting of 70\% of the data.
\item
  \texttt{leaderboard\_frame} = valid\_h2o: Our validation set consisting of 15\% of the data. H2O uses this to ensure the model does not overfit the data.
\item
  \texttt{max\_runtime\_secs\ =\ 30}: We supply this to speed up H2O's modeling. The algorithm has a large number of complex models so we want to keep things moving at the expense of some accuracy.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Run the automated machine learning }
\NormalTok{automl_models_h2o <-}\StringTok{ }\KeywordTok{h2o.automl}\NormalTok{(}
    \DataTypeTok{x =}\NormalTok{ x, }
    \DataTypeTok{y =}\NormalTok{ y,}
    \DataTypeTok{training_frame    =}\NormalTok{ train_h2o,}
    \DataTypeTok{leaderboard_frame =}\NormalTok{ valid_h2o,}
    \DataTypeTok{max_runtime_secs  =} \DecValTok{30}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

All of the models are stored the \texttt{automl\_models\_h2o} object. However, we are only concerned with the leader, which is the best model in terms of accuracy on the validation set. We'll extract it from the models object.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Extract leader model}
\NormalTok{automl_leader <-}\StringTok{ }\NormalTok{automl_models_h2o}\OperatorTok{@}\NormalTok{leader}
\end{Highlighting}
\end{Shaded}

\hypertarget{predict}{%
\section{Predict}\label{predict}}

Now we are ready to predict on our test set, which is unseen from during our modeling process. This is the true test of performance. We use the \texttt{h2o.predict()} function to make predictions.\\

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Predict on hold-out set, test_h2o}
\NormalTok{pred_h2o <-}\StringTok{ }\KeywordTok{h2o.predict}\NormalTok{(}\DataTypeTok{object =}\NormalTok{ automl_leader, }\DataTypeTok{newdata =}\NormalTok{ test_h2o)}
\end{Highlighting}
\end{Shaded}

\hypertarget{performance}{%
\section{Performance}\label{performance}}

Now we can evaluate our leader model. We'll reformat the test set an add the predictions as column so we have the actual and prediction columns side-by-side.\\

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Prep for performance assessment}
\NormalTok{test_performance <-}\StringTok{ }\NormalTok{test_h2o }\OperatorTok{%>%}
\StringTok{    }\NormalTok{tibble}\OperatorTok{::}\KeywordTok{as_tibble}\NormalTok{() }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{select}\NormalTok{(Attrition) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{add_column}\NormalTok{(}\DataTypeTok{pred =} \KeywordTok{as.vector}\NormalTok{(pred_h2o}\OperatorTok{$}\NormalTok{predict)) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{mutate_if}\NormalTok{(is.character, as.factor)}
\NormalTok{test_performance}
\CommentTok{#> # A tibble: 211 x 2}
\CommentTok{#>   Attrition pred }
\CommentTok{#>   <fct>     <fct>}
\CommentTok{#> 1 No        No   }
\CommentTok{#> 2 No        No   }
\CommentTok{#> 3 Yes       Yes  }
\CommentTok{#> 4 No        No   }
\CommentTok{#> 5 No        No   }
\CommentTok{#> 6 No        No   }
\CommentTok{#> # ... with 205 more rows}
\end{Highlighting}
\end{Shaded}

We can use the table() function to quickly get a confusion table of the results. We see that the leader model wasn't perfect, but it did a decent job identifying employees that are likely to quit. For perspective, a logistic regression would not perform nearly this well.\\

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Confusion table counts}
\NormalTok{confusion_matrix <-}\StringTok{ }\NormalTok{test_performance }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{table}\NormalTok{() }
\NormalTok{confusion_matrix}
\CommentTok{#>          pred}
\CommentTok{#> Attrition  No Yes}
\CommentTok{#>       No  163  19}
\CommentTok{#>       Yes   6  23}
\end{Highlighting}
\end{Shaded}

We'll run through a binary classification analysis to understand the model performance.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Performance analysis}
\NormalTok{tn <-}\StringTok{ }\NormalTok{confusion_matrix[}\DecValTok{1}\NormalTok{]}
\NormalTok{tp <-}\StringTok{ }\NormalTok{confusion_matrix[}\DecValTok{4}\NormalTok{]}
\NormalTok{fp <-}\StringTok{ }\NormalTok{confusion_matrix[}\DecValTok{3}\NormalTok{]}
\NormalTok{fn <-}\StringTok{ }\NormalTok{confusion_matrix[}\DecValTok{2}\NormalTok{]}

\NormalTok{accuracy <-}\StringTok{ }\NormalTok{(tp }\OperatorTok{+}\StringTok{ }\NormalTok{tn) }\OperatorTok{/}\StringTok{ }\NormalTok{(tp }\OperatorTok{+}\StringTok{ }\NormalTok{tn }\OperatorTok{+}\StringTok{ }\NormalTok{fp }\OperatorTok{+}\StringTok{ }\NormalTok{fn)}
\NormalTok{misclassification_rate <-}\StringTok{ }\DecValTok{1} \OperatorTok{-}\StringTok{ }\NormalTok{accuracy}
\NormalTok{recall <-}\StringTok{ }\NormalTok{tp }\OperatorTok{/}\StringTok{ }\NormalTok{(tp }\OperatorTok{+}\StringTok{ }\NormalTok{fn)}
\NormalTok{precision <-}\StringTok{ }\NormalTok{tp }\OperatorTok{/}\StringTok{ }\NormalTok{(tp }\OperatorTok{+}\StringTok{ }\NormalTok{fp)}
\NormalTok{null_error_rate <-}\StringTok{ }\NormalTok{tn }\OperatorTok{/}\StringTok{ }\NormalTok{(tp }\OperatorTok{+}\StringTok{ }\NormalTok{tn }\OperatorTok{+}\StringTok{ }\NormalTok{fp }\OperatorTok{+}\StringTok{ }\NormalTok{fn)}

\KeywordTok{tibble}\NormalTok{(}
\NormalTok{    accuracy,}
\NormalTok{    misclassification_rate,}
\NormalTok{    recall,}
\NormalTok{    precision,}
\NormalTok{    null_error_rate}
\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{transpose}\NormalTok{() }
\CommentTok{#> [[1]]}
\CommentTok{#> [[1]]$accuracy}
\CommentTok{#> [1] 0.882}
\CommentTok{#> }
\CommentTok{#> [[1]]$misclassification_rate}
\CommentTok{#> [1] 0.118}
\CommentTok{#> }
\CommentTok{#> [[1]]$recall}
\CommentTok{#> [1] 0.793}
\CommentTok{#> }
\CommentTok{#> [[1]]$precision}
\CommentTok{#> [1] 0.548}
\CommentTok{#> }
\CommentTok{#> [[1]]$null_error_rate}
\CommentTok{#> [1] 0.773}
\end{Highlighting}
\end{Shaded}

It is important to understand is that the accuracy can be misleading: 88\% sounds pretty good especially for modeling HR data, but if we just pick \texttt{Attrition\ =\ NO} we would get an accuracy of about 79\%. Doesn't sound so great now.

Before we make our final judgement, let's dive a little deeper into precision and recall. Precision is when the model predicts yes, how often is it actually yes. Recall (also true positive rate or specificity) is when the actual value is yes how often is the model correct. Confused yet? Let's explain in terms of what's important to HR.

Most HR groups would probably prefer to incorrectly classify folks not looking to quit as high potential of quiting rather than classify those that are likely to quit as not at risk. Because it's important to not miss at risk employees, HR will really care about recall or when the actual value is \texttt{Attrition\ =\ YES} how often the model predicts YES.

Recall for our model is 62\%. In an HR context, this is 62\% more employees that could potentially be targeted prior to quiting. From that standpoint, an organization that loses 100 people per year could possibly target 62 implementing measures to retain.

\hypertarget{the-lime-package}{%
\section{\texorpdfstring{The \texttt{lime} package}{The lime package}}\label{the-lime-package}}

We have a very good model that is capable of making very accurate predictions on unseen data, but what can it tell us about what causes attrition? Let's find out using LIME.

\hypertarget{set-up}{%
\subsection{Set up}\label{set-up}}

The \texttt{lime} package implements LIME in R. One thing to note is that it's not setup out-of-the-box to work with \texttt{h2o}. The good news is with a few functions we can get everything working properly. We'll need to make two custom functions:

\begin{itemize}
\item
  \texttt{model\_type}: Used to tell lime what type of model we are dealing with. It could be classification, regression, survival, etc.
\item
  \texttt{predict\_model}: Used to allow lime to perform predictions that its algorithm can interpret.
\end{itemize}

The first thing we need to do is identify the class of our model leader object. We do this with the \texttt{class()} function.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{(automl_leader)}
\CommentTok{#> [1] "H2OBinomialModel"}
\CommentTok{#> attr(,"package")}
\CommentTok{#> [1] "h2o"}
\end{Highlighting}
\end{Shaded}

Next we create our \texttt{model\_type} function. It's only input is x the h2o model. The function simply returns ``classification'', which tells LIME we are classifying.\\

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Setup lime::model_type() function for h2o}
\NormalTok{model_type.H2OBinomialModel <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x, ...) \{}
    \CommentTok{# Function tells lime() what model type we are dealing with}
    \CommentTok{# 'classification', 'regression', 'survival', 'clustering', 'multilabel', etc}
    \CommentTok{#}
    \CommentTok{# x is our h2o model}
    
    \KeywordTok{return}\NormalTok{(}\StringTok{"classification"}\NormalTok{)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Now we can create our \texttt{predict\_model} function. The trick here is to realize that it's inputs must be x a model, newdata a dataframe object (this is important), and type which is not used but can be use to switch the output type. The output is also a little tricky because it must be in the format of probabilities by classification (this is important; shown next). Internally we just call the \texttt{h2o.predict()} function.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Setup lime::predict_model() function for h2o}
\NormalTok{predict_model.H2OBinomialModel <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x, newdata, type, ...) \{}
    \CommentTok{# Function performs prediction and returns dataframe with Response}
    \CommentTok{#}
    \CommentTok{# x is h2o model}
    \CommentTok{# newdata is data frame}
    \CommentTok{# type is only setup for data frame}
    
\NormalTok{    pred <-}\StringTok{ }\KeywordTok{h2o.predict}\NormalTok{(x, }\KeywordTok{as.h2o}\NormalTok{(newdata))}
    
    \CommentTok{# return probs}
    \KeywordTok{return}\NormalTok{(}\KeywordTok{as.data.frame}\NormalTok{(pred[,}\OperatorTok{-}\DecValTok{1}\NormalTok{]))}
    
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Run this next script to show you what the output looks like and to test our \texttt{predict\_model} function. See how it's the probabilities by classification. It must be in this form for \texttt{model\_type\ =\ classification}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Test our predict_model() function}
\KeywordTok{predict_model}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ automl_leader, }\DataTypeTok{newdata =} \KeywordTok{as.data.frame}\NormalTok{(test_h2o[,}\OperatorTok{-}\DecValTok{1}\NormalTok{]), }\DataTypeTok{type =} \StringTok{'raw'}\NormalTok{) }\OperatorTok{%>%}
\StringTok{    }\NormalTok{tibble}\OperatorTok{::}\KeywordTok{as_tibble}\NormalTok{()}
\CommentTok{#> # A tibble: 211 x 2}
\CommentTok{#>       No    Yes}
\CommentTok{#>    <dbl>  <dbl>}
\CommentTok{#> 1 0.807  0.193 }
\CommentTok{#> 2 0.958  0.0423}
\CommentTok{#> 3 0.0448 0.955 }
\CommentTok{#> 4 0.959  0.0411}
\CommentTok{#> 5 0.860  0.140 }
\CommentTok{#> 6 0.965  0.0350}
\CommentTok{#> # ... with 205 more rows}
\end{Highlighting}
\end{Shaded}

Now the fun part, we create an explainer using the \texttt{lime()} function. Just pass the training data set without the ``Attribution column''. The form must be a data frame, which is OK since our predict\_model function will switch it to an h2o object. Set\texttt{model\ =\ automl\_leader} our leader model, and \texttt{bin\_continuous\ =\ FALSE}. We could tell the algorithm to bin continuous variables, but this may not make sense for categorical numeric data that we didn't change to factors.\\

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Run lime() on training set}
\NormalTok{explainer <-}\StringTok{ }\NormalTok{lime}\OperatorTok{::}\KeywordTok{lime}\NormalTok{(}
    \KeywordTok{as.data.frame}\NormalTok{(train_h2o[,}\OperatorTok{-}\DecValTok{1}\NormalTok{]), }
    \DataTypeTok{model          =}\NormalTok{ automl_leader, }
    \DataTypeTok{bin_continuous =} \OtherTok{FALSE}\NormalTok{)}
\CommentTok{#> Warning: Data contains numeric columns with zero variance}
\end{Highlighting}
\end{Shaded}

Now we run the \texttt{explain()} function, which returns our explanation. This can take a minute to run so we limit it to just the first ten rows of the test data set. We set n\_labels = 1 because we care about explaining a single class. Setting n\_features = 4 returns the top four features that are critical to each case. Finally, setting kernel\_width = 0.5 allows us to increase the ``model\_r2'' value by shrinking the localized evaluation.\\

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Run explain() on explainer}
\NormalTok{explanation <-}\StringTok{ }\NormalTok{lime}\OperatorTok{::}\KeywordTok{explain}\NormalTok{(}
    \KeywordTok{as.data.frame}\NormalTok{(test_h2o[}\DecValTok{1}\OperatorTok{:}\DecValTok{10}\NormalTok{,}\OperatorTok{-}\DecValTok{1}\NormalTok{]), }
    \DataTypeTok{explainer    =}\NormalTok{ explainer, }
    \DataTypeTok{n_labels     =} \DecValTok{1}\NormalTok{, }
    \DataTypeTok{n_features   =} \DecValTok{4}\NormalTok{,}
    \DataTypeTok{kernel_width =} \FloatTok{0.5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{feature-importance-visualization}{%
\section{Feature Importance Visualization}\label{feature-importance-visualization}}

The payoff for the work we put in using LIME is this feature importance plot. This allows us to visualize each of the ten cases (observations) from the test data. The top four features for each case are shown. Note that they are not the same for each case. The green bars mean that the feature supports the model conclusion, and the red bars contradict. We'll focus in on Cases with \texttt{Label\ =\ Yes}, which are predicted to have attrition. We can see a common theme with Case 3 and Case 7: Training Time, Job Role, and Over Time are among the top factors influencing attrition. These are only two cases, but they can be used to potentially generalize to the larger population as we will see next.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot_features}\NormalTok{(explanation) }\OperatorTok{+}
\StringTok{    }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{title =} \StringTok{"HR Predictive Analytics: LIME Feature Importance Visualization"}\NormalTok{,}
         \DataTypeTok{subtitle =} \StringTok{"Hold Out (Test) Set, First 10 Cases Shown"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{fe-comparison_342-classification_employee_attrition-lime-multi_files/figure-latex/plot-features-1} \end{center}

\hypertarget{what-features-are-linked-to-employee-attrition}{%
\subsection{What features are linked to employee attrition}\label{what-features-are-linked-to-employee-attrition}}

Now we turn to our three critical features from the LIME Feature Importance Plot:

\begin{itemize}
\tightlist
\item
  Training Time
\item
  Job Role
\item
  Over Time
\end{itemize}

We'll subset this data and visualize to detect trends.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Focus on critical features of attrition}
\NormalTok{attrition_critical_features <-}\StringTok{ }\NormalTok{hr_data }\OperatorTok{%>%}
\StringTok{    }\NormalTok{tibble}\OperatorTok{::}\KeywordTok{as_tibble}\NormalTok{() }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{select}\NormalTok{(Attrition, TrainingTimesLastYear, JobRole, OverTime) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{rowid_to_column}\NormalTok{(}\DataTypeTok{var =} \StringTok{"Case"}\NormalTok{)}
\NormalTok{attrition_critical_features}
\CommentTok{#> # A tibble: 1,470 x 5}
\CommentTok{#>    Case Attrition TrainingTimesLastYear JobRole               OverTime}
\CommentTok{#>   <int> <fct>                     <dbl> <fct>                 <fct>   }
\CommentTok{#> 1     1 Yes                           0 Sales Executive       Yes     }
\CommentTok{#> 2     2 No                            3 Research Scientist    No      }
\CommentTok{#> 3     3 Yes                           3 Laboratory Technician Yes     }
\CommentTok{#> 4     4 No                            3 Research Scientist    Yes     }
\CommentTok{#> 5     5 No                            3 Laboratory Technician No      }
\CommentTok{#> 6     6 No                            2 Laboratory Technician No      }
\CommentTok{#> # ... with 1,464 more rows}
\end{Highlighting}
\end{Shaded}

\hypertarget{training}{%
\subsection{Training}\label{training}}

From the violin plot, the employees that stay tend to have a large peaks at two and three trainings per year whereas the employees that leave tend to have a large peak at two trainings per year. This suggests that employees with more trainings may be less likely to leave.\\

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(attrition_critical_features, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Attrition, }
                                        \DataTypeTok{y =}\NormalTok{ TrainingTimesLastYear)) }\OperatorTok{+}
\StringTok{    }\KeywordTok{geom_violin}\NormalTok{()  }\OperatorTok{+}
\StringTok{    }\KeywordTok{geom_jitter}\NormalTok{(}\DataTypeTok{alpha =} \FloatTok{0.25}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{fe-comparison_342-classification_employee_attrition-lime-multi_files/figure-latex/unnamed-chunk-5-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{attrition_critical_features }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(Attrition, TrainingTimesLastYear)) }\OperatorTok{+}
\StringTok{    }\KeywordTok{geom_jitter}\NormalTok{(}\DataTypeTok{alpha =} \FloatTok{0.5}\NormalTok{, }\DataTypeTok{fill =} \KeywordTok{palette_light}\NormalTok{()[[}\DecValTok{1}\NormalTok{]]) }\OperatorTok{+}
\StringTok{    }\KeywordTok{geom_violin}\NormalTok{(}\DataTypeTok{alpha =} \FloatTok{0.7}\NormalTok{, }\DataTypeTok{fill =} \KeywordTok{palette_light}\NormalTok{()[[}\DecValTok{1}\NormalTok{]]) }\OperatorTok{+}
\StringTok{    }\KeywordTok{theme_tq}\NormalTok{() }\OperatorTok{+}
\StringTok{    }\KeywordTok{labs}\NormalTok{(}
    \DataTypeTok{title =} \StringTok{"Prevalance of Training is Lower in Attrition = Yes"}\NormalTok{,}
    \DataTypeTok{subtitle =} \StringTok{"Suggests that increased training is related to lower attrition"}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{fe-comparison_342-classification_employee_attrition-lime-multi_files/figure-latex/plot-critical_training-1} \end{center}

\hypertarget{overtime}{%
\subsection{Overtime}\label{overtime}}

The plot below shows a very interesting relationship: a very high proportion of employees that turnover are working over time. The opposite is true for employees that stay.\\

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{attrition_critical_features }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{OverTime =} \KeywordTok{case_when}\NormalTok{(}
\NormalTok{                        OverTime }\OperatorTok{==}\StringTok{ "Yes"} \OperatorTok{~}\StringTok{ }\DecValTok{1}\NormalTok{,}
\NormalTok{                        OverTime }\OperatorTok{==}\StringTok{ "No"}  \OperatorTok{~}\StringTok{ }\DecValTok{0}\NormalTok{ )) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(Attrition, OverTime)) }\OperatorTok{+}
\StringTok{    }\KeywordTok{geom_jitter}\NormalTok{(}\DataTypeTok{alpha =} \FloatTok{0.5}\NormalTok{, }\DataTypeTok{fill =} \KeywordTok{palette_light}\NormalTok{()[[}\DecValTok{1}\NormalTok{]]) }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{geom_violin}\NormalTok{(}\DataTypeTok{alpha =} \FloatTok{0.7}\NormalTok{, }\DataTypeTok{fill =} \KeywordTok{palette_light}\NormalTok{()[[}\DecValTok{1}\NormalTok{]]) }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{theme_tq}\NormalTok{() }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{labs}\NormalTok{(}
    \DataTypeTok{title =} \StringTok{"Prevalance of Over Time is Higher in Attrition = Yes"}\NormalTok{,}
    \DataTypeTok{subtitle =} \StringTok{"Suggests that increased overtime is related to higher attrition"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{fe-comparison_342-classification_employee_attrition-lime-multi_files/figure-latex/plot-overtime-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(attrition_critical_features, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Attrition, }
                                        \DataTypeTok{y =}\NormalTok{ OverTime,}
\NormalTok{                                        )) }\OperatorTok{+}
\StringTok{    }\CommentTok{# geom_violin(aes(y = ..prop.., group = 1)) +}
\StringTok{    }\KeywordTok{geom_jitter}\NormalTok{(}\DataTypeTok{alpha =} \FloatTok{0.5}\NormalTok{)}
    
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{fe-comparison_342-classification_employee_attrition-lime-multi_files/figure-latex/unnamed-chunk-6-1} \end{center}

\hypertarget{job-role}{%
\subsection{Job Role}\label{job-role}}

Several job roles are experiencing more turnover. Sales reps have the highest turnover at about 40\% followed by Lab Technician, Human Resources, Sales Executive, and Research Scientist. It may be worthwhile to investigate what localized issues could be creating the high turnover among these groups within the organization.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \KeywordTok{subset}\NormalTok{(attrition_critical_features, Attrition }\OperatorTok{==}\StringTok{ "Yes"}\NormalTok{),}
            \DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ JobRole))}
\NormalTok{p }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{y =}\NormalTok{ ..prop.., }\DataTypeTok{group =} \DecValTok{1}\NormalTok{)) }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{coord_flip}\NormalTok{()}

\CommentTok{# geom_bar(mapping = aes(y = ..prop.., group = 1)) }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{fe-comparison_342-classification_employee_attrition-lime-multi_files/figure-latex/unnamed-chunk-7-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ attrition_critical_features,}
            \DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ JobRole))}
\NormalTok{p }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{y =}\NormalTok{ ..prop.., }\DataTypeTok{group =} \DecValTok{1}\NormalTok{)) }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{coord_flip}\NormalTok{() }\OperatorTok{+}
\StringTok{    }\KeywordTok{facet_wrap}\NormalTok{(Attrition }\OperatorTok{~}\StringTok{ }\NormalTok{.)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{fe-comparison_342-classification_employee_attrition-lime-multi_files/figure-latex/unnamed-chunk-8-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{attrition_critical_features }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{group_by}\NormalTok{(JobRole, Attrition) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{summarize}\NormalTok{(}\DataTypeTok{total =} \KeywordTok{n}\NormalTok{())}
\CommentTok{#> # A tibble: 18 x 3}
\CommentTok{#> # Groups:   JobRole [9]}
\CommentTok{#>   JobRole                   Attrition total}
\CommentTok{#>   <fct>                     <fct>     <int>}
\CommentTok{#> 1 Healthcare Representative No          122}
\CommentTok{#> 2 Healthcare Representative Yes           9}
\CommentTok{#> 3 Human Resources           No           40}
\CommentTok{#> 4 Human Resources           Yes          12}
\CommentTok{#> 5 Laboratory Technician     No          197}
\CommentTok{#> 6 Laboratory Technician     Yes          62}
\CommentTok{#> # ... with 12 more rows}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{attrition_critical_features }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{group_by}\NormalTok{(JobRole, Attrition) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{summarize}\NormalTok{(}\DataTypeTok{total =} \KeywordTok{n}\NormalTok{()) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{spread}\NormalTok{(}\DataTypeTok{key =}\NormalTok{ Attrition, }\DataTypeTok{value =}\NormalTok{ total) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{pct_attrition =}\NormalTok{ Yes }\OperatorTok{/}\StringTok{ }\NormalTok{(Yes }\OperatorTok{+}\StringTok{ }\NormalTok{No))}
\CommentTok{#> # A tibble: 9 x 4}
\CommentTok{#> # Groups:   JobRole [9]}
\CommentTok{#>   JobRole                      No   Yes pct_attrition}
\CommentTok{#>   <fct>                     <int> <int>         <dbl>}
\CommentTok{#> 1 Healthcare Representative   122     9        0.0687}
\CommentTok{#> 2 Human Resources              40    12        0.231 }
\CommentTok{#> 3 Laboratory Technician       197    62        0.239 }
\CommentTok{#> 4 Manager                      97     5        0.0490}
\CommentTok{#> 5 Manufacturing Director      135    10        0.0690}
\CommentTok{#> 6 Research Director            78     2        0.025 }
\CommentTok{#> # ... with 3 more rows}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{attrition_critical_features }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{group_by}\NormalTok{(JobRole, Attrition) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{summarize}\NormalTok{(}\DataTypeTok{total =} \KeywordTok{n}\NormalTok{()) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{spread}\NormalTok{(}\DataTypeTok{key =}\NormalTok{ Attrition, }\DataTypeTok{value =}\NormalTok{ total) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{pct_attrition =}\NormalTok{ Yes }\OperatorTok{/}\StringTok{ }\NormalTok{(Yes }\OperatorTok{+}\StringTok{ }\NormalTok{No)) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ forcats}\OperatorTok{::}\KeywordTok{fct_reorder}\NormalTok{(JobRole, pct_attrition), }\DataTypeTok{y =}\NormalTok{ pct_attrition)) }\OperatorTok{+}
\StringTok{    }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{stat =} \StringTok{"identity"}\NormalTok{, }\DataTypeTok{alpha =} \DecValTok{1}\NormalTok{, }\DataTypeTok{fill =} \KeywordTok{palette_light}\NormalTok{()[[}\DecValTok{1}\NormalTok{]]) }\OperatorTok{+}
\StringTok{    }\KeywordTok{expand_limits}\NormalTok{(}\DataTypeTok{y =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)) }\OperatorTok{+}
\StringTok{    }\KeywordTok{coord_flip}\NormalTok{() }\OperatorTok{+}
\StringTok{    }\KeywordTok{theme_tq}\NormalTok{() }\OperatorTok{+}
\StringTok{    }\KeywordTok{labs}\NormalTok{(}
        \DataTypeTok{title =} \StringTok{"Attrition Varies By Job Role"}\NormalTok{,}
        \DataTypeTok{subtitle =} \StringTok{"Sales Rep, Lab Tech, HR, Sales Exec, and Research Scientist }
\StringTok{        have much higher turnover"}\NormalTok{,}
        \DataTypeTok{y =} \StringTok{"Attrition Percentage (Yes / Total)"}\NormalTok{,}
        \DataTypeTok{x =} \StringTok{"JobRole"}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{fe-comparison_342-classification_employee_attrition-lime-multi_files/figure-latex/unnamed-chunk-11-1} \end{center}

\hypertarget{conclusions-1}{%
\section{Conclusions}\label{conclusions-1}}

There's a lot to take away from this article. We showed how you can use predictive analytics to develop sophisticated models that very accurately detect employees that are at risk of turnover. The autoML algorithm from H2O.ai worked well for classifying attrition with an accuracy around 87\% on unseen / unmodeled data. We then used LIME to breakdown the complex ensemble model returned from H2O into critical features that are related to attrition. Overall, this is a really useful example where we can see how machine learning and data science can be used in business applications.

\hypertarget{dealing-with-unbalanced-data}{%
\chapter{Dealing with unbalanced data}\label{dealing-with-unbalanced-data}}

\hypertarget{breast-cancer-dataset}{%
\section{Breast cancer dataset}\label{breast-cancer-dataset}}

\hypertarget{introduction-11}{%
\section{Introduction}\label{introduction-11}}

Source: \url{https://shiring.github.io/machine_learning/2017/04/02/unbalanced}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(caret)}
\CommentTok{#> Loading required package: lattice}
\CommentTok{#> Loading required package: ggplot2}
\CommentTok{#> Registered S3 methods overwritten by 'ggplot2':}
\CommentTok{#>   method         from }
\CommentTok{#>   [.quosures     rlang}
\CommentTok{#>   c.quosures     rlang}
\CommentTok{#>   print.quosures rlang}
\KeywordTok{library}\NormalTok{(mice)}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'mice'}
\CommentTok{#> The following objects are masked from 'package:base':}
\CommentTok{#> }
\CommentTok{#>     cbind, rbind}
\KeywordTok{library}\NormalTok{(ggplot2)}
\end{Highlighting}
\end{Shaded}

In my last post, where I shared the code that I used to produce an example analysis to go along with my webinar on building meaningful models for disease prediction, I mentioned that it is advised to consider over- or under-sampling when you have unbalanced data sets. Because my focus in this webinar was on evaluating model performance, I did not want to add an additional layer of complexity and therefore did not further discuss how to specifically deal with unbalanced data.

But because I had gotten a few questions regarding this, I thought it would be worthwhile to explain over- and under-sampling techniques in more detail and show how you can very easily implement them with \texttt{caret}.

\hypertarget{read-and-process-the-data-1}{%
\section{Read and process the data}\label{read-and-process-the-data-1}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bc_data <-}\StringTok{ }\KeywordTok{read.table}\NormalTok{(}\KeywordTok{file.path}\NormalTok{(data_raw_dir, }\StringTok{"breast-cancer-wisconsin.data"}\NormalTok{), }
                      \DataTypeTok{header =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{sep =} \StringTok{","}\NormalTok{)}

\KeywordTok{colnames}\NormalTok{(bc_data) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"sample_code_number"}\NormalTok{, }\StringTok{"clump_thickness"}\NormalTok{, }
                       \StringTok{"uniformity_of_cell_size"}\NormalTok{, }\StringTok{"uniformity_of_cell_shape"}\NormalTok{,}
                       \StringTok{"marginal_adhesion"}\NormalTok{, }\StringTok{"single_epithelial_cell_size"}\NormalTok{, }
                       \StringTok{"bare_nuclei"}\NormalTok{, }\StringTok{"bland_chromatin"}\NormalTok{, }\StringTok{"normal_nucleoli"}\NormalTok{, }
                       \StringTok{"mitosis"}\NormalTok{, }\StringTok{"classes"}\NormalTok{)}

\NormalTok{bc_data}\OperatorTok{$}\NormalTok{classes <-}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(bc_data}\OperatorTok{$}\NormalTok{classes }\OperatorTok{==}\StringTok{ "2"}\NormalTok{, }\StringTok{"benign"}\NormalTok{,}
                          \KeywordTok{ifelse}\NormalTok{(bc_data}\OperatorTok{$}\NormalTok{classes }\OperatorTok{==}\StringTok{ "4"}\NormalTok{, }\StringTok{"malignant"}\NormalTok{, }\OtherTok{NA}\NormalTok{))}

\NormalTok{bc_data[bc_data }\OperatorTok{==}\StringTok{ "?"}\NormalTok{] <-}\StringTok{ }\OtherTok{NA}

\CommentTok{# how many NAs are in the data}
\KeywordTok{length}\NormalTok{(}\KeywordTok{which}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(bc_data)))}
\CommentTok{#> [1] 16}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# impute missing data}

\CommentTok{# skip columns: sample_code_number and classes}
\NormalTok{bc_data[,}\DecValTok{2}\OperatorTok{:}\DecValTok{10}\NormalTok{] <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(bc_data[, }\DecValTok{2}\OperatorTok{:}\DecValTok{10}\NormalTok{], }\DecValTok{2}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{as.character}\NormalTok{(x)))}

\CommentTok{# impute but stay mute}
\NormalTok{dataset_impute <-}\StringTok{ }\KeywordTok{mice}\NormalTok{(bc_data[, }\DecValTok{2}\OperatorTok{:}\DecValTok{10}\NormalTok{],  }\DataTypeTok{print =} \OtherTok{FALSE}\NormalTok{)}

\CommentTok{# bind "classes" with the rest. skip "sample_code_number"}
\NormalTok{bc_data <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(bc_data[, }\DecValTok{11}\NormalTok{, }\DataTypeTok{drop =} \OtherTok{FALSE}\NormalTok{], }
\NormalTok{                 mice}\OperatorTok{::}\KeywordTok{complete}\NormalTok{(dataset_impute, }\DataTypeTok{action =} \DecValTok{1}\NormalTok{))}

\NormalTok{bc_data}\OperatorTok{$}\NormalTok{classes <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(bc_data}\OperatorTok{$}\NormalTok{classes)}
\end{Highlighting}
\end{Shaded}

\hypertarget{unbalanced-data}{%
\subsection{Unbalanced data}\label{unbalanced-data}}

In this context, unbalanced data refers to classification problems where we have unequal instances for different classes. Having unbalanced data is actually very common in general, but it is especially prevalent when working with disease data where we usually have more healthy control samples than disease cases. Even more extreme unbalance is seen with fraud detection, where e.g.~most credit card uses are okay and only very few will be fraudulent. In the example I used for my webinar, a \emph{breast cancer} dataset, we had about twice as many benign than malignant samples.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# how many benign and malignant cases are there?}
\KeywordTok{summary}\NormalTok{(bc_data}\OperatorTok{$}\NormalTok{classes)}
\CommentTok{#>    benign malignant }
\CommentTok{#>       458       241}
\end{Highlighting}
\end{Shaded}

\hypertarget{why-is-unbalanced-data-a-problem-in-machine-learning}{%
\subsubsection{Why is unbalanced data a problem in machine learning?}\label{why-is-unbalanced-data-a-problem-in-machine-learning}}

Most machine learning classification algorithms are sensitive to unbalance in the predictor classes. Let's consider an even more extreme example than our breast cancer dataset: assume we had 10 malignant vs 90 benign samples. A machine learning model that has been trained and tested on such a dataset could now predict ``benign'' for all samples and still gain a very high accuracy. An unbalanced dataset will bias the prediction model towards the more common class!

\hypertarget{how-to-balance-data-for-modeling}{%
\subsubsection{How to balance data for modeling}\label{how-to-balance-data-for-modeling}}

The basic theoretical concepts behind over- and under-sampling are very simple:

With under-sampling, we randomly select a subset of samples from the class with more instances to match the number of samples coming from each class. In our example, we would randomly pick 241 out of the 458 benign cases. The main disadvantage of under-sampling is that we lose potentially relevant information from the left-out samples.

With oversampling, we randomly duplicate samples from the class with fewer instances or we generate additional instances based on the data that we have, so as to match the number of samples in each class. While we avoid losing information with this approach, we also run the risk of overfitting our model as we are more likely to get the same samples in the training and in the test data, i.e.~the test data is no longer independent from training data. This would lead to an overestimation of our model's performance and generalizability.

In reality though, we should not simply perform over- or under-sampling on our training data and then run the model. We need to account for cross-validation and perform over- or under-sampling on each fold independently to get an honest estimate of model performance!

\hypertarget{modeling-the-original-unbalanced-data}{%
\subsubsection{Modeling the original unbalanced data}\label{modeling-the-original-unbalanced-data}}

Here is the same model I used in my webinar example: I randomly divide the data into training and test sets (stratified by class) and perform Random Forest modeling with 10 x 10 repeated cross-validation. Final model performance is then measured on the test set.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{42}\NormalTok{)}
\NormalTok{index <-}\StringTok{ }\KeywordTok{createDataPartition}\NormalTok{(bc_data}\OperatorTok{$}\NormalTok{classes, }\DataTypeTok{p =} \FloatTok{0.7}\NormalTok{, }\DataTypeTok{list =} \OtherTok{FALSE}\NormalTok{)}
\NormalTok{train_data <-}\StringTok{ }\NormalTok{bc_data[index, ]}
\NormalTok{test_data  <-}\StringTok{ }\NormalTok{bc_data[}\OperatorTok{-}\NormalTok{index, ]}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{42}\NormalTok{)}
\NormalTok{model_rf <-}\StringTok{ }\NormalTok{caret}\OperatorTok{::}\KeywordTok{train}\NormalTok{(classes }\OperatorTok{~}\StringTok{ }\NormalTok{.,}
                         \DataTypeTok{data =}\NormalTok{ train_data,}
                         \DataTypeTok{method =} \StringTok{"rf"}\NormalTok{,}
                         \DataTypeTok{preProcess =} \KeywordTok{c}\NormalTok{(}\StringTok{"scale"}\NormalTok{, }\StringTok{"center"}\NormalTok{),}
                         \DataTypeTok{trControl =} \KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"repeatedcv"}\NormalTok{, }
                                                  \DataTypeTok{number =} \DecValTok{10}\NormalTok{, }
                                                  \DataTypeTok{repeats =} \DecValTok{10}\NormalTok{, }
                                                  \DataTypeTok{verboseIter =} \OtherTok{FALSE}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{final <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{actual =}\NormalTok{ test_data}\OperatorTok{$}\NormalTok{classes,}
                    \KeywordTok{predict}\NormalTok{(model_rf, }
                            \DataTypeTok{newdata =}\NormalTok{ test_data, }
                            \DataTypeTok{type =} \StringTok{"prob"}\NormalTok{))}

\NormalTok{final}\OperatorTok{$}\NormalTok{predict <-}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(final}\OperatorTok{$}\NormalTok{benign }\OperatorTok{>}\StringTok{ }\FloatTok{0.5}\NormalTok{, }\StringTok{"benign"}\NormalTok{, }\StringTok{"malignant"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{final_predict <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(final}\OperatorTok{$}\NormalTok{predict)}
\NormalTok{test_data_classes <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(test_data}\OperatorTok{$}\NormalTok{classes)}

\NormalTok{cm_original <-}\StringTok{ }\KeywordTok{confusionMatrix}\NormalTok{(final_predict, test_data_classes)}
\NormalTok{cm_original}\OperatorTok{$}\NormalTok{byClass[}\StringTok{'Sensitivity'}\NormalTok{]}
\CommentTok{#> Sensitivity }
\CommentTok{#>       0.978}
\end{Highlighting}
\end{Shaded}

\hypertarget{under-sampling}{%
\section{Under-sampling}\label{under-sampling}}

Luckily, \texttt{caret} makes it very easy to incorporate over- and under-sampling techniques with cross-validation resampling. We can simply add the sampling option to our \texttt{trainControl} and choose down for under- (also called \texttt{down}-) sampling. The rest stays the same as with our original model.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{42}\NormalTok{)}
\NormalTok{ctrl <-}\StringTok{ }\KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"repeatedcv"}\NormalTok{, }
                     \DataTypeTok{number =} \DecValTok{10}\NormalTok{, }
                     \DataTypeTok{repeats =} \DecValTok{10}\NormalTok{, }
                     \DataTypeTok{verboseIter =} \OtherTok{FALSE}\NormalTok{,}
                     \DataTypeTok{sampling =} \StringTok{"down"}\NormalTok{)}


\NormalTok{model_rf_under <-}\StringTok{ }\NormalTok{caret}\OperatorTok{::}\KeywordTok{train}\NormalTok{(classes }\OperatorTok{~}\StringTok{ }\NormalTok{.,}
                         \DataTypeTok{data =}\NormalTok{ train_data,}
                         \DataTypeTok{method =} \StringTok{"rf"}\NormalTok{,}
                         \DataTypeTok{preProcess =} \KeywordTok{c}\NormalTok{(}\StringTok{"scale"}\NormalTok{, }\StringTok{"center"}\NormalTok{),}
                         \DataTypeTok{trControl =}\NormalTok{ ctrl)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{final_under <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{actual =}\NormalTok{ test_data}\OperatorTok{$}\NormalTok{classes,}
                    \KeywordTok{predict}\NormalTok{(model_rf_under, }
                            \DataTypeTok{newdata =}\NormalTok{ test_data, }
                            \DataTypeTok{type =} \StringTok{"prob"}\NormalTok{))}

\NormalTok{final_under}\OperatorTok{$}\NormalTok{predict <-}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(final_under}\OperatorTok{$}\NormalTok{benign }\OperatorTok{>}\StringTok{ }\FloatTok{0.5}\NormalTok{, }\StringTok{"benign"}\NormalTok{, }\StringTok{"malignant"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{final_under_predict <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(final_under}\OperatorTok{$}\NormalTok{predict)}
\NormalTok{test_data_classes <-}\StringTok{ }\NormalTok{test_data}\OperatorTok{$}\NormalTok{classes}

\NormalTok{cm_under <-}\StringTok{ }\KeywordTok{confusionMatrix}\NormalTok{(final_under_predict, test_data_classes)}
\NormalTok{cm_under}\OperatorTok{$}\NormalTok{byClass[}\StringTok{'Sensitivity'}\NormalTok{]}
\CommentTok{#> Sensitivity }
\CommentTok{#>       0.978}
\end{Highlighting}
\end{Shaded}

\hypertarget{oversampling}{%
\section{Oversampling}\label{oversampling}}

For over- (also called up-) sampling we simply specify sampling = ``up''.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{42}\NormalTok{)}
\NormalTok{ctrl <-}\StringTok{ }\KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"repeatedcv"}\NormalTok{, }
                     \DataTypeTok{number =} \DecValTok{10}\NormalTok{, }
                     \DataTypeTok{repeats =} \DecValTok{10}\NormalTok{, }
                     \DataTypeTok{verboseIter =} \OtherTok{FALSE}\NormalTok{,}
                     \DataTypeTok{sampling =} \StringTok{"up"}\NormalTok{)}


\NormalTok{model_rf_over <-}\StringTok{ }\NormalTok{caret}\OperatorTok{::}\KeywordTok{train}\NormalTok{(classes }\OperatorTok{~}\StringTok{ }\NormalTok{.,}
                         \DataTypeTok{data =}\NormalTok{ train_data,}
                         \DataTypeTok{method =} \StringTok{"rf"}\NormalTok{,}
                         \DataTypeTok{preProcess =} \KeywordTok{c}\NormalTok{(}\StringTok{"scale"}\NormalTok{, }\StringTok{"center"}\NormalTok{),}
                         \DataTypeTok{trControl =}\NormalTok{ ctrl)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{final_over <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{actual =}\NormalTok{ test_data}\OperatorTok{$}\NormalTok{classes,}
                          \KeywordTok{predict}\NormalTok{(model_rf_over, }
                                  \DataTypeTok{newdata =}\NormalTok{ test_data, }
                                  \DataTypeTok{type =} \StringTok{"prob"}\NormalTok{))}

\NormalTok{final_over}\OperatorTok{$}\NormalTok{predict <-}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(final_over}\OperatorTok{$}\NormalTok{benign }\OperatorTok{>}\StringTok{ }\FloatTok{0.5}\NormalTok{, }\StringTok{"benign"}\NormalTok{, }\StringTok{"malignant"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{final_over_predict <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(final_over}\OperatorTok{$}\NormalTok{predict)}
\NormalTok{test_data_classes <-}\StringTok{ }\NormalTok{test_data}\OperatorTok{$}\NormalTok{classes}

\NormalTok{cm_over <-}\StringTok{ }\KeywordTok{confusionMatrix}\NormalTok{(final_over_predict, test_data_classes)}
\NormalTok{cm_over}\OperatorTok{$}\NormalTok{byClass[}\StringTok{'Sensitivity'}\NormalTok{]}
\CommentTok{#> Sensitivity }
\CommentTok{#>       0.978}
\end{Highlighting}
\end{Shaded}

\hypertarget{rose}{%
\subsection{ROSE}\label{rose}}

Besides over- and under-sampling, there are hybrid methods that combine under-sampling with the generation of additional data. Two of the most popular are ROSE and SMOTE.

\begin{quote}
From Nicola Lunardon, Giovanna Menardi and Nicola Torelli's ``ROSE: A Package for Binary Imbalanced Learning'' (R Journal, 2014, Vol. 6 Issue 1, p.~79): ``The ROSE package provides functions to deal with binary classification problems in the presence of imbalanced classes. Artificial balanced samples are generated according to a smoothed bootstrap approach and allow for aiding both the phases of estimation and accuracy evaluation of a binary classifier in the presence of a rare class. Functions that implement more traditional remedies for the class imbalance and different metrics to evaluate accuracy are also provided. These are estimated by holdout, bootstrap, or cross-validation methods.''
\end{quote}

You implement them the same way as before, this time choosing sampling = ``rose''\ldots{}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{42}\NormalTok{)}
\NormalTok{ctrl <-}\StringTok{ }\KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"repeatedcv"}\NormalTok{, }
                     \DataTypeTok{number =} \DecValTok{10}\NormalTok{, }
                     \DataTypeTok{repeats =} \DecValTok{10}\NormalTok{, }
                     \DataTypeTok{verboseIter =} \OtherTok{FALSE}\NormalTok{,}
                     \DataTypeTok{sampling =} \StringTok{"rose"}\NormalTok{)}

\NormalTok{model_rf_rose <-}\StringTok{ }\NormalTok{caret}\OperatorTok{::}\KeywordTok{train}\NormalTok{(classes }\OperatorTok{~}\StringTok{ }\NormalTok{.,}
                              \DataTypeTok{data =}\NormalTok{ train_data,}
                              \DataTypeTok{method =} \StringTok{"rf"}\NormalTok{,}
                              \DataTypeTok{preProcess =} \KeywordTok{c}\NormalTok{(}\StringTok{"scale"}\NormalTok{, }\StringTok{"center"}\NormalTok{),}
                              \DataTypeTok{trControl =}\NormalTok{ ctrl)}
\CommentTok{#> Loaded ROSE 0.0-3}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{final_rose <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{actual =}\NormalTok{ test_data}\OperatorTok{$}\NormalTok{classes,}
                         \KeywordTok{predict}\NormalTok{(model_rf_rose, }
                                 \DataTypeTok{newdata =}\NormalTok{ test_data, }
                                 \DataTypeTok{type =} \StringTok{"prob"}\NormalTok{))}

\NormalTok{final_rose}\OperatorTok{$}\NormalTok{predict <-}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(final_rose}\OperatorTok{$}\NormalTok{benign }\OperatorTok{>}\StringTok{ }\FloatTok{0.5}\NormalTok{, }\StringTok{"benign"}\NormalTok{, }\StringTok{"malignant"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cm_rose <-}\StringTok{ }\KeywordTok{confusionMatrix}\NormalTok{(}\KeywordTok{as.factor}\NormalTok{(final_rose}\OperatorTok{$}\NormalTok{predict), }
                           \KeywordTok{as.factor}\NormalTok{(test_data}\OperatorTok{$}\NormalTok{classes))}
\NormalTok{cm_rose}\OperatorTok{$}\NormalTok{byClass[}\StringTok{'Sensitivity'}\NormalTok{]}
\CommentTok{#> Sensitivity }
\CommentTok{#>       0.985}
\end{Highlighting}
\end{Shaded}

\hypertarget{smote}{%
\subsection{SMOTE}\label{smote}}

\ldots{} or by choosing sampling = ``smote'' in the \texttt{trainControl} settings.

\begin{quote}
From Nitesh V. Chawla, Kevin W. Bowyer, Lawrence O. Hall and W. Philip Kegelmeyer's ``SMOTE: Synthetic Minority Over-sampling Technique'' (Journal of Artificial Intelligence Research, 2002, Vol. 16, pp.~321--357): ``This paper shows that a combination of our method of over-sampling the minority (abnormal) class and under-sampling the majority (normal) class can achieve better classifier performance (in ROC space) than only under-sampling the majority class. This paper also shows that a combination of our method of over-sampling the minority class and under-sampling the majority class can achieve better classifier performance (in ROC space) than varying the loss ratios in Ripper or class priors in Naive Bayes. Our method of over-sampling the minority class involves creating synthetic minority class examples.''
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{42}\NormalTok{)}
\NormalTok{ctrl <-}\StringTok{ }\KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"repeatedcv"}\NormalTok{, }
                     \DataTypeTok{number =} \DecValTok{10}\NormalTok{, }
                     \DataTypeTok{repeats =} \DecValTok{10}\NormalTok{, }
                     \DataTypeTok{verboseIter =} \OtherTok{FALSE}\NormalTok{,}
                     \DataTypeTok{sampling =} \StringTok{"smote"}\NormalTok{)}

\NormalTok{model_rf_smote <-}\StringTok{ }\NormalTok{caret}\OperatorTok{::}\KeywordTok{train}\NormalTok{(classes }\OperatorTok{~}\StringTok{ }\NormalTok{.,}
                              \DataTypeTok{data =}\NormalTok{ train_data,}
                              \DataTypeTok{method =} \StringTok{"rf"}\NormalTok{,}
                              \DataTypeTok{preProcess =} \KeywordTok{c}\NormalTok{(}\StringTok{"scale"}\NormalTok{, }\StringTok{"center"}\NormalTok{),}
                              \DataTypeTok{trControl =}\NormalTok{ ctrl)}
\CommentTok{#> Loading required package: grid}
\CommentTok{#> Registered S3 method overwritten by 'xts':}
\CommentTok{#>   method     from}
\CommentTok{#>   as.zoo.xts zoo}
\CommentTok{#> Registered S3 method overwritten by 'quantmod':}
\CommentTok{#>   method            from}
\CommentTok{#>   as.zoo.data.frame zoo}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{final_smote <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{actual =}\NormalTok{ test_data}\OperatorTok{$}\NormalTok{classes,}
                         \KeywordTok{predict}\NormalTok{(model_rf_smote, }
                                 \DataTypeTok{newdata =}\NormalTok{ test_data, }
                                 \DataTypeTok{type =} \StringTok{"prob"}\NormalTok{))}

\NormalTok{final_smote}\OperatorTok{$}\NormalTok{predict <-}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(final_smote}\OperatorTok{$}\NormalTok{benign }\OperatorTok{>}\StringTok{ }\FloatTok{0.5}\NormalTok{, }\StringTok{"benign"}\NormalTok{, }\StringTok{"malignant"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cm_smote <-}\StringTok{ }\KeywordTok{confusionMatrix}\NormalTok{(}\KeywordTok{as.factor}\NormalTok{(final_smote}\OperatorTok{$}\NormalTok{predict), }
                            \KeywordTok{as.factor}\NormalTok{(test_data}\OperatorTok{$}\NormalTok{classes))}
\NormalTok{cm_smote}\OperatorTok{$}\NormalTok{byClass[}\StringTok{'Sensitivity'}\NormalTok{]}
\CommentTok{#> Sensitivity }
\CommentTok{#>       0.978}
\end{Highlighting}
\end{Shaded}

\hypertarget{predictions}{%
\section{Predictions}\label{predictions}}

Now let's compare the predictions of all these models:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{models <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}
                \DataTypeTok{original =}\NormalTok{ model_rf,}
                \DataTypeTok{under =}\NormalTok{ model_rf_under,}
                \DataTypeTok{over =}\NormalTok{ model_rf_over,}
                \DataTypeTok{smote =}\NormalTok{ model_rf_smote,}
                \DataTypeTok{rose =}\NormalTok{ model_rf_rose)}

\NormalTok{resampling <-}\StringTok{ }\KeywordTok{resamples}\NormalTok{(models)}
\KeywordTok{bwplot}\NormalTok{(resampling)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{fe-meta_131-dealing_with_unbalanced_data-sglander_files/figure-latex/plot-resampling-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(dplyr)}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'dplyr'}
\CommentTok{#> The following objects are masked from 'package:stats':}
\CommentTok{#> }
\CommentTok{#>     filter, lag}
\CommentTok{#> The following objects are masked from 'package:base':}
\CommentTok{#> }
\CommentTok{#>     intersect, setdiff, setequal, union}
\NormalTok{comparison <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{model =} \KeywordTok{names}\NormalTok{(models),}
                         \DataTypeTok{Sensitivity =} \KeywordTok{rep}\NormalTok{(}\OtherTok{NA}\NormalTok{, }\KeywordTok{length}\NormalTok{(models)),}
                         \DataTypeTok{Specificity =} \KeywordTok{rep}\NormalTok{(}\OtherTok{NA}\NormalTok{, }\KeywordTok{length}\NormalTok{(models)),}
                         \DataTypeTok{Precision   =} \KeywordTok{rep}\NormalTok{(}\OtherTok{NA}\NormalTok{, }\KeywordTok{length}\NormalTok{(models)),}
                         \DataTypeTok{Recall      =} \KeywordTok{rep}\NormalTok{(}\OtherTok{NA}\NormalTok{, }\KeywordTok{length}\NormalTok{(models)),}
                         \DataTypeTok{F1          =} \KeywordTok{rep}\NormalTok{(}\OtherTok{NA}\NormalTok{, }\KeywordTok{length}\NormalTok{(models)))}

\ControlFlowTok{for}\NormalTok{ (name }\ControlFlowTok{in} \KeywordTok{names}\NormalTok{(models)) \{}
\NormalTok{    cm_model <-}\StringTok{ }\KeywordTok{get}\NormalTok{(}\KeywordTok{paste0}\NormalTok{(}\StringTok{"cm_"}\NormalTok{, name))}
\NormalTok{    comparison[comparison}\OperatorTok{$}\NormalTok{model}\OperatorTok{==}\NormalTok{name, ] <-}\StringTok{ }\KeywordTok{filter}\NormalTok{(comparison, model}\OperatorTok{==}\NormalTok{name) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{Sensitivity =}\NormalTok{ cm_model}\OperatorTok{$}\NormalTok{byClass[}\StringTok{"Sensitivity"}\NormalTok{],}
           \DataTypeTok{Specificity =}\NormalTok{ cm_model}\OperatorTok{$}\NormalTok{byClass[}\StringTok{"Specificity"}\NormalTok{],}
           \DataTypeTok{Precision   =}\NormalTok{ cm_model}\OperatorTok{$}\NormalTok{byClass[}\StringTok{"Precision"}\NormalTok{],}
           \DataTypeTok{Recall      =}\NormalTok{ cm_model}\OperatorTok{$}\NormalTok{byClass[}\StringTok{"Recall"}\NormalTok{],}
           \DataTypeTok{F1          =}\NormalTok{ cm_model}\OperatorTok{$}\NormalTok{byClass[}\StringTok{"F1"}\NormalTok{]}
\NormalTok{  )}
\NormalTok{\}    }

\KeywordTok{print}\NormalTok{(comparison)}
\CommentTok{#>      model Sensitivity Specificity Precision Recall    F1}
\CommentTok{#> 1 original       0.978       0.986     0.993  0.978 0.985}
\CommentTok{#> 2    under       0.978       1.000     1.000  0.978 0.989}
\CommentTok{#> 3     over       0.978       0.986     0.993  0.978 0.985}
\CommentTok{#> 4    smote       0.978       0.986     0.993  0.978 0.985}
\CommentTok{#> 5     rose       0.985       0.986     0.993  0.985 0.989}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyr)}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'tidyr'}
\CommentTok{#> The following object is masked from 'package:mice':}
\CommentTok{#> }
\CommentTok{#>     complete}
\NormalTok{comparison }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{gather}\NormalTok{(x, y, Sensitivity}\OperatorTok{:}\NormalTok{F1) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ x, }\DataTypeTok{y =}\NormalTok{ y, }\DataTypeTok{color =}\NormalTok{ model)) }\OperatorTok{+}
\StringTok{    }\KeywordTok{geom_jitter}\NormalTok{(}\DataTypeTok{width =} \FloatTok{0.2}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{0.5}\NormalTok{, }\DataTypeTok{size =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{fe-meta_131-dealing_with_unbalanced_data-sglander_files/figure-latex/comparison-plot-1} \end{center}

With this small dataset, we can already see how the different techniques can influence model performance. \textbf{Sensitivity} (or \textbf{recall}) describes the proportion of benign cases that have been predicted correctly, while \textbf{specificity} describes the proportion of malignant cases that have been predicted correctly. \textbf{Precision describes} the true positives, i.e.~the proportion of benign predictions that were actual from benign samples. \textbf{F1} is the weighted average of precision and sensitivity/ recall.

\hypertarget{final-notes}{%
\section{Final notes}\label{final-notes}}

Here, all four methods improved specificity and precision compared to the original model. Under-sampling, over-sampling and ROSE additionally improved precision and the F1 score.

This post shows a simple example of how to correct for unbalance in datasets for machine learning. For more advanced instructions and potential caveats with these techniques, check out the excellent \texttt{caret} documentation.

If you are interested in more machine learning posts, check out the category listing for machine\_learning on my blog.

\hypertarget{ten-different-methods-to-assess-variable-importance}{%
\chapter{Ten different methods to assess Variable Importance}\label{ten-different-methods-to-assess-variable-importance}}

\hypertarget{glaucoma-dataset}{%
\section{Glaucoma dataset}\label{glaucoma-dataset}}

Source: \url{https://www.machinelearningplus.com/machine-learning/feature-selection/}

\hypertarget{introduction-12}{%
\section{Introduction}\label{introduction-12}}

In real-world datasets, it is fairly common to have columns that are nothing but noise.

You are better off getting rid of such variables because of the memory space they occupy, the time and the computational esources it is going to cost, especially in large datasets.

Sometimes, you have a variable that makes business sense, but you are not sure if it actually helps in predicting the Y. You also need to consider the fact that, a feature that could be useful in one ML algorithm (say a decision tree) may go underrepresented or unused by another (like a regression model).

Having said that, it is still possible that a variable that shows poor signs of helping to explain the response variable (Y), can turn out to be significantly useful in the presence of (or combination with) other predictors. What I mean by that is, a variable might have a low correlation value of (\textasciitilde{}0.2) with Y. But in the presence of other variables, it can help to explain certain patterns/phenomenon that other variables can't explain.

In such cases, it can be hard to make a call whether to include or exclude such variables.

The strategies we are about to discuss can help fix such problems. Not only that, it will also help understand if a particular variable is important or not and how much it is contributing to the model

An important caveat. It is always best to have variables that have sound business logic backing the inclusion of a variable and rely solely on variable importance metrics.

Alright. Let's load up the `Glaucoma' dataset where the goal is to predict if a patient has Glaucoma or not based on 63 different physiological measurements. You can directly run the codes or download the dataset here.

A lot of interesting examples ahead. Let's get started.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Load Packages and prepare dataset}
\KeywordTok{library}\NormalTok{(TH.data)}
\CommentTok{#> Loading required package: survival}
\CommentTok{#> Loading required package: MASS}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'TH.data'}
\CommentTok{#> The following object is masked from 'package:MASS':}
\CommentTok{#> }
\CommentTok{#>     geyser}
\KeywordTok{library}\NormalTok{(caret)}
\CommentTok{#> Loading required package: lattice}
\CommentTok{#> Loading required package: ggplot2}
\CommentTok{#> Registered S3 methods overwritten by 'ggplot2':}
\CommentTok{#>   method         from }
\CommentTok{#>   [.quosures     rlang}
\CommentTok{#>   c.quosures     rlang}
\CommentTok{#>   print.quosures rlang}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'caret'}
\CommentTok{#> The following object is masked from 'package:survival':}
\CommentTok{#> }
\CommentTok{#>     cluster}
\KeywordTok{library}\NormalTok{(tictoc)}

\KeywordTok{data}\NormalTok{(}\StringTok{"GlaucomaM"}\NormalTok{, }\DataTypeTok{package =} \StringTok{"TH.data"}\NormalTok{)}
\NormalTok{trainData <-}\StringTok{ }\NormalTok{GlaucomaM}
\KeywordTok{head}\NormalTok{(trainData)}
\CommentTok{#>      ag    at    as    an    ai   eag   eat   eas   ean   eai  abrg  abrt}
\CommentTok{#> 2  2.22 0.354 0.580 0.686 0.601 1.267 0.336 0.346 0.255 0.331 0.479 0.260}
\CommentTok{#> 43 2.68 0.475 0.672 0.868 0.667 2.053 0.440 0.520 0.639 0.454 1.090 0.377}
\CommentTok{#> 25 1.98 0.343 0.508 0.624 0.504 1.200 0.299 0.396 0.259 0.246 0.465 0.209}
\CommentTok{#> 65 1.75 0.269 0.476 0.525 0.476 0.612 0.147 0.017 0.044 0.405 0.170 0.062}
\CommentTok{#> 70 2.99 0.599 0.686 1.039 0.667 2.513 0.543 0.607 0.871 0.492 1.800 0.431}
\CommentTok{#> 16 2.92 0.483 0.763 0.901 0.770 2.200 0.462 0.637 0.504 0.597 1.311 0.394}
\CommentTok{#>     abrs  abrn  abri    hic   mhcg  mhct   mhcs   mhcn   mhci   phcg}
\CommentTok{#> 2  0.107 0.014 0.098  0.214  0.111 0.412  0.036  0.105 -0.022 -0.139}
\CommentTok{#> 43 0.257 0.212 0.245  0.382  0.140 0.338  0.104  0.080  0.109 -0.015}
\CommentTok{#> 25 0.112 0.041 0.103  0.195  0.062 0.356  0.045 -0.009 -0.048 -0.149}
\CommentTok{#> 65 0.000 0.000 0.108 -0.030 -0.015 0.074 -0.084 -0.050  0.035 -0.182}
\CommentTok{#> 70 0.494 0.601 0.274  0.383  0.089 0.233  0.145  0.023  0.007 -0.131}
\CommentTok{#> 16 0.365 0.251 0.301  0.442  0.128 0.375  0.049  0.111  0.052 -0.088}
\CommentTok{#>      phct   phcs   phcn   phci   hvc  vbsg  vbst  vbss  vbsn  vbsi  vasg}
\CommentTok{#> 2   0.242 -0.053  0.010 -0.139 0.613 0.303 0.103 0.088 0.022 0.090 0.062}
\CommentTok{#> 43  0.296 -0.015 -0.015  0.036 0.382 0.676 0.181 0.186 0.141 0.169 0.029}
\CommentTok{#> 25  0.206 -0.092 -0.081 -0.149 0.557 0.300 0.084 0.088 0.046 0.082 0.036}
\CommentTok{#> 65 -0.097 -0.125 -0.138 -0.182 0.373 0.048 0.011 0.000 0.000 0.036 0.070}
\CommentTok{#> 70  0.163  0.055 -0.131 -0.115 0.405 0.889 0.151 0.253 0.330 0.155 0.020}
\CommentTok{#> 16  0.281 -0.067 -0.062 -0.088 0.507 0.972 0.213 0.316 0.197 0.246 0.043}
\CommentTok{#>     vast  vass  vasn  vasi  vbrg  vbrt  vbrs  vbrn  vbri  varg  vart  vars}
\CommentTok{#> 2  0.000 0.011 0.032 0.018 0.075 0.039 0.021 0.002 0.014 0.756 0.009 0.209}
\CommentTok{#> 43 0.001 0.007 0.011 0.010 0.370 0.127 0.099 0.050 0.093 0.410 0.006 0.105}
\CommentTok{#> 25 0.002 0.004 0.016 0.013 0.081 0.034 0.019 0.007 0.021 0.565 0.014 0.132}
\CommentTok{#> 65 0.005 0.030 0.033 0.002 0.005 0.001 0.000 0.000 0.004 0.380 0.032 0.147}
\CommentTok{#> 70 0.001 0.004 0.008 0.007 0.532 0.103 0.173 0.181 0.075 0.228 0.011 0.026}
\CommentTok{#> 16 0.001 0.005 0.028 0.009 0.467 0.136 0.148 0.078 0.104 0.540 0.008 0.133}
\CommentTok{#>     varn  vari   mdg   mdt   mds   mdn   mdi    tmg    tmt    tms    tmn}
\CommentTok{#> 2  0.298 0.240 0.705 0.637 0.738 0.596 0.691 -0.236 -0.018 -0.230 -0.510}
\CommentTok{#> 43 0.181 0.117 0.898 0.850 0.907 0.771 0.940 -0.211 -0.014 -0.165 -0.317}
\CommentTok{#> 25 0.243 0.177 0.687 0.643 0.689 0.684 0.700 -0.185 -0.097 -0.235 -0.337}
\CommentTok{#> 65 0.151 0.050 0.207 0.171 0.022 0.046 0.221 -0.148 -0.035 -0.449 -0.217}
\CommentTok{#> 70 0.105 0.087 0.721 0.638 0.730 0.730 0.640 -0.052 -0.105  0.084 -0.012}
\CommentTok{#> 16 0.232 0.167 0.927 0.842 0.953 0.906 0.898 -0.040  0.087  0.018 -0.094}
\CommentTok{#>       tmi    mr   rnf  mdic   emd    mv  Class}
\CommentTok{#> 2  -0.158 0.841 0.410 0.137 0.239 0.035 normal}
\CommentTok{#> 43 -0.192 0.924 0.256 0.252 0.329 0.022 normal}
\CommentTok{#> 25 -0.020 0.795 0.378 0.152 0.250 0.029 normal}
\CommentTok{#> 65 -0.091 0.746 0.200 0.027 0.078 0.023 normal}
\CommentTok{#> 70 -0.054 0.977 0.193 0.297 0.354 0.034 normal}
\CommentTok{#> 16 -0.051 0.965 0.339 0.333 0.442 0.028 normal}
\end{Highlighting}
\end{Shaded}

\hypertarget{boruta}{%
\section{1. Boruta}\label{boruta}}

Boruta is a feature ranking and selection algorithm based on random forests algorithm.

The advantage with Boruta is that it clearly decides if a variable is important or not and helps to select variables that are statistically significant. Besides, you can adjust the strictness of the algorithm by adjusting the \(p\) values that defaults to 0.01 and the \texttt{maxRuns}.

\texttt{maxRuns} is the number of times the algorithm is run. The higher the \texttt{maxRuns} the more selective you get in picking the variables. The default value is 100.

In the process of deciding if a feature is important or not, some features may be marked by Boruta as `Tentative'. Sometimes increasing the maxRuns can help resolve the `Tentativeness' of the feature.

Lets see an example based on the Glaucoma dataset from \texttt{TH.data} package that I created earlier.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# install.packages('Boruta')}
\KeywordTok{library}\NormalTok{(Boruta)}
\CommentTok{#> Loading required package: ranger}
\end{Highlighting}
\end{Shaded}

The \texttt{boruta} function uses a formula interface just like most predictive modeling functions. So the first argument to \texttt{boruta()} is the formula with the response variable on the left and all the predictors on the right.

By placing a dot, all the variables in \texttt{trainData} other than Class will be included in the model.

The \texttt{doTrace} argument controls the amount of output printed to the console. Higher the value, more the log details you get. So save space I have set it to 0, but try setting it to 1 and 2 if you are running the code.

Finally the output is stored in \texttt{boruta\_output}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Perform Boruta search}
\NormalTok{boruta_output <-}\StringTok{ }\KeywordTok{Boruta}\NormalTok{(Class }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data=}\KeywordTok{na.omit}\NormalTok{(trainData), }\DataTypeTok{doTrace=}\DecValTok{0}\NormalTok{)  }
\end{Highlighting}
\end{Shaded}

Let's see what the boruta\_output contains.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{names}\NormalTok{(boruta_output)}
\CommentTok{#>  [1] "finalDecision" "ImpHistory"    "pValue"        "maxRuns"      }
\CommentTok{#>  [5] "light"         "mcAdj"         "timeTaken"     "roughfixed"   }
\CommentTok{#>  [9] "call"          "impSource"}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Get significant variables including tentatives}
\NormalTok{boruta_signif <-}\StringTok{ }\KeywordTok{getSelectedAttributes}\NormalTok{(boruta_output, }\DataTypeTok{withTentative =} \OtherTok{TRUE}\NormalTok{)}
\KeywordTok{print}\NormalTok{(boruta_signif)  }
\CommentTok{#>  [1] "as"   "ai"   "eas"  "ean"  "abrg" "abrs" "abrn" "abri" "hic"  "mhcg"}
\CommentTok{#> [11] "mhcs" "mhcn" "mhci" "phcg" "phcn" "phci" "hvc"  "vbsg" "vbss" "vbsn"}
\CommentTok{#> [21] "vbsi" "vasg" "vass" "vasi" "vbrg" "vbrs" "vbrn" "vbri" "varg" "vart"}
\CommentTok{#> [31] "vars" "varn" "vari" "mdn"  "tmg"  "tmt"  "tms"  "tmi"  "mr"   "rnf" }
\CommentTok{#> [41] "mdic" "emd"}
\end{Highlighting}
\end{Shaded}

If you are not sure about the tentative variables being selected for granted, you can choose a \texttt{TentativeRoughFix} on \texttt{boruta\_output}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Do a tentative rough fix}
\NormalTok{roughFixMod <-}\StringTok{ }\KeywordTok{TentativeRoughFix}\NormalTok{(boruta_output)}
\NormalTok{boruta_signif <-}\StringTok{ }\KeywordTok{getSelectedAttributes}\NormalTok{(roughFixMod)}
\KeywordTok{print}\NormalTok{(boruta_signif)}
\CommentTok{#>  [1] "as"   "ai"   "ean"  "abrg" "abrs" "abrn" "abri" "hic"  "mhcg" "mhcn"}
\CommentTok{#> [11] "mhci" "phcg" "phcn" "phci" "hvc"  "vbsn" "vbsi" "vasg" "vass" "vasi"}
\CommentTok{#> [21] "vbrg" "vbrs" "vbrn" "vbri" "varg" "vart" "vars" "varn" "vari" "mdn" }
\CommentTok{#> [31] "tmg"  "tms"  "tmi"  "mr"   "rnf"  "mdic"}
\end{Highlighting}
\end{Shaded}

There you go. Boruta has decided on the `Tentative' variables on our behalf. Let's find out the importance scores of these variables.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Variable Importance Scores}
\NormalTok{imps <-}\StringTok{ }\KeywordTok{attStats}\NormalTok{(roughFixMod)}
\NormalTok{imps2 =}\StringTok{ }\NormalTok{imps[imps}\OperatorTok{$}\NormalTok{decision }\OperatorTok{!=}\StringTok{ 'Rejected'}\NormalTok{, }\KeywordTok{c}\NormalTok{(}\StringTok{'meanImp'}\NormalTok{, }\StringTok{'decision'}\NormalTok{)]}
\KeywordTok{head}\NormalTok{(imps2[}\KeywordTok{order}\NormalTok{(}\OperatorTok{-}\NormalTok{imps2}\OperatorTok{$}\NormalTok{meanImp), ])  }\CommentTok{# descending sort}
\CommentTok{#>      meanImp  decision}
\CommentTok{#> vari   12.37 Confirmed}
\CommentTok{#> varg   11.74 Confirmed}
\CommentTok{#> vars   10.74 Confirmed}
\CommentTok{#> phci    8.34 Confirmed}
\CommentTok{#> hic     8.21 Confirmed}
\CommentTok{#> varn    7.88 Confirmed}
\end{Highlighting}
\end{Shaded}

Let's plot it to see the importances of these variables.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Plot variable importance}
\KeywordTok{plot}\NormalTok{(boruta_output, }\DataTypeTok{cex.axis=}\NormalTok{.}\DecValTok{7}\NormalTok{, }\DataTypeTok{las=}\DecValTok{2}\NormalTok{, }\DataTypeTok{xlab=}\StringTok{""}\NormalTok{, }\DataTypeTok{main=}\StringTok{"Variable Importance"}\NormalTok{)  }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{fe-meta_133-variable_importance_files/figure-latex/boruta-plot-1} \end{center}

This plot reveals the importance of each of the features.

The columns in green are `confirmed' and the ones in red are not. There are couple of blue bars representing \texttt{ShadowMax} and \texttt{ShadowMin.} They are not actual features, but are used by the boruta algorithm to decide if a variable is important or not.

\hypertarget{variable-importance-from-machine-learning-algorithms}{%
\section{Variable Importance from Machine Learning Algorithms}\label{variable-importance-from-machine-learning-algorithms}}

Another way to look at feature selection is to consider variables most used by various ML algorithms the most to be important.

Depending on how the machine learning algorithm learns the relationship between X's and Y, different machine learning algorithms may possibly end up using different variables (but mostly common vars) to various degrees.

What I mean by that is, the variables that proved useful in a tree-based algorithm like \texttt{rpart}, can turn out to be less useful in a regression-based model. So all variables need not be equally useful to all algorithms.

So how do we find the variable importance for a given ML algo?

\texttt{train()} the desired model using the caret package.
Then, use \texttt{varImp()} to determine the feature importances.

You may want to try out multiple algorithms, to get a feel of the usefulness of the features across algos.

\hypertarget{rpart}{%
\subsection{rpart}\label{rpart}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Train an rpart model and compute variable importance.}
\KeywordTok{library}\NormalTok{(caret)}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{100}\NormalTok{)}
\NormalTok{rPartMod <-}\StringTok{ }\KeywordTok{train}\NormalTok{(Class }\OperatorTok{~}\StringTok{ }\NormalTok{., }
                  \DataTypeTok{data=}\NormalTok{trainData, }
                  \DataTypeTok{method=}\StringTok{"rpart"}\NormalTok{)}

\NormalTok{rpartImp <-}\StringTok{ }\KeywordTok{varImp}\NormalTok{(rPartMod)}
\KeywordTok{print}\NormalTok{(rpartImp)}
\CommentTok{#> rpart variable importance}
\CommentTok{#> }
\CommentTok{#>   only 20 most important variables shown (out of 62)}
\CommentTok{#> }
\CommentTok{#>      Overall}
\CommentTok{#> varg   100.0}
\CommentTok{#> vari    93.2}
\CommentTok{#> vars    85.2}
\CommentTok{#> varn    76.9}
\CommentTok{#> tmi     72.3}
\CommentTok{#> mhcn     0.0}
\CommentTok{#> as       0.0}
\CommentTok{#> phcs     0.0}
\CommentTok{#> vbst     0.0}
\CommentTok{#> abrt     0.0}
\CommentTok{#> vbsg     0.0}
\CommentTok{#> eai      0.0}
\CommentTok{#> vbrs     0.0}
\CommentTok{#> vbsi     0.0}
\CommentTok{#> eag      0.0}
\CommentTok{#> tmt      0.0}
\CommentTok{#> phcn     0.0}
\CommentTok{#> vart     0.0}
\CommentTok{#> mds      0.0}
\CommentTok{#> an       0.0}
\end{Highlighting}
\end{Shaded}

Only 5 of the 63 features was used by rpart and if you look closely, the 5 variables used here are in the top 6 that boruta selected.

Let's do one more: the variable importances from Regularized Random Forest (RRF) algorithm.

\hypertarget{regularized-random-forest-rrf}{%
\subsection{Regularized Random Forest (RRF)}\label{regularized-random-forest-rrf}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{tic}\NormalTok{()}
\CommentTok{# Train an RRF model and compute variable importance.}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{100}\NormalTok{)}
\NormalTok{rrfMod <-}\StringTok{ }\KeywordTok{train}\NormalTok{(Class }\OperatorTok{~}\StringTok{ }\NormalTok{., }
                \DataTypeTok{data =}\NormalTok{ trainData, }
                \DataTypeTok{method =} \StringTok{"RRF"}\NormalTok{)}
\CommentTok{#> Registered S3 method overwritten by 'RRF':}
\CommentTok{#>   method      from        }
\CommentTok{#>   plot.margin randomForest}

\NormalTok{rrfImp <-}\StringTok{ }\KeywordTok{varImp}\NormalTok{(rrfMod, }\DataTypeTok{scale=}\NormalTok{F)}
\KeywordTok{toc}\NormalTok{()}
\CommentTok{#> 341.618 sec elapsed}
\NormalTok{rrfImp}
\CommentTok{#> RRF variable importance}
\CommentTok{#> }
\CommentTok{#>   only 20 most important variables shown (out of 62)}
\CommentTok{#> }
\CommentTok{#>      Overall}
\CommentTok{#> varg   25.07}
\CommentTok{#> vari   18.78}
\CommentTok{#> vars    5.29}
\CommentTok{#> tmi     4.09}
\CommentTok{#> mhcg    3.25}
\CommentTok{#> mhci    2.81}
\CommentTok{#> hic     2.69}
\CommentTok{#> hvc     2.50}
\CommentTok{#> mv      2.00}
\CommentTok{#> vasg    1.99}
\CommentTok{#> phci    1.77}
\CommentTok{#> phcn    1.53}
\CommentTok{#> phct    1.43}
\CommentTok{#> vass    1.37}
\CommentTok{#> phcg    1.37}
\CommentTok{#> tms     1.32}
\CommentTok{#> tmg     1.16}
\CommentTok{#> abrs    1.16}
\CommentTok{#> tmt     1.13}
\CommentTok{#> mdic    1.13}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(rrfImp, }\DataTypeTok{top =} \DecValTok{20}\NormalTok{, }\DataTypeTok{main=}\StringTok{'Variable Importance'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{fe-meta_133-variable_importance_files/figure-latex/unnamed-chunk-9-1} \end{center}

The topmost important variables are pretty much from the top tier of Boruta's selections.

Some of the other algorithms available in \texttt{train()} that you can use to compute varImp are the following:

\begin{verbatim}
ada, AdaBag, AdaBoost.M1, adaboost, bagEarth, bagEarthGCV, bagFDA, bagFDAGCV, bartMachine, blasso, BstLm, bstSm, C5.0, C5.0Cost, C5.0Rules, C5.0Tree, cforest, chaid, ctree, ctree2, cubist, deepboost, earth, enet, evtree, extraTrees, fda, gamboost, gbm_h2o, gbm, gcvEarth, glmnet_h2o, glmnet, glmStepAIC, J48, JRip, lars, lars2, lasso, LMT, LogitBoost, M5, M5Rules, msaenet, nodeHarvest, OneR, ordinalNet, ORFlog, ORFpls, ORFridge, ORFsvm, pam, parRF, PART, penalized, PenalizedLDA, qrf, ranger, Rborist, relaxo, rf, rFerns, rfRules, rotationForest, rotationForestCp, rpart, rpart1SE, rpart2, rpartCost, rpartScore, rqlasso, rqnc, RRF, RRFglobal, sdwd, smda, sparseLDA, spikeslab, wsrf, xgbLinear, xgbTree.
\end{verbatim}

\hypertarget{lasso-regression}{%
\section{Lasso Regression}\label{lasso-regression}}

Least Absolute Shrinkage and Selection Operator (LASSO) regression is a type of regularization method that penalizes with L1-norm.

It basically imposes a cost to having large weights (value of coefficients). And its called L1 regularization, because the cost added, is proportional to the absolute value of weight coefficients.

As a result, in the process of shrinking the coefficients, it eventually reduces the coefficients of certain unwanted features all the to zero. That is, it removes the unneeded variables altogether.

So effectively, LASSO regression can be considered as a variable selection technique as well.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(glmnet)}
\CommentTok{#> Loading required package: Matrix}
\CommentTok{#> Loading required package: foreach}
\CommentTok{#> Loaded glmnet 2.0-16}

\CommentTok{# online data}
\CommentTok{# trainData <- read.csv('https://raw.githubusercontent.com/selva86/datasets/master/GlaucomaM.csv')}

\NormalTok{trainData <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\KeywordTok{file.path}\NormalTok{(data_raw_dir, }\StringTok{"glaucoma.csv"}\NormalTok{))}

\NormalTok{x <-}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{(trainData[,}\OperatorTok{-}\DecValTok{63}\NormalTok{]) }\CommentTok{# all X vars}
\NormalTok{y <-}\StringTok{ }\KeywordTok{as.double}\NormalTok{(}\KeywordTok{as.matrix}\NormalTok{(}\KeywordTok{ifelse}\NormalTok{(trainData[, }\DecValTok{63}\NormalTok{]}\OperatorTok{==}\StringTok{'normal'}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{))) }\CommentTok{# Only Class}

\CommentTok{# Fit the LASSO model (Lasso: Alpha = 1)}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{100}\NormalTok{)}
\NormalTok{cv.lasso <-}\StringTok{ }\KeywordTok{cv.glmnet}\NormalTok{(x, y, }\DataTypeTok{family=}\StringTok{'binomial'}\NormalTok{, }\DataTypeTok{alpha=}\DecValTok{1}\NormalTok{, }\DataTypeTok{parallel=}\OtherTok{TRUE}\NormalTok{, }\DataTypeTok{standardize=}\OtherTok{TRUE}\NormalTok{, }\DataTypeTok{type.measure=}\StringTok{'auc'}\NormalTok{)}
\CommentTok{#> Warning: executing %dopar% sequentially: no parallel backend registered}

\CommentTok{# Results}
\KeywordTok{plot}\NormalTok{(cv.lasso)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{fe-meta_133-variable_importance_files/figure-latex/lasso-train-1} \end{center}

Let's see how to interpret this plot.

The X axis of the plot is the log of \texttt{lambda}. That means when it is 2 here, the lambda value is actually 100.

The numbers at the top of the plot show how many predictors were included in the model. The position of red dots along the Y-axis tells what \texttt{AUC} we got when you include as many variables shown on the top x-axis.

You can also see two dashed vertical lines.

The first one on the left points to the lambda with the lowest mean squared error. The one on the right point to the number of variables with the highest deviance within 1 standard deviation.

The best lambda value is stored inside `cv.lasso\$lambda.min'.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot(cv.lasso$glmnet.fit, xvar="lambda", label=TRUE)}
\KeywordTok{cat}\NormalTok{(}\StringTok{'Min Lambda: '}\NormalTok{, cv.lasso}\OperatorTok{$}\NormalTok{lambda.min, }\StringTok{'}\CharTok{\textbackslash{}n}\StringTok{ 1Sd Lambda: '}\NormalTok{, cv.lasso}\OperatorTok{$}\NormalTok{lambda}\FloatTok{.1}\NormalTok{se)}
\CommentTok{#> Min Lambda:  0.0224 }
\CommentTok{#>  1Sd Lambda:  0.144}
\NormalTok{df_coef <-}\StringTok{ }\KeywordTok{round}\NormalTok{(}\KeywordTok{as.matrix}\NormalTok{(}\KeywordTok{coef}\NormalTok{(cv.lasso, }\DataTypeTok{s=}\NormalTok{cv.lasso}\OperatorTok{$}\NormalTok{lambda.min)), }\DecValTok{2}\NormalTok{)}

\CommentTok{# See all contributing variables}
\NormalTok{df_coef[df_coef[, }\DecValTok{1}\NormalTok{] }\OperatorTok{!=}\StringTok{ }\DecValTok{0}\NormalTok{, ]}
\CommentTok{#> (Intercept)          as        mhci        phci         hvc        vast }
\CommentTok{#>        2.68       -1.59        3.85        5.60       -2.41      -13.90 }
\CommentTok{#>        vars        vari         mdn         mdi         tmg         tms }
\CommentTok{#>      -20.18       -1.58        0.50        0.99        0.06        2.56 }
\CommentTok{#>         tmi }
\CommentTok{#>        2.23}
\end{Highlighting}
\end{Shaded}

The above output shows what variables LASSO considered important. A high positive or low negative implies more important is that variable.

\hypertarget{step-wise-forward-and-backward-selection}{%
\section{Step wise Forward and Backward Selection}\label{step-wise-forward-and-backward-selection}}

Stepwise regression can be used to select features if the Y variable is a numeric variable. It is particularly used in selecting best linear regression models.

It searches for the best possible regression model by iteratively selecting and dropping variables to arrive at a model with the lowest possible AIC.

It can be implemented using the \texttt{step()} function and you need to provide it with a lower model, which is the base model from which it won't remove any features and an upper model, which is a full model that has all possible features you want to have.

Our case is not so complicated (\textless{} 20 vars), so lets just do a simple stepwise in `both' directions.

I will use the \texttt{ozone} dataset for this where the objective is to predict the \texttt{ozone\_reading} based on other weather related observations.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Load data}
\CommentTok{# online}
\CommentTok{# trainData <- read.csv("http://rstatistics.net/wp-content/uploads/2015/09/ozone1.csv",}
\CommentTok{#                      stringsAsFactors=F)}
\NormalTok{trainData <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\KeywordTok{file.path}\NormalTok{(data_raw_dir, }\StringTok{"ozone1.csv"}\NormalTok{))}
\KeywordTok{print}\NormalTok{(}\KeywordTok{head}\NormalTok{(trainData))}
\CommentTok{#>   Month Day_of_month Day_of_week ozone_reading pressure_height Wind_speed}
\CommentTok{#> 1     1            1           4             3            5480          8}
\CommentTok{#> 2     1            2           5             3            5660          6}
\CommentTok{#> 3     1            3           6             3            5710          4}
\CommentTok{#> 4     1            4           7             5            5700          3}
\CommentTok{#> 5     1            5           1             5            5760          3}
\CommentTok{#> 6     1            6           2             6            5720          4}
\CommentTok{#>   Humidity Temperature_Sandburg Temperature_ElMonte Inversion_base_height}
\CommentTok{#> 1       20                 40.5                39.8                  5000}
\CommentTok{#> 2       41                 38.0                46.7                  4109}
\CommentTok{#> 3       28                 40.0                49.5                  2693}
\CommentTok{#> 4       37                 45.0                52.3                   590}
\CommentTok{#> 5       51                 54.0                45.3                  1450}
\CommentTok{#> 6       69                 35.0                49.6                  1568}
\CommentTok{#>   Pressure_gradient Inversion_temperature Visibility}
\CommentTok{#> 1               -15                  30.6        200}
\CommentTok{#> 2               -14                  48.0        300}
\CommentTok{#> 3               -25                  47.7        250}
\CommentTok{#> 4               -24                  55.0        100}
\CommentTok{#> 5                25                  57.0         60}
\CommentTok{#> 6                15                  53.8         60}
\end{Highlighting}
\end{Shaded}

The data is ready. Let's perform the stepwise.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Step 1: Define base intercept only model}
\NormalTok{base.mod <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(ozone_reading }\OperatorTok{~}\StringTok{ }\DecValTok{1}\NormalTok{ , }\DataTypeTok{data=}\NormalTok{trainData)  }

\CommentTok{# Step 2: Full model with all predictors}
\NormalTok{all.mod <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(ozone_reading }\OperatorTok{~}\StringTok{ }\NormalTok{. , }\DataTypeTok{data=}\NormalTok{ trainData) }

\CommentTok{# Step 3: Perform step-wise algorithm. direction='both' implies both forward and backward stepwise}
\NormalTok{stepMod <-}\StringTok{ }\KeywordTok{step}\NormalTok{(base.mod, }\DataTypeTok{scope =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{lower =}\NormalTok{ base.mod, }\DataTypeTok{upper =}\NormalTok{ all.mod), }\DataTypeTok{direction =} \StringTok{"both"}\NormalTok{, }\DataTypeTok{trace =} \DecValTok{0}\NormalTok{, }\DataTypeTok{steps =} \DecValTok{1000}\NormalTok{)  }

\CommentTok{# Step 4: Get the shortlisted variable.}
\NormalTok{shortlistedVars <-}\StringTok{ }\KeywordTok{names}\NormalTok{(}\KeywordTok{unlist}\NormalTok{(stepMod[[}\DecValTok{1}\NormalTok{]])) }
\NormalTok{shortlistedVars <-}\StringTok{ }\NormalTok{shortlistedVars[}\OperatorTok{!}\NormalTok{shortlistedVars }\OperatorTok{%in%}\StringTok{ "(Intercept)"}\NormalTok{] }\CommentTok{# remove intercept}

\CommentTok{# Show}
\KeywordTok{print}\NormalTok{(shortlistedVars)}
\CommentTok{#> [1] "Temperature_Sandburg"  "Humidity"              "Temperature_ElMonte"  }
\CommentTok{#> [4] "Month"                 "pressure_height"       "Inversion_base_height"}
\end{Highlighting}
\end{Shaded}

The selected model has the above 6 features in it.

But if you have too many features (\textgreater{} 100) in training data, then it might be a good idea to split the dataset into chunks of 10 variables each with Y as mandatory in each dataset. Loop through all the chunks and collect the best features.

We are doing it this way because some variables that came as important in a training data with fewer features may not show up in a linear reg model built on lots of features.

Finally, from a pool of shortlisted features (from small chunk models), run a full stepwise model to get the final set of selected features.

You can take this as a learning assignment to be solved within 20 minutes.

\hypertarget{relative-importance-from-linear-regression}{%
\section{Relative Importance from Linear Regression}\label{relative-importance-from-linear-regression}}

This technique is specific to linear regression models.

Relative importance can be used to assess which variables contributed how much in explaining the linear model's R-squared value. So, if you sum up the produced importances, it will add up to the model's R-sq value.

In essence, it is not directly a feature selection method, because you have already provided the features that go in the model. But after building the model, the \texttt{relaimpo} can provide a sense of how important each feature is in contributing to the R-sq, or in other words, in `explaining the Y variable'.

So, how to calculate relative importance?

It is implemented in the \texttt{relaimpo} package. Basically, you build a linear regression model and pass that as the main argument to \texttt{calc.relimp()}. relaimpo has multiple options to compute the relative importance, but the recommended method is to use \texttt{type=\textquotesingle{}lmg\textquotesingle{}}, as I have done below.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# install.packages('relaimpo')}
\KeywordTok{library}\NormalTok{(relaimpo)}
\CommentTok{#> Loading required package: boot}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'boot'}
\CommentTok{#> The following object is masked from 'package:lattice':}
\CommentTok{#> }
\CommentTok{#>     melanoma}
\CommentTok{#> The following object is masked from 'package:survival':}
\CommentTok{#> }
\CommentTok{#>     aml}
\CommentTok{#> Loading required package: survey}
\CommentTok{#> Loading required package: grid}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'survey'}
\CommentTok{#> The following object is masked from 'package:graphics':}
\CommentTok{#> }
\CommentTok{#>     dotchart}
\CommentTok{#> Loading required package: mitools}
\CommentTok{#> This is the global version of package relaimpo.}
\CommentTok{#> If you are a non-US user, a version with the interesting additional metric pmvd is available}
\CommentTok{#> from Ulrike Groempings web site at prof.beuth-hochschule.de/groemping.}

\CommentTok{# Build linear regression model}
\NormalTok{model_formula =}\StringTok{ }\NormalTok{ozone_reading }\OperatorTok{~}\StringTok{ }\NormalTok{Temperature_Sandburg }\OperatorTok{+}\StringTok{ }\NormalTok{Humidity }\OperatorTok{+}\StringTok{ }\NormalTok{Temperature_ElMonte }\OperatorTok{+}\StringTok{ }\NormalTok{Month }\OperatorTok{+}\StringTok{ }\NormalTok{pressure_height }\OperatorTok{+}\StringTok{ }\NormalTok{Inversion_base_height}
\NormalTok{lmMod <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(model_formula, }\DataTypeTok{data=}\NormalTok{trainData)}

\CommentTok{# calculate relative importance}
\NormalTok{relImportance <-}\StringTok{ }\KeywordTok{calc.relimp}\NormalTok{(lmMod, }\DataTypeTok{type =} \StringTok{"lmg"}\NormalTok{, }\DataTypeTok{rela =}\NormalTok{ F)  }

\CommentTok{# Sort}
\KeywordTok{cat}\NormalTok{(}\StringTok{'Relative Importances: }\CharTok{\textbackslash{}n}\StringTok{'}\NormalTok{)}
\CommentTok{#> Relative Importances:}
\KeywordTok{sort}\NormalTok{(}\KeywordTok{round}\NormalTok{(relImportance}\OperatorTok{$}\NormalTok{lmg, }\DecValTok{3}\NormalTok{), }\DataTypeTok{decreasing=}\OtherTok{TRUE}\NormalTok{)}
\CommentTok{#>   Temperature_ElMonte  Temperature_Sandburg       pressure_height }
\CommentTok{#>                 0.214                 0.203                 0.104 }
\CommentTok{#> Inversion_base_height              Humidity                 Month }
\CommentTok{#>                 0.096                 0.086                 0.012}
\end{Highlighting}
\end{Shaded}

Additionally, you can use bootstrapping (using \texttt{boot.relimp}) to compute the confidence intervals of the produced relative importances.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bootsub <-}\StringTok{ }\KeywordTok{boot.relimp}\NormalTok{(ozone_reading }\OperatorTok{~}\StringTok{ }\NormalTok{Temperature_Sandburg }\OperatorTok{+}\StringTok{ }\NormalTok{Humidity }\OperatorTok{+}\StringTok{ }\NormalTok{Temperature_ElMonte }\OperatorTok{+}\StringTok{ }\NormalTok{Month }\OperatorTok{+}\StringTok{ }\NormalTok{pressure_height }\OperatorTok{+}\StringTok{ }\NormalTok{Inversion_base_height, }\DataTypeTok{data=}\NormalTok{trainData,}
                       \DataTypeTok{b =} \DecValTok{1000}\NormalTok{, }\DataTypeTok{type =} \StringTok{'lmg'}\NormalTok{, }\DataTypeTok{rank =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{diff =} \OtherTok{TRUE}\NormalTok{)}

\KeywordTok{plot}\NormalTok{(}\KeywordTok{booteval.relimp}\NormalTok{(bootsub, }\DataTypeTok{level=}\NormalTok{.}\DecValTok{95}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{fe-meta_133-variable_importance_files/figure-latex/unnamed-chunk-11-1} \end{center}

\hypertarget{recursive-feature-elimination-rfe-1}{%
\section{Recursive Feature Elimination (RFE)}\label{recursive-feature-elimination-rfe-1}}

Recursive feature elimnation (rfe) offers a rigorous way to determine the important variables before you even feed them into a ML algo.

It can be implemented using the \texttt{rfe()} from caret package.

The rfe() also takes two important parameters.

\begin{itemize}
\tightlist
\item
  \texttt{sizes}
\item
  \texttt{rfeControl}
\end{itemize}

So, what does \texttt{sizes} and \texttt{rfeControl} represent?

The sizes determines the number of most important features the rfe should iterate. Below, I have set the size as 1 to 5, 10, 15 and 18.

Secondly, the \texttt{rfeControl} parameter receives the output of the \texttt{rfeControl()}. You can set what type of variable evaluation algorithm must be used. Here, I have used random forests based rfFuncs. The \texttt{method=\textquotesingle{}repeatedCV\textquotesingle{}} means it will do a repeated k-Fold cross validation with \texttt{repeats=5}.

Once complete, you get the accuracy and kappa for each model size you provided. The final selected model subset size is marked with a * in the rightmost selected column.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str}\NormalTok{(trainData)}
\CommentTok{#> 'data.frame':    366 obs. of  13 variables:}
\CommentTok{#>  $ Month                : int  1 1 1 1 1 1 1 1 1 1 ...}
\CommentTok{#>  $ Day_of_month         : int  1 2 3 4 5 6 7 8 9 10 ...}
\CommentTok{#>  $ Day_of_week          : int  4 5 6 7 1 2 3 4 5 6 ...}
\CommentTok{#>  $ ozone_reading        : num  3 3 3 5 5 6 4 4 6 7 ...}
\CommentTok{#>  $ pressure_height      : num  5480 5660 5710 5700 5760 5720 5790 5790 5700 5700 ...}
\CommentTok{#>  $ Wind_speed           : int  8 6 4 3 3 4 6 3 3 3 ...}
\CommentTok{#>  $ Humidity             : num  20 41 28 37 51 ...}
\CommentTok{#>  $ Temperature_Sandburg : num  40.5 38 40 45 54 ...}
\CommentTok{#>  $ Temperature_ElMonte  : num  39.8 46.7 49.5 52.3 45.3 ...}
\CommentTok{#>  $ Inversion_base_height: num  5000 4109 2693 590 1450 ...}
\CommentTok{#>  $ Pressure_gradient    : num  -15 -14 -25 -24 25 15 -33 -28 23 -2 ...}
\CommentTok{#>  $ Inversion_temperature: num  30.6 48 47.7 55 57 ...}
\CommentTok{#>  $ Visibility           : int  200 300 250 100 60 60 100 250 120 120 ...}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{tic}\NormalTok{()}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{100}\NormalTok{)}
\KeywordTok{options}\NormalTok{(}\DataTypeTok{warn=}\OperatorTok{-}\DecValTok{1}\NormalTok{)}

\NormalTok{subsets <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{5}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{18}\NormalTok{)}

\NormalTok{ctrl <-}\StringTok{ }\KeywordTok{rfeControl}\NormalTok{(}\DataTypeTok{functions =}\NormalTok{ rfFuncs,}
                   \DataTypeTok{method =} \StringTok{"repeatedcv"}\NormalTok{,}
                   \DataTypeTok{repeats =} \DecValTok{5}\NormalTok{,}
                   \DataTypeTok{verbose =} \OtherTok{FALSE}\NormalTok{)}

\NormalTok{lmProfile <-}\StringTok{ }\KeywordTok{rfe}\NormalTok{(}\DataTypeTok{x=}\NormalTok{trainData[, }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{3}\NormalTok{, }\DecValTok{5}\OperatorTok{:}\DecValTok{13}\NormalTok{)], }\DataTypeTok{y=}\NormalTok{trainData}\OperatorTok{$}\NormalTok{ozone_reading,}
                 \DataTypeTok{sizes =}\NormalTok{ subsets,}
                 \DataTypeTok{rfeControl =}\NormalTok{ ctrl)}
\KeywordTok{toc}\NormalTok{()}
\CommentTok{#> 91.881 sec elapsed}
\NormalTok{lmProfile}
\CommentTok{#> }
\CommentTok{#> Recursive feature selection}
\CommentTok{#> }
\CommentTok{#> Outer resampling method: Cross-Validated (10 fold, repeated 5 times) }
\CommentTok{#> }
\CommentTok{#> Resampling performance over subset size:}
\CommentTok{#> }
\CommentTok{#>  Variables RMSE Rsquared  MAE RMSESD RsquaredSD MAESD Selected}
\CommentTok{#>          1 5.13    0.595 3.92  0.826     0.1275 0.586         }
\CommentTok{#>          2 4.03    0.746 3.11  0.542     0.0743 0.416         }
\CommentTok{#>          3 3.95    0.756 3.06  0.472     0.0670 0.380         }
\CommentTok{#>          4 3.93    0.759 3.01  0.468     0.0683 0.361         }
\CommentTok{#>          5 3.90    0.763 2.98  0.467     0.0659 0.350         }
\CommentTok{#>         10 3.77    0.782 2.85  0.496     0.0734 0.393        *}
\CommentTok{#>         12 3.77    0.781 2.86  0.508     0.0756 0.401         }
\CommentTok{#> }
\CommentTok{#> The top 5 variables (out of 10):}
\CommentTok{#>    Temperature_ElMonte, Pressure_gradient, Temperature_Sandburg, Inversion_temperature, Humidity}
\end{Highlighting}
\end{Shaded}

So, it says, Temperature\_ElMonte, Pressure\_gradient, Temperature\_Sandburg, Inversion\_temperature, Humidity are the top 5 variables in that order.

And the best model size out of the provided models sizes (in subsets) is 10.

You can see all of the top 10 variables from `\texttt{lmProfile\$optVariables}' that was created using \texttt{rfe} function above.

\hypertarget{genetic-algorithm}{%
\section{Genetic Algorithm}\label{genetic-algorithm}}

You can perform a supervised feature selection with genetic algorithms using the \texttt{gafs()}. This is \textbf{quite resource expensive} so consider that before choosing the number of iterations (iters) and the number of repeats in \texttt{gafsControl()}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{tic}\NormalTok{()}
\CommentTok{# Define control function}
\NormalTok{ga_ctrl <-}\StringTok{ }\KeywordTok{gafsControl}\NormalTok{(}\DataTypeTok{functions =}\NormalTok{ rfGA,  }\CommentTok{# another option is `caretGA`.}
                        \DataTypeTok{method =} \StringTok{"cv"}\NormalTok{,}
                        \DataTypeTok{repeats =} \DecValTok{3}\NormalTok{)}

\CommentTok{# Genetic Algorithm feature selection}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{100}\NormalTok{)}
\NormalTok{ga_obj <-}\StringTok{ }\KeywordTok{gafs}\NormalTok{(}\DataTypeTok{x=}\NormalTok{trainData[, }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{3}\NormalTok{, }\DecValTok{5}\OperatorTok{:}\DecValTok{13}\NormalTok{)], }
               \DataTypeTok{y=}\NormalTok{trainData[, }\DecValTok{4}\NormalTok{], }
               \DataTypeTok{iters =} \DecValTok{3}\NormalTok{,   }\CommentTok{# normally much higher (100+)}
               \DataTypeTok{gafsControl =}\NormalTok{ ga_ctrl)}
\KeywordTok{toc}\NormalTok{()}
\CommentTok{#> 636.08 sec elapsed}
\NormalTok{ga_obj}
\CommentTok{#> }
\CommentTok{#> Genetic Algorithm Feature Selection}
\CommentTok{#> }
\CommentTok{#> 366 samples}
\CommentTok{#> 12 predictors}
\CommentTok{#> }
\CommentTok{#> Maximum generations: 3 }
\CommentTok{#> Population per generation: 50 }
\CommentTok{#> Crossover probability: 0.8 }
\CommentTok{#> Mutation probability: 0.1 }
\CommentTok{#> Elitism: 0 }
\CommentTok{#> }
\CommentTok{#> Internal performance values: RMSE, Rsquared}
\CommentTok{#> Subset selection driven to minimize internal RMSE }
\CommentTok{#> }
\CommentTok{#> External performance values: RMSE, Rsquared, MAE}
\CommentTok{#> Best iteration chose by minimizing external RMSE }
\CommentTok{#> External resampling method: Cross-Validated (10 fold) }
\CommentTok{#> }
\CommentTok{#> During resampling:}
\CommentTok{#>   * the top 5 selected variables (out of a possible 12):}
\CommentTok{#>     Month (100%), Pressure_gradient (100%), Temperature_ElMonte (100%), Humidity (80%), Visibility (80%)}
\CommentTok{#>   * on average, 6.8 variables were selected (min = 5, max = 9)}
\CommentTok{#> }
\CommentTok{#> In the final search using the entire training set:}
\CommentTok{#>    * 9 features selected at iteration 2 including:}
\CommentTok{#>      Month, Day_of_month, pressure_height, Wind_speed, Humidity ... }
\CommentTok{#>    * external performance at this iteration is}
\CommentTok{#> }
\CommentTok{#>       RMSE   Rsquared        MAE }
\CommentTok{#>      3.721      0.788      2.800}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Optimal variables}
\NormalTok{ga_obj}\OperatorTok{$}\NormalTok{optVariables}
\CommentTok{#> [1] "Month"                 "Day_of_month"          "pressure_height"      }
\CommentTok{#> [4] "Wind_speed"            "Humidity"              "Temperature_ElMonte"  }
\CommentTok{#> [7] "Inversion_base_height" "Pressure_gradient"     "Inversion_temperature"}
\end{Highlighting}
\end{Shaded}

`Month'
`Day\_of\_month'
`Wind\_speed'
`Temperature\_ElMonte'
`Pressure\_gradient'
`Visibility'

So the optimal variables according to the genetic algorithms are listed above. But, I wouldn't use it just yet because, the above variant was tuned for only 3 iterations, which is quite low. I had to set it so low to save computing time.

\hypertarget{simulated-annealing}{%
\section{Simulated Annealing}\label{simulated-annealing}}

Simulated annealing is a global search algorithm that allows a suboptimal solution to be accepted in hope that a better solution will show up eventually.

It works by making small random changes to an initial solution and sees if the performance improved. The change is accepted if it improves, else it can still be accepted if the difference of performances meet an acceptance criteria.

In caret it has been implemented in the \texttt{safs()} which accepts a control parameter that can be set using the \texttt{safsControl()} function.

\texttt{safsControl} is similar to other control functions in caret (like you saw in rfe and ga), and additionally it accepts an improve parameter which is the number of iterations it should wait without improvement until the values are reset to previous iteration.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{tic}\NormalTok{()}
\CommentTok{# Define control function}
\NormalTok{sa_ctrl <-}\StringTok{ }\KeywordTok{safsControl}\NormalTok{(}\DataTypeTok{functions =}\NormalTok{ rfSA,}
                        \DataTypeTok{method =} \StringTok{"repeatedcv"}\NormalTok{,}
                        \DataTypeTok{repeats =} \DecValTok{3}\NormalTok{,}
                        \DataTypeTok{improve =} \DecValTok{5}\NormalTok{) }\CommentTok{# n iterations without improvement before a reset}

\CommentTok{# Genetic Algorithm feature selection}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{100}\NormalTok{)}
\NormalTok{sa_obj <-}\StringTok{ }\KeywordTok{safs}\NormalTok{(}\DataTypeTok{x=}\NormalTok{trainData[, }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{3}\NormalTok{, }\DecValTok{5}\OperatorTok{:}\DecValTok{13}\NormalTok{)], }
               \DataTypeTok{y=}\NormalTok{trainData[, }\DecValTok{4}\NormalTok{],}
               \DataTypeTok{safsControl =}\NormalTok{ sa_ctrl)}
\KeywordTok{toc}\NormalTok{()}
\CommentTok{#> 109.61 sec elapsed}
\NormalTok{sa_obj}
\CommentTok{#> }
\CommentTok{#> Simulated Annealing Feature Selection}
\CommentTok{#> }
\CommentTok{#> 366 samples}
\CommentTok{#> 12 predictors}
\CommentTok{#> }
\CommentTok{#> Maximum search iterations: 10 }
\CommentTok{#> Restart after 5 iterations without improvement (0.3 restarts on average)}
\CommentTok{#> }
\CommentTok{#> Internal performance values: RMSE, Rsquared}
\CommentTok{#> Subset selection driven to minimize internal RMSE }
\CommentTok{#> }
\CommentTok{#> External performance values: RMSE, Rsquared, MAE}
\CommentTok{#> Best iteration chose by minimizing external RMSE }
\CommentTok{#> External resampling method: Cross-Validated (10 fold, repeated 3 times) }
\CommentTok{#> }
\CommentTok{#> During resampling:}
\CommentTok{#>   * the top 5 selected variables (out of a possible 12):}
\CommentTok{#>     Temperature_Sandburg (80%), Month (66.7%), Pressure_gradient (66.7%), Temperature_ElMonte (63.3%), Visibility (60%)}
\CommentTok{#>   * on average, 6.5 variables were selected (min = 3, max = 11)}
\CommentTok{#> }
\CommentTok{#> In the final search using the entire training set:}
\CommentTok{#>    * 6 features selected at iteration 9 including:}
\CommentTok{#>      Day_of_week, pressure_height, Wind_speed, Humidity, Inversion_base_height ... }
\CommentTok{#>    * external performance at this iteration is}
\CommentTok{#> }
\CommentTok{#>       RMSE   Rsquared        MAE }
\CommentTok{#>      4.108      0.743      3.111}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Optimal variables}
\KeywordTok{print}\NormalTok{(sa_obj}\OperatorTok{$}\NormalTok{optVariables)}
\CommentTok{#> [1] "Day_of_week"           "pressure_height"       "Wind_speed"           }
\CommentTok{#> [4] "Humidity"              "Inversion_base_height" "Pressure_gradient"}
\end{Highlighting}
\end{Shaded}

\hypertarget{information-value-and-weights-of-evidence}{%
\section{Information Value and Weights of Evidence}\label{information-value-and-weights-of-evidence}}

The Information Value can be used to judge how important a given categorical variable is in explaining the binary Y variable. It goes well with logistic regression and other classification models that can model binary variables.

Let's try to find out how important the categorical variables are in predicting if an individual will earn \textgreater{} 50k from the \texttt{adult.csv} dataset. Just run the code below to import the dataset.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(InformationValue)}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'InformationValue'}
\CommentTok{#> The following objects are masked from 'package:caret':}
\CommentTok{#> }
\CommentTok{#>     confusionMatrix, precision, sensitivity, specificity}

\CommentTok{# online data}
\CommentTok{# inputData <- read.csv("http://rstatistics.net/wp-content/uploads/2015/09/adult.csv")}

\NormalTok{inputData <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\KeywordTok{file.path}\NormalTok{(data_raw_dir, }\StringTok{"adult.csv"}\NormalTok{))}
\KeywordTok{print}\NormalTok{(}\KeywordTok{head}\NormalTok{(inputData))}
\CommentTok{#>   AGE        WORKCLASS FNLWGT EDUCATION EDUCATIONNUM      MARITALSTATUS}
\CommentTok{#> 1  39        State-gov  77516 Bachelors           13      Never-married}
\CommentTok{#> 2  50 Self-emp-not-inc  83311 Bachelors           13 Married-civ-spouse}
\CommentTok{#> 3  38          Private 215646   HS-grad            9           Divorced}
\CommentTok{#> 4  53          Private 234721      11th            7 Married-civ-spouse}
\CommentTok{#> 5  28          Private 338409 Bachelors           13 Married-civ-spouse}
\CommentTok{#> 6  37          Private 284582   Masters           14 Married-civ-spouse}
\CommentTok{#>          OCCUPATION  RELATIONSHIP  RACE    SEX CAPITALGAIN CAPITALLOSS}
\CommentTok{#> 1      Adm-clerical Not-in-family White   Male        2174           0}
\CommentTok{#> 2   Exec-managerial       Husband White   Male           0           0}
\CommentTok{#> 3 Handlers-cleaners Not-in-family White   Male           0           0}
\CommentTok{#> 4 Handlers-cleaners       Husband Black   Male           0           0}
\CommentTok{#> 5    Prof-specialty          Wife Black Female           0           0}
\CommentTok{#> 6   Exec-managerial          Wife White Female           0           0}
\CommentTok{#>   HOURSPERWEEK NATIVECOUNTRY ABOVE50K}
\CommentTok{#> 1           40 United-States        0}
\CommentTok{#> 2           13 United-States        0}
\CommentTok{#> 3           40 United-States        0}
\CommentTok{#> 4           40 United-States        0}
\CommentTok{#> 5           40          Cuba        0}
\CommentTok{#> 6           40 United-States        0}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Choose Categorical Variables to compute Info Value.}
\NormalTok{cat_vars <-}\StringTok{ }\KeywordTok{c}\NormalTok{ (}\StringTok{"WORKCLASS"}\NormalTok{, }\StringTok{"EDUCATION"}\NormalTok{, }\StringTok{"MARITALSTATUS"}\NormalTok{, }\StringTok{"OCCUPATION"}\NormalTok{, }\StringTok{"RELATIONSHIP"}\NormalTok{, }\StringTok{"RACE"}\NormalTok{, }\StringTok{"SEX"}\NormalTok{, }\StringTok{"NATIVECOUNTRY"}\NormalTok{)  }\CommentTok{# get all categorical variables}

\NormalTok{factor_vars <-}\StringTok{ }\NormalTok{cat_vars}


\CommentTok{# Init Output}
\NormalTok{df_iv <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{VARS=}\NormalTok{cat_vars, }\DataTypeTok{IV=}\KeywordTok{numeric}\NormalTok{(}\KeywordTok{length}\NormalTok{(cat_vars)), }\DataTypeTok{STRENGTH=}\KeywordTok{character}\NormalTok{(}\KeywordTok{length}\NormalTok{(cat_vars)), }\DataTypeTok{stringsAsFactors =}\NormalTok{ F)  }\CommentTok{# init output dataframe}

\CommentTok{# Get Information Value for each variable}
\ControlFlowTok{for}\NormalTok{ (factor_var }\ControlFlowTok{in}\NormalTok{ factor_vars)\{}
\NormalTok{  df_iv[df_iv}\OperatorTok{$}\NormalTok{VARS }\OperatorTok{==}\StringTok{ }\NormalTok{factor_var, }\StringTok{"IV"}\NormalTok{] <-}\StringTok{ }\NormalTok{InformationValue}\OperatorTok{::}\KeywordTok{IV}\NormalTok{(}\DataTypeTok{X=}\NormalTok{inputData[, factor_var], }\DataTypeTok{Y=}\NormalTok{inputData}\OperatorTok{$}\NormalTok{ABOVE50K)}
\NormalTok{  df_iv[df_iv}\OperatorTok{$}\NormalTok{VARS }\OperatorTok{==}\StringTok{ }\NormalTok{factor_var, }\StringTok{"STRENGTH"}\NormalTok{] <-}\StringTok{ }\KeywordTok{attr}\NormalTok{(InformationValue}\OperatorTok{::}\KeywordTok{IV}\NormalTok{(}\DataTypeTok{X=}\NormalTok{inputData[, factor_var], }\DataTypeTok{Y=}\NormalTok{inputData}\OperatorTok{$}\NormalTok{ABOVE50K), }\StringTok{"howgood"}\NormalTok{)}
\NormalTok{\}}

\CommentTok{# Sort}
\NormalTok{df_iv <-}\StringTok{ }\NormalTok{df_iv[}\KeywordTok{order}\NormalTok{(}\OperatorTok{-}\NormalTok{df_iv}\OperatorTok{$}\NormalTok{IV), ]}

\NormalTok{df_iv}
\CommentTok{#>            VARS     IV            STRENGTH}
\CommentTok{#> 5  RELATIONSHIP 1.5356   Highly Predictive}
\CommentTok{#> 3 MARITALSTATUS 1.3388   Highly Predictive}
\CommentTok{#> 4    OCCUPATION 0.7762   Highly Predictive}
\CommentTok{#> 2     EDUCATION 0.7411   Highly Predictive}
\CommentTok{#> 7           SEX 0.3033   Highly Predictive}
\CommentTok{#> 1     WORKCLASS 0.1634   Highly Predictive}
\CommentTok{#> 8 NATIVECOUNTRY 0.0794 Somewhat Predictive}
\CommentTok{#> 6          RACE 0.0693 Somewhat Predictive}
\end{Highlighting}
\end{Shaded}

Here is what the quantum of Information Value means:

Less than 0.02, then the predictor is not useful for modeling (separating the Goods from the Bads)

0.02 to 0.1, then the predictor has only a weak relationship.
0.1 to 0.3, then the predictor has a medium strength relationship.
0.3 or higher, then the predictor has a strong relationship.
That was about IV. Then what is Weight of Evidence?

Weights of evidence can be useful to find out how important a given categorical variable is in explaining the `events' (called `Goods' in below table.)

The `Information Value' of the categorical variable can then be derived from the respective WOE values.

IV=(perc good of all goodsperc bad of all bads) *WOE

The `WOETable' below given the computation in more detail.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{WOETable}\NormalTok{(}\DataTypeTok{X=}\NormalTok{inputData[, }\StringTok{'WORKCLASS'}\NormalTok{], }\DataTypeTok{Y=}\NormalTok{inputData}\OperatorTok{$}\NormalTok{ABOVE50K)}
\CommentTok{#>                CAT GOODS  BADS TOTAL   PCT_G    PCT_B    WOE       IV}
\CommentTok{#> 1                ?   191  1645  1836 0.02429 0.066545 -1.008 0.042574}
\CommentTok{#> 2      Federal-gov   371   589   960 0.04719 0.023827  0.683 0.015964}
\CommentTok{#> 3        Local-gov   617  1476  2093 0.07848 0.059709  0.273 0.005131}
\CommentTok{#> 4     Never-worked     7     7     7 0.00089 0.000283  1.146 0.000696}
\CommentTok{#> 5          Private  4963 17733 22696 0.63126 0.717354 -0.128 0.011006}
\CommentTok{#> 6     Self-emp-inc   622   494  1116 0.07911 0.019984  1.376 0.081363}
\CommentTok{#> 7 Self-emp-not-inc   724  1817  2541 0.09209 0.073503  0.225 0.004190}
\CommentTok{#> 8        State-gov   353   945  1298 0.04490 0.038228  0.161 0.001073}
\CommentTok{#> 9      Without-pay    14    14    14 0.00178 0.000566  1.146 0.001391}
\end{Highlighting}
\end{Shaded}

The total IV of a variable is the sum of IV's of its categories.

\hypertarget{dalex-package}{%
\section{DALEX Package}\label{dalex-package}}

The \texttt{DALEX} is a powerful package that explains various things about the variables used in an ML model.

For example, using the \texttt{variable\_dropout()} function you can find out how important a variable is based on a dropout loss, that is how much loss is incurred by removing a variable from the model.

Apart from this, it also has the \texttt{single\_variable()} function that gives you an idea of how the model's output will change by changing the values of one of the X's in the model.

It also has the \texttt{single\_prediction()} that can decompose a single model prediction so as to understand which variable caused what effect in predicting the value of Y.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(randomForest)}
\CommentTok{#> randomForest 4.6-14}
\CommentTok{#> Type rfNews() to see new features/changes/bug fixes.}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'randomForest'}
\CommentTok{#> The following object is masked from 'package:dplyr':}
\CommentTok{#> }
\CommentTok{#>     combine}
\CommentTok{#> The following object is masked from 'package:ranger':}
\CommentTok{#> }
\CommentTok{#>     importance}
\CommentTok{#> The following object is masked from 'package:ggplot2':}
\CommentTok{#> }
\CommentTok{#>     margin}
\KeywordTok{library}\NormalTok{(DALEX)}
\CommentTok{#> Welcome to DALEX (version: 0.3.0).}
\CommentTok{#> This is a plain DALEX. Use 'install_dependencies()' to get all required packages.}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'DALEX'}
\CommentTok{#> The following object is masked from 'package:dplyr':}
\CommentTok{#> }
\CommentTok{#>     explain}

\CommentTok{# Load data}
\CommentTok{# inputData <- read.csv("http://rstatistics.net/wp-content/uploads/2015/09/adult.csv")}

\NormalTok{inputData <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\KeywordTok{file.path}\NormalTok{(data_raw_dir, }\StringTok{"adult.csv"}\NormalTok{))}

\CommentTok{# Train random forest model}
\NormalTok{rf_mod <-}\StringTok{ }\KeywordTok{randomForest}\NormalTok{(}\KeywordTok{factor}\NormalTok{(ABOVE50K) }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data=}\NormalTok{inputData, }\DataTypeTok{ntree=}\DecValTok{100}\NormalTok{)}
\NormalTok{rf_mod}
\CommentTok{#> }
\CommentTok{#> Call:}
\CommentTok{#>  randomForest(formula = factor(ABOVE50K) ~ ., data = inputData,      ntree = 100) }
\CommentTok{#>                Type of random forest: classification}
\CommentTok{#>                      Number of trees: 100}
\CommentTok{#> No. of variables tried at each split: 3}
\CommentTok{#> }
\CommentTok{#>         OOB estimate of  error rate: 13.6%}
\CommentTok{#> Confusion matrix:}
\CommentTok{#>       0    1 class.error}
\CommentTok{#> 0 23051 1669      0.0675}
\CommentTok{#> 1  2754 5087      0.3512}

\CommentTok{# Variable importance with DALEX}
\NormalTok{explained_rf <-}\StringTok{ }\KeywordTok{explain}\NormalTok{(rf_mod, }\DataTypeTok{data=}\NormalTok{inputData, }\DataTypeTok{y=}\NormalTok{inputData}\OperatorTok{$}\NormalTok{ABOVE50K)}

\CommentTok{# Get the variable importances}
\NormalTok{varimps =}\StringTok{ }\KeywordTok{variable_dropout}\NormalTok{(explained_rf, }\DataTypeTok{type=}\StringTok{'raw'}\NormalTok{)}

\KeywordTok{print}\NormalTok{(varimps)}
\CommentTok{#>         variable dropout_loss        label}
\CommentTok{#> 1   _full_model_         31.6 randomForest}
\CommentTok{#> 2       ABOVE50K         31.6 randomForest}
\CommentTok{#> 3           RACE         36.6 randomForest}
\CommentTok{#> 4            SEX         39.4 randomForest}
\CommentTok{#> 5    CAPITALLOSS         39.9 randomForest}
\CommentTok{#> 6  NATIVECOUNTRY         40.3 randomForest}
\CommentTok{#> 7      WORKCLASS         51.0 randomForest}
\CommentTok{#> 8    CAPITALGAIN         53.8 randomForest}
\CommentTok{#> 9         FNLWGT         56.2 randomForest}
\CommentTok{#> 10  HOURSPERWEEK         56.7 randomForest}
\CommentTok{#> 11     EDUCATION         58.0 randomForest}
\CommentTok{#> 12  RELATIONSHIP         58.5 randomForest}
\CommentTok{#> 13  EDUCATIONNUM         59.2 randomForest}
\CommentTok{#> 14 MARITALSTATUS         71.0 randomForest}
\CommentTok{#> 15    OCCUPATION         83.1 randomForest}
\CommentTok{#> 16           AGE         86.8 randomForest}
\CommentTok{#> 17    _baseline_        304.4 randomForest}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(varimps)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{fe-meta_133-variable_importance_files/figure-latex/unnamed-chunk-15-1} \end{center}

\hypertarget{conclusion}{%
\section{Conclusion}\label{conclusion}}

Hope you find these methods useful. As it turns out different methods showed different variables as important, or at least the degree of importance changed. This need not be a conflict, because each method gives a different perspective of how the variable can be useful depending on how the algorithms learn \texttt{Y\ \textasciitilde{}\ x}. So its cool.

If you find any code breaks or bugs, report the issue here or just write it below.

\hypertarget{imputting-missing-values-with-random-forest}{%
\chapter{Imputting missing values with Random Forest}\label{imputting-missing-values-with-random-forest}}

\hypertarget{flu-prediction.-fluh7n9_china_2013-dataset}{%
\section{\texorpdfstring{Flu Prediction. \texttt{fluH7N9\_china\_2013} dataset}{Flu Prediction. fluH7N9\_china\_2013 dataset}}\label{flu-prediction.-fluh7n9_china_2013-dataset}}

Source: \url{https://shirinsplayground.netlify.com/2018/04/flu_prediction/}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(outbreaks)}
\KeywordTok{library}\NormalTok{(tidyverse)}
\KeywordTok{library}\NormalTok{(plyr)}
\KeywordTok{library}\NormalTok{(mice)}
\KeywordTok{library}\NormalTok{(caret)}
\KeywordTok{library}\NormalTok{(purrr)}
\KeywordTok{library}\NormalTok{(}\StringTok{"tibble"}\NormalTok{)}
\KeywordTok{library}\NormalTok{(}\StringTok{"dplyr"}\NormalTok{)}
\KeywordTok{library}\NormalTok{(}\StringTok{"tidyr"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Since I migrated my blog from Github Pages to blogdown and Netlify, I wanted to start migrating (most of) my old posts too - and use that opportunity to update them and make sure the code still works.

Here I am updating my very first machine learning post from 27 Nov 2016: Can we predict flu deaths with Machine Learning and R?. Changes are marked as bold comments.

The main changes I made are:

\begin{itemize}
\item
  using the tidyverse more consistently throughout the analysis
\item
  focusing on comparing multiple imputations from the mice package, rather than comparing different algorithms
\item
  using purrr, map(), nest() and unnest() to model and predict the machine learning algorithm over the different imputed datasets
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

Among the many nice R packages containing data collections is the \texttt{outbreaks} package. It contains a dataset on epidemics and among them is data from the 2013 outbreak of influenza A H7N9 in China as analysed by Kucharski et al. (2014):

\begin{quote}
A. Kucharski, H. Mills, A. Pinsent, C. Fraser, M. Van Kerkhove, C. A. Donnelly, and S. Riley. 2014. Distinguishing between reservoir exposure and human-to-human transmission for emerging pathogens using case onset data. PLOS Currents Outbreaks. Mar 7, edition 1. doi: 10.1371/currents.outbreaks.e1473d9bfc99d080ca242139a06c455f.
\end{quote}

\begin{quote}
A. Kucharski, H. Mills, A. Pinsent, C. Fraser, M. Van Kerkhove, C. A. Donnelly, and S. Riley. 2014. Data from: Distinguishing between reservoir exposure and human-to-human transmission for emerging pathogens using case onset data. Dryad Digital Repository. \url{http://dx.doi.org/10.5061/dryad.2g43n}.
\end{quote}

I will be using their data as an example to show how to use Machine Learning algorithms for predicting disease outcome.

\hypertarget{the-data-1}{%
\section{The data}\label{the-data-1}}

The dataset contains case ID, date of onset, date of hospitalization, date of outcome, gender, age, province and of course outcome: Death or Recovery.

\hypertarget{pre-processing}{%
\subsection{Pre-processing}\label{pre-processing}}

Change: variable names (i.e.~column names) have been renamed, dots have been replaced with underscores, letters are all lower case now.

Change: I am using the tidyverse notation more consistently.

First, I'm doing some preprocessing, including:

\begin{itemize}
\tightlist
\item
  renaming missing data as NA
\item
  adding an ID column
\item
  setting column types
\item
  gathering date columns
\item
  changing factor names of dates (to make them look nicer in plots) and of province (to combine provinces with few cases)
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{from1 <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"date_of_onset"}\NormalTok{, }\StringTok{"date_of_hospitalisation"}\NormalTok{, }\StringTok{"date_of_outcome"}\NormalTok{)}
\NormalTok{to1   <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"date of onset"}\NormalTok{, }\StringTok{"date of hospitalisation"}\NormalTok{, }\StringTok{"date of outcome"}\NormalTok{)}
\NormalTok{from2 <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Anhui"}\NormalTok{, }\StringTok{"Beijing"}\NormalTok{, }\StringTok{"Fujian"}\NormalTok{, }\StringTok{"Guangdong"}\NormalTok{, }\StringTok{"Hebei"}\NormalTok{, }\StringTok{"Henan"}\NormalTok{, }
           \StringTok{"Hunan"}\NormalTok{, }\StringTok{"Jiangxi"}\NormalTok{, }\StringTok{"Shandong"}\NormalTok{, }\StringTok{"Taiwan"}\NormalTok{)}
\NormalTok{to2   <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\StringTok{"Other"}\NormalTok{, }\DecValTok{10}\NormalTok{)}

\NormalTok{fluH7N9_china_}\DecValTok{2013}\OperatorTok{$}\NormalTok{age[}\KeywordTok{which}\NormalTok{(fluH7N9_china_}\DecValTok{2013}\OperatorTok{$}\NormalTok{age }\OperatorTok{==}\StringTok{ "?"}\NormalTok{)] <-}\StringTok{ }\OtherTok{NA}
\NormalTok{fluH7N9_china_}\DecValTok{2013}\NormalTok{_gather <-}\StringTok{ }\NormalTok{fluH7N9_china_}\DecValTok{2013} \OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{case_id =} \KeywordTok{paste}\NormalTok{(}\StringTok{"case"}\NormalTok{, case_id, }\DataTypeTok{sep =} \StringTok{"_"}\NormalTok{),}
         \DataTypeTok{age =} \KeywordTok{as.numeric}\NormalTok{(age)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{gather}\NormalTok{(Group, Date, date_of_onset}\OperatorTok{:}\NormalTok{date_of_outcome) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{Group =} \KeywordTok{as.factor}\NormalTok{(}\KeywordTok{mapvalues}\NormalTok{(Group, }\DataTypeTok{from =}\NormalTok{ from1, }\DataTypeTok{to =}\NormalTok{ to1)),}
         \DataTypeTok{province =} \KeywordTok{mapvalues}\NormalTok{(province, }\DataTypeTok{from =}\NormalTok{ from2, }\DataTypeTok{to =}\NormalTok{ to2))}

\NormalTok{fluH7N9_china_}\DecValTok{2013}\NormalTok{ <-}\StringTok{ }\KeywordTok{as.tibble}\NormalTok{(fluH7N9_china_}\DecValTok{2013}\NormalTok{)}
\CommentTok{#> Warning: `as.tibble()` is deprecated, use `as_tibble()` (but mind the new semantics).}
\CommentTok{#> This warning is displayed once per session.}
\NormalTok{fluH7N9_china_}\DecValTok{2013}\NormalTok{_gather <-}\StringTok{ }\KeywordTok{as.tibble}\NormalTok{(fluH7N9_china_}\DecValTok{2013}\NormalTok{_gather)}
\KeywordTok{print}\NormalTok{(fluH7N9_china_}\DecValTok{2013}\NormalTok{)}
\CommentTok{#> # A tibble: 136 x 8}
\CommentTok{#>   case_id date_of_onset date_of_hospita~ date_of_outcome outcome gender}
\CommentTok{#>   <fct>   <date>        <date>           <date>          <fct>   <fct> }
\CommentTok{#> 1 1       2013-02-19    NA               2013-03-04      Death   m     }
\CommentTok{#> 2 2       2013-02-27    2013-03-03       2013-03-10      Death   m     }
\CommentTok{#> 3 3       2013-03-09    2013-03-19       2013-04-09      Death   f     }
\CommentTok{#> 4 4       2013-03-19    2013-03-27       NA              <NA>    f     }
\CommentTok{#> 5 5       2013-03-19    2013-03-30       2013-05-15      Recover f     }
\CommentTok{#> 6 6       2013-03-21    2013-03-28       2013-04-26      Death   f     }
\CommentTok{#> # ... with 130 more rows, and 2 more variables: age <fct>, province <fct>}
\end{Highlighting}
\end{Shaded}

I'm also adding a third gender level for unknown gender

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{levels}\NormalTok{(fluH7N9_china_}\DecValTok{2013}\NormalTok{_gather}\OperatorTok{$}\NormalTok{gender) <-}\StringTok{ }
\StringTok{  }\KeywordTok{c}\NormalTok{(}\KeywordTok{levels}\NormalTok{(fluH7N9_china_}\DecValTok{2013}\NormalTok{_gather}\OperatorTok{$}\NormalTok{gender), }\StringTok{"unknown"}\NormalTok{)}
\NormalTok{fluH7N9_china_}\DecValTok{2013}\NormalTok{_gather}\OperatorTok{$}\NormalTok{gender[}\KeywordTok{is.na}\NormalTok{(fluH7N9_china_}\DecValTok{2013}\NormalTok{_gather}\OperatorTok{$}\NormalTok{gender)] <-}\StringTok{ "unknown"}
\KeywordTok{print}\NormalTok{(fluH7N9_china_}\DecValTok{2013}\NormalTok{_gather)}
\CommentTok{#> # A tibble: 408 x 7}
\CommentTok{#>   case_id outcome gender   age province Group         Date      }
\CommentTok{#>   <chr>   <fct>   <fct>  <dbl> <fct>    <fct>         <date>    }
\CommentTok{#> 1 case_1  Death   m         58 Shanghai date of onset 2013-02-19}
\CommentTok{#> 2 case_2  Death   m          7 Shanghai date of onset 2013-02-27}
\CommentTok{#> 3 case_3  Death   f         11 Other    date of onset 2013-03-09}
\CommentTok{#> 4 case_4  <NA>    f         18 Jiangsu  date of onset 2013-03-19}
\CommentTok{#> 5 case_5  Recover f         20 Jiangsu  date of onset 2013-03-19}
\CommentTok{#> 6 case_6  Death   f          9 Jiangsu  date of onset 2013-03-21}
\CommentTok{#> # ... with 402 more rows}
\end{Highlighting}
\end{Shaded}

For plotting, I am defining a custom ggplot2 theme:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my_theme <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(}\DataTypeTok{base_size =} \DecValTok{12}\NormalTok{, }\DataTypeTok{base_family =} \StringTok{"sans"}\NormalTok{)\{}
  \KeywordTok{theme_minimal}\NormalTok{(}\DataTypeTok{base_size =}\NormalTok{ base_size, }\DataTypeTok{base_family =}\NormalTok{ base_family) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}
    \DataTypeTok{axis.text =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{size =} \DecValTok{12}\NormalTok{),}
    \DataTypeTok{axis.text.x =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{angle =} \DecValTok{45}\NormalTok{, }\DataTypeTok{vjust =} \FloatTok{0.5}\NormalTok{, }\DataTypeTok{hjust =} \FloatTok{0.5}\NormalTok{),}
    \DataTypeTok{axis.title =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{size =} \DecValTok{14}\NormalTok{),}
    \DataTypeTok{panel.grid.major =} \KeywordTok{element_line}\NormalTok{(}\DataTypeTok{color =} \StringTok{"grey"}\NormalTok{),}
    \DataTypeTok{panel.grid.minor =} \KeywordTok{element_blank}\NormalTok{(),}
    \DataTypeTok{panel.background =} \KeywordTok{element_rect}\NormalTok{(}\DataTypeTok{fill =} \StringTok{"aliceblue"}\NormalTok{),}
    \DataTypeTok{strip.background =} \KeywordTok{element_rect}\NormalTok{(}\DataTypeTok{fill =} \StringTok{"lightgrey"}\NormalTok{, }\DataTypeTok{color =} \StringTok{"grey"}\NormalTok{, }\DataTypeTok{size =} \DecValTok{1}\NormalTok{),}
    \DataTypeTok{strip.text =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{face =} \StringTok{"bold"}\NormalTok{, }\DataTypeTok{size =} \DecValTok{12}\NormalTok{, }\DataTypeTok{color =} \StringTok{"black"}\NormalTok{),}
    \DataTypeTok{legend.position =} \StringTok{"bottom"}\NormalTok{,}
    \DataTypeTok{legend.justification =} \StringTok{"top"}\NormalTok{, }
    \DataTypeTok{legend.box =} \StringTok{"horizontal"}\NormalTok{,}
    \DataTypeTok{legend.box.background =} \KeywordTok{element_rect}\NormalTok{(}\DataTypeTok{colour =} \StringTok{"grey50"}\NormalTok{),}
    \DataTypeTok{legend.background =} \KeywordTok{element_blank}\NormalTok{(),}
    \DataTypeTok{panel.border =} \KeywordTok{element_rect}\NormalTok{(}\DataTypeTok{color =} \StringTok{"grey"}\NormalTok{, }\DataTypeTok{fill =} \OtherTok{NA}\NormalTok{, }\DataTypeTok{size =} \FloatTok{0.5}\NormalTok{)}
\NormalTok{  )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

And use that theme to visualize the data:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ fluH7N9_china_}\DecValTok{2013}\NormalTok{_gather, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Date, }\DataTypeTok{y =}\NormalTok{ age, }\DataTypeTok{fill =}\NormalTok{ outcome)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{stat_density2d}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{alpha =}\NormalTok{ ..level..), }\DataTypeTok{geom =} \StringTok{"polygon"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_jitter}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{color =}\NormalTok{ outcome, }\DataTypeTok{shape =}\NormalTok{ gender), }\DataTypeTok{size =} \FloatTok{1.5}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_rug}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{color =}\NormalTok{ outcome)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_y_continuous}\NormalTok{(}\DataTypeTok{limits =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{90}\NormalTok{)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}
    \DataTypeTok{fill =} \StringTok{"Outcome"}\NormalTok{,}
    \DataTypeTok{color =} \StringTok{"Outcome"}\NormalTok{,}
    \DataTypeTok{alpha =} \StringTok{"Level"}\NormalTok{,}
    \DataTypeTok{shape =} \StringTok{"Gender"}\NormalTok{,}
    \DataTypeTok{x =} \StringTok{"Date in 2013"}\NormalTok{,}
    \DataTypeTok{y =} \StringTok{"Age"}\NormalTok{,}
    \DataTypeTok{title =} \StringTok{"2013 Influenza A H7N9 cases in China"}\NormalTok{,}
    \DataTypeTok{subtitle =} \StringTok{"Dataset from 'outbreaks' package (Kucharski et al. 2014)"}\NormalTok{,}
    \DataTypeTok{caption =} \StringTok{""}
\NormalTok{  ) }\OperatorTok{+}
\StringTok{  }\KeywordTok{facet_grid}\NormalTok{(Group }\OperatorTok{~}\StringTok{ }\NormalTok{province) }\OperatorTok{+}
\StringTok{  }\KeywordTok{my_theme}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_shape_manual}\NormalTok{(}\DataTypeTok{values =} \KeywordTok{c}\NormalTok{(}\DecValTok{15}\NormalTok{, }\DecValTok{16}\NormalTok{, }\DecValTok{17}\NormalTok{)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_color_brewer}\NormalTok{(}\DataTypeTok{palette=}\StringTok{"Set1"}\NormalTok{, }\DataTypeTok{na.value =} \StringTok{"grey50"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_fill_brewer}\NormalTok{(}\DataTypeTok{palette=}\StringTok{"Set1"}\NormalTok{)}
\CommentTok{#> Warning: Removed 149 rows containing non-finite values (stat_density2d).}
\CommentTok{#> Warning: Removed 149 rows containing missing values (geom_point).}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{fe-meta_230-missing_flu_prediction_micex5-sglander_files/figure-latex/unnamed-chunk-6-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ fluH7N9_china_}\DecValTok{2013}\NormalTok{_gather, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Date, }\DataTypeTok{y =}\NormalTok{ age, }\DataTypeTok{color =}\NormalTok{ outcome)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{color =}\NormalTok{ outcome, }\DataTypeTok{shape =}\NormalTok{ gender), }\DataTypeTok{size =} \FloatTok{1.5}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{0.6}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_path}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{group =}\NormalTok{ case_id)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{facet_wrap}\NormalTok{( }\OperatorTok{~}\StringTok{ }\NormalTok{province, }\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{my_theme}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_shape_manual}\NormalTok{(}\DataTypeTok{values =} \KeywordTok{c}\NormalTok{(}\DecValTok{15}\NormalTok{, }\DecValTok{16}\NormalTok{, }\DecValTok{17}\NormalTok{)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_color_brewer}\NormalTok{(}\DataTypeTok{palette=}\StringTok{"Set1"}\NormalTok{, }\DataTypeTok{na.value =} \StringTok{"grey50"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_fill_brewer}\NormalTok{(}\DataTypeTok{palette=}\StringTok{"Set1"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}
    \DataTypeTok{color =} \StringTok{"Outcome"}\NormalTok{,}
    \DataTypeTok{shape =} \StringTok{"Gender"}\NormalTok{,}
    \DataTypeTok{x =} \StringTok{"Date in 2013"}\NormalTok{,}
    \DataTypeTok{y =} \StringTok{"Age"}\NormalTok{,}
    \DataTypeTok{title =} \StringTok{"2013 Influenza A H7N9 cases in China"}\NormalTok{,}
    \DataTypeTok{subtitle =} \StringTok{"Dataset from 'outbreaks' package (Kucharski et al. 2014)"}\NormalTok{,}
    \DataTypeTok{caption =} \StringTok{"}\CharTok{\textbackslash{}n}\StringTok{Time from onset of flu to outcome."}
\NormalTok{  )}
\CommentTok{#> Warning: Removed 149 rows containing missing values (geom_point).}
\CommentTok{#> Warning: Removed 122 rows containing missing values (geom_path).}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{fe-meta_230-missing_flu_prediction_micex5-sglander_files/figure-latex/unnamed-chunk-7-1} \end{center}

\hypertarget{features-1}{%
\section{Features}\label{features-1}}

In machine learning-speak features are what we call the variables used for model training. Using the right features dramatically influences the accuracy and success of your model.

For this example, I am keeping age, but I am also generating new features from the date information and converting gender and province into numerical values.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{delta_dates <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(onset, ref) \{}
\NormalTok{    d2 =}\StringTok{ }\KeywordTok{as.Date}\NormalTok{(}\KeywordTok{as.character}\NormalTok{(onset), }\DataTypeTok{format =} \StringTok{"%Y-%m-%d"}\NormalTok{)}
\NormalTok{    d1 =}\StringTok{ }\KeywordTok{as.Date}\NormalTok{(}\KeywordTok{as.character}\NormalTok{(ref), }\DataTypeTok{format =} \StringTok{"%Y-%m-%d"}\NormalTok{)}
    \KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{as.character}\NormalTok{(}\KeywordTok{gsub}\NormalTok{(}\StringTok{" days"}\NormalTok{, }\StringTok{""}\NormalTok{, d1 }\OperatorTok{-}\StringTok{ }\NormalTok{d2)))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dataset <-}\StringTok{ }\NormalTok{fluH7N9_china_}\DecValTok{2013} \OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}
      \DataTypeTok{hospital =} \KeywordTok{as.factor}\NormalTok{(}\KeywordTok{ifelse}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(date_of_hospitalisation), }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)), }
      \DataTypeTok{gender_f =} \KeywordTok{as.factor}\NormalTok{(}\KeywordTok{ifelse}\NormalTok{(gender }\OperatorTok{==}\StringTok{ "f"}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)), }
      \DataTypeTok{province_Jiangsu =} \KeywordTok{as.factor}\NormalTok{(}\KeywordTok{ifelse}\NormalTok{(province }\OperatorTok{==}\StringTok{ "Jiangsu"}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)), }
      \DataTypeTok{province_Shanghai =} \KeywordTok{as.factor}\NormalTok{(}\KeywordTok{ifelse}\NormalTok{(province }\OperatorTok{==}\StringTok{ "Shanghai"}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)), }
      \DataTypeTok{province_Zhejiang =} \KeywordTok{as.factor}\NormalTok{(}\KeywordTok{ifelse}\NormalTok{(province }\OperatorTok{==}\StringTok{ "Zhejiang"}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)), }
      \DataTypeTok{province_other =} \KeywordTok{as.factor}\NormalTok{(}\KeywordTok{ifelse}\NormalTok{(province }\OperatorTok{==}\StringTok{ "Zhejiang"} 
                                           \OperatorTok{|}\StringTok{ }\NormalTok{province }\OperatorTok{==}\StringTok{ "Jiangsu"} 
                                           \OperatorTok{|}\StringTok{ }\NormalTok{province }\OperatorTok{==}\StringTok{ "Shanghai"}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)),}
      
      \DataTypeTok{days_onset_to_outcome =} \KeywordTok{delta_dates}\NormalTok{(date_of_onset, date_of_outcome),}
      \DataTypeTok{days_onset_to_hospital =} \KeywordTok{delta_dates}\NormalTok{(date_of_onset, date_of_hospitalisation),}
      \DataTypeTok{age =}\NormalTok{ age,}
      \DataTypeTok{early_onset =} \KeywordTok{as.factor}\NormalTok{(}\KeywordTok{ifelse}\NormalTok{(date_of_onset }\OperatorTok{<}\StringTok{ }
\StringTok{                                     }\KeywordTok{summary}\NormalTok{(date_of_onset)[[}\DecValTok{3}\NormalTok{]], }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)),}
      \DataTypeTok{early_outcome =} \KeywordTok{as.factor}\NormalTok{(}\KeywordTok{ifelse}\NormalTok{(date_of_outcome }\OperatorTok{<}
\StringTok{                                       }\KeywordTok{summary}\NormalTok{(date_of_outcome)[[}\DecValTok{3}\NormalTok{]], }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{))}
\NormalTok{    ) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{subset}\NormalTok{(}\DataTypeTok{select =} \OperatorTok{-}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\OperatorTok{:}\DecValTok{4}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{8}\NormalTok{))}

\KeywordTok{rownames}\NormalTok{(dataset) <-}\StringTok{ }\NormalTok{dataset}\OperatorTok{$}\NormalTok{case_id}
\CommentTok{#> Warning: Setting row names on a tibble is deprecated.}
\NormalTok{dataset[, }\DecValTok{-2}\NormalTok{] <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{as.matrix}\NormalTok{(dataset[, }\DecValTok{-2}\NormalTok{]))}
\KeywordTok{print}\NormalTok{(dataset)}
\CommentTok{#> # A tibble: 136 x 13}
\CommentTok{#>   case_id outcome   age hospital gender_f province_Jiangsu province_Shangh~}
\CommentTok{#> *   <dbl> <fct>   <dbl>    <dbl>    <dbl>            <dbl>            <dbl>}
\CommentTok{#> 1       1 Death      87        0        0                0                1}
\CommentTok{#> 2       2 Death      27        1        0                0                1}
\CommentTok{#> 3       3 Death      35        1        1                0                0}
\CommentTok{#> 4       4 <NA>       45        1        1                1                0}
\CommentTok{#> 5       5 Recover    48        1        1                1                0}
\CommentTok{#> 6       6 Death      32        1        1                1                0}
\CommentTok{#> # ... with 130 more rows, and 6 more variables: province_Zhejiang <dbl>,}
\CommentTok{#> #   province_other <dbl>, days_onset_to_outcome <dbl>,}
\CommentTok{#> #   days_onset_to_hospital <dbl>, early_onset <dbl>, early_outcome <dbl>}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(dataset}\OperatorTok{$}\NormalTok{outcome)}
\CommentTok{#>   Death Recover    NA's }
\CommentTok{#>      32      47      57}
\end{Highlighting}
\end{Shaded}

\hypertarget{imputing-missing-values-1}{%
\section{Imputing missing values}\label{imputing-missing-values-1}}

I am using the \texttt{mice} package for imputing missing values

Note: Since publishing this blogpost I learned that the idea behind using mice is to compare different imputations to see how stable they are, instead of picking one imputed set as fixed for the remainder of the analysis. Therefore, I changed the focus of this post a little bit: in the old post I compared many different algorithms and their outcome; in this updated version I am only showing the \textbf{Random Forest} algorithm and focus on \textbf{comparing the different imputed datasets}. I am ignoring feature importance and feature plots because nothing changed compared to the old post.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot the missing data in a matrix by variables}
\NormalTok{md_pattern <-}\StringTok{ }\KeywordTok{md.pattern}\NormalTok{(dataset, }\DataTypeTok{rotate.names =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{fe-meta_230-missing_flu_prediction_micex5-sglander_files/figure-latex/unnamed-chunk-11-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dataset_impute <-}\StringTok{ }\KeywordTok{mice}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ dataset[, }\DecValTok{-2}\NormalTok{],  }\DataTypeTok{print =} \OtherTok{FALSE}\NormalTok{)}
\CommentTok{#> Warning: Number of logged events: 150}
\end{Highlighting}
\end{Shaded}

\hypertarget{generate-a-dataframe-of-five-imputting-strategies}{%
\subsection{Generate a dataframe of five imputting strategies}\label{generate-a-dataframe-of-five-imputting-strategies}}

\begin{itemize}
\tightlist
\item
  by default, mice() calculates five (m = 5) imputed data sets
\item
  we can combine them all in one output with the complete(``long'') function
\item
  I did not want to impute missing values in the outcome column, so I have to merge it back in with the imputed data
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# c(1,2): case_id, outcome}
\NormalTok{datasets_complete <-}\StringTok{ }\KeywordTok{right_join}\NormalTok{(dataset[, }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{)], }
                           \KeywordTok{complete}\NormalTok{(dataset_impute, }\StringTok{"long"}\NormalTok{),}
                           \DataTypeTok{by =} \StringTok{"case_id"}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{.imp =} \KeywordTok{as.factor}\NormalTok{(.imp)) }\OperatorTok{%>%}\StringTok{   }
\StringTok{  }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{.id) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{print}\NormalTok{()}
\CommentTok{#> # A tibble: 680 x 14}
\CommentTok{#>   case_id outcome .imp    age hospital gender_f province_Jiangsu}
\CommentTok{#>     <dbl> <fct>   <fct> <dbl>    <dbl>    <dbl>            <dbl>}
\CommentTok{#> 1       1 Death   1        87        0        0                0}
\CommentTok{#> 2       2 Death   1        27        1        0                0}
\CommentTok{#> 3       3 Death   1        35        1        1                0}
\CommentTok{#> 4       4 <NA>    1        45        1        1                1}
\CommentTok{#> 5       5 Recover 1        48        1        1                1}
\CommentTok{#> 6       6 Death   1        32        1        1                1}
\CommentTok{#> # ... with 674 more rows, and 7 more variables: province_Shanghai <dbl>,}
\CommentTok{#> #   province_Zhejiang <dbl>, province_other <dbl>,}
\CommentTok{#> #   days_onset_to_outcome <dbl>, days_onset_to_hospital <dbl>,}
\CommentTok{#> #   early_onset <dbl>, early_outcome <dbl>}
\end{Highlighting}
\end{Shaded}

Let's compare the distributions of the five different imputed datasets:

\hypertarget{plot-effect-of-imputting-on-features}{%
\subsection{plot effect of imputting on features}\label{plot-effect-of-imputting-on-features}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{datasets_complete }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{gather}\NormalTok{(x, y, age}\OperatorTok{:}\NormalTok{early_outcome) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ y, }\DataTypeTok{fill =}\NormalTok{ .imp, }\DataTypeTok{color =}\NormalTok{ .imp)) }\OperatorTok{+}
\StringTok{    }\KeywordTok{geom_density}\NormalTok{(}\DataTypeTok{alpha =} \FloatTok{0.20}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{facet_wrap}\NormalTok{(}\OperatorTok{~}\StringTok{ }\NormalTok{x, }\DataTypeTok{ncol =} \DecValTok{3}\NormalTok{, }\DataTypeTok{scales =} \StringTok{"free"}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{scale_fill_brewer}\NormalTok{(}\DataTypeTok{palette=}\StringTok{"Set1"}\NormalTok{, }\DataTypeTok{na.value =} \StringTok{"grey50"}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{scale_color_brewer}\NormalTok{(}\DataTypeTok{palette=}\StringTok{"Set1"}\NormalTok{, }\DataTypeTok{na.value =} \StringTok{"grey50"}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{my_theme}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{fe-meta_230-missing_flu_prediction_micex5-sglander_files/figure-latex/unnamed-chunk-14-1} \end{center}

\hypertarget{test-train-and-validation-data-sets-1}{%
\section{Test, train and validation data sets}\label{test-train-and-validation-data-sets-1}}

Now, we can go ahead with machine learning!

The dataset contains a few missing values in the outcome column; those will be the test set used for final predictions (see the old blog post for this).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{length}\NormalTok{(}\KeywordTok{which}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(datasets_complete}\OperatorTok{$}\NormalTok{outcome)))}
\KeywordTok{length}\NormalTok{(}\KeywordTok{which}\NormalTok{(}\OperatorTok{!}\KeywordTok{is.na}\NormalTok{(datasets_complete}\OperatorTok{$}\NormalTok{outcome)))}
\CommentTok{#> [1] 285}
\CommentTok{#> [1] 395}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{train_index <-}\StringTok{ }\KeywordTok{which}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(datasets_complete}\OperatorTok{$}\NormalTok{outcome))}
\NormalTok{train_data <-}\StringTok{ }\NormalTok{datasets_complete[}\OperatorTok{-}\NormalTok{train_index, ]}
\NormalTok{test_data  <-}\StringTok{ }\NormalTok{datasets_complete[train_index, }\DecValTok{-2}\NormalTok{]       }\CommentTok{# remove variable outcome}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{print}\NormalTok{(train_data)}
\CommentTok{#> # A tibble: 395 x 14}
\CommentTok{#>   case_id outcome .imp    age hospital gender_f province_Jiangsu}
\CommentTok{#>     <dbl> <fct>   <fct> <dbl>    <dbl>    <dbl>            <dbl>}
\CommentTok{#> 1       1 Death   1        87        0        0                0}
\CommentTok{#> 2       2 Death   1        27        1        0                0}
\CommentTok{#> 3       3 Death   1        35        1        1                0}
\CommentTok{#> 4       5 Recover 1        48        1        1                1}
\CommentTok{#> 5       6 Death   1        32        1        1                1}
\CommentTok{#> 6       7 Death   1        83        1        0                1}
\CommentTok{#> # ... with 389 more rows, and 7 more variables: province_Shanghai <dbl>,}
\CommentTok{#> #   province_Zhejiang <dbl>, province_other <dbl>,}
\CommentTok{#> #   days_onset_to_outcome <dbl>, days_onset_to_hospital <dbl>,}
\CommentTok{#> #   early_onset <dbl>, early_outcome <dbl>}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# outcome variable removed}
\KeywordTok{print}\NormalTok{(test_data)}
\CommentTok{#> # A tibble: 285 x 13}
\CommentTok{#>   case_id .imp    age hospital gender_f province_Jiangsu province_Shangh~}
\CommentTok{#>     <dbl> <fct> <dbl>    <dbl>    <dbl>            <dbl>            <dbl>}
\CommentTok{#> 1       4 1        45        1        1                1                0}
\CommentTok{#> 2       9 1        67        1        0                0                0}
\CommentTok{#> 3      15 1        61        0        1                1                0}
\CommentTok{#> 4      16 1        79        0        0                1                0}
\CommentTok{#> 5      22 1        85        1        0                1                0}
\CommentTok{#> 6      28 1        79        0        0                0                0}
\CommentTok{#> # ... with 279 more rows, and 6 more variables: province_Zhejiang <dbl>,}
\CommentTok{#> #   province_other <dbl>, days_onset_to_outcome <dbl>,}
\CommentTok{#> #   days_onset_to_hospital <dbl>, early_onset <dbl>, early_outcome <dbl>}
\end{Highlighting}
\end{Shaded}

The remainder of the data will be used for modeling. Here, I am splitting the data into 70\% training and 30\% test data.

Because I want to model each imputed dataset separately, I am using the \texttt{nest()} and \texttt{map()} functions.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{train_data_nest <-}\StringTok{ }\NormalTok{train_data }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(.imp) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{nest}\NormalTok{() }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{print}\NormalTok{()}
\CommentTok{#> # A tibble: 5 x 2}
\CommentTok{#>   .imp  data              }
\CommentTok{#>   <fct> <list>            }
\CommentTok{#> 1 1     <tibble [79 x 13]>}
\CommentTok{#> 2 2     <tibble [79 x 13]>}
\CommentTok{#> 3 3     <tibble [79 x 13]>}
\CommentTok{#> 4 4     <tibble [79 x 13]>}
\CommentTok{#> 5 5     <tibble [79 x 13]>}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# split the training data in validation training and validation test}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{42}\NormalTok{)}
\NormalTok{val_data <-}\StringTok{ }\NormalTok{train_data_nest }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{val_index =} \KeywordTok{map}\NormalTok{(data, }\OperatorTok{~}\StringTok{ }\KeywordTok{createDataPartition}\NormalTok{(.}\OperatorTok{$}\NormalTok{outcome, }
                                                     \DataTypeTok{p =} \FloatTok{0.7}\NormalTok{, }
                                               \DataTypeTok{list =} \OtherTok{FALSE}\NormalTok{)),}
         \DataTypeTok{val_train_data =} \KeywordTok{map2}\NormalTok{(data, val_index, }\OperatorTok{~}\StringTok{ }\NormalTok{.x[.y, ]),}
         \DataTypeTok{val_test_data  =} \KeywordTok{map2}\NormalTok{(data, val_index, }\OperatorTok{~}\StringTok{ }\NormalTok{.x[}\OperatorTok{-}\NormalTok{.y, ])) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{print}\NormalTok{()}
\CommentTok{#> # A tibble: 5 x 5}
\CommentTok{#>   .imp  data            val_index        val_train_data    val_test_data   }
\CommentTok{#>   <fct> <list>          <list>           <list>            <list>          }
\CommentTok{#> 1 1     <tibble [79 x ~ <int[,1] [56 x ~ <tibble [56 x 13~ <tibble [23 x 1~}
\CommentTok{#> 2 2     <tibble [79 x ~ <int[,1] [56 x ~ <tibble [56 x 13~ <tibble [23 x 1~}
\CommentTok{#> 3 3     <tibble [79 x ~ <int[,1] [56 x ~ <tibble [56 x 13~ <tibble [23 x 1~}
\CommentTok{#> 4 4     <tibble [79 x ~ <int[,1] [56 x ~ <tibble [56 x 13~ <tibble [23 x 1~}
\CommentTok{#> 5 5     <tibble [79 x ~ <int[,1] [56 x ~ <tibble [56 x 13~ <tibble [23 x 1~}
\end{Highlighting}
\end{Shaded}

\hypertarget{machine-learning-algorithms}{%
\section{Machine Learning algorithms}\label{machine-learning-algorithms}}

\hypertarget{random-forest-2}{%
\subsection{Random Forest}\label{random-forest-2}}

To make the code tidier, I am first defining the modeling function with the parameters I want.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model_function <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(df) \{}
\NormalTok{  caret}\OperatorTok{::}\KeywordTok{train}\NormalTok{(outcome }\OperatorTok{~}\StringTok{ }\NormalTok{.,}
               \DataTypeTok{data =}\NormalTok{ df,}
               \DataTypeTok{method =} \StringTok{"rf"}\NormalTok{,}
               \DataTypeTok{preProcess =} \KeywordTok{c}\NormalTok{(}\StringTok{"scale"}\NormalTok{, }\StringTok{"center"}\NormalTok{),}
               \DataTypeTok{trControl =} \KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"repeatedcv"}\NormalTok{, }
                                        \DataTypeTok{number =} \DecValTok{5}\NormalTok{, }
                                        \DataTypeTok{repeats =} \DecValTok{3}\NormalTok{, }
                                        \DataTypeTok{verboseIter =} \OtherTok{FALSE}\NormalTok{))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\hypertarget{add-model-and-prediction-to-nested-dataframe-and-calculate}{%
\subsection{Add model and prediction to nested dataframe and calculate}\label{add-model-and-prediction-to-nested-dataframe-and-calculate}}

Next, I am using the nested tibble from before to map() the model function, predict the outcome and calculate confusion matrices.

\hypertarget{add-model-list-column}{%
\subsubsection{add model list-column}\label{add-model-list-column}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{val_data_model <-}\StringTok{ }\NormalTok{val_data }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{model =} \KeywordTok{map}\NormalTok{(val_train_data, }\OperatorTok{~}\StringTok{ }\KeywordTok{model_function}\NormalTok{(.x))) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{val_index) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{print}\NormalTok{()}
\CommentTok{#> # A tibble: 5 x 5}
\CommentTok{#>   .imp  data               val_train_data     val_test_data      model  }
\CommentTok{#>   <fct> <list>             <list>             <list>             <list> }
\CommentTok{#> 1 1     <tibble [79 x 13]> <tibble [56 x 13]> <tibble [23 x 13]> <train>}
\CommentTok{#> 2 2     <tibble [79 x 13]> <tibble [56 x 13]> <tibble [23 x 13]> <train>}
\CommentTok{#> 3 3     <tibble [79 x 13]> <tibble [56 x 13]> <tibble [23 x 13]> <train>}
\CommentTok{#> 4 4     <tibble [79 x 13]> <tibble [56 x 13]> <tibble [23 x 13]> <train>}
\CommentTok{#> 5 5     <tibble [79 x 13]> <tibble [56 x 13]> <tibble [23 x 13]> <train>}
\end{Highlighting}
\end{Shaded}

\hypertarget{add-prediction-and-confusion-matrix-list-columns}{%
\subsubsection{add prediction and confusion matrix list-columns}\label{add-prediction-and-confusion-matrix-list-columns}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{42}\NormalTok{)}
\NormalTok{val_data_model <-}\StringTok{ }\NormalTok{val_data_model }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}
         \DataTypeTok{predict =} \KeywordTok{map2}\NormalTok{(model, val_test_data, }\OperatorTok{~}\StringTok{ }
\StringTok{                            }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{prediction =} \KeywordTok{predict}\NormalTok{(.x, .y[, }\DecValTok{-2}\NormalTok{]))),}
         \DataTypeTok{predict_prob =} \KeywordTok{map2}\NormalTok{(model, val_test_data, }\OperatorTok{~}\StringTok{ }
\StringTok{                            }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{outcome =}\NormalTok{ .y[, }\DecValTok{2}\NormalTok{], }
                            \DataTypeTok{prediction =} \KeywordTok{predict}\NormalTok{(.x, .y[, }\DecValTok{-2}\NormalTok{], }\DataTypeTok{type =} \StringTok{"prob"}\NormalTok{))),}
         \DataTypeTok{confusion_matrix =} \KeywordTok{map2}\NormalTok{(val_test_data, predict, }\OperatorTok{~}\StringTok{ }
\StringTok{                                     }\KeywordTok{confusionMatrix}\NormalTok{(.x}\OperatorTok{$}\NormalTok{outcome, .y}\OperatorTok{$}\NormalTok{prediction)),}
         \DataTypeTok{confusion_matrix_tbl =} \KeywordTok{map}\NormalTok{(confusion_matrix, }\OperatorTok{~}\StringTok{ }\KeywordTok{as.tibble}\NormalTok{(.x}\OperatorTok{$}\NormalTok{table))) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{print}\NormalTok{()}
\CommentTok{#> # A tibble: 5 x 9}
\CommentTok{#>   .imp  data  val_train_data val_test_data model predict predict_prob}
\CommentTok{#>   <fct> <lis> <list>         <list>        <lis> <list>  <list>      }
\CommentTok{#> 1 1     <tib~ <tibble [56 x~ <tibble [23 ~ <tra~ <df[,1~ <df[,3] [23~}
\CommentTok{#> 2 2     <tib~ <tibble [56 x~ <tibble [23 ~ <tra~ <df[,1~ <df[,3] [23~}
\CommentTok{#> 3 3     <tib~ <tibble [56 x~ <tibble [23 ~ <tra~ <df[,1~ <df[,3] [23~}
\CommentTok{#> 4 4     <tib~ <tibble [56 x~ <tibble [23 ~ <tra~ <df[,1~ <df[,3] [23~}
\CommentTok{#> 5 5     <tib~ <tibble [56 x~ <tibble [23 ~ <tra~ <df[,1~ <df[,3] [23~}
\CommentTok{#> # ... with 2 more variables: confusion_matrix <list>,}
\CommentTok{#> #   confusion_matrix_tbl <list>}
\end{Highlighting}
\end{Shaded}

\begin{quote}
Finally, we have a nested dataframe of 5 rows or cases, one per imputting strategy with its corresponding models and prediction results.
\end{quote}

\hypertarget{comparing-accuracy-of-models-1}{%
\section{Comparing accuracy of models}\label{comparing-accuracy-of-models-1}}

To compare how the different imputations did, I am plotting the confusion matrices:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{val_data_model_unnest <-}\StringTok{ }\NormalTok{val_data_model }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{unnest}\NormalTok{(confusion_matrix_tbl) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{print}\NormalTok{()}
\CommentTok{#> # A tibble: 20 x 4}
\CommentTok{#>   .imp  Prediction Reference     n}
\CommentTok{#>   <fct> <chr>      <chr>     <int>}
\CommentTok{#> 1 1     Death      Death         5}
\CommentTok{#> 2 1     Recover    Death         3}
\CommentTok{#> 3 1     Death      Recover       4}
\CommentTok{#> 4 1     Recover    Recover      11}
\CommentTok{#> 5 2     Death      Death         3}
\CommentTok{#> 6 2     Recover    Death         3}
\CommentTok{#> # ... with 14 more rows}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{val_data_model_unnest }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Prediction, }\DataTypeTok{y =}\NormalTok{ Reference, }\DataTypeTok{fill =}\NormalTok{ n)) }\OperatorTok{+}
\StringTok{    }\KeywordTok{facet_wrap}\NormalTok{(}\OperatorTok{~}\StringTok{ }\NormalTok{.imp, }\DataTypeTok{ncol =} \DecValTok{5}\NormalTok{, }\DataTypeTok{scales =} \StringTok{"free"}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{geom_tile}\NormalTok{() }\OperatorTok{+}
\StringTok{    }\KeywordTok{my_theme}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{fe-meta_230-missing_flu_prediction_micex5-sglander_files/figure-latex/unnamed-chunk-25-1} \end{center}

and the prediction probabilities for correct and wrong predictions:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{val_data_model_gather <-}\StringTok{ }\NormalTok{val_data_model }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{unnest}\NormalTok{(predict_prob) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{gather}\NormalTok{(x, y, prediction.Death}\OperatorTok{:}\NormalTok{prediction.Recover) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{print}\NormalTok{()}
\CommentTok{#> # A tibble: 230 x 4}
\CommentTok{#>   .imp  outcome x                    y}
\CommentTok{#>   <fct> <fct>   <chr>            <dbl>}
\CommentTok{#> 1 1     Death   prediction.Death 0.758}
\CommentTok{#> 2 1     Recover prediction.Death 0.864}
\CommentTok{#> 3 1     Death   prediction.Death 0.828}
\CommentTok{#> 4 1     Death   prediction.Death 0.828}
\CommentTok{#> 5 1     Recover prediction.Death 0.342}
\CommentTok{#> 6 1     Recover prediction.Death 0.552}
\CommentTok{#> # ... with 224 more rows}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{val_data_model_gather }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ x, }\DataTypeTok{y =}\NormalTok{ y, }\DataTypeTok{fill =}\NormalTok{ outcome)) }\OperatorTok{+}
\StringTok{    }\KeywordTok{facet_wrap}\NormalTok{(}\OperatorTok{~}\StringTok{ }\NormalTok{.imp, }\DataTypeTok{ncol =} \DecValTok{5}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{geom_boxplot}\NormalTok{() }\OperatorTok{+}
\StringTok{    }\KeywordTok{scale_fill_brewer}\NormalTok{(}\DataTypeTok{palette=}\StringTok{"Set1"}\NormalTok{, }\DataTypeTok{na.value =} \StringTok{"grey50"}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{my_theme}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{fe-meta_230-missing_flu_prediction_micex5-sglander_files/figure-latex/unnamed-chunk-27-1} \end{center}

Hope, you found that example interesting and helpful!

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sessionInfo}\NormalTok{()}
\CommentTok{#> R version 3.6.0 (2019-04-26)}
\CommentTok{#> Platform: x86_64-pc-linux-gnu (64-bit)}
\CommentTok{#> Running under: Ubuntu 18.04.3 LTS}
\CommentTok{#> }
\CommentTok{#> Matrix products: default}
\CommentTok{#> BLAS/LAPACK: /usr/lib/x86_64-linux-gnu/libopenblasp-r0.2.20.so}
\CommentTok{#> }
\CommentTok{#> locale:}
\CommentTok{#>  [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              }
\CommentTok{#>  [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    }
\CommentTok{#>  [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   }
\CommentTok{#>  [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                 }
\CommentTok{#>  [9] LC_ADDRESS=C               LC_TELEPHONE=C            }
\CommentTok{#> [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       }
\CommentTok{#> }
\CommentTok{#> attached base packages:}
\CommentTok{#> [1] stats     graphics  grDevices utils     datasets  methods   base     }
\CommentTok{#> }
\CommentTok{#> other attached packages:}
\CommentTok{#>  [1] caret_6.0-84    mice_3.4.0      lattice_0.20-38 plyr_1.8.4     }
\CommentTok{#>  [5] forcats_0.4.0   stringr_1.4.0   dplyr_0.8.0.1   purrr_0.3.2    }
\CommentTok{#>  [9] readr_1.3.1     tidyr_0.8.3     tibble_2.1.1    ggplot2_3.1.1  }
\CommentTok{#> [13] tidyverse_1.2.1 outbreaks_1.5.0 logging_0.9-107}
\CommentTok{#> }
\CommentTok{#> loaded via a namespace (and not attached):}
\CommentTok{#>  [1] nlme_3.1-139        lubridate_1.7.4     RColorBrewer_1.1-2 }
\CommentTok{#>  [4] httr_1.4.0          rprojroot_1.3-2     tools_3.6.0        }
\CommentTok{#>  [7] backports_1.1.4     utf8_1.1.4          R6_2.4.0           }
\CommentTok{#> [10] rpart_4.1-15        lazyeval_0.2.2      colorspace_1.4-1   }
\CommentTok{#> [13] jomo_2.6-7          nnet_7.3-12         withr_2.1.2        }
\CommentTok{#> [16] tidyselect_0.2.5    compiler_3.6.0      cli_1.1.0          }
\CommentTok{#> [19] rvest_0.3.3         xml2_1.2.0          labeling_0.3       }
\CommentTok{#> [22] bookdown_0.10       scales_1.0.0        randomForest_4.6-14}
\CommentTok{#> [25] digest_0.6.18       minqa_1.2.4         rmarkdown_1.12     }
\CommentTok{#> [28] pkgconfig_2.0.2     htmltools_0.3.6     lme4_1.1-21        }
\CommentTok{#> [31] rlang_0.3.4         readxl_1.3.1        rstudioapi_0.10    }
\CommentTok{#> [34] generics_0.0.2      jsonlite_1.6        ModelMetrics_1.2.2 }
\CommentTok{#> [37] magrittr_1.5        Matrix_1.2-17       Rcpp_1.0.1         }
\CommentTok{#> [40] munsell_0.5.0       fansi_0.4.0         stringi_1.4.3      }
\CommentTok{#> [43] yaml_2.2.0          MASS_7.3-51.4       recipes_0.1.5      }
\CommentTok{#> [46] grid_3.6.0          parallel_3.6.0      mitml_0.3-7        }
\CommentTok{#> [49] crayon_1.3.4        haven_2.1.0         splines_3.6.0      }
\CommentTok{#> [52] hms_0.4.2           zeallot_0.1.0       knitr_1.22         }
\CommentTok{#> [55] pillar_1.4.0        boot_1.3-22         reshape2_1.4.3     }
\CommentTok{#> [58] codetools_0.2-16    stats4_3.6.0        pan_1.6            }
\CommentTok{#> [61] glue_1.3.1          evaluate_0.13       data.table_1.12.2  }
\CommentTok{#> [64] modelr_0.1.4        vctrs_0.1.0         nloptr_1.2.1       }
\CommentTok{#> [67] foreach_1.4.4       cellranger_1.1.0    gtable_0.3.0       }
\CommentTok{#> [70] assertthat_0.2.1    xfun_0.6            gower_0.2.0        }
\CommentTok{#> [73] prodlim_2018.04.18  broom_0.5.2         e1071_1.7-1        }
\CommentTok{#> [76] class_7.3-15        survival_2.44-1.1   timeDate_3043.102  }
\CommentTok{#> [79] iterators_1.0.10    lava_1.6.5          ipred_0.9-9}
\end{Highlighting}
\end{Shaded}

\hypertarget{part-meta-ml}{%
\part{Meta ML}\label{part-meta-ml}}

\hypertarget{pca-prcomp-vs-princomp}{%
\chapter{PCA: prcomp vs princomp}\label{pca-prcomp-vs-princomp}}

\url{http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/118-principal-component-analysis-in-r-prcomp-vs-princomp/}

\hypertarget{general-methods-for-principal-component-analysis}{%
\section{General methods for principal component analysis}\label{general-methods-for-principal-component-analysis}}

There are two general methods to perform PCA in R :

\begin{itemize}
\tightlist
\item
  Spectral decomposition which examines the covariances / correlations between variables
\item
  Singular value decomposition which examines the covariances / correlations between individuals
\end{itemize}

The function princomp() uses the spectral decomposition approach. The functions prcomp() and PCA(){[}FactoMineR{]} use the singular value decomposition (SVD).

\hypertarget{prcomp-and-princomp-functions}{%
\section{prcomp() and princomp() functions}\label{prcomp-and-princomp-functions}}

The simplified format of these 2 functions are :

\begin{verbatim}
prcomp(x, scale = FALSE)
princomp(x, cor = FALSE, scores = TRUE)
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Arguments for prcomp():\\
  \texttt{x}: a numeric matrix or data frame\\
  \texttt{scale}: a logical value indicating whether the variables should be scaled to have unit variance before the analysis takes place
\item
  Arguments for princomp():\\
  \texttt{x}: a numeric matrix or data frame
  \texttt{cor}: a logical value. If TRUE, the data will be centered and scaled before the analysis
  \texttt{scores}: a logical value. If TRUE, the coordinates on each principal component are calculated
\end{enumerate}

\hypertarget{factoextra}{%
\section{factoextra}\label{factoextra}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# install.packages("factoextra")}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(factoextra)}
\CommentTok{#> Loading required package: ggplot2}
\CommentTok{#> Registered S3 methods overwritten by 'ggplot2':}
\CommentTok{#>   method         from }
\CommentTok{#>   [.quosures     rlang}
\CommentTok{#>   c.quosures     rlang}
\CommentTok{#>   print.quosures rlang}
\CommentTok{#> Welcome! Related Books: `Practical Guide To Cluster Analysis in R` at https://goo.gl/13EFCZ}
\end{Highlighting}
\end{Shaded}

\hypertarget{demo-dataset}{%
\section{demo dataset}\label{demo-dataset}}

We'll use the data sets \texttt{decathlon2} {[}in factoextra{]}, which has been already described at: PCA - Data format.

Briefly, it contains:

\begin{itemize}
\tightlist
\item
  Active individuals (rows 1 to 23) and active variables (columns 1 to 10), which are used to perform the principal component analysis
\item
  Supplementary individuals (rows 24 to 27) and supplementary variables (columns 11 to 13), which coordinates will be predicted using the PCA information and parameters obtained with active individuals/variables.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(}\StringTok{"factoextra"}\NormalTok{)}
\KeywordTok{data}\NormalTok{(decathlon2)}
\NormalTok{decathlon2.active <-}\StringTok{ }\NormalTok{decathlon2[}\DecValTok{1}\OperatorTok{:}\DecValTok{23}\NormalTok{, }\DecValTok{1}\OperatorTok{:}\DecValTok{10}\NormalTok{]}
\KeywordTok{head}\NormalTok{(decathlon2.active[, }\DecValTok{1}\OperatorTok{:}\DecValTok{6}\NormalTok{])}
\CommentTok{#>           X100m Long.jump Shot.put High.jump X400m X110m.hurdle}
\CommentTok{#> SEBRLE     11.0      7.58     14.8      2.07  49.8         14.7}
\CommentTok{#> CLAY       10.8      7.40     14.3      1.86  49.4         14.1}
\CommentTok{#> BERNARD    11.0      7.23     14.2      1.92  48.9         15.0}
\CommentTok{#> YURKOV     11.3      7.09     15.2      2.10  50.4         15.3}
\CommentTok{#> ZSIVOCZKY  11.1      7.30     13.5      2.01  48.6         14.2}
\CommentTok{#> McMULLEN   10.8      7.31     13.8      2.13  49.9         14.4}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{decathlon2.supplementary <-}\StringTok{ }\NormalTok{decathlon2[}\DecValTok{24}\OperatorTok{:}\DecValTok{27}\NormalTok{, }\DecValTok{1}\OperatorTok{:}\DecValTok{10}\NormalTok{]}
\KeywordTok{head}\NormalTok{(decathlon2.supplementary[, }\DecValTok{1}\OperatorTok{:}\DecValTok{6}\NormalTok{])}
\CommentTok{#>         X100m Long.jump Shot.put High.jump X400m X110m.hurdle}
\CommentTok{#> KARPOV   11.0      7.30     14.8      2.04  48.4         14.1}
\CommentTok{#> WARNERS  11.1      7.60     14.3      1.98  48.7         14.2}
\CommentTok{#> Nool     10.8      7.53     14.3      1.88  48.8         14.8}
\CommentTok{#> Drews    10.9      7.38     13.1      1.88  48.5         14.0}
\end{Highlighting}
\end{Shaded}

\hypertarget{compute-pca-in-r-using-prcomp}{%
\section{Compute PCA in R using prcomp()}\label{compute-pca-in-r-using-prcomp}}

In this section we'll provide an easy-to-use R code to compute and visualize PCA in R using the prcomp() function and the factoextra package.

`. Load factoextra for visualization

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(factoextra)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  compute PCA
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# compute PCA}
\NormalTok{res.pca <-}\StringTok{ }\KeywordTok{prcomp}\NormalTok{(decathlon2.active, }\DataTypeTok{scale =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Visualize eigenvalues (scree plot). Show the percentage of variances explained by each principal component.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Visualize eigenvalues (scree plot).}
\KeywordTok{fviz_eig}\NormalTok{(res.pca, }\DataTypeTok{addlabels =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{50}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{meta_110a-PCA-prcomp_vs_princomp_files/figure-latex/unnamed-chunk-8-1} \end{center}

\begin{quote}
From the plot above, we might want to stop at the fifth principal component. 87\% of the information (variances) contained in the data are retained by the first five principal components.
\end{quote}

\hypertarget{plots-quality-and-contribution}{%
\section{Plots: quality and contribution}\label{plots-quality-and-contribution}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Graph of individuals. Individuals with a similar profile are grouped together.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Graph of individuals.}
\KeywordTok{fviz_pca_ind}\NormalTok{(res.pca,}
             \DataTypeTok{col.ind =} \StringTok{"cos2"}\NormalTok{, }\CommentTok{# Color by the quality of representation}
             \DataTypeTok{gradient.cols =} \KeywordTok{c}\NormalTok{(}\StringTok{"#00AFBB"}\NormalTok{, }\StringTok{"#E7B800"}\NormalTok{, }\StringTok{"#FC4E07"}\NormalTok{),}
             \DataTypeTok{repel =} \OtherTok{TRUE}     \CommentTok{# Avoid text overlapping}
\NormalTok{             )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{meta_110a-PCA-prcomp_vs_princomp_files/figure-latex/unnamed-chunk-9-1} \end{center}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Graph of variables. Positive correlated variables point to the same side of the plot. Negative correlated variables point to opposite sides of the graph.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Graph of variables.}
\KeywordTok{fviz_pca_var}\NormalTok{(res.pca,}
             \DataTypeTok{col.var =} \StringTok{"contrib"}\NormalTok{, }\CommentTok{# Color by contributions to the PC}
             \DataTypeTok{gradient.cols =} \KeywordTok{c}\NormalTok{(}\StringTok{"#00AFBB"}\NormalTok{, }\StringTok{"#E7B800"}\NormalTok{, }\StringTok{"#FC4E07"}\NormalTok{),}
             \DataTypeTok{repel =} \OtherTok{TRUE}     \CommentTok{# Avoid text overlapping}
\NormalTok{             )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{meta_110a-PCA-prcomp_vs_princomp_files/figure-latex/unnamed-chunk-10-1} \end{center}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  Biplot of individuals and variables
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Biplot of individuals and variables}
\KeywordTok{fviz_pca_biplot}\NormalTok{(res.pca, }\DataTypeTok{repel =} \OtherTok{TRUE}\NormalTok{,}
                \DataTypeTok{col.var =} \StringTok{"#2E9FDF"}\NormalTok{, }\CommentTok{# Variables color}
                \DataTypeTok{col.ind =} \StringTok{"#696969"}  \CommentTok{# Individuals color}
\NormalTok{                )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{meta_110a-PCA-prcomp_vs_princomp_files/figure-latex/unnamed-chunk-11-1} \end{center}

\hypertarget{access-to-the-pca-results}{%
\section{Access to the PCA results}\label{access-to-the-pca-results}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(factoextra)}
\CommentTok{# Eigenvalues}
\NormalTok{eig.val <-}\StringTok{ }\KeywordTok{get_eigenvalue}\NormalTok{(res.pca)}
\NormalTok{eig.val}
\CommentTok{#>        eigenvalue variance.percent cumulative.variance.percent}
\CommentTok{#> Dim.1       4.124            41.24                        41.2}
\CommentTok{#> Dim.2       1.839            18.39                        59.6}
\CommentTok{#> Dim.3       1.239            12.39                        72.0}
\CommentTok{#> Dim.4       0.819             8.19                        80.2}
\CommentTok{#> Dim.5       0.702             7.02                        87.2}
\CommentTok{#> Dim.6       0.423             4.23                        91.5}
\CommentTok{#> Dim.7       0.303             3.03                        94.5}
\CommentTok{#> Dim.8       0.274             2.74                        97.2}
\CommentTok{#> Dim.9       0.155             1.55                        98.8}
\CommentTok{#> Dim.10      0.122             1.22                       100.0}
  
\CommentTok{# Results for Variables}
\NormalTok{res.var <-}\StringTok{ }\KeywordTok{get_pca_var}\NormalTok{(res.pca)}
\NormalTok{res.var}\OperatorTok{$}\NormalTok{coord          }\CommentTok{# Coordinates}
\CommentTok{#>                 Dim.1   Dim.2   Dim.3   Dim.4  Dim.5    Dim.6    Dim.7}
\CommentTok{#> X100m        -0.85063  0.1794 -0.3016  0.0336 -0.194  0.03537 -0.09134}
\CommentTok{#> Long.jump     0.79418 -0.2809  0.1905 -0.1154  0.233 -0.03373 -0.15433}
\CommentTok{#> Shot.put      0.73391 -0.0854 -0.5176  0.1285 -0.249 -0.23979 -0.00989}
\CommentTok{#> High.jump     0.61008  0.4652 -0.3301  0.1446  0.403 -0.28464  0.02816}
\CommentTok{#> X400m        -0.70160 -0.2902 -0.2835  0.4308  0.104 -0.04929  0.28611}
\CommentTok{#> X110m.hurdle -0.76413  0.0247 -0.4489 -0.0169  0.224  0.00263 -0.37007}
\CommentTok{#> Discus        0.74321 -0.0497 -0.1765  0.3950 -0.408  0.19854 -0.14273}
\CommentTok{#> Pole.vault   -0.21727 -0.8075 -0.0941 -0.3390 -0.222 -0.32746 -0.01039}
\CommentTok{#> Javeline      0.42823 -0.3861 -0.6041 -0.3317  0.198  0.36210  0.13356}
\CommentTok{#> X1500m        0.00428 -0.7845  0.2195  0.4480  0.263  0.04205 -0.11137}
\CommentTok{#>                 Dim.8   Dim.9   Dim.10}
\CommentTok{#> X100m        -0.10472 -0.3031  0.04442}
\CommentTok{#> Long.jump    -0.39738 -0.0516  0.02972}
\CommentTok{#> Shot.put      0.02436  0.0478  0.21745}
\CommentTok{#> High.jump     0.08441 -0.1121 -0.13357}
\CommentTok{#> X400m        -0.23355  0.0822 -0.03417}
\CommentTok{#> X110m.hurdle -0.00834  0.1618 -0.01563}
\CommentTok{#> Discus       -0.03956  0.0134 -0.17259}
\CommentTok{#> Pole.vault    0.03291 -0.0258 -0.13721}
\CommentTok{#> Javeline      0.05284 -0.0405 -0.00385}
\CommentTok{#> X1500m        0.19447 -0.1022  0.06283}
\NormalTok{res.var}\OperatorTok{$}\NormalTok{contrib        }\CommentTok{# Contributions to the PCs}
\CommentTok{#>                 Dim.1   Dim.2  Dim.3   Dim.4 Dim.5    Dim.6   Dim.7}
\CommentTok{#> X100m        1.75e+01  1.7505  7.339  0.1376  5.39  0.29592  2.7571}
\CommentTok{#> Long.jump    1.53e+01  4.2904  2.930  1.6249  7.75  0.26900  7.8716}
\CommentTok{#> Shot.put     1.31e+01  0.3967 21.620  2.0141  8.82 13.59686  0.0323}
\CommentTok{#> High.jump    9.02e+00 11.7716  8.793  2.5499 23.12 19.15961  0.2620}
\CommentTok{#> X400m        1.19e+01  4.5799  6.488 22.6509  1.54  0.57451 27.0527}
\CommentTok{#> X110m.hurdle 1.42e+01  0.0333 16.261  0.0348  7.17  0.00164 45.2616}
\CommentTok{#> Discus       1.34e+01  0.1341  2.515 19.0413 23.76  9.32175  6.7323}
\CommentTok{#> Pole.vault   1.14e+00 35.4619  0.714 14.0231  7.01 25.35762  0.0357}
\CommentTok{#> Javeline     4.45e+00  8.1087 29.453 13.4296  5.58 31.00496  5.8957}
\CommentTok{#> X1500m       4.44e-04 33.4729  3.887 24.4939  9.88  0.41813  4.0989}
\CommentTok{#>                Dim.8  Dim.9  Dim.10}
\CommentTok{#> X100m         3.9952 59.174  1.6176}
\CommentTok{#> Long.jump    57.5332  1.715  0.7241}
\CommentTok{#> Shot.put      0.2162  1.471 38.7677}
\CommentTok{#> High.jump     2.5957  8.102 14.6265}
\CommentTok{#> X400m        19.8734  4.349  0.9573}
\CommentTok{#> X110m.hurdle  0.0254 16.858  0.2003}
\CommentTok{#> Discus        0.5702  0.115 24.4217}
\CommentTok{#> Pole.vault    0.3947  0.428 15.4356}
\CommentTok{#> Javeline      1.0173  1.054  0.0122}
\CommentTok{#> X1500m       13.7787  6.734  3.2370}
\NormalTok{res.var}\OperatorTok{$}\NormalTok{cos2           }\CommentTok{# Quality of representation }
\CommentTok{#>                 Dim.1    Dim.2   Dim.3    Dim.4  Dim.5    Dim.6    Dim.7}
\CommentTok{#> X100m        7.24e-01 0.032184 0.09094 0.001127 0.0378 1.25e-03 8.34e-03}
\CommentTok{#> Long.jump    6.31e-01 0.078881 0.03631 0.013315 0.0544 1.14e-03 2.38e-02}
\CommentTok{#> Shot.put     5.39e-01 0.007294 0.26791 0.016504 0.0619 5.75e-02 9.77e-05}
\CommentTok{#> High.jump    3.72e-01 0.216424 0.10896 0.020895 0.1622 8.10e-02 7.93e-04}
\CommentTok{#> X400m        4.92e-01 0.084203 0.08039 0.185611 0.0108 2.43e-03 8.19e-02}
\CommentTok{#> X110m.hurdle 5.84e-01 0.000612 0.20150 0.000285 0.0503 6.93e-06 1.37e-01}
\CommentTok{#> Discus       5.52e-01 0.002466 0.03116 0.156032 0.1667 3.94e-02 2.04e-02}
\CommentTok{#> Pole.vault   4.72e-02 0.651977 0.00885 0.114911 0.0491 1.07e-01 1.08e-04}
\CommentTok{#> Javeline     1.83e-01 0.149080 0.36497 0.110048 0.0391 1.31e-01 1.78e-02}
\CommentTok{#> X1500m       1.83e-05 0.615409 0.04817 0.200713 0.0693 1.77e-03 1.24e-02}
\CommentTok{#>                 Dim.8    Dim.9   Dim.10}
\CommentTok{#> X100m        1.10e-02 0.091848 1.97e-03}
\CommentTok{#> Long.jump    1.58e-01 0.002661 8.83e-04}
\CommentTok{#> Shot.put     5.93e-04 0.002284 4.73e-02}
\CommentTok{#> High.jump    7.12e-03 0.012575 1.78e-02}
\CommentTok{#> X400m        5.45e-02 0.006750 1.17e-03}
\CommentTok{#> X110m.hurdle 6.96e-05 0.026166 2.44e-04}
\CommentTok{#> Discus       1.56e-03 0.000179 2.98e-02}
\CommentTok{#> Pole.vault   1.08e-03 0.000664 1.88e-02}
\CommentTok{#> Javeline     2.79e-03 0.001637 1.49e-05}
\CommentTok{#> X1500m       3.78e-02 0.010453 3.95e-03}
\CommentTok{# Results for individuals}
\NormalTok{res.ind <-}\StringTok{ }\KeywordTok{get_pca_ind}\NormalTok{(res.pca)}
\NormalTok{res.ind}\OperatorTok{$}\NormalTok{coord          }\CommentTok{# Coordinates}
\CommentTok{#>              Dim.1  Dim.2  Dim.3   Dim.4     Dim.5   Dim.6   Dim.7}
\CommentTok{#> SEBRLE       0.191 -1.554 -0.628  0.0821  1.142614 -0.4639 -0.2080}
\CommentTok{#> CLAY         0.790 -2.420  1.357  1.2698 -0.806848  1.3042 -0.2129}
\CommentTok{#> BERNARD     -1.329 -1.612 -0.196 -1.9209  0.082343 -0.4006 -0.4064}
\CommentTok{#> YURKOV      -0.869  0.433 -2.474  0.6972  0.398858  0.1029 -0.3249}
\CommentTok{#> ZSIVOCZKY   -0.106  2.023  1.305 -0.0993 -0.197024  0.8955  0.0883}
\CommentTok{#> McMULLEN     0.119  0.992  0.844  1.3122  1.585871  0.1866  0.4783}
\CommentTok{#> MARTINEAU   -2.392  1.285 -0.898  0.3731 -2.243352 -0.4567 -0.2998}
\CommentTok{#> HERNU       -1.891 -1.178 -0.156  0.8913 -0.126741  0.4362 -0.5661}
\CommentTok{#> BARRAS      -1.774  0.413  0.658  0.2287 -0.233837  0.0903  0.2159}
\CommentTok{#> NOOL        -2.777  1.573  0.607 -1.5555  1.424184  0.4972 -0.5321}
\CommentTok{#> BOURGUIGNON -4.414 -1.264 -0.010  0.6668  0.419152 -0.0820 -0.5983}
\CommentTok{#> Sebrle       3.451 -1.217 -1.678 -0.8087 -0.025053 -0.0828  0.0102}
\CommentTok{#> Clay         3.316 -1.623 -0.618 -0.3168  0.569165  0.7772  0.2575}
\CommentTok{#> Karpov       4.070  0.798  1.015  0.3134 -0.797426 -0.3296 -1.3637}
\CommentTok{#> Macey        1.848  2.064 -0.979  0.5847 -0.000216 -0.1973 -0.2693}
\CommentTok{#> Warners      1.387 -0.282  2.000 -1.0196 -0.040540 -0.5567 -0.2674}
\CommentTok{#> Zsivoczky    0.472  0.927 -1.728 -0.1848  0.407303 -0.1138  0.0399}
\CommentTok{#> Hernu        0.276  1.166  0.171 -0.8487 -0.689480 -0.3317  0.4431}
\CommentTok{#> Bernard      1.367  1.478  0.831  0.7453  0.859802 -0.3281  0.3636}
\CommentTok{#> Schwarzl    -0.710 -0.658  1.041 -0.9272 -0.288757 -0.6889  0.5657}
\CommentTok{#> Pogorelov   -0.214 -0.861  0.298  1.3556 -0.015053 -1.5938  0.7837}
\CommentTok{#> Schoenbeck  -0.495 -1.300  0.103 -0.2493 -0.645226  0.1617  0.8575}
\CommentTok{#> Barras      -0.316  0.819 -0.862 -0.5894 -0.779739  1.1742  0.9451}
\CommentTok{#>                Dim.8    Dim.9  Dim.10}
\CommentTok{#> SEBRLE       0.04346 -0.65934  0.0327}
\CommentTok{#> CLAY         0.61724 -0.06013 -0.3172}
\CommentTok{#> BERNARD      0.70386  0.17008 -0.0991}
\CommentTok{#> YURKOV       0.11500 -0.10952 -0.1197}
\CommentTok{#> ZSIVOCZKY   -0.20234 -0.52310 -0.3484}
\CommentTok{#> McMULLEN     0.29309 -0.10562 -0.3932}
\CommentTok{#> MARTINEAU   -0.29163 -0.22342 -0.6164}
\CommentTok{#> HERNU       -1.52940  0.00618  0.5537}
\CommentTok{#> BARRAS       0.68258 -0.66928  0.5309}
\CommentTok{#> NOOL        -0.43339 -0.11578 -0.0962}
\CommentTok{#> BOURGUIGNON  0.56362  0.52581  0.0586}
\CommentTok{#> Sebrle      -0.03059 -0.84721  0.2197}
\CommentTok{#> Clay        -0.58064  0.40978 -0.6160}
\CommentTok{#> Karpov       0.34531  0.19306  0.2172}
\CommentTok{#> Macey       -0.36322  0.36826  0.2125}
\CommentTok{#> Warners     -0.10947  0.18028  0.2421}
\CommentTok{#> Zsivoczky    0.53804  0.58597 -0.1427}
\CommentTok{#> Hernu        0.24729  0.06691 -0.2087}
\CommentTok{#> Bernard      0.00617  0.27949  0.3207}
\CommentTok{#> Schwarzl    -0.68705 -0.00836 -0.3021}
\CommentTok{#> Pogorelov   -0.03762 -0.13053 -0.0370}
\CommentTok{#> Schoenbeck  -0.25585  0.56422  0.2968}
\CommentTok{#> Barras       0.36555  0.10226  0.6119}
\NormalTok{res.ind}\OperatorTok{$}\NormalTok{contrib        }\CommentTok{# Contributions to the PCs}
\CommentTok{#>               Dim.1  Dim.2    Dim.3   Dim.4    Dim.5   Dim.6    Dim.7}
\CommentTok{#> SEBRLE       0.0385  5.712 1.39e+00  0.0357 8.09e+00  2.2126  0.62143}
\CommentTok{#> CLAY         0.6581 13.854 6.46e+00  8.5557 4.03e+00 17.4880  0.65141}
\CommentTok{#> BERNARD      1.8627  6.144 1.35e-01 19.5783 4.20e-02  1.6502  2.37365}
\CommentTok{#> YURKOV       0.7969  0.443 2.15e+01  2.5794 9.86e-01  0.1088  1.51656}
\CommentTok{#> ZSIVOCZKY    0.0118  9.682 5.97e+00  0.0523 2.41e-01  8.2456  0.11192}
\CommentTok{#> McMULLEN     0.0148  2.325 2.50e+00  9.1353 1.56e+01  0.3579  3.28702}
\CommentTok{#> MARTINEAU    6.0337  3.904 2.83e+00  0.7386 3.12e+01  2.1441  1.29111}
\CommentTok{#> HERNU        3.7700  3.284 8.58e-02  4.2151 9.96e-02  1.9566  4.60485}
\CommentTok{#> BARRAS       3.3194  0.402 1.52e+00  0.2776 3.39e-01  0.0838  0.67004}
\CommentTok{#> NOOL         8.1299  5.849 1.29e+00 12.8376 1.26e+01  2.5413  4.06767}
\CommentTok{#> BOURGUIGNON 20.5373  3.776 3.53e-04  2.3588 1.09e+00  0.0691  5.14425}
\CommentTok{#> Sebrle      12.5584  3.502 9.88e+00  3.4701 3.89e-03  0.0705  0.00148}
\CommentTok{#> Clay        11.5936  6.232 1.34e+00  0.5325 2.01e+00  6.2097  0.95282}
\CommentTok{#> Karpov      17.4661  1.507 3.61e+00  0.5210 3.94e+00  1.1168 26.72016}
\CommentTok{#> Macey        3.6021 10.073 3.36e+00  1.8139 2.89e-07  0.4001  1.04191}
\CommentTok{#> Warners      2.0291  0.188 1.40e+01  5.5159 1.02e-02  3.1867  1.02738}
\CommentTok{#> Zsivoczky    0.2344  2.031 1.05e+01  0.1813 1.03e+00  0.1332  0.02289}
\CommentTok{#> Hernu        0.0805  3.214 1.02e-01  3.8217 2.95e+00  1.1311  2.82103}
\CommentTok{#> Bernard      1.9708  5.166 2.43e+00  2.9474 4.58e+00  1.1066  1.89945}
\CommentTok{#> Schwarzl     0.5318  1.025 3.80e+00  4.5612 5.17e-01  4.8796  4.59812}
\CommentTok{#> Pogorelov    0.0484  1.753 3.11e-01  9.7503 1.40e-03 26.1167  8.82532}
\CommentTok{#> Schoenbeck   0.2586  3.997 3.72e-02  0.3297 2.58e+00  0.2689 10.56627}
\CommentTok{#> Barras       0.1052  1.588 2.61e+00  1.8430 3.77e+00 14.1743 12.83542}
\CommentTok{#>                Dim.8    Dim.9  Dim.10}
\CommentTok{#> SEBRLE      2.99e-02 12.17748  0.0382}
\CommentTok{#> CLAY        6.04e+00  0.10126  3.5857}
\CommentTok{#> BERNARD     7.85e+00  0.81032  0.3499}
\CommentTok{#> YURKOV      2.09e-01  0.33601  0.5107}
\CommentTok{#> ZSIVOCZKY   6.49e-01  7.66492  4.3274}
\CommentTok{#> McMULLEN    1.36e+00  0.31250  5.5105}
\CommentTok{#> MARTINEAU   1.35e+00  1.39820 13.5440}
\CommentTok{#> HERNU       3.71e+01  0.00107 10.9278}
\CommentTok{#> BARRAS      7.38e+00 12.54733 10.0454}
\CommentTok{#> NOOL        2.98e+00  0.37548  0.3300}
\CommentTok{#> BOURGUIGNON 5.03e+00  7.74457  0.1222}
\CommentTok{#> Sebrle      1.48e-02 20.10555  1.7206}
\CommentTok{#> Clay        5.34e+00  4.70357 13.5271}
\CommentTok{#> Karpov      1.89e+00  1.04399  1.6819}
\CommentTok{#> Macey       2.09e+00  3.79877  1.6096}
\CommentTok{#> Warners     1.90e-01  0.91042  2.0890}
\CommentTok{#> Zsivoczky   4.59e+00  9.61785  0.7261}
\CommentTok{#> Hernu       9.69e-01  0.12540  1.5523}
\CommentTok{#> Bernard     6.02e-04  2.18807  3.6657}
\CommentTok{#> Schwarzl    7.48e+00  0.00196  3.2536}
\CommentTok{#> Pogorelov   2.24e-02  0.47727  0.0487}
\CommentTok{#> Schoenbeck  1.04e+00  8.91730  3.1402}
\CommentTok{#> Barras      2.12e+00  0.29289 13.3453}
\NormalTok{res.ind}\OperatorTok{$}\NormalTok{cos2           }\CommentTok{# Quality of representation }
\CommentTok{#>               Dim.1  Dim.2    Dim.3   Dim.4    Dim.5    Dim.6    Dim.7}
\CommentTok{#> SEBRLE      0.00753 0.4975 8.13e-02 0.00139 2.69e-01 0.044324 8.91e-03}
\CommentTok{#> CLAY        0.04870 0.4570 1.44e-01 0.12579 5.08e-02 0.132691 3.54e-03}
\CommentTok{#> BERNARD     0.19720 0.2900 4.29e-03 0.41182 7.57e-04 0.017913 1.84e-02}
\CommentTok{#> YURKOV      0.09611 0.0238 7.78e-01 0.06181 2.02e-02 0.001345 1.34e-02}
\CommentTok{#> ZSIVOCZKY   0.00157 0.5764 2.40e-01 0.00139 5.47e-03 0.112918 1.10e-03}
\CommentTok{#> McMULLEN    0.00218 0.1522 1.10e-01 0.26649 3.89e-01 0.005388 3.54e-02}
\CommentTok{#> MARTINEAU   0.40401 0.1165 5.69e-02 0.00983 3.55e-01 0.014721 6.34e-03}
\CommentTok{#> HERNU       0.39928 0.1551 2.73e-03 0.08870 1.79e-03 0.021248 3.58e-02}
\CommentTok{#> BARRAS      0.61624 0.0333 8.48e-02 0.01024 1.07e-02 0.001594 9.13e-03}
\CommentTok{#> NOOL        0.48987 0.1571 2.34e-02 0.15369 1.29e-01 0.015701 1.80e-02}
\CommentTok{#> BOURGUIGNON 0.85970 0.0705 4.45e-06 0.01962 7.75e-03 0.000297 1.58e-02}
\CommentTok{#> Sebrle      0.67538 0.0840 1.60e-01 0.03708 3.56e-05 0.000389 5.85e-06}
\CommentTok{#> Clay        0.68759 0.1648 2.39e-02 0.00627 2.03e-02 0.037763 4.15e-03}
\CommentTok{#> Karpov      0.78367 0.0301 4.87e-02 0.00464 3.01e-02 0.005138 8.80e-02}
\CommentTok{#> Macey       0.36344 0.4531 1.02e-01 0.03636 4.95e-09 0.004140 7.71e-03}
\CommentTok{#> Warners     0.25565 0.0106 5.31e-01 0.13808 2.18e-04 0.041169 9.50e-03}
\CommentTok{#> Zsivoczky   0.04505 0.1740 6.05e-01 0.00692 3.36e-02 0.002625 3.23e-04}
\CommentTok{#> Hernu       0.02482 0.4418 9.46e-03 0.23420 1.55e-01 0.035771 6.38e-02}
\CommentTok{#> Bernard     0.28935 0.3381 1.07e-01 0.08598 1.14e-01 0.016659 2.05e-02}
\CommentTok{#> Schwarzl    0.11672 0.1003 2.51e-01 0.19889 1.93e-02 0.109806 7.40e-02}
\CommentTok{#> Pogorelov   0.00780 0.1259 1.50e-02 0.31210 3.85e-05 0.431416 1.04e-01}
\CommentTok{#> Schoenbeck  0.06707 0.4620 2.90e-03 0.01699 1.14e-01 0.007150 2.01e-01}
\CommentTok{#> Barras      0.01897 0.1277 1.41e-01 0.06604 1.16e-01 0.262130 1.70e-01}
\CommentTok{#>                Dim.8    Dim.9   Dim.10}
\CommentTok{#> SEBRLE      3.89e-04 8.95e-02 0.000221}
\CommentTok{#> CLAY        2.97e-02 2.82e-04 0.007847}
\CommentTok{#> BERNARD     5.53e-02 3.23e-03 0.001096}
\CommentTok{#> YURKOV      1.68e-03 1.53e-03 0.001822}
\CommentTok{#> ZSIVOCZKY   5.76e-03 3.85e-02 0.017092}
\CommentTok{#> McMULLEN    1.33e-02 1.73e-03 0.023927}
\CommentTok{#> MARTINEAU   6.00e-03 3.52e-03 0.026821}
\CommentTok{#> HERNU       2.61e-01 4.27e-06 0.034229}
\CommentTok{#> BARRAS      9.12e-02 8.77e-02 0.055153}
\CommentTok{#> NOOL        1.19e-02 8.51e-04 0.000588}
\CommentTok{#> BOURGUIGNON 1.40e-02 1.22e-02 0.000151}
\CommentTok{#> Sebrle      5.30e-05 4.07e-02 0.002737}
\CommentTok{#> Clay        2.11e-02 1.05e-02 0.023726}
\CommentTok{#> Karpov      5.64e-03 1.76e-03 0.002232}
\CommentTok{#> Macey       1.40e-02 1.44e-02 0.004803}
\CommentTok{#> Warners     1.59e-03 4.32e-03 0.007784}
\CommentTok{#> Zsivoczky   5.87e-02 6.96e-02 0.004127}
\CommentTok{#> Hernu       1.99e-02 1.46e-03 0.014160}
\CommentTok{#> Bernard     5.88e-06 1.21e-02 0.015917}
\CommentTok{#> Schwarzl    1.09e-01 1.62e-05 0.021117}
\CommentTok{#> Pogorelov   2.40e-04 2.89e-03 0.000232}
\CommentTok{#> Schoenbeck  1.79e-02 8.70e-02 0.024083}
\CommentTok{#> Barras      2.54e-02 1.99e-03 0.071184}
\end{Highlighting}
\end{Shaded}

\hypertarget{predict-using-pca}{%
\section{Predict using PCA}\label{predict-using-pca}}

In this section, we'll show how to predict the coordinates of supplementary individuals and variables using only the information provided by the previously performed PCA.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Data: rows 24 to 27 and columns 1 to to 10 {[}in decathlon2 data sets{]}. The new data must contain columns (variables) with the same names and in the same order as the active data used to compute PCA.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Data for the supplementary individuals}
\NormalTok{ind.sup <-}\StringTok{ }\NormalTok{decathlon2[}\DecValTok{24}\OperatorTok{:}\DecValTok{27}\NormalTok{, }\DecValTok{1}\OperatorTok{:}\DecValTok{10}\NormalTok{]}
\NormalTok{ind.sup[, }\DecValTok{1}\OperatorTok{:}\DecValTok{6}\NormalTok{]}
\CommentTok{#>         X100m Long.jump Shot.put High.jump X400m X110m.hurdle}
\CommentTok{#> KARPOV   11.0      7.30     14.8      2.04  48.4         14.1}
\CommentTok{#> WARNERS  11.1      7.60     14.3      1.98  48.7         14.2}
\CommentTok{#> Nool     10.8      7.53     14.3      1.88  48.8         14.8}
\CommentTok{#> Drews    10.9      7.38     13.1      1.88  48.5         14.0}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Predict the coordinates of new individuals data. Use the R base function predict():
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ind.sup.coord <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(res.pca, }\DataTypeTok{newdata =}\NormalTok{ ind.sup)}
\NormalTok{ind.sup.coord[, }\DecValTok{1}\OperatorTok{:}\DecValTok{4}\NormalTok{]}
\CommentTok{#>            PC1    PC2   PC3    PC4}
\CommentTok{#> KARPOV   0.777 -0.762 1.597  1.686}
\CommentTok{#> WARNERS -0.378  0.119 1.701 -0.691}
\CommentTok{#> Nool    -0.547 -1.934 0.472 -2.228}
\CommentTok{#> Drews   -1.085 -0.017 2.982 -1.501}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Graph of individuals including the supplementary individuals:
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Plot of active individuals}
\NormalTok{p <-}\StringTok{ }\KeywordTok{fviz_pca_ind}\NormalTok{(res.pca, }\DataTypeTok{repel =} \OtherTok{TRUE}\NormalTok{)}
\CommentTok{# Add supplementary individuals}
\KeywordTok{fviz_add}\NormalTok{(p, ind.sup.coord, }\DataTypeTok{color =}\StringTok{"blue"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{meta_110a-PCA-prcomp_vs_princomp_files/figure-latex/unnamed-chunk-15-1} \end{center}

The predicted coordinates of individuals can be manually calculated as follow:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Center and scale the new individuals data using the center and the scale of the PCA
\item
  Calculate the predicted coordinates by multiplying the scaled values with the eigenvectors (loadings) of the principal components.
  The R code below can be used :
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Centering and scaling the supplementary individuals}
\NormalTok{ind.scaled <-}\StringTok{ }\KeywordTok{scale}\NormalTok{(ind.sup, }
                    \DataTypeTok{center =}\NormalTok{ res.pca}\OperatorTok{$}\NormalTok{center,}
                    \DataTypeTok{scale =}\NormalTok{ res.pca}\OperatorTok{$}\NormalTok{scale)}
\CommentTok{# Coordinates of the individividuals}
\NormalTok{coord_func <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(ind, loadings)\{}
\NormalTok{  r <-}\StringTok{ }\NormalTok{loadings}\OperatorTok{*}\NormalTok{ind}
  \KeywordTok{apply}\NormalTok{(r, }\DecValTok{2}\NormalTok{, sum)}
\NormalTok{\}}
\NormalTok{pca.loadings <-}\StringTok{ }\NormalTok{res.pca}\OperatorTok{$}\NormalTok{rotation}
\NormalTok{ind.sup.coord <-}\StringTok{ }\KeywordTok{t}\NormalTok{(}\KeywordTok{apply}\NormalTok{(ind.scaled, }\DecValTok{1}\NormalTok{, coord_func, pca.loadings ))}
\NormalTok{ind.sup.coord[, }\DecValTok{1}\OperatorTok{:}\DecValTok{4}\NormalTok{]}
\CommentTok{#>            PC1    PC2   PC3    PC4}
\CommentTok{#> KARPOV   0.777 -0.762 1.597  1.686}
\CommentTok{#> WARNERS -0.378  0.119 1.701 -0.691}
\CommentTok{#> Nool    -0.547 -1.934 0.472 -2.228}
\CommentTok{#> Drews   -1.085 -0.017 2.982 -1.501}
\end{Highlighting}
\end{Shaded}

\hypertarget{supplementary-variables}{%
\section{Supplementary variables}\label{supplementary-variables}}

\hypertarget{qualitative-categorical-variables}{%
\subsection{Qualitative / categorical variables}\label{qualitative-categorical-variables}}

The data sets decathlon2 contain a supplementary qualitative variable at columns 13 corresponding to the type of competitions.

Qualitative / categorical variables can be used to color individuals by groups. The grouping variable should be of same length as the number of active individuals (here 23).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{groups <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(decathlon2}\OperatorTok{$}\NormalTok{Competition[}\DecValTok{1}\OperatorTok{:}\DecValTok{23}\NormalTok{])}
\KeywordTok{fviz_pca_ind}\NormalTok{(res.pca,}
             \DataTypeTok{col.ind =}\NormalTok{ groups, }\CommentTok{# color by groups}
             \DataTypeTok{palette =} \KeywordTok{c}\NormalTok{(}\StringTok{"#00AFBB"}\NormalTok{,  }\StringTok{"#FC4E07"}\NormalTok{),}
             \DataTypeTok{addEllipses =} \OtherTok{TRUE}\NormalTok{, }\CommentTok{# Concentration ellipses}
             \DataTypeTok{ellipse.type =} \StringTok{"confidence"}\NormalTok{,}
             \DataTypeTok{legend.title =} \StringTok{"Groups"}\NormalTok{,}
             \DataTypeTok{repel =} \OtherTok{TRUE}
\NormalTok{             )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{meta_110a-PCA-prcomp_vs_princomp_files/figure-latex/unnamed-chunk-17-1} \end{center}

Calculate the coordinates for the levels of grouping variables. The coordinates for a given group is calculated as the mean coordinates of the individuals in the group.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(magrittr) }\CommentTok{# for pipe %>%}
\KeywordTok{library}\NormalTok{(dplyr)   }\CommentTok{# everything else}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'dplyr'}
\CommentTok{#> The following objects are masked from 'package:stats':}
\CommentTok{#> }
\CommentTok{#>     filter, lag}
\CommentTok{#> The following objects are masked from 'package:base':}
\CommentTok{#> }
\CommentTok{#>     intersect, setdiff, setequal, union}

\CommentTok{# 1. Individual coordinates}
\NormalTok{res.ind <-}\StringTok{ }\KeywordTok{get_pca_ind}\NormalTok{(res.pca)}
\CommentTok{# 2. Coordinate of groups}
\NormalTok{coord.groups <-}\StringTok{ }\NormalTok{res.ind}\OperatorTok{$}\NormalTok{coord }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{as_data_frame}\NormalTok{() }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(Dim}\FloatTok{.1}\NormalTok{, Dim}\FloatTok{.2}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{competition =}\NormalTok{ groups) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(competition) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}
    \DataTypeTok{Dim.1 =} \KeywordTok{mean}\NormalTok{(Dim}\FloatTok{.1}\NormalTok{),}
    \DataTypeTok{Dim.2 =} \KeywordTok{mean}\NormalTok{(Dim}\FloatTok{.2}\NormalTok{)}
\NormalTok{    )}
\CommentTok{#> Warning: `as_data_frame()` is deprecated, use `as_tibble()` (but mind the new semantics).}
\CommentTok{#> This warning is displayed once per session.}
\NormalTok{coord.groups}
\CommentTok{#> # A tibble: 2 x 3}
\CommentTok{#>   competition Dim.1  Dim.2}
\CommentTok{#>   <fct>       <dbl>  <dbl>}
\CommentTok{#> 1 Decastar    -1.31 -0.119}
\CommentTok{#> 2 OlympicG     1.20  0.109}
\end{Highlighting}
\end{Shaded}

\hypertarget{quantitative-variables}{%
\subsection{Quantitative variables}\label{quantitative-variables}}

Data: columns 11:12. Should be of same length as the number of active individuals (here 23)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{quanti.sup <-}\StringTok{ }\NormalTok{decathlon2[}\DecValTok{1}\OperatorTok{:}\DecValTok{23}\NormalTok{, }\DecValTok{11}\OperatorTok{:}\DecValTok{12}\NormalTok{, drop =}\StringTok{ }\OtherTok{FALSE}\NormalTok{]}
\KeywordTok{head}\NormalTok{(quanti.sup)}
\CommentTok{#>           Rank Points}
\CommentTok{#> SEBRLE       1   8217}
\CommentTok{#> CLAY         2   8122}
\CommentTok{#> BERNARD      4   8067}
\CommentTok{#> YURKOV       5   8036}
\CommentTok{#> ZSIVOCZKY    7   8004}
\CommentTok{#> McMULLEN     8   7995}
\end{Highlighting}
\end{Shaded}

The coordinates of a given quantitative variable are calculated as the correlation between the quantitative variables and the principal components.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Predict coordinates and compute cos2}
\NormalTok{quanti.coord <-}\StringTok{ }\KeywordTok{cor}\NormalTok{(quanti.sup, res.pca}\OperatorTok{$}\NormalTok{x)}
\NormalTok{quanti.cos2 <-}\StringTok{ }\NormalTok{quanti.coord}\OperatorTok{^}\DecValTok{2}
\CommentTok{# Graph of variables including supplementary variables}
\NormalTok{p <-}\StringTok{ }\KeywordTok{fviz_pca_var}\NormalTok{(res.pca)}
\KeywordTok{fviz_add}\NormalTok{(p, quanti.coord, }\DataTypeTok{color =}\StringTok{"blue"}\NormalTok{, }\DataTypeTok{geom=}\StringTok{"arrow"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{meta_110a-PCA-prcomp_vs_princomp_files/figure-latex/unnamed-chunk-20-1} \end{center}

\hypertarget{theory-behind-pca-results}{%
\section{Theory behind PCA results}\label{theory-behind-pca-results}}

\hypertarget{pca-results-for-variables}{%
\subsection{PCA results for variables}\label{pca-results-for-variables}}

Here we'll show how to calculate the PCA results for variables: coordinates, cos2 and contributions:

\texttt{var.coord} = loadings * the component standard deviations
\texttt{var.cos2} = var.coord\^{}2
\texttt{var.contrib}. The contribution of a variable to a given principal component is (in percentage) : (var.cos2 * 100) / (total cos2 of the component)

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Helper function }
\CommentTok{#::::::::::::::::::::::::::::::::::::::::}
\NormalTok{var_coord_func <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(loadings, comp.sdev)\{}
\NormalTok{  loadings}\OperatorTok{*}\NormalTok{comp.sdev}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Compute Coordinates}
\CommentTok{#::::::::::::::::::::::::::::::::::::::::}
\NormalTok{loadings <-}\StringTok{ }\NormalTok{res.pca}\OperatorTok{$}\NormalTok{rotation}
\NormalTok{sdev <-}\StringTok{ }\NormalTok{res.pca}\OperatorTok{$}\NormalTok{sdev}
\NormalTok{var.coord <-}\StringTok{ }\KeywordTok{t}\NormalTok{(}\KeywordTok{apply}\NormalTok{(loadings, }\DecValTok{1}\NormalTok{, var_coord_func, sdev)) }
\KeywordTok{head}\NormalTok{(var.coord[, }\DecValTok{1}\OperatorTok{:}\DecValTok{4}\NormalTok{])}
\CommentTok{#>                 PC1     PC2    PC3     PC4}
\CommentTok{#> X100m        -0.851  0.1794 -0.302  0.0336}
\CommentTok{#> Long.jump     0.794 -0.2809  0.191 -0.1154}
\CommentTok{#> Shot.put      0.734 -0.0854 -0.518  0.1285}
\CommentTok{#> High.jump     0.610  0.4652 -0.330  0.1446}
\CommentTok{#> X400m        -0.702 -0.2902 -0.284  0.4308}
\CommentTok{#> X110m.hurdle -0.764  0.0247 -0.449 -0.0169}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Compute Cos2}
\CommentTok{#::::::::::::::::::::::::::::::::::::::::}
\NormalTok{var.cos2 <-}\StringTok{ }\NormalTok{var.coord}\OperatorTok{^}\DecValTok{2}
\KeywordTok{head}\NormalTok{(var.cos2[, }\DecValTok{1}\OperatorTok{:}\DecValTok{4}\NormalTok{])}
\CommentTok{#>                PC1      PC2    PC3      PC4}
\CommentTok{#> X100m        0.724 0.032184 0.0909 0.001127}
\CommentTok{#> Long.jump    0.631 0.078881 0.0363 0.013315}
\CommentTok{#> Shot.put     0.539 0.007294 0.2679 0.016504}
\CommentTok{#> High.jump    0.372 0.216424 0.1090 0.020895}
\CommentTok{#> X400m        0.492 0.084203 0.0804 0.185611}
\CommentTok{#> X110m.hurdle 0.584 0.000612 0.2015 0.000285}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Compute contributions}
\CommentTok{#::::::::::::::::::::::::::::::::::::::::}
\NormalTok{comp.cos2 <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(var.cos2, }\DecValTok{2}\NormalTok{, sum)}
\NormalTok{contrib <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(var.cos2, comp.cos2)\{var.cos2}\OperatorTok{*}\DecValTok{100}\OperatorTok{/}\NormalTok{comp.cos2\}}
\NormalTok{var.contrib <-}\StringTok{ }\KeywordTok{t}\NormalTok{(}\KeywordTok{apply}\NormalTok{(var.cos2,}\DecValTok{1}\NormalTok{, contrib, comp.cos2))}
\KeywordTok{head}\NormalTok{(var.contrib[, }\DecValTok{1}\OperatorTok{:}\DecValTok{4}\NormalTok{])}
\CommentTok{#>                PC1     PC2   PC3     PC4}
\CommentTok{#> X100m        17.54  1.7505  7.34  0.1376}
\CommentTok{#> Long.jump    15.29  4.2904  2.93  1.6249}
\CommentTok{#> Shot.put     13.06  0.3967 21.62  2.0141}
\CommentTok{#> High.jump     9.02 11.7716  8.79  2.5499}
\CommentTok{#> X400m        11.94  4.5799  6.49 22.6509}
\CommentTok{#> X110m.hurdle 14.16  0.0333 16.26  0.0348}
\end{Highlighting}
\end{Shaded}

\hypertarget{pca-results-for-individuals}{%
\subsection{PCA results for individuals}\label{pca-results-for-individuals}}

\begin{itemize}
\tightlist
\item
  \texttt{ind.coord} = res.pca\$x
\item
  Cos2 of individuals. Two steps:

  \begin{itemize}
  \tightlist
  \item
    Calculate the square distance between each individual and the PCA center of gravity: d2 = {[}(var1\_ind\_i - mean\_var1)/sd\_var1{]}\^{}2 + \ldots{}+ {[}(var10\_ind\_i - mean\_var10)/sd\_var10{]}\^{}2 + \ldots{}+..
  \item
    Calculate the cos2 as ind.coord\^{}2/d2
  \end{itemize}
\item
  Contributions of individuals to the principal components: 100 * (1 / number\_of\_individuals)*(ind.coord\^{}2 / comp\_sdev\^{}2). Note that the sum of all the contributions per column is 100
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Coordinates of individuals}
\CommentTok{#::::::::::::::::::::::::::::::::::}
\NormalTok{ind.coord <-}\StringTok{ }\NormalTok{res.pca}\OperatorTok{$}\NormalTok{x}
\KeywordTok{head}\NormalTok{(ind.coord[, }\DecValTok{1}\OperatorTok{:}\DecValTok{4}\NormalTok{])}
\CommentTok{#>              PC1    PC2    PC3     PC4}
\CommentTok{#> SEBRLE     0.191 -1.554 -0.628  0.0821}
\CommentTok{#> CLAY       0.790 -2.420  1.357  1.2698}
\CommentTok{#> BERNARD   -1.329 -1.612 -0.196 -1.9209}
\CommentTok{#> YURKOV    -0.869  0.433 -2.474  0.6972}
\CommentTok{#> ZSIVOCZKY -0.106  2.023  1.305 -0.0993}
\CommentTok{#> McMULLEN   0.119  0.992  0.844  1.3122}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Cos2 of individuals}
\CommentTok{#:::::::::::::::::::::::::::::::::}
\CommentTok{# 1. square of the distance between an individual and the}
\CommentTok{# PCA center of gravity}
\NormalTok{center <-}\StringTok{ }\NormalTok{res.pca}\OperatorTok{$}\NormalTok{center}
\NormalTok{scale<-}\StringTok{ }\NormalTok{res.pca}\OperatorTok{$}\NormalTok{scale}

\NormalTok{getdistance <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(ind_row, center, scale)\{}
  \KeywordTok{return}\NormalTok{(}\KeywordTok{sum}\NormalTok{(((ind_row}\OperatorTok{-}\NormalTok{center)}\OperatorTok{/}\NormalTok{scale)}\OperatorTok{^}\DecValTok{2}\NormalTok{))}
\NormalTok{\}}

\NormalTok{d2 <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(decathlon2.active,}\DecValTok{1}\NormalTok{, getdistance, center, scale)}
\CommentTok{# 2. Compute the cos2. The sum of each row is 1}
\NormalTok{cos2 <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(ind.coord, d2)\{}\KeywordTok{return}\NormalTok{(ind.coord}\OperatorTok{^}\DecValTok{2}\OperatorTok{/}\NormalTok{d2)\}}
\NormalTok{ind.cos2 <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(ind.coord, }\DecValTok{2}\NormalTok{, cos2, d2)}
\KeywordTok{head}\NormalTok{(ind.cos2[, }\DecValTok{1}\OperatorTok{:}\DecValTok{4}\NormalTok{])}
\CommentTok{#>               PC1    PC2     PC3     PC4}
\CommentTok{#> SEBRLE    0.00753 0.4975 0.08133 0.00139}
\CommentTok{#> CLAY      0.04870 0.4570 0.14363 0.12579}
\CommentTok{#> BERNARD   0.19720 0.2900 0.00429 0.41182}
\CommentTok{#> YURKOV    0.09611 0.0238 0.77823 0.06181}
\CommentTok{#> ZSIVOCZKY 0.00157 0.5764 0.23975 0.00139}
\CommentTok{#> McMULLEN  0.00218 0.1522 0.11014 0.26649}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Contributions of individuals}
\CommentTok{#:::::::::::::::::::::::::::::::}
\NormalTok{contrib <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(ind.coord, comp.sdev, n.ind)\{}
  \DecValTok{100}\OperatorTok{*}\NormalTok{(}\DecValTok{1}\OperatorTok{/}\NormalTok{n.ind)}\OperatorTok{*}\NormalTok{ind.coord}\OperatorTok{^}\DecValTok{2}\OperatorTok{/}\NormalTok{comp.sdev}\OperatorTok{^}\DecValTok{2}
\NormalTok{\}}
\NormalTok{ind.contrib <-}\StringTok{ }\KeywordTok{t}\NormalTok{(}\KeywordTok{apply}\NormalTok{(ind.coord, }\DecValTok{1}\NormalTok{, contrib, }
\NormalTok{                       res.pca}\OperatorTok{$}\NormalTok{sdev, }\KeywordTok{nrow}\NormalTok{(ind.coord)))}
\KeywordTok{head}\NormalTok{(ind.contrib[, }\DecValTok{1}\OperatorTok{:}\DecValTok{4}\NormalTok{])}
\CommentTok{#>              PC1    PC2    PC3     PC4}
\CommentTok{#> SEBRLE    0.0385  5.712  1.385  0.0357}
\CommentTok{#> CLAY      0.6581 13.854  6.460  8.5557}
\CommentTok{#> BERNARD   1.8627  6.144  0.135 19.5783}
\CommentTok{#> YURKOV    0.7969  0.443 21.476  2.5794}
\CommentTok{#> ZSIVOCZKY 0.0118  9.682  5.975  0.0523}
\CommentTok{#> McMULLEN  0.0148  2.325  2.497  9.1353}
\end{Highlighting}
\end{Shaded}

\hypertarget{principal-components-methods}{%
\chapter{Principal Components Methods}\label{principal-components-methods}}

\url{http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/112-pca-principal-component-analysis-essentials/}

Principal component analysis (PCA) allows us to summarize and to visualize the information in a data set containing individuals/observations described by multiple inter-correlated quantitative variables. Each variable could be considered as a different dimension. If you have more than 3 variables in your data sets, it could be very difficult to visualize a multi-dimensional hyperspace.

Principal component analysis is used to extract the important information from a multivariate data table and to express this information as a set of few new variables called principal components. These new variables correspond to a linear combination of the originals. The number of principal components is less than or equal to the number of original variables.

The information in a given data set corresponds to the total variation it contains. The goal of PCA is to identify directions (or principal components) along which the variation in the data is maximal.

In other words, PCA reduces the dimensionality of a multivariate data to two or three principal components, that can be visualized graphically, with minimal loss of information.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# install.packages(c("FactoMineR", "factoextra"))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(FactoMineR)}
\KeywordTok{library}\NormalTok{(factoextra)}
\CommentTok{#> Loading required package: ggplot2}
\CommentTok{#> Registered S3 methods overwritten by 'ggplot2':}
\CommentTok{#>   method         from }
\CommentTok{#>   [.quosures     rlang}
\CommentTok{#>   c.quosures     rlang}
\CommentTok{#>   print.quosures rlang}
\CommentTok{#> Welcome! Related Books: `Practical Guide To Cluster Analysis in R` at https://goo.gl/13EFCZ}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{data}\NormalTok{(decathlon2)}
\CommentTok{# head(decathlon2)}
\end{Highlighting}
\end{Shaded}

In PCA terminology, our data contains :

\begin{itemize}
\item
  Active individuals (in light blue, rows 1:23) : Individuals that are used during the principal component analysis.
\item
  Supplementary individuals (in dark blue, rows 24:27) : The coordinates of these individuals will be predicted using the PCA information and parameters obtained with active individuals/variables
\item
  Active variables (in pink, columns 1:10) : Variables that are used for the principal component analysis.
\item
  Supplementary variables: As supplementary individuals, the coordinates of these variables will be predicted also. These can be:

  \begin{itemize}
  \tightlist
  \item
    Supplementary continuous variables (red): Columns 11 and 12 corresponding respectively to the rank and the points of athletes.
  \item
    Supplementary qualitative variables (green): Column 13 corresponding to the two athlete-tic meetings (2004 Olympic Game or 2004 Decastar). This is a categorical (or factor) variable factor. It can be used to color individuals by groups.
  \end{itemize}
\end{itemize}

We start by subsetting active individuals and active variables for the principal component analysis:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{decathlon2.active <-}\StringTok{ }\NormalTok{decathlon2[}\DecValTok{1}\OperatorTok{:}\DecValTok{23}\NormalTok{, }\DecValTok{1}\OperatorTok{:}\DecValTok{10}\NormalTok{]}
\KeywordTok{head}\NormalTok{(decathlon2.active[, }\DecValTok{1}\OperatorTok{:}\DecValTok{6}\NormalTok{], }\DecValTok{4}\NormalTok{)}
\CommentTok{#>         X100m Long.jump Shot.put High.jump X400m X110m.hurdle}
\CommentTok{#> SEBRLE   11.0      7.58     14.8      2.07  49.8         14.7}
\CommentTok{#> CLAY     10.8      7.40     14.3      1.86  49.4         14.1}
\CommentTok{#> BERNARD  11.0      7.23     14.2      1.92  48.9         15.0}
\CommentTok{#> YURKOV   11.3      7.09     15.2      2.10  50.4         15.3}
\end{Highlighting}
\end{Shaded}

\hypertarget{data-standardization}{%
\section{Data standardization}\label{data-standardization}}

In principal component analysis, variables are often scaled (i.e.~standardized). This is particularly recommended when variables are measured in different scales (e.g: kilograms, kilometers, centimeters, \ldots{}); otherwise, the PCA outputs obtained will be severely affected.

The goal is to make the variables comparable. Generally variables are scaled to have i) standard deviation one and ii) mean zero.

The function PCA() {[}FactoMineR package{]} can be used. A simplified format is:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(FactoMineR)}
\NormalTok{res.pca <-}\StringTok{ }\KeywordTok{PCA}\NormalTok{(decathlon2.active, }\DataTypeTok{graph =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{print}\NormalTok{(res.pca)}
\CommentTok{#> **Results for the Principal Component Analysis (PCA)**}
\CommentTok{#> The analysis was performed on 23 individuals, described by 10 variables}
\CommentTok{#> *The results are available in the following objects:}
\CommentTok{#> }
\CommentTok{#>    name               description                          }
\CommentTok{#> 1  "$eig"             "eigenvalues"                        }
\CommentTok{#> 2  "$var"             "results for the variables"          }
\CommentTok{#> 3  "$var$coord"       "coord. for the variables"           }
\CommentTok{#> 4  "$var$cor"         "correlations variables - dimensions"}
\CommentTok{#> 5  "$var$cos2"        "cos2 for the variables"             }
\CommentTok{#> 6  "$var$contrib"     "contributions of the variables"     }
\CommentTok{#> 7  "$ind"             "results for the individuals"        }
\CommentTok{#> 8  "$ind$coord"       "coord. for the individuals"         }
\CommentTok{#> 9  "$ind$cos2"        "cos2 for the individuals"           }
\CommentTok{#> 10 "$ind$contrib"     "contributions of the individuals"   }
\CommentTok{#> 11 "$call"            "summary statistics"                 }
\CommentTok{#> 12 "$call$centre"     "mean of the variables"              }
\CommentTok{#> 13 "$call$ecart.type" "standard error of the variables"    }
\CommentTok{#> 14 "$call$row.w"      "weights for the individuals"        }
\CommentTok{#> 15 "$call$col.w"      "weights for the variables"}
\end{Highlighting}
\end{Shaded}

\begin{quote}
The object that is created using the function PCA() contains many information found in many different lists and matrices. These values are described in the next section.
\end{quote}

\hypertarget{eigenvalues-variances}{%
\section{Eigenvalues / Variances}\label{eigenvalues-variances}}

As described in previous sections, the eigenvalues measure the amount of variation retained by each principal component. Eigenvalues are large for the first PCs and small for the subsequent PCs. That is, the first PCs corresponds to the directions with the maximum amount of variation in the data set.

We examine the eigenvalues to determine the number of principal components to be considered. The eigenvalues and the proportion of variances (i.e., information) retained by the principal components (PCs) can be extracted using the function get\_eigenvalue() {[}factoextra package{]}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(factoextra)}
\NormalTok{eig.val <-}\StringTok{ }\KeywordTok{get_eigenvalue}\NormalTok{(res.pca)}
\NormalTok{eig.val}
\CommentTok{#>        eigenvalue variance.percent cumulative.variance.percent}
\CommentTok{#> Dim.1       4.124            41.24                        41.2}
\CommentTok{#> Dim.2       1.839            18.39                        59.6}
\CommentTok{#> Dim.3       1.239            12.39                        72.0}
\CommentTok{#> Dim.4       0.819             8.19                        80.2}
\CommentTok{#> Dim.5       0.702             7.02                        87.2}
\CommentTok{#> Dim.6       0.423             4.23                        91.5}
\CommentTok{#> Dim.7       0.303             3.03                        94.5}
\CommentTok{#> Dim.8       0.274             2.74                        97.2}
\CommentTok{#> Dim.9       0.155             1.55                        98.8}
\CommentTok{#> Dim.10      0.122             1.22                       100.0}
\end{Highlighting}
\end{Shaded}

The sum of all the eigenvalues give a total variance of 10.

The proportion of variation explained by each eigenvalue is given in the second column. For example, 4.124 divided by 10 equals 0.4124, or, about 41.24\% of the variation is explained by this first eigenvalue. The cumulative percentage explained is obtained by adding the successive proportions of variation explained to obtain the running total. For instance, 41.242\% plus 18.385\% equals 59.627\%, and so forth. Therefore, about 59.627\% of the variation is explained by the first two eigenvalues together.

Unfortunately, there is no well-accepted objective way to decide how many principal components are enough. This will depend on the specific field of application and the specific data set. In practice, we tend to look at the first few principal components in order to find interesting patterns in the data.

In our analysis, the first three principal components explain 72\% of the variation. This is an acceptably large percentage.

An alternative method to determine the number of principal components is to look at a Scree Plot, which is the plot of eigenvalues ordered from largest to the smallest. The number of component is determined at the point, beyond which the remaining eigenvalues are all relatively small and of comparable size (Jollife 2002, Peres-Neto, Jackson, and Somers (2005)).

The scree plot can be produced using the function fviz\_eig() or fviz\_screeplot() {[}factoextra package{]}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{fviz_eig}\NormalTok{(res.pca, }\DataTypeTok{addlabels =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{50}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{meta_110b-PCA-Principal_Component_Methods_in_R_files/figure-latex/unnamed-chunk-9-1} \end{center}

From the plot above, we might want to stop at the fifth principal component. 87\% of the information (variances) contained in the data are retained by the first five principal components.

\hypertarget{graph-of-variables}{%
\section{Graph of variables}\label{graph-of-variables}}

Results
A simple method to extract the results, for variables, from a PCA output is to use the function get\_pca\_var() {[}factoextra package{]}. This function provides a list of matrices containing all the results for the active variables (coordinates, correlation between variables and axes, squared cosine and contributions)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{var <-}\StringTok{ }\KeywordTok{get_pca_var}\NormalTok{(res.pca)}
\NormalTok{var}
\CommentTok{#> Principal Component Analysis Results for variables}
\CommentTok{#>  ===================================================}
\CommentTok{#>   Name       Description                                    }
\CommentTok{#> 1 "$coord"   "Coordinates for the variables"                }
\CommentTok{#> 2 "$cor"     "Correlations between variables and dimensions"}
\CommentTok{#> 3 "$cos2"    "Cos2 for the variables"                       }
\CommentTok{#> 4 "$contrib" "contributions of the variables"}
\end{Highlighting}
\end{Shaded}

The components of the get\_pca\_var() can be used in the plot of variables as follow:

\begin{itemize}
\tightlist
\item
  var\$coord: coordinates of variables to create a scatter plot
\item
  var\$cos2: represents the quality of representation for variables on the factor map. It's calculated as the squared coordinates: var.cos2 = var.coord * var.coord.
\item
  var\$contrib: contains the contributions (in percentage) of the variables to the principal components. The contribution of a variable (var) to a given principal component is (in percentage) : (var.cos2 * 100) / (total cos2 of the component).
\end{itemize}

\begin{quote}
Note that, it's possible to plot variables and to color them according to either i) their quality on the factor map (cos2) or ii) their contribution values to the principal components (contrib).
\end{quote}

The different components can be accessed as follow:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Coordinates}
\KeywordTok{head}\NormalTok{(var}\OperatorTok{$}\NormalTok{coord)}
\CommentTok{#>               Dim.1   Dim.2  Dim.3   Dim.4  Dim.5}
\CommentTok{#> X100m        -0.851 -0.1794  0.302  0.0336 -0.194}
\CommentTok{#> Long.jump     0.794  0.2809 -0.191 -0.1154  0.233}
\CommentTok{#> Shot.put      0.734  0.0854  0.518  0.1285 -0.249}
\CommentTok{#> High.jump     0.610 -0.4652  0.330  0.1446  0.403}
\CommentTok{#> X400m        -0.702  0.2902  0.284  0.4308  0.104}
\CommentTok{#> X110m.hurdle -0.764 -0.0247  0.449 -0.0169  0.224}
\CommentTok{# Cos2: quality on the factore map}
\KeywordTok{head}\NormalTok{(var}\OperatorTok{$}\NormalTok{cos2)}
\CommentTok{#>              Dim.1    Dim.2  Dim.3    Dim.4  Dim.5}
\CommentTok{#> X100m        0.724 0.032184 0.0909 0.001127 0.0378}
\CommentTok{#> Long.jump    0.631 0.078881 0.0363 0.013315 0.0544}
\CommentTok{#> Shot.put     0.539 0.007294 0.2679 0.016504 0.0619}
\CommentTok{#> High.jump    0.372 0.216424 0.1090 0.020895 0.1622}
\CommentTok{#> X400m        0.492 0.084203 0.0804 0.185611 0.0108}
\CommentTok{#> X110m.hurdle 0.584 0.000612 0.2015 0.000285 0.0503}
\CommentTok{# Contributions to the principal components}
\KeywordTok{head}\NormalTok{(var}\OperatorTok{$}\NormalTok{contrib)}
\CommentTok{#>              Dim.1   Dim.2 Dim.3   Dim.4 Dim.5}
\CommentTok{#> X100m        17.54  1.7505  7.34  0.1376  5.39}
\CommentTok{#> Long.jump    15.29  4.2904  2.93  1.6249  7.75}
\CommentTok{#> Shot.put     13.06  0.3967 21.62  2.0141  8.82}
\CommentTok{#> High.jump     9.02 11.7716  8.79  2.5499 23.12}
\CommentTok{#> X400m        11.94  4.5799  6.49 22.6509  1.54}
\CommentTok{#> X110m.hurdle 14.16  0.0333 16.26  0.0348  7.17}
\end{Highlighting}
\end{Shaded}

In this section, we describe how to visualize variables and draw conclusions about their correlations. Next, we highlight variables according to either i) their quality of representation on the factor map or ii) their contributions to the principal components.

\hypertarget{correlation-circle}{%
\section{Correlation circle}\label{correlation-circle}}

The correlation between a variable and a principal component (PC) is used as the coordinates of the variable on the PC. The representation of variables differs from the plot of the observations: The observations are represented by their projections, but the variables are represented by their correlations (Abdi and Williams 2010).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Coordinates of variables}
\KeywordTok{head}\NormalTok{(var}\OperatorTok{$}\NormalTok{coord, }\DecValTok{4}\NormalTok{)}
\CommentTok{#>            Dim.1   Dim.2  Dim.3   Dim.4  Dim.5}
\CommentTok{#> X100m     -0.851 -0.1794  0.302  0.0336 -0.194}
\CommentTok{#> Long.jump  0.794  0.2809 -0.191 -0.1154  0.233}
\CommentTok{#> Shot.put   0.734  0.0854  0.518  0.1285 -0.249}
\CommentTok{#> High.jump  0.610 -0.4652  0.330  0.1446  0.403}
\end{Highlighting}
\end{Shaded}

To plot variables, type this:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{fviz_pca_var}\NormalTok{(res.pca, }\DataTypeTok{col.var =} \StringTok{"black"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{meta_110b-PCA-Principal_Component_Methods_in_R_files/figure-latex/unnamed-chunk-13-1} \end{center}

The plot above is also known as variable correlation plots. It shows the relationships between all variables. It can be interpreted as follow:

\begin{itemize}
\tightlist
\item
  Positively correlated variables are grouped together.
\item
  Negatively correlated variables are positioned on opposite sides of the plot origin (opposed quadrants).
\item
  The distance between variables and the origin measures the quality of the variables on the factor map. Variables that are away from the origin are well represented on the factor map.
\end{itemize}

\hypertarget{quality-of-representation}{%
\section{Quality of representation}\label{quality-of-representation}}

The quality of representation of the variables on factor map is called cos2 (square cosine, squared coordinates) . You can access to the cos2 as follow:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(var}\OperatorTok{$}\NormalTok{cos2, }\DecValTok{4}\NormalTok{)}
\CommentTok{#>           Dim.1   Dim.2  Dim.3   Dim.4  Dim.5}
\CommentTok{#> X100m     0.724 0.03218 0.0909 0.00113 0.0378}
\CommentTok{#> Long.jump 0.631 0.07888 0.0363 0.01331 0.0544}
\CommentTok{#> Shot.put  0.539 0.00729 0.2679 0.01650 0.0619}
\CommentTok{#> High.jump 0.372 0.21642 0.1090 0.02089 0.1622}
\end{Highlighting}
\end{Shaded}

You can visualize the cos2 of variables on all the dimensions using the corrplot package:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(corrplot)}
\CommentTok{#> corrplot 0.84 loaded}
\KeywordTok{corrplot}\NormalTok{(var}\OperatorTok{$}\NormalTok{cos2, }\DataTypeTok{is.corr=}\OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{meta_110b-PCA-Principal_Component_Methods_in_R_files/figure-latex/unnamed-chunk-15-1} \end{center}

It's also possible to create a bar plot of variables cos2 using the function \texttt{fviz\_cos2()} {[}in factoextra{]}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Total cos2 of variables on Dim.1 and Dim.2}
\KeywordTok{fviz_cos2}\NormalTok{(res.pca, }\DataTypeTok{choice =} \StringTok{"var"}\NormalTok{, }\DataTypeTok{axes =} \DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{meta_110b-PCA-Principal_Component_Methods_in_R_files/figure-latex/unnamed-chunk-16-1} \end{center}

Note that,

\begin{itemize}
\item
  A high cos2 indicates a good representation of the variable on the principal component. In this case the variable is positioned close to the circumference of the correlation circle.
\item
  A low cos2 indicates that the variable is not perfectly represented by the PCs. In this case the variable is close to the center of the circle.
\end{itemize}

For a given variable, the sum of the cos2 on all the principal components is equal to one.

If a variable is perfectly represented by only two principal components (Dim.1 \& Dim.2), the sum of the cos2 on these two PCs is equal to one. In this case the variables will be positioned on the circle of correlations.

For some of the variables, more than 2 components might be required to perfectly represent the data. In this case the variables are positioned inside the circle of correlations.

In summary:

\begin{itemize}
\tightlist
\item
  The cos2 values are used to estimate the quality of the representation
\item
  The closer a variable is to the circle of correlations, the better its representation on the factor map (and the more important it is to interpret these components)
\item
  Variables that are closed to the center of the plot are less important for the first components.
\end{itemize}

It's possible to color variables by their cos2 values using the argument col.var = ``cos2''. This produces a gradient colors. In this case, the argument gradient.cols can be used to provide a custom color. For instance, gradient.cols = c(``white'', ``blue'', ``red'') means that:

\begin{itemize}
\tightlist
\item
  variables with low cos2 values will be colored in ``white''
\item
  variables with mid cos2 values will be colored in ``blue''
\item
  variables with high cos2 values will be colored in red
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Color by cos2 values: quality on the factor map}
\KeywordTok{fviz_pca_var}\NormalTok{(res.pca, }\DataTypeTok{col.var =} \StringTok{"cos2"}\NormalTok{,}
             \DataTypeTok{gradient.cols =} \KeywordTok{c}\NormalTok{(}\StringTok{"#00AFBB"}\NormalTok{, }\StringTok{"#E7B800"}\NormalTok{, }\StringTok{"#FC4E07"}\NormalTok{), }
             \DataTypeTok{repel =} \OtherTok{TRUE} \CommentTok{# Avoid text overlapping}
\NormalTok{             )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{meta_110b-PCA-Principal_Component_Methods_in_R_files/figure-latex/unnamed-chunk-17-1} \end{center}

Note that, it's also possible to change the transparency of the variables according to their cos2 values using the option alpha.var = ``cos2''. For example, type this:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Change the transparency by cos2 values}
\KeywordTok{fviz_pca_var}\NormalTok{(res.pca, }\DataTypeTok{alpha.var =} \StringTok{"cos2"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{meta_110b-PCA-Principal_Component_Methods_in_R_files/figure-latex/unnamed-chunk-18-1} \end{center}

\hypertarget{contributions-of-variables-to-pcs}{%
\section{Contributions of variables to PCs}\label{contributions-of-variables-to-pcs}}

The contributions of variables in accounting for the variability in a given principal component are expressed in percentage.

\begin{itemize}
\tightlist
\item
  Variables that are correlated with PC1 (i.e., Dim.1) and PC2 (i.e., Dim.2) are the most important in explaining the variability in the data set.
\item
  Variables that do not correlated with any PC or correlated with the last dimensions are variables with low contribution and might be removed to simplify the overall analysis.
\end{itemize}

The contribution of variables can be extracted as follow :

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(var}\OperatorTok{$}\NormalTok{contrib, }\DecValTok{4}\NormalTok{)}
\CommentTok{#>           Dim.1  Dim.2 Dim.3 Dim.4 Dim.5}
\CommentTok{#> X100m     17.54  1.751  7.34 0.138  5.39}
\CommentTok{#> Long.jump 15.29  4.290  2.93 1.625  7.75}
\CommentTok{#> Shot.put  13.06  0.397 21.62 2.014  8.82}
\CommentTok{#> High.jump  9.02 11.772  8.79 2.550 23.12}
\end{Highlighting}
\end{Shaded}

\begin{quote}
The larger the value of the contribution, the more the variable contributes to the component.
\end{quote}

It's possible to use the function corrplot() {[}corrplot package{]} to highlight the most contributing variables for each dimension:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(}\StringTok{"corrplot"}\NormalTok{)}

\KeywordTok{corrplot}\NormalTok{(var}\OperatorTok{$}\NormalTok{contrib, }\DataTypeTok{is.corr=}\OtherTok{FALSE}\NormalTok{)    }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{meta_110b-PCA-Principal_Component_Methods_in_R_files/figure-latex/unnamed-chunk-20-1} \end{center}

The function fviz\_contrib() {[}factoextra package{]} can be used to draw a bar plot of variable contributions. If your data contains many variables, you can decide to show only the top contributing variables. The R code below shows the top 10 variables contributing to the principal components:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Contributions of variables to PC1}
\KeywordTok{fviz_contrib}\NormalTok{(res.pca, }\DataTypeTok{choice =} \StringTok{"var"}\NormalTok{, }\DataTypeTok{axes =} \DecValTok{1}\NormalTok{, }\DataTypeTok{top =} \DecValTok{10}\NormalTok{)}
\CommentTok{# Contributions of variables to PC2}
\KeywordTok{fviz_contrib}\NormalTok{(res.pca, }\DataTypeTok{choice =} \StringTok{"var"}\NormalTok{, }\DataTypeTok{axes =} \DecValTok{2}\NormalTok{, }\DataTypeTok{top =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{meta_110b-PCA-Principal_Component_Methods_in_R_files/figure-latex/unnamed-chunk-21-1} \includegraphics[width=0.7\linewidth]{meta_110b-PCA-Principal_Component_Methods_in_R_files/figure-latex/unnamed-chunk-21-2} \end{center}

The total contribution to PC1 and PC2 is obtained with the following R code:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{fviz_contrib}\NormalTok{(res.pca, }\DataTypeTok{choice =} \StringTok{"var"}\NormalTok{, }\DataTypeTok{axes =} \DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{, }\DataTypeTok{top =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{meta_110b-PCA-Principal_Component_Methods_in_R_files/figure-latex/unnamed-chunk-22-1} \end{center}

The red dashed line on the graph above indicates the expected average contribution. If the contribution of the variables were uniform, the expected value would be 1/length(variables) = 1/10 = 10\%. For a given component, a variable with a contribution larger than this cutoff could be considered as important in contributing to the component.

Note that, the total contribution of a given variable, on explaining the variations retained by two principal components, say PC1 and PC2, is calculated as contrib = {[}(C1 * Eig1) + (C2 * Eig2){]}/(Eig1 + Eig2), where

\begin{itemize}
\tightlist
\item
  C1 and C2 are the contributions of the variable on PC1 and PC2, respectively
\item
  Eig1 and Eig2 are the eigenvalues of PC1 and PC2, respectively. Recall that eigenvalues measure the amount of variation retained by each PC.
\end{itemize}

In this case, the expected average contribution (cutoff) is calculated as follow: As mentioned above, if the contributions of the 10 variables were uniform, the expected average contribution on a given PC would be 1/10 = 10\%. The expected average contribution of a variable for PC1 and PC2 is : {[}(10* Eig1) + (10 * Eig2){]}/(Eig1 + Eig2)

It can be seen that the variables - X100m, Long.jump and Pole.vault - contribute the most to the dimensions 1 and 2.

The most important (or, contributing) variables can be highlighted on the correlation plot as follow:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{fviz_pca_var}\NormalTok{(res.pca, }\DataTypeTok{col.var =} \StringTok{"contrib"}\NormalTok{,}
             \DataTypeTok{gradient.cols =} \KeywordTok{c}\NormalTok{(}\StringTok{"#00AFBB"}\NormalTok{, }\StringTok{"#E7B800"}\NormalTok{, }\StringTok{"#FC4E07"}\NormalTok{)}
\NormalTok{             )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{meta_110b-PCA-Principal_Component_Methods_in_R_files/figure-latex/unnamed-chunk-23-1} \end{center}

Note that, it's also possible to change the transparency of variables according to their contrib values using the option alpha.var = ``contrib''. For example, type this:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Change the transparency by contrib values}
\KeywordTok{fviz_pca_var}\NormalTok{(res.pca, }\DataTypeTok{alpha.var =} \StringTok{"contrib"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{meta_110b-PCA-Principal_Component_Methods_in_R_files/figure-latex/unnamed-chunk-24-1} \end{center}

\hypertarget{color-by-a-custom-continuous-variable}{%
\section{Color by a custom continuous variable}\label{color-by-a-custom-continuous-variable}}

In the previous sections, we showed how to color variables by their contributions and their cos2. Note that, it's possible to color variables by any custom continuous variable. The coloring variable should have the same length as the number of active variables in the PCA (here n = 10).

For example, type this:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Create a random continuous variable of length 10}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{my.cont.var <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DecValTok{10}\NormalTok{)}
\CommentTok{# Color variables by the continuous variable}
\KeywordTok{fviz_pca_var}\NormalTok{(res.pca, }\DataTypeTok{col.var =}\NormalTok{ my.cont.var,}
             \DataTypeTok{gradient.cols =} \KeywordTok{c}\NormalTok{(}\StringTok{"blue"}\NormalTok{, }\StringTok{"yellow"}\NormalTok{, }\StringTok{"red"}\NormalTok{),}
             \DataTypeTok{legend.title =} \StringTok{"Cont.Var"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{meta_110b-PCA-Principal_Component_Methods_in_R_files/figure-latex/unnamed-chunk-25-1} \end{center}

\hypertarget{color-by-groups}{%
\section{Color by groups}\label{color-by-groups}}

It's also possible to change the color of variables by groups defined by a qualitative/categorical variable, also called factor in R terminology.

As we don't have any grouping variable in our data sets for classifying variables, we'll create it.

In the following demo example, we start by classifying the variables into 3 groups using the kmeans clustering algorithm. Next, we use the clusters returned by the kmeans algorithm to color variables.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Create a grouping variable using kmeans}
\CommentTok{# Create 3 groups of variables (centers = 3)}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{res.km <-}\StringTok{ }\KeywordTok{kmeans}\NormalTok{(var}\OperatorTok{$}\NormalTok{coord, }\DataTypeTok{centers =} \DecValTok{3}\NormalTok{, }\DataTypeTok{nstart =} \DecValTok{25}\NormalTok{)}
\NormalTok{grp <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(res.km}\OperatorTok{$}\NormalTok{cluster)}
\CommentTok{# Color variables by groups}
\KeywordTok{fviz_pca_var}\NormalTok{(res.pca, }\DataTypeTok{col.var =}\NormalTok{ grp, }
             \DataTypeTok{palette =} \KeywordTok{c}\NormalTok{(}\StringTok{"#0073C2FF"}\NormalTok{, }\StringTok{"#EFC000FF"}\NormalTok{, }\StringTok{"#868686FF"}\NormalTok{),}
             \DataTypeTok{legend.title =} \StringTok{"Cluster"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{meta_110b-PCA-Principal_Component_Methods_in_R_files/figure-latex/unnamed-chunk-26-1} \end{center}

\hypertarget{dimension-description}{%
\section{Dimension description}\label{dimension-description}}

In the section \citet{ref}(pca-variable-contributions), we described how to highlight variables according to their contributions to the principal components.

Note also that, the function dimdesc() {[}in FactoMineR{]}, for dimension description, can be used to identify the most significantly associated variables with a given principal component . It can be used as follow:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res.desc <-}\StringTok{ }\KeywordTok{dimdesc}\NormalTok{(res.pca, }\DataTypeTok{axes =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{), }\DataTypeTok{proba =} \FloatTok{0.05}\NormalTok{)}
\CommentTok{# Description of dimension 1}
\NormalTok{res.desc}\OperatorTok{$}\NormalTok{Dim}\FloatTok{.1}
\CommentTok{#> $quanti}
\CommentTok{#>              correlation  p.value}
\CommentTok{#> Long.jump          0.794 6.06e-06}
\CommentTok{#> Discus             0.743 4.84e-05}
\CommentTok{#> Shot.put           0.734 6.72e-05}
\CommentTok{#> High.jump          0.610 1.99e-03}
\CommentTok{#> Javeline           0.428 4.15e-02}
\CommentTok{#> X400m             -0.702 1.91e-04}
\CommentTok{#> X110m.hurdle      -0.764 2.20e-05}
\CommentTok{#> X100m             -0.851 2.73e-07}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Description of dimension 2}
\NormalTok{res.desc}\OperatorTok{$}\NormalTok{Dim}\FloatTok{.2}
\CommentTok{#> $quanti}
\CommentTok{#>            correlation  p.value}
\CommentTok{#> Pole.vault       0.807 3.21e-06}
\CommentTok{#> X1500m           0.784 9.38e-06}
\CommentTok{#> High.jump       -0.465 2.53e-02}
\end{Highlighting}
\end{Shaded}

\hypertarget{graph-of-individuals}{%
\section{Graph of individuals}\label{graph-of-individuals}}

Results
The results, for individuals can be extracted using the function get\_pca\_ind() {[}factoextra package{]}. Similarly to the get\_pca\_var(), the function get\_pca\_ind() provides a list of matrices containing all the results for the individuals (coordinates, correlation between individuals and axes, squared cosine and contributions)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ind <-}\StringTok{ }\KeywordTok{get_pca_ind}\NormalTok{(res.pca)}
\NormalTok{ind}
\CommentTok{#> Principal Component Analysis Results for individuals}
\CommentTok{#>  ===================================================}
\CommentTok{#>   Name       Description                       }
\CommentTok{#> 1 "$coord"   "Coordinates for the individuals" }
\CommentTok{#> 2 "$cos2"    "Cos2 for the individuals"        }
\CommentTok{#> 3 "$contrib" "contributions of the individuals"}
\end{Highlighting}
\end{Shaded}

To get access to the different components, use this:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Coordinates of individuals}
\KeywordTok{head}\NormalTok{(ind}\OperatorTok{$}\NormalTok{coord)}
\CommentTok{#>            Dim.1  Dim.2  Dim.3   Dim.4   Dim.5}
\CommentTok{#> SEBRLE     0.196  1.589  0.642  0.0839  1.1683}
\CommentTok{#> CLAY       0.808  2.475 -1.387  1.2984 -0.8250}
\CommentTok{#> BERNARD   -1.359  1.648  0.201 -1.9641  0.0842}
\CommentTok{#> YURKOV    -0.889 -0.443  2.530  0.7129  0.4078}
\CommentTok{#> ZSIVOCZKY -0.108 -2.069 -1.334 -0.1015 -0.2015}
\CommentTok{#> McMULLEN   0.121 -1.014 -0.863  1.3416  1.6215}
\CommentTok{# Quality of individuals}
\KeywordTok{head}\NormalTok{(ind}\OperatorTok{$}\NormalTok{cos2)}
\CommentTok{#>             Dim.1  Dim.2   Dim.3   Dim.4    Dim.5}
\CommentTok{#> SEBRLE    0.00753 0.4975 0.08133 0.00139 0.268903}
\CommentTok{#> CLAY      0.04870 0.4570 0.14363 0.12579 0.050785}
\CommentTok{#> BERNARD   0.19720 0.2900 0.00429 0.41182 0.000757}
\CommentTok{#> YURKOV    0.09611 0.0238 0.77823 0.06181 0.020228}
\CommentTok{#> ZSIVOCZKY 0.00157 0.5764 0.23975 0.00139 0.005465}
\CommentTok{#> McMULLEN  0.00218 0.1522 0.11014 0.26649 0.389262}
\CommentTok{# Contributions of individuals}
\KeywordTok{head}\NormalTok{(ind}\OperatorTok{$}\NormalTok{contrib)}
\CommentTok{#>            Dim.1  Dim.2  Dim.3   Dim.4   Dim.5}
\CommentTok{#> SEBRLE    0.0403  5.971  1.448  0.0373  8.4589}
\CommentTok{#> CLAY      0.6881 14.484  6.754  8.9446  4.2179}
\CommentTok{#> BERNARD   1.9474  6.423  0.141 20.4682  0.0439}
\CommentTok{#> YURKOV    0.8331  0.463 22.452  2.6966  1.0308}
\CommentTok{#> ZSIVOCZKY 0.0123 10.122  6.246  0.0547  0.2515}
\CommentTok{#> McMULLEN  0.0155  2.431  2.610  9.5506 16.2949}
\end{Highlighting}
\end{Shaded}

\hypertarget{plots-quality-and-contribution-1}{%
\section{Plots: quality and contribution}\label{plots-quality-and-contribution-1}}

The fviz\_pca\_ind() is used to produce the graph of individuals. To create a simple plot, type this:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{fviz_pca_ind}\NormalTok{(res.pca)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{meta_110b-PCA-Principal_Component_Methods_in_R_files/figure-latex/unnamed-chunk-31-1} \end{center}

Like variables, it's also possible to color individuals by their cos2 values:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{fviz_pca_ind}\NormalTok{(res.pca, }\DataTypeTok{col.ind =} \StringTok{"cos2"}\NormalTok{, }
             \DataTypeTok{gradient.cols =} \KeywordTok{c}\NormalTok{(}\StringTok{"#00AFBB"}\NormalTok{, }\StringTok{"#E7B800"}\NormalTok{, }\StringTok{"#FC4E07"}\NormalTok{),}
             \DataTypeTok{repel =} \OtherTok{TRUE} \CommentTok{# Avoid text overlapping (slow if many points)}
\NormalTok{             )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{meta_110b-PCA-Principal_Component_Methods_in_R_files/figure-latex/unnamed-chunk-32-1} \end{center}

\begin{quote}
Note that, individuals that are similar are grouped together on the plot.
\end{quote}

You can also change the point size according the cos2 of the corresponding individuals:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{fviz_pca_ind}\NormalTok{(res.pca, }\DataTypeTok{pointsize =} \StringTok{"cos2"}\NormalTok{, }
             \DataTypeTok{pointshape =} \DecValTok{21}\NormalTok{, }\DataTypeTok{fill =} \StringTok{"#E7B800"}\NormalTok{,}
             \DataTypeTok{repel =} \OtherTok{TRUE} \CommentTok{# Avoid text overlapping (slow if many points)}
\NormalTok{             )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{meta_110b-PCA-Principal_Component_Methods_in_R_files/figure-latex/unnamed-chunk-33-1} \end{center}

To change both point size and color by cos2, try this:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{fviz_pca_ind}\NormalTok{(res.pca, }\DataTypeTok{col.ind =} \StringTok{"cos2"}\NormalTok{, }\DataTypeTok{pointsize =} \StringTok{"cos2"}\NormalTok{,}
             \DataTypeTok{gradient.cols =} \KeywordTok{c}\NormalTok{(}\StringTok{"#00AFBB"}\NormalTok{, }\StringTok{"#E7B800"}\NormalTok{, }\StringTok{"#FC4E07"}\NormalTok{),}
             \DataTypeTok{repel =} \OtherTok{TRUE} \CommentTok{# Avoid text overlapping (slow if many points)}
\NormalTok{             )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{meta_110b-PCA-Principal_Component_Methods_in_R_files/figure-latex/unnamed-chunk-34-1} \end{center}

To create a bar plot of the quality of representation (cos2) of individuals on the factor map, you can use the function fviz\_cos2() as previously described for variables:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{fviz_cos2}\NormalTok{(res.pca, }\DataTypeTok{choice =} \StringTok{"ind"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{meta_110b-PCA-Principal_Component_Methods_in_R_files/figure-latex/unnamed-chunk-35-1} \end{center}

To visualize the contribution of individuals to the first two principal components, type this:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Total contribution on PC1 and PC2}
\KeywordTok{fviz_contrib}\NormalTok{(res.pca, }\DataTypeTok{choice =} \StringTok{"ind"}\NormalTok{, }\DataTypeTok{axes =} \DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{meta_110b-PCA-Principal_Component_Methods_in_R_files/figure-latex/unnamed-chunk-36-1} \end{center}

\hypertarget{color-by-a-custom-continuous-variable-1}{%
\section{Color by a custom continuous variable}\label{color-by-a-custom-continuous-variable-1}}

As for variables, individuals can be colored by any custom continuous variable by specifying the argument col.ind.

For example, type this:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Create a random continuous variable of length 23,}
\CommentTok{# Same length as the number of active individuals in the PCA}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{my.cont.var <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DecValTok{23}\NormalTok{)}
\CommentTok{# Color individuals by the continuous variable}
\KeywordTok{fviz_pca_ind}\NormalTok{(res.pca, }\DataTypeTok{col.ind =}\NormalTok{ my.cont.var,}
             \DataTypeTok{gradient.cols =} \KeywordTok{c}\NormalTok{(}\StringTok{"blue"}\NormalTok{, }\StringTok{"yellow"}\NormalTok{, }\StringTok{"red"}\NormalTok{),}
             \DataTypeTok{legend.title =} \StringTok{"Cont.Var"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{meta_110b-PCA-Principal_Component_Methods_in_R_files/figure-latex/unnamed-chunk-37-1} \end{center}

\hypertarget{color-by-groups-1}{%
\section{Color by groups}\label{color-by-groups-1}}

Here, we describe how to color individuals by group. Additionally, we show how to add concentration ellipses and confidence ellipses by groups. For this, we'll use the iris data as demo data sets.

Iris data sets look like this:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(iris, }\DecValTok{3}\NormalTok{)}
\CommentTok{#>   Sepal.Length Sepal.Width Petal.Length Petal.Width Species}
\CommentTok{#> 1          5.1         3.5          1.4         0.2  setosa}
\CommentTok{#> 2          4.9         3.0          1.4         0.2  setosa}
\CommentTok{#> 3          4.7         3.2          1.3         0.2  setosa}
\end{Highlighting}
\end{Shaded}

The column ``Species'' will be used as grouping variable. We start by computing principal component analysis as follow:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# The variable Species (index = 5) is removed}
\CommentTok{# before PCA analysis}
\NormalTok{iris.pca <-}\StringTok{ }\KeywordTok{PCA}\NormalTok{(iris[,}\OperatorTok{-}\DecValTok{5}\NormalTok{], }\DataTypeTok{graph =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

In the R code below: the argument habillage or col.ind can be used to specify the factor variable for coloring the individuals by groups.

To add a concentration ellipse around each group, specify the argument addEllipses = TRUE. The argument palette can be used to change group colors.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{fviz_pca_ind}\NormalTok{(iris.pca,}
             \DataTypeTok{geom.ind =} \StringTok{"point"}\NormalTok{, }\CommentTok{# show points only (nbut not "text")}
             \DataTypeTok{col.ind =}\NormalTok{ iris}\OperatorTok{$}\NormalTok{Species, }\CommentTok{# color by groups}
             \DataTypeTok{palette =} \KeywordTok{c}\NormalTok{(}\StringTok{"#00AFBB"}\NormalTok{, }\StringTok{"#E7B800"}\NormalTok{, }\StringTok{"#FC4E07"}\NormalTok{),}
             \DataTypeTok{addEllipses =} \OtherTok{TRUE}\NormalTok{, }\CommentTok{# Concentration ellipses}
             \DataTypeTok{legend.title =} \StringTok{"Groups"}
\NormalTok{             )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{meta_110b-PCA-Principal_Component_Methods_in_R_files/figure-latex/unnamed-chunk-40-1} \end{center}

\begin{quote}
To remove the group mean point, specify the argument mean.point = FALSE.
If you want confidence ellipses instead of concentration ellipses, use ellipse.type = ``confidence''.
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Add confidence ellipses}
\KeywordTok{fviz_pca_ind}\NormalTok{(iris.pca, }\DataTypeTok{geom.ind =} \StringTok{"point"}\NormalTok{, }\DataTypeTok{col.ind =}\NormalTok{ iris}\OperatorTok{$}\NormalTok{Species, }
             \DataTypeTok{palette =} \KeywordTok{c}\NormalTok{(}\StringTok{"#00AFBB"}\NormalTok{, }\StringTok{"#E7B800"}\NormalTok{, }\StringTok{"#FC4E07"}\NormalTok{),}
             \DataTypeTok{addEllipses =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{ellipse.type =} \StringTok{"confidence"}\NormalTok{,}
             \DataTypeTok{legend.title =} \StringTok{"Groups"}
\NormalTok{             )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{meta_110b-PCA-Principal_Component_Methods_in_R_files/figure-latex/unnamed-chunk-41-1} \end{center}

Note that, allowed values for palette include:

\begin{itemize}
\tightlist
\item
  ``grey'' for grey color palettes;
\item
  brewer palettes e.g. ``RdBu'', ``Blues'', \ldots{}; To view all, type this in R: RColorBrewer::display.brewer.all().
\item
  custom color palette e.g.~c(``blue'', ``red'');
  and scientific journal palettes from ggsci R package, e.g.: ``npg'', ``aaas'', * ``lancet'', ``jco'', ``ucscgb'', ``uchicago'', ``simpsons'' and ``rickandmorty''.
  For example, to use the jco (journal of clinical oncology) color palette, type this:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{fviz_pca_ind}\NormalTok{(iris.pca,}
             \DataTypeTok{label =} \StringTok{"none"}\NormalTok{, }\CommentTok{# hide individual labels}
             \DataTypeTok{habillage =}\NormalTok{ iris}\OperatorTok{$}\NormalTok{Species, }\CommentTok{# color by groups}
             \DataTypeTok{addEllipses =} \OtherTok{TRUE}\NormalTok{, }\CommentTok{# Concentration ellipses}
             \DataTypeTok{palette =} \StringTok{"jco"}
\NormalTok{             )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{meta_110b-PCA-Principal_Component_Methods_in_R_files/figure-latex/unnamed-chunk-42-1} \end{center}

\hypertarget{graph-customization}{%
\section{Graph customization}\label{graph-customization}}

Note that, fviz\_pca\_ind() and fviz\_pca\_var() and related functions are wrapper around the core function fviz() {[}in factoextra{]}. fviz() is a wrapper around the function ggscatter() {[}in ggpubr{]}. Therefore, further arguments, to be passed to the function fviz() and ggscatter(), can be specified in fviz\_pca\_ind() and fviz\_pca\_var().

Here, we present some of these additional arguments to customize the PCA graph of variables and individuals.

\hypertarget{dimensions}{%
\subsection{Dimensions}\label{dimensions}}

By default, variables/individuals are represented on dimensions 1 and 2. If you want to visualize them on dimensions 2 and 3, for example, you should specify the argument axes = c(2, 3).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Variables on dimensions 2 and 3}
\KeywordTok{fviz_pca_var}\NormalTok{(res.pca, }\DataTypeTok{axes =} \KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{))}
\CommentTok{# Individuals on dimensions 2 and 3}
\KeywordTok{fviz_pca_ind}\NormalTok{(res.pca, }\DataTypeTok{axes =} \KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{meta_110b-PCA-Principal_Component_Methods_in_R_files/figure-latex/unnamed-chunk-43-1} \includegraphics[width=0.7\linewidth]{meta_110b-PCA-Principal_Component_Methods_in_R_files/figure-latex/unnamed-chunk-43-2} \end{center}

Plot elements: point, text, arrow
The argument geom (for geometry) and derivatives are used to specify the geometry elements or graphical elements to be used for plotting.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  geom.var: a text specifying the geometry to be used for plotting variables. Allowed values are the combination of c(``point'', ``arrow'', ``text'').
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Use geom.var = ``point'', to show only points;
\item
  Use geom.var = ``text'' to show only text labels;
\item
  Use geom.var = c(``point'', ``text'') to show both points and text labels
\item
  Use geom.var = c(``arrow'', ``text'') to show arrows and labels (default).
\end{itemize}

For example, type this:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Show variable points and text labels}
\KeywordTok{fviz_pca_var}\NormalTok{(res.pca, }\DataTypeTok{geom.var =} \KeywordTok{c}\NormalTok{(}\StringTok{"point"}\NormalTok{, }\StringTok{"text"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{meta_110b-PCA-Principal_Component_Methods_in_R_files/figure-latex/unnamed-chunk-44-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Show individuals text labels only}
\KeywordTok{fviz_pca_ind}\NormalTok{(res.pca, }\DataTypeTok{geom.ind =}  \StringTok{"text"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{meta_110b-PCA-Principal_Component_Methods_in_R_files/figure-latex/unnamed-chunk-45-1} \end{center}

\hypertarget{size-and-shape-of-plot-elements}{%
\section{Size and shape of plot elements}\label{size-and-shape-of-plot-elements}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Change the size of arrows an labels}
\KeywordTok{fviz_pca_var}\NormalTok{(res.pca, }\DataTypeTok{arrowsize =} \DecValTok{1}\NormalTok{, }\DataTypeTok{labelsize =} \DecValTok{5}\NormalTok{, }
             \DataTypeTok{repel =} \OtherTok{TRUE}\NormalTok{)}
\CommentTok{# Change points size, shape and fill color}
\CommentTok{# Change labelsize}
\KeywordTok{fviz_pca_ind}\NormalTok{(res.pca, }
             \DataTypeTok{pointsize =} \DecValTok{3}\NormalTok{, }\DataTypeTok{pointshape =} \DecValTok{21}\NormalTok{, }\DataTypeTok{fill =} \StringTok{"lightblue"}\NormalTok{,}
             \DataTypeTok{labelsize =} \DecValTok{5}\NormalTok{, }\DataTypeTok{repel =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{meta_110b-PCA-Principal_Component_Methods_in_R_files/figure-latex/unnamed-chunk-46-1} \includegraphics[width=0.7\linewidth]{meta_110b-PCA-Principal_Component_Methods_in_R_files/figure-latex/unnamed-chunk-46-2} \end{center}

\hypertarget{ellipses}{%
\section{Ellipses}\label{ellipses}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Add confidence ellipses}
\KeywordTok{fviz_pca_ind}\NormalTok{(iris.pca, }\DataTypeTok{geom.ind =} \StringTok{"point"}\NormalTok{, }
             \DataTypeTok{col.ind =}\NormalTok{ iris}\OperatorTok{$}\NormalTok{Species, }\CommentTok{# color by groups}
             \DataTypeTok{palette =} \KeywordTok{c}\NormalTok{(}\StringTok{"#00AFBB"}\NormalTok{, }\StringTok{"#E7B800"}\NormalTok{, }\StringTok{"#FC4E07"}\NormalTok{),}
             \DataTypeTok{addEllipses =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{ellipse.type =} \StringTok{"confidence"}\NormalTok{,}
             \DataTypeTok{legend.title =} \StringTok{"Groups"}
\NormalTok{             )}
\CommentTok{# Convex hull}
\KeywordTok{fviz_pca_ind}\NormalTok{(iris.pca, }\DataTypeTok{geom.ind =} \StringTok{"point"}\NormalTok{,}
             \DataTypeTok{col.ind =}\NormalTok{ iris}\OperatorTok{$}\NormalTok{Species, }\CommentTok{# color by groups}
             \DataTypeTok{palette =} \KeywordTok{c}\NormalTok{(}\StringTok{"#00AFBB"}\NormalTok{, }\StringTok{"#E7B800"}\NormalTok{, }\StringTok{"#FC4E07"}\NormalTok{),}
             \DataTypeTok{addEllipses =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{ellipse.type =} \StringTok{"convex"}\NormalTok{,}
             \DataTypeTok{legend.title =} \StringTok{"Groups"}
\NormalTok{             )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{meta_110b-PCA-Principal_Component_Methods_in_R_files/figure-latex/unnamed-chunk-47-1} \includegraphics[width=0.7\linewidth]{meta_110b-PCA-Principal_Component_Methods_in_R_files/figure-latex/unnamed-chunk-47-2} \end{center}

\hypertarget{group-mean-points}{%
\section{Group mean points}\label{group-mean-points}}

When coloring individuals by groups (section \citet{ref}(color-ind-by-groups)), the mean points of groups (barycenters) are also displayed by default.

To remove the mean points, use the argument mean.point = FALSE.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{fviz_pca_ind}\NormalTok{(iris.pca,}
             \DataTypeTok{geom.ind =} \StringTok{"point"}\NormalTok{, }\CommentTok{# show points only (but not "text")}
             \DataTypeTok{group.ind =}\NormalTok{ iris}\OperatorTok{$}\NormalTok{Species, }\CommentTok{# color by groups}
             \DataTypeTok{legend.title =} \StringTok{"Groups"}\NormalTok{,}
             \DataTypeTok{mean.point =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{meta_110b-PCA-Principal_Component_Methods_in_R_files/figure-latex/unnamed-chunk-48-1} \end{center}

\hypertarget{axis-lines}{%
\section{Axis lines}\label{axis-lines}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{fviz_pca_var}\NormalTok{(res.pca, }\DataTypeTok{axes.linetype =} \StringTok{"blank"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{meta_110b-PCA-Principal_Component_Methods_in_R_files/figure-latex/unnamed-chunk-49-1} \end{center}

\hypertarget{graphical-parameters}{%
\section{Graphical parameters}\label{graphical-parameters}}

To change easily the graphical of any ggplots, you can use the function ggpar() {[}ggpubr package{]}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ind.p <-}\StringTok{ }\KeywordTok{fviz_pca_ind}\NormalTok{(iris.pca, }\DataTypeTok{geom =} \StringTok{"point"}\NormalTok{, }\DataTypeTok{col.ind =}\NormalTok{ iris}\OperatorTok{$}\NormalTok{Species)}
\NormalTok{ggpubr}\OperatorTok{::}\KeywordTok{ggpar}\NormalTok{(ind.p,}
              \DataTypeTok{title =} \StringTok{"Principal Component Analysis"}\NormalTok{,}
              \DataTypeTok{subtitle =} \StringTok{"Iris data set"}\NormalTok{,}
              \DataTypeTok{caption =} \StringTok{"Source: factoextra"}\NormalTok{,}
              \DataTypeTok{xlab =} \StringTok{"PC1"}\NormalTok{, }\DataTypeTok{ylab =} \StringTok{"PC2"}\NormalTok{,}
              \DataTypeTok{legend.title =} \StringTok{"Species"}\NormalTok{, }\DataTypeTok{legend.position =} \StringTok{"top"}\NormalTok{,}
              \DataTypeTok{ggtheme =} \KeywordTok{theme_gray}\NormalTok{(), }\DataTypeTok{palette =} \StringTok{"jco"}
\NormalTok{              )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{meta_110b-PCA-Principal_Component_Methods_in_R_files/figure-latex/unnamed-chunk-50-1} \end{center}

\hypertarget{biplot}{%
\section{Biplot}\label{biplot}}

To make a simple biplot of individuals and variables, type this:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{fviz_pca_biplot}\NormalTok{(res.pca, }\DataTypeTok{repel =} \OtherTok{TRUE}\NormalTok{,}
                \DataTypeTok{col.var =} \StringTok{"#2E9FDF"}\NormalTok{, }\CommentTok{# Variables color}
                \DataTypeTok{col.ind =} \StringTok{"#696969"}  \CommentTok{# Individuals color}
\NormalTok{                )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{meta_110b-PCA-Principal_Component_Methods_in_R_files/figure-latex/unnamed-chunk-51-1} \end{center}

\begin{quote}
Note that, the biplot might be only useful when there is a low number of variables and individuals in the data set; otherwise the final plot would be unreadable.
\end{quote}

\begin{quote}
Note also that, the coordinate of individuals and variables are not constructed on the same space. Therefore, in the biplot, you should mainly focus on the direction of variables but not on their absolute positions on the plot.
\end{quote}

\begin{quote}
Roughly speaking a biplot can be interpreted as follow:
* an individual that is on the same side of a given variable has a high value for this variable;
* an individual that is on the opposite side of a given variable has a low value for this variable.
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{fviz_pca_biplot}\NormalTok{(iris.pca, }
                \DataTypeTok{col.ind =}\NormalTok{ iris}\OperatorTok{$}\NormalTok{Species, }\DataTypeTok{palette =} \StringTok{"jco"}\NormalTok{, }
                \DataTypeTok{addEllipses =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{label =} \StringTok{"var"}\NormalTok{,}
                \DataTypeTok{col.var =} \StringTok{"black"}\NormalTok{, }\DataTypeTok{repel =} \OtherTok{TRUE}\NormalTok{,}
                \DataTypeTok{legend.title =} \StringTok{"Species"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{meta_110b-PCA-Principal_Component_Methods_in_R_files/figure-latex/unnamed-chunk-52-1} \end{center}

In the following example, we want to color both individuals and variables by groups. The trick is to use pointshape = 21 for individual points. This particular point shape can be filled by a color using the argument fill.ind. The border line color of individual points is set to ``black'' using col.ind. To color variable by groups, the argument col.var will be used.

To customize individuals and variable colors, we use the helper functions fill\_palette() and color\_palette() {[}in ggpubr package{]}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{fviz_pca_biplot}\NormalTok{(iris.pca, }
                \CommentTok{# Fill individuals by groups}
                \DataTypeTok{geom.ind =} \StringTok{"point"}\NormalTok{,}
                \DataTypeTok{pointshape =} \DecValTok{21}\NormalTok{,}
                \DataTypeTok{pointsize =} \FloatTok{2.5}\NormalTok{,}
                \DataTypeTok{fill.ind =}\NormalTok{ iris}\OperatorTok{$}\NormalTok{Species,}
                \DataTypeTok{col.ind =} \StringTok{"black"}\NormalTok{,}
                \CommentTok{# Color variable by groups}
                \DataTypeTok{col.var =} \KeywordTok{factor}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"sepal"}\NormalTok{, }\StringTok{"sepal"}\NormalTok{, }\StringTok{"petal"}\NormalTok{, }\StringTok{"petal"}\NormalTok{)),}
                
                \DataTypeTok{legend.title =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{fill =} \StringTok{"Species"}\NormalTok{, }\DataTypeTok{color =} \StringTok{"Clusters"}\NormalTok{),}
                \DataTypeTok{repel =} \OtherTok{TRUE}        \CommentTok{# Avoid label overplotting}
\NormalTok{             )}\OperatorTok{+}
\StringTok{  }\NormalTok{ggpubr}\OperatorTok{::}\KeywordTok{fill_palette}\NormalTok{(}\StringTok{"jco"}\NormalTok{)}\OperatorTok{+}\StringTok{      }\CommentTok{# Indiviual fill color}
\StringTok{  }\NormalTok{ggpubr}\OperatorTok{::}\KeywordTok{color_palette}\NormalTok{(}\StringTok{"npg"}\NormalTok{)      }\CommentTok{# Variable colors}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{meta_110b-PCA-Principal_Component_Methods_in_R_files/figure-latex/unnamed-chunk-53-1} \end{center}

Another complex example is to color individuals by groups (discrete color) and variables by their contributions to the principal components (gradient colors). Additionally, we'll change the transparency of variables by their contributions using the argument alpha.var.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{fviz_pca_biplot}\NormalTok{(iris.pca, }
                \CommentTok{# Individuals}
                \DataTypeTok{geom.ind =} \StringTok{"point"}\NormalTok{,}
                \DataTypeTok{fill.ind =}\NormalTok{ iris}\OperatorTok{$}\NormalTok{Species, }\DataTypeTok{col.ind =} \StringTok{"black"}\NormalTok{,}
                \DataTypeTok{pointshape =} \DecValTok{21}\NormalTok{, }\DataTypeTok{pointsize =} \DecValTok{2}\NormalTok{,}
                \DataTypeTok{palette =} \StringTok{"jco"}\NormalTok{,}
                \DataTypeTok{addEllipses =} \OtherTok{TRUE}\NormalTok{,}
                \CommentTok{# Variables}
                \DataTypeTok{alpha.var =}\StringTok{"contrib"}\NormalTok{, }\DataTypeTok{col.var =} \StringTok{"contrib"}\NormalTok{,}
                \DataTypeTok{gradient.cols =} \StringTok{"RdYlBu"}\NormalTok{,}
                
                \DataTypeTok{legend.title =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{fill =} \StringTok{"Species"}\NormalTok{, }\DataTypeTok{color =} \StringTok{"Contrib"}\NormalTok{,}
                                    \DataTypeTok{alpha =} \StringTok{"Contrib"}\NormalTok{)}
\NormalTok{                )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{meta_110b-PCA-Principal_Component_Methods_in_R_files/figure-latex/unnamed-chunk-54-1} \end{center}

\hypertarget{supplementary-elements}{%
\section{Supplementary elements}\label{supplementary-elements}}

Definition and types
As described above (section \citet{ref}(pca-data-format)), the decathlon2 data sets contain supplementary continuous variables (quanti.sup, columns 11:12), supplementary qualitative variables (quali.sup, column 13) and supplementary individuals (ind.sup, rows 24:27).

Supplementary variables and individuals are not used for the determination of the principal components. Their coordinates are predicted using only the information provided by the performed principal component analysis on active variables/individuals.

Specification in PCA
To specify supplementary individuals and variables, the function PCA() can be used as follow:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res.pca <-}\StringTok{ }\KeywordTok{PCA}\NormalTok{(decathlon2, }\DataTypeTok{ind.sup =} \DecValTok{24}\OperatorTok{:}\DecValTok{27}\NormalTok{, }
               \DataTypeTok{quanti.sup =} \DecValTok{11}\OperatorTok{:}\DecValTok{12}\NormalTok{, }\DataTypeTok{quali.sup =} \DecValTok{13}\NormalTok{, }\DataTypeTok{graph=}\OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{quantitative-variables-1}{%
\section{Quantitative variables}\label{quantitative-variables-1}}

Predicted results (coordinates, correlation and cos2) for the supplementary quantitative variables:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res.pca}\OperatorTok{$}\NormalTok{quanti.sup}
\CommentTok{#> $coord}
\CommentTok{#>         Dim.1   Dim.2  Dim.3   Dim.4   Dim.5}
\CommentTok{#> Rank   -0.701 -0.2452 -0.183  0.0558 -0.0738}
\CommentTok{#> Points  0.964  0.0777  0.158 -0.1662 -0.0311}
\CommentTok{#> }
\CommentTok{#> $cor}
\CommentTok{#>         Dim.1   Dim.2  Dim.3   Dim.4   Dim.5}
\CommentTok{#> Rank   -0.701 -0.2452 -0.183  0.0558 -0.0738}
\CommentTok{#> Points  0.964  0.0777  0.158 -0.1662 -0.0311}
\CommentTok{#> }
\CommentTok{#> $cos2}
\CommentTok{#>        Dim.1   Dim.2  Dim.3   Dim.4   Dim.5}
\CommentTok{#> Rank   0.492 0.06012 0.0336 0.00311 0.00545}
\CommentTok{#> Points 0.929 0.00603 0.0250 0.02763 0.00097}
\end{Highlighting}
\end{Shaded}

Visualize all variables (active and supplementary ones):

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{fviz_pca_var}\NormalTok{(res.pca)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{meta_110b-PCA-Principal_Component_Methods_in_R_files/figure-latex/unnamed-chunk-57-1} \end{center}

\begin{quote}
Note that, by default, supplementary quantitative variables are shown in blue color and dashed lines.
\end{quote}

Further arguments to customize the plot:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Change color of variables}
\KeywordTok{fviz_pca_var}\NormalTok{(res.pca,}
             \DataTypeTok{col.var =} \StringTok{"black"}\NormalTok{,     }\CommentTok{# Active variables}
             \DataTypeTok{col.quanti.sup =} \StringTok{"red"} \CommentTok{# Suppl. quantitative variables}
\NormalTok{             )}
\CommentTok{# Hide active variables on the plot, }
\CommentTok{# show only supplementary variables}
\KeywordTok{fviz_pca_var}\NormalTok{(res.pca, }\DataTypeTok{invisible =} \StringTok{"var"}\NormalTok{)}
\CommentTok{# Hide supplementary variables}
\KeywordTok{fviz_pca_var}\NormalTok{(res.pca, }\DataTypeTok{invisible =} \StringTok{"quanti.sup"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{meta_110b-PCA-Principal_Component_Methods_in_R_files/figure-latex/unnamed-chunk-58-1} \includegraphics[width=0.7\linewidth]{meta_110b-PCA-Principal_Component_Methods_in_R_files/figure-latex/unnamed-chunk-58-2} \includegraphics[width=0.7\linewidth]{meta_110b-PCA-Principal_Component_Methods_in_R_files/figure-latex/unnamed-chunk-58-3} \end{center}

\begin{quote}
Using the fviz\_pca\_var(), the quantitative supplementary variables are displayed automatically on the correlation circle plot. Note that, you can add the quanti.sup variables manually, using the fviz\_add() function, for further customization. An example is shown below.
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Plot of active variables}
\NormalTok{p <-}\StringTok{ }\KeywordTok{fviz_pca_var}\NormalTok{(res.pca, }\DataTypeTok{invisible =} \StringTok{"quanti.sup"}\NormalTok{)}
\CommentTok{# Add supplementary active variables}
\KeywordTok{fviz_add}\NormalTok{(p, res.pca}\OperatorTok{$}\NormalTok{quanti.sup}\OperatorTok{$}\NormalTok{coord, }
         \DataTypeTok{geom =} \KeywordTok{c}\NormalTok{(}\StringTok{"arrow"}\NormalTok{, }\StringTok{"text"}\NormalTok{), }
         \DataTypeTok{color =} \StringTok{"red"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{meta_110b-PCA-Principal_Component_Methods_in_R_files/figure-latex/unnamed-chunk-59-1} \end{center}

\hypertarget{individuals}{%
\section{Individuals}\label{individuals}}

Predicted results for the supplementary individuals (ind.sup):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res.pca}\OperatorTok{$}\NormalTok{ind.sup}
\CommentTok{#> $coord}
\CommentTok{#>          Dim.1   Dim.2  Dim.3  Dim.4   Dim.5}
\CommentTok{#> KARPOV   0.795  0.7795 -1.633  1.724 -0.7507}
\CommentTok{#> WARNERS -0.386 -0.1216 -1.739 -0.706 -0.0323}
\CommentTok{#> Nool    -0.559  1.9775 -0.483 -2.278 -0.2546}
\CommentTok{#> Drews   -1.109  0.0174 -3.049 -1.534 -0.3264}
\CommentTok{#> }
\CommentTok{#> $cos2}
\CommentTok{#>          Dim.1    Dim.2  Dim.3  Dim.4    Dim.5}
\CommentTok{#> KARPOV  0.0510 4.91e-02 0.2155 0.2403 0.045549}
\CommentTok{#> WARNERS 0.0242 2.40e-03 0.4904 0.0809 0.000169}
\CommentTok{#> Nool    0.0290 3.62e-01 0.0216 0.4811 0.006008}
\CommentTok{#> Drews   0.0921 2.27e-05 0.6956 0.1762 0.007974}
\CommentTok{#> }
\CommentTok{#> $dist}
\CommentTok{#>  KARPOV WARNERS    Nool   Drews }
\CommentTok{#>    3.52    2.48    3.28    3.66}
\end{Highlighting}
\end{Shaded}

Visualize all individuals (active and supplementary ones). On the graph, you can add also the supplementary qualitative variables (quali.sup), which coordinates is accessible using res.pca\(quali.supp\)coord.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p <-}\StringTok{ }\KeywordTok{fviz_pca_ind}\NormalTok{(res.pca, }\DataTypeTok{col.ind.sup =} \StringTok{"blue"}\NormalTok{, }\DataTypeTok{repel =} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{p <-}\StringTok{ }\KeywordTok{fviz_add}\NormalTok{(p, res.pca}\OperatorTok{$}\NormalTok{quali.sup}\OperatorTok{$}\NormalTok{coord, }\DataTypeTok{color =} \StringTok{"red"}\NormalTok{)}
\NormalTok{p}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{meta_110b-PCA-Principal_Component_Methods_in_R_files/figure-latex/unnamed-chunk-61-1} \end{center}

\begin{quote}
Supplementary individuals are shown in blue. The levels of the supplementary qualitative variable are shown in red color.
\end{quote}

\hypertarget{qualitative-variables}{%
\section{Qualitative variables}\label{qualitative-variables}}

In the previous section, we showed that you can add the supplementary qualitative variables on individuals plot using fviz\_add().

Note that, the supplementary qualitative variables can be also used for coloring individuals by groups. This can help to interpret the data. The data sets decathlon2 contain a supplementary qualitative variable at columns 13 corresponding to the type of competitions.

The results concerning the supplementary qualitative variable are:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res.pca}\OperatorTok{$}\NormalTok{quali}
\CommentTok{#> $coord}
\CommentTok{#>          Dim.1  Dim.2   Dim.3  Dim.4  Dim.5}
\CommentTok{#> Decastar -1.34  0.122 -0.0379  0.181  0.134}
\CommentTok{#> OlympicG  1.23 -0.112  0.0347 -0.166 -0.123}
\CommentTok{#> }
\CommentTok{#> $cos2}
\CommentTok{#>          Dim.1   Dim.2   Dim.3  Dim.4   Dim.5}
\CommentTok{#> Decastar 0.905 0.00744 0.00072 0.0164 0.00905}
\CommentTok{#> OlympicG 0.905 0.00744 0.00072 0.0164 0.00905}
\CommentTok{#> }
\CommentTok{#> $v.test}
\CommentTok{#>          Dim.1  Dim.2  Dim.3  Dim.4 Dim.5}
\CommentTok{#> Decastar -2.97  0.403 -0.153  0.897  0.72}
\CommentTok{#> OlympicG  2.97 -0.403  0.153 -0.897 -0.72}
\CommentTok{#> }
\CommentTok{#> $dist}
\CommentTok{#> Decastar OlympicG }
\CommentTok{#>     1.41     1.29 }
\CommentTok{#> }
\CommentTok{#> $eta2}
\CommentTok{#>             Dim.1  Dim.2   Dim.3  Dim.4  Dim.5}
\CommentTok{#> Competition 0.401 0.0074 0.00106 0.0366 0.0236}
\end{Highlighting}
\end{Shaded}

To color individuals by a supplementary qualitative variable, the argument habillage is used to specify the index of the supplementary qualitative variable. Historically, this argument name comes from the FactoMineR package. It's a french word meaning ``dressing'' in english. To keep consistency between FactoMineR and factoextra, we decided to keep the same argument name

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{fviz_pca_ind}\NormalTok{(res.pca, }\DataTypeTok{habillage =} \DecValTok{13}\NormalTok{,}
             \DataTypeTok{addEllipses =}\OtherTok{TRUE}\NormalTok{, }\DataTypeTok{ellipse.type =} \StringTok{"confidence"}\NormalTok{,}
             \DataTypeTok{palette =} \StringTok{"jco"}\NormalTok{, }\DataTypeTok{repel =} \OtherTok{TRUE}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{meta_110b-PCA-Principal_Component_Methods_in_R_files/figure-latex/unnamed-chunk-63-1} \end{center}

\begin{quote}
Recall that, to remove the mean points of groups, specify the argument mean.point = FALSE.
\end{quote}

\hypertarget{filtering-results}{%
\section{Filtering results}\label{filtering-results}}

If you have many individuals/variable, it's possible to visualize only some of them using the arguments select.ind and select.var.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Visualize variable with cos2 >= 0.6}
\KeywordTok{fviz_pca_var}\NormalTok{(res.pca, }\DataTypeTok{select.var =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{cos2 =} \FloatTok{0.6}\NormalTok{))}
\CommentTok{# Top 5 active variables with the highest cos2}
\KeywordTok{fviz_pca_var}\NormalTok{(res.pca, }\DataTypeTok{select.var=} \KeywordTok{list}\NormalTok{(}\DataTypeTok{cos2 =} \DecValTok{5}\NormalTok{))}
\CommentTok{# Select by names}
\NormalTok{name <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}\DataTypeTok{name =} \KeywordTok{c}\NormalTok{(}\StringTok{"Long.jump"}\NormalTok{, }\StringTok{"High.jump"}\NormalTok{, }\StringTok{"X100m"}\NormalTok{))}
\KeywordTok{fviz_pca_var}\NormalTok{(res.pca, }\DataTypeTok{select.var =}\NormalTok{ name)}
\CommentTok{# top 5 contributing individuals and variable}
\KeywordTok{fviz_pca_biplot}\NormalTok{(res.pca, }\DataTypeTok{select.ind =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{contrib =} \DecValTok{5}\NormalTok{), }
               \DataTypeTok{select.var =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{contrib =} \DecValTok{5}\NormalTok{),}
               \DataTypeTok{ggtheme =} \KeywordTok{theme_minimal}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{meta_110b-PCA-Principal_Component_Methods_in_R_files/figure-latex/unnamed-chunk-64-1} \includegraphics[width=0.7\linewidth]{meta_110b-PCA-Principal_Component_Methods_in_R_files/figure-latex/unnamed-chunk-64-2} \includegraphics[width=0.7\linewidth]{meta_110b-PCA-Principal_Component_Methods_in_R_files/figure-latex/unnamed-chunk-64-3} \includegraphics[width=0.7\linewidth]{meta_110b-PCA-Principal_Component_Methods_in_R_files/figure-latex/unnamed-chunk-64-4} \end{center}

\begin{quote}
When the selection is done according to the contribution values, supplementary individuals/variables are not shown because they don't contribute to the construction of the axes.
\end{quote}

\hypertarget{exporting-results}{%
\section{Exporting results}\label{exporting-results}}

Export plots to PDF/PNG files
The factoextra package produces a ggplot2-based graphs. To save any ggplots, the standard R code is as follow:

\begin{verbatim}
# Print the plot to a pdf file
pdf("myplot.pdf")
print(myplot)
dev.off()
\end{verbatim}

In the following examples, we'll show you how to save the different graphs into pdf or png files.

The first step is to create the plots you want as an R object:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Scree plot}
\NormalTok{scree.plot <-}\StringTok{ }\KeywordTok{fviz_eig}\NormalTok{(res.pca)}
\CommentTok{# Plot of individuals}
\NormalTok{ind.plot <-}\StringTok{ }\KeywordTok{fviz_pca_ind}\NormalTok{(res.pca)}
\CommentTok{# Plot of variables}
\NormalTok{var.plot <-}\StringTok{ }\KeywordTok{fviz_pca_var}\NormalTok{(res.pca)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{pdf}\NormalTok{(}\KeywordTok{file.path}\NormalTok{(data_out_dir, }\StringTok{"PCA.pdf"}\NormalTok{))   }\CommentTok{# Create a new pdf device}
\KeywordTok{print}\NormalTok{(scree.plot)}
\KeywordTok{print}\NormalTok{(ind.plot)}
\KeywordTok{print}\NormalTok{(var.plot)}
\KeywordTok{dev.off}\NormalTok{() }\CommentTok{# Close the pdf device}
\CommentTok{#> pdf }
\CommentTok{#>   2}
\end{Highlighting}
\end{Shaded}

\begin{quote}
Note that, using the above R code will create the PDF file into your current working directory. To see the path of your current working directory, type getwd() in the R console.
\end{quote}

To print each plot to specific png file, the R code looks like this:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Print scree plot to a png file}
\KeywordTok{png}\NormalTok{(}\KeywordTok{file.path}\NormalTok{(data_out_dir, }\StringTok{"pca-scree-plot.png"}\NormalTok{))}
\KeywordTok{print}\NormalTok{(scree.plot)}
\KeywordTok{dev.off}\NormalTok{()}
\CommentTok{#> pdf }
\CommentTok{#>   2}
\CommentTok{# Print individuals plot to a png file}
\KeywordTok{png}\NormalTok{(}\KeywordTok{file.path}\NormalTok{(data_out_dir, }\StringTok{"pca-variables.png"}\NormalTok{))}
\KeywordTok{print}\NormalTok{(var.plot)}
\KeywordTok{dev.off}\NormalTok{()}
\CommentTok{#> pdf }
\CommentTok{#>   2}
\CommentTok{# Print variables plot to a png file}
\KeywordTok{png}\NormalTok{(}\KeywordTok{file.path}\NormalTok{(data_out_dir, }\StringTok{"pca-individuals.png"}\NormalTok{))}
\KeywordTok{print}\NormalTok{(ind.plot)}
\KeywordTok{dev.off}\NormalTok{()}
\CommentTok{#> pdf }
\CommentTok{#>   2}
\end{Highlighting}
\end{Shaded}

Another alternative, to export ggplots, is to use the function ggexport() {[}in ggpubr package{]}. We like ggexport(), because it's very simple. With one line R code, it allows us to export individual plots to a file (pdf, eps or png) (one plot per page). It can also arrange the plots (2 plot per page, for example) before exporting them. The examples below demonstrates how to export ggplots using ggexport().

Export individual plots to a pdf file (one plot per page):

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(ggpubr)}
\CommentTok{#> Loading required package: magrittr}
\KeywordTok{ggexport}\NormalTok{(}\DataTypeTok{plotlist =} \KeywordTok{list}\NormalTok{(scree.plot, ind.plot, var.plot), }
         \DataTypeTok{filename =} \KeywordTok{file.path}\NormalTok{(data_out_dir, }\StringTok{"PCA.pdf"}\NormalTok{))}
\CommentTok{#> file saved to /home/datascience/repos/machine-learning-rsuite/export/PCA.pdf}
\end{Highlighting}
\end{Shaded}

Arrange and export. Specify nrow and ncol to display multiple plots on the same page:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggexport}\NormalTok{(}\DataTypeTok{plotlist =} \KeywordTok{list}\NormalTok{(scree.plot, ind.plot, var.plot), }
         \DataTypeTok{nrow =} \DecValTok{2}\NormalTok{, }\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{,}
         \DataTypeTok{filename =} \KeywordTok{file.path}\NormalTok{(data_out_dir, }\StringTok{"PCA.pdf"}\NormalTok{))}
\CommentTok{#> file saved to /home/datascience/repos/machine-learning-rsuite/export/PCA.pdf}
\end{Highlighting}
\end{Shaded}

Export plots to png files. If you specify a list of plots, then multiple png files will be automatically created to hold each plot.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggexport}\NormalTok{(}\DataTypeTok{plotlist =} \KeywordTok{list}\NormalTok{(scree.plot, ind.plot, var.plot),}
         \DataTypeTok{filename =} \KeywordTok{file.path}\NormalTok{(data_out_dir, }\StringTok{"PCA.png"}\NormalTok{))}
\CommentTok{#> [1] "/home/datascience/repos/machine-learning-rsuite/export/PCA%03d.png"}
\CommentTok{#> file saved to /home/datascience/repos/machine-learning-rsuite/export/PCA%03d.png}
\end{Highlighting}
\end{Shaded}

\hypertarget{export-results-to-txtcsv-files}{%
\section{Export results to txt/csv files}\label{export-results-to-txtcsv-files}}

All the outputs of the PCA (individuals/variables coordinates, contributions, etc) can be exported at once, into a TXT/CSV file, using the function write.infile() {[}in FactoMineR{]} package:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Export into a TXT file}
\KeywordTok{write.infile}\NormalTok{(res.pca, }\KeywordTok{file.path}\NormalTok{(data_out_dir, }\StringTok{"pca.txt"}\NormalTok{), }\DataTypeTok{sep =} \StringTok{"}\CharTok{\textbackslash{}t}\StringTok{"}\NormalTok{)}
\CommentTok{# Export into a CSV file}
\KeywordTok{write.infile}\NormalTok{(res.pca, }\KeywordTok{file.path}\NormalTok{(data_out_dir, }\StringTok{"pca.csv"}\NormalTok{), }\DataTypeTok{sep =} \StringTok{";"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{summary}{%
\section{Summary}\label{summary}}

In conclusion, we described how to perform and interpret principal component analysis (PCA). We computed PCA using the PCA() function {[}FactoMineR{]}. Next, we used the factoextra R package to produce ggplot2-based visualization of the PCA results.

There are other functions {[}packages{]} to compute PCA in R:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Using prcomp() {[}stats{]}
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res.pca <-}\StringTok{ }\KeywordTok{prcomp}\NormalTok{(iris[, }\DecValTok{-5}\NormalTok{], }\DataTypeTok{scale. =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res.pca <-}\StringTok{ }\KeywordTok{princomp}\NormalTok{(iris[, }\DecValTok{-5}\NormalTok{], }\DataTypeTok{cor =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Using dudi.pca() {[}ade4{]}
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(ade4)}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'ade4'}
\CommentTok{#> The following object is masked from 'package:FactoMineR':}
\CommentTok{#> }
\CommentTok{#>     reconst}
\NormalTok{res.pca <-}\StringTok{ }\KeywordTok{dudi.pca}\NormalTok{(iris[, }\DecValTok{-5}\NormalTok{], }\DataTypeTok{scannf =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{nf =} \DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Using epPCA() {[}ExPosition{]}
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(ExPosition)}
\CommentTok{#> Loading required package: prettyGraphs}
\NormalTok{res.pca <-}\StringTok{ }\KeywordTok{epPCA}\NormalTok{(iris[, }\DecValTok{-5}\NormalTok{], }\DataTypeTok{graph =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

No matter what functions you decide to use, in the list above, the factoextra package can handle the output for creating beautiful plots similar to what we described in the previous sections for FactoMineR:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{fviz_eig}\NormalTok{(res.pca)     }\CommentTok{# Scree plot}
\KeywordTok{fviz_pca_ind}\NormalTok{(res.pca) }\CommentTok{# Graph of individuals}
\KeywordTok{fviz_pca_var}\NormalTok{(res.pca) }\CommentTok{# Graph of variables}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{meta_110b-PCA-Principal_Component_Methods_in_R_files/figure-latex/unnamed-chunk-76-1} \includegraphics[width=0.7\linewidth]{meta_110b-PCA-Principal_Component_Methods_in_R_files/figure-latex/unnamed-chunk-76-2} \includegraphics[width=0.7\linewidth]{meta_110b-PCA-Principal_Component_Methods_in_R_files/figure-latex/unnamed-chunk-76-3} \end{center}

\hypertarget{biplot-of-the-iris-data-set}{%
\chapter{Biplot of the Iris data set}\label{biplot-of-the-iris-data-set}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# devtools::install_github("vqv/ggbiplot")}
\KeywordTok{library}\NormalTok{(ggbiplot)}
\CommentTok{#> Loading required package: ggplot2}
\CommentTok{#> Registered S3 methods overwritten by 'ggplot2':}
\CommentTok{#>   method         from }
\CommentTok{#>   [.quosures     rlang}
\CommentTok{#>   c.quosures     rlang}
\CommentTok{#>   print.quosures rlang}
\CommentTok{#> Loading required package: plyr}
\CommentTok{#> Loading required package: scales}
\CommentTok{#> Loading required package: grid}

\NormalTok{iris.pca <-}\StringTok{ }\KeywordTok{prcomp}\NormalTok{(iris[, }\DecValTok{1}\OperatorTok{:}\DecValTok{4}\NormalTok{], }\DataTypeTok{center =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{scale =} \OtherTok{TRUE}\NormalTok{)}
\KeywordTok{print}\NormalTok{(iris.pca)}
\CommentTok{#> Standard deviations (1, .., p=4):}
\CommentTok{#> [1] 1.708 0.956 0.383 0.144}
\CommentTok{#> }
\CommentTok{#> Rotation (n x k) = (4 x 4):}
\CommentTok{#>                 PC1     PC2    PC3    PC4}
\CommentTok{#> Sepal.Length  0.521 -0.3774  0.720  0.261}
\CommentTok{#> Sepal.Width  -0.269 -0.9233 -0.244 -0.124}
\CommentTok{#> Petal.Length  0.580 -0.0245 -0.142 -0.801}
\CommentTok{#> Petal.Width   0.565 -0.0669 -0.634  0.524}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(iris.pca)}
\CommentTok{#> Importance of components:}
\CommentTok{#>                         PC1   PC2    PC3     PC4}
\CommentTok{#> Standard deviation     1.71 0.956 0.3831 0.14393}
\CommentTok{#> Proportion of Variance 0.73 0.229 0.0367 0.00518}
\CommentTok{#> Cumulative Proportion  0.73 0.958 0.9948 1.00000}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{g <-}\StringTok{ }\KeywordTok{ggbiplot}\NormalTok{(iris.pca,}
              \DataTypeTok{obs.scale =} \DecValTok{1}\NormalTok{,}
              \DataTypeTok{var.scale =} \DecValTok{1}\NormalTok{,}
              \DataTypeTok{groups =}\NormalTok{ iris}\OperatorTok{$}\NormalTok{Species,}
              \DataTypeTok{ellipse =} \OtherTok{TRUE}\NormalTok{,}
              \DataTypeTok{circle =} \OtherTok{TRUE}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{scale_color_discrete}\NormalTok{(}\DataTypeTok{name =} \StringTok{""}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{legend.direction =} \StringTok{"horizontal"}\NormalTok{, }\DataTypeTok{legend.position =} \StringTok{"top"}\NormalTok{)}

\KeywordTok{print}\NormalTok{(g)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{meta_110c-PCA-Phan_2016-Introduction_To_PCA_with_examples_in_R_files/figure-latex/unnamed-chunk-4-1} \end{center}

The PC1 axis explains 0.730 of the variance, while the PC2 axis explains
0.229 of the variance.

\hypertarget{iris-underlying-principal-components}{%
\section{Iris: underlying principal components}\label{iris-underlying-principal-components}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Run PCA here with prcomp ()}
\NormalTok{iris.pca <-}\StringTok{ }\KeywordTok{prcomp}\NormalTok{(iris[, }\DecValTok{1}\OperatorTok{:}\DecValTok{4}\NormalTok{], }\DataTypeTok{center =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{scale =} \OtherTok{TRUE}\NormalTok{)}

\KeywordTok{print}\NormalTok{(iris.pca)}
\CommentTok{#> Standard deviations (1, .., p=4):}
\CommentTok{#> [1] 1.708 0.956 0.383 0.144}
\CommentTok{#> }
\CommentTok{#> Rotation (n x k) = (4 x 4):}
\CommentTok{#>                 PC1     PC2    PC3    PC4}
\CommentTok{#> Sepal.Length  0.521 -0.3774  0.720  0.261}
\CommentTok{#> Sepal.Width  -0.269 -0.9233 -0.244 -0.124}
\CommentTok{#> Petal.Length  0.580 -0.0245 -0.142 -0.801}
\CommentTok{#> Petal.Width   0.565 -0.0669 -0.634  0.524}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Now, compute the new dataset aligned to the PCs by}
\CommentTok{# using the predict() function .}
\NormalTok{df.new <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(iris.pca, iris[, }\DecValTok{1}\OperatorTok{:}\DecValTok{4}\NormalTok{])}
\KeywordTok{head}\NormalTok{(df.new)}
\CommentTok{#>        PC1    PC2     PC3      PC4}
\CommentTok{#> [1,] -2.26 -0.478  0.1273  0.02409}
\CommentTok{#> [2,] -2.07  0.672  0.2338  0.10266}
\CommentTok{#> [3,] -2.36  0.341 -0.0441  0.02828}
\CommentTok{#> [4,] -2.29  0.595 -0.0910 -0.06574}
\CommentTok{#> [5,] -2.38 -0.645 -0.0157 -0.03580}
\CommentTok{#> [6,] -2.07 -1.484 -0.0269  0.00659}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Show the PCA models sdev values are the square root}
\CommentTok{# of the projected variances, which are along the diagonal}
\CommentTok{# of the covariance matrix of the projected data.}
\NormalTok{iris.pca}\OperatorTok{$}\NormalTok{sdev}\OperatorTok{^}\DecValTok{2}
\CommentTok{#> [1] 2.9185 0.9140 0.1468 0.0207}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# # Compute covariance matrix for new dataset.}
\CommentTok{# Recall that the standard deviation is the square root of the variance.}
\KeywordTok{round}\NormalTok{(}\KeywordTok{cov}\NormalTok{(df.new), }\DecValTok{5}\NormalTok{)}
\CommentTok{#>      PC1   PC2   PC3    PC4}
\CommentTok{#> PC1 2.92 0.000 0.000 0.0000}
\CommentTok{#> PC2 0.00 0.914 0.000 0.0000}
\CommentTok{#> PC3 0.00 0.000 0.147 0.0000}
\CommentTok{#> PC4 0.00 0.000 0.000 0.0207}
\end{Highlighting}
\end{Shaded}

\hypertarget{iris.-compute-the-eigenvectors-and-eigenvalues}{%
\section{Iris. Compute the eigenvectors and eigenvalues}\label{iris.-compute-the-eigenvectors-and-eigenvalues}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Scale and center the data.}
\NormalTok{df.scaled <-}\StringTok{ }\KeywordTok{scale}\NormalTok{(iris[, }\DecValTok{1}\OperatorTok{:}\DecValTok{4}\NormalTok{], }\DataTypeTok{center =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{scale =} \OtherTok{TRUE}\NormalTok{)}

\CommentTok{# Compute the covariance matrix.}
\NormalTok{cov.df.scaled <-}\StringTok{ }\KeywordTok{cov}\NormalTok{(df.scaled)}

\CommentTok{# Compute the eigenvectors and eigen values.}
\CommentTok{# Each eigenvector (column) is a principal component.}
\CommentTok{# Each eigenvalue is the variance explained by the}
\CommentTok{# associated eigenvector.}
\NormalTok{eigenInformation <-}\StringTok{ }\KeywordTok{eigen}\NormalTok{(cov.df.scaled)}

\KeywordTok{print}\NormalTok{(eigenInformation)}
\CommentTok{#> eigen() decomposition}
\CommentTok{#> $values}
\CommentTok{#> [1] 2.9185 0.9140 0.1468 0.0207}
\CommentTok{#> }
\CommentTok{#> $vectors}
\CommentTok{#>        [,1]    [,2]   [,3]   [,4]}
\CommentTok{#> [1,]  0.521 -0.3774  0.720  0.261}
\CommentTok{#> [2,] -0.269 -0.9233 -0.244 -0.124}
\CommentTok{#> [3,]  0.580 -0.0245 -0.142 -0.801}
\CommentTok{#> [4,]  0.565 -0.0669 -0.634  0.524}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Now, compute the new dataset aligned to the PCs by}
\CommentTok{# multiplying the eigenvector and data matrices.}


\CommentTok{# Create transposes in preparation for matrix multiplication}
\NormalTok{eigenvectors.t <-}\StringTok{ }\KeywordTok{t}\NormalTok{(eigenInformation}\OperatorTok{$}\NormalTok{vectors)     }\CommentTok{# 4x4}
\NormalTok{df.scaled.t <-}\StringTok{ }\KeywordTok{t}\NormalTok{(df.scaled)    }\CommentTok{# 4x150}

\CommentTok{# Perform matrix multiplication.}
\NormalTok{df.new <-}\StringTok{ }\NormalTok{eigenvectors.t }\OperatorTok{%*%}\StringTok{ }\NormalTok{df.scaled.t   }\CommentTok{# 4x150}

\CommentTok{# Create new data frame. First take transpose and}
\CommentTok{# then add column names.}
\NormalTok{df.new.t <-}\StringTok{ }\KeywordTok{t}\NormalTok{(df.new)    }\CommentTok{# 150x4}
\KeywordTok{colnames}\NormalTok{(df.new.t) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"PC1"}\NormalTok{, }\StringTok{"PC2"}\NormalTok{, }\StringTok{"PC3"}\NormalTok{, }\StringTok{"PC4"}\NormalTok{)}

\KeywordTok{head}\NormalTok{(df.new.t)}
\CommentTok{#>        PC1    PC2     PC3      PC4}
\CommentTok{#> [1,] -2.26 -0.478  0.1273  0.02409}
\CommentTok{#> [2,] -2.07  0.672  0.2338  0.10266}
\CommentTok{#> [3,] -2.36  0.341 -0.0441  0.02828}
\CommentTok{#> [4,] -2.29  0.595 -0.0910 -0.06574}
\CommentTok{#> [5,] -2.38 -0.645 -0.0157 -0.03580}
\CommentTok{#> [6,] -2.07 -1.484 -0.0269  0.00659}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Compute covariance matrix for new dataset }
\KeywordTok{round}\NormalTok{(}\KeywordTok{cov}\NormalTok{(df.new.t), }\DecValTok{5}\NormalTok{)}
\CommentTok{#>      PC1   PC2   PC3    PC4}
\CommentTok{#> PC1 2.92 0.000 0.000 0.0000}
\CommentTok{#> PC2 0.00 0.914 0.000 0.0000}
\CommentTok{#> PC3 0.00 0.000 0.147 0.0000}
\CommentTok{#> PC4 0.00 0.000 0.000 0.0207}
\end{Highlighting}
\end{Shaded}

\hypertarget{logistic-regression.-diabetes}{%
\chapter{Logistic Regression. Diabetes}\label{logistic-regression.-diabetes}}

\hypertarget{introduction-13}{%
\section{Introduction}\label{introduction-13}}

Source: \url{https://github.com/AntoineGuillot2/Logistic-Regression-R/blob/master/script.R}
Source: \url{http://enhancedatascience.com/2017/04/26/r-basics-logistic-regression-with-r/}
Data: \url{https://www.kaggle.com/uciml/pima-indians-diabetes-database}

The goal of logistic regression is to predict whether an outcome will be positive (aka 1) or negative (i.e: 0). Some real life example could be:

\begin{itemize}
\tightlist
\item
  Will Emmanuel Macron win the French Presidential election or will he lose?
\item
  Does Mr.X has this illness or not?
\item
  Will this visitor click on my link or not?
\end{itemize}

So, logistic regression can be used in a lot of binary classification cases and will often be run before more advanced methods. For this tutorial, we will use the diabetes detection dataset from Kaggle.

This dataset contains data from Pima Indians Women such as the number of pregnancies, the blood pressure, the skin thickness, \ldots{} the goal of the tutorial is to be able to detect diabetes using only these measures.

\hypertarget{exploring-the-data}{%
\section{Exploring the data}\label{exploring-the-data}}

As usual, first, let's take a look at our data. You can download the data here then please put the file diabetes.csv in your working directory. With the summary function, we can easily summarise the different variables:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(ggplot2)}
\CommentTok{#> Registered S3 methods overwritten by 'ggplot2':}
\CommentTok{#>   method         from }
\CommentTok{#>   [.quosures     rlang}
\CommentTok{#>   c.quosures     rlang}
\CommentTok{#>   print.quosures rlang}
\KeywordTok{library}\NormalTok{(data.table)}

\NormalTok{DiabetesData <-}\StringTok{ }\KeywordTok{data.table}\NormalTok{(}\KeywordTok{read.csv}\NormalTok{(}\KeywordTok{file.path}\NormalTok{(data_raw_dir, }\StringTok{'diabetes.csv'}\NormalTok{)))}

\CommentTok{# Quick data summary}
\KeywordTok{summary}\NormalTok{(DiabetesData)}
\CommentTok{#>   Pregnancies       Glucose    BloodPressure   SkinThickness }
\CommentTok{#>  Min.   : 0.00   Min.   :  0   Min.   :  0.0   Min.   : 0.0  }
\CommentTok{#>  1st Qu.: 1.00   1st Qu.: 99   1st Qu.: 62.0   1st Qu.: 0.0  }
\CommentTok{#>  Median : 3.00   Median :117   Median : 72.0   Median :23.0  }
\CommentTok{#>  Mean   : 3.85   Mean   :121   Mean   : 69.1   Mean   :20.5  }
\CommentTok{#>  3rd Qu.: 6.00   3rd Qu.:140   3rd Qu.: 80.0   3rd Qu.:32.0  }
\CommentTok{#>  Max.   :17.00   Max.   :199   Max.   :122.0   Max.   :99.0  }
\CommentTok{#>     Insulin         BMI       DiabetesPedigreeFunction      Age      }
\CommentTok{#>  Min.   :  0   Min.   : 0.0   Min.   :0.078            Min.   :21.0  }
\CommentTok{#>  1st Qu.:  0   1st Qu.:27.3   1st Qu.:0.244            1st Qu.:24.0  }
\CommentTok{#>  Median : 30   Median :32.0   Median :0.372            Median :29.0  }
\CommentTok{#>  Mean   : 80   Mean   :32.0   Mean   :0.472            Mean   :33.2  }
\CommentTok{#>  3rd Qu.:127   3rd Qu.:36.6   3rd Qu.:0.626            3rd Qu.:41.0  }
\CommentTok{#>  Max.   :846   Max.   :67.1   Max.   :2.420            Max.   :81.0  }
\CommentTok{#>     Outcome     }
\CommentTok{#>  Min.   :0.000  }
\CommentTok{#>  1st Qu.:0.000  }
\CommentTok{#>  Median :0.000  }
\CommentTok{#>  Mean   :0.349  }
\CommentTok{#>  3rd Qu.:1.000  }
\CommentTok{#>  Max.   :1.000}
\end{Highlighting}
\end{Shaded}

The mean of the outcome is 0.35 which shows an imbalance between the classes. However, the imbalance should not be too strong to be a problem.

To understand the relationship between variables, a Scatter Plot Matrix will be used. To plot it, the GGally package was used.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Scatter plot matrix}
\KeywordTok{library}\NormalTok{(GGally)}
\CommentTok{#> Registered S3 method overwritten by 'GGally':}
\CommentTok{#>   method from   }
\CommentTok{#>   +.gg   ggplot2}
\KeywordTok{ggpairs}\NormalTok{(DiabetesData, }\DataTypeTok{lower =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{continuous=}\StringTok{'smooth'}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{meta_137-logistic_regression-diabetes_files/figure-latex/plot-matrix-1} \end{center}

The correlations between explanatory variables do not seem too strong. Hence the model is not likely to suffer from multicollinearity. All explanatory variable are correlated with the Outcome. At first sight, glucose rate is the most important factor to detect the outcome.

\hypertarget{logistic-regression-with-r}{%
\section{Logistic regression with R}\label{logistic-regression-with-r}}

After variable exploration, a first model can be fitted using the glm function. With stargazer, it is easy to get nice output in ASCII or even Latex.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# first model: all features}
\NormalTok{glm1 =}\StringTok{ }\KeywordTok{glm}\NormalTok{(Outcome}\OperatorTok{~}\NormalTok{., }
\NormalTok{           DiabetesData, }
           \DataTypeTok{family =} \KeywordTok{binomial}\NormalTok{(}\DataTypeTok{link=}\StringTok{"logit"}\NormalTok{))}

\KeywordTok{summary}\NormalTok{(glm1)}
\CommentTok{#> }
\CommentTok{#> Call:}
\CommentTok{#> glm(formula = Outcome ~ ., family = binomial(link = "logit"), }
\CommentTok{#>     data = DiabetesData)}
\CommentTok{#> }
\CommentTok{#> Deviance Residuals: }
\CommentTok{#>    Min      1Q  Median      3Q     Max  }
\CommentTok{#> -2.557  -0.727  -0.416   0.727   2.930  }
\CommentTok{#> }
\CommentTok{#> Coefficients:}
\CommentTok{#>                           Estimate Std. Error z value Pr(>|z|)    }
\CommentTok{#> (Intercept)              -8.404696   0.716636  -11.73  < 2e-16 ***}
\CommentTok{#> Pregnancies               0.123182   0.032078    3.84  0.00012 ***}
\CommentTok{#> Glucose                   0.035164   0.003709    9.48  < 2e-16 ***}
\CommentTok{#> BloodPressure            -0.013296   0.005234   -2.54  0.01107 *  }
\CommentTok{#> SkinThickness             0.000619   0.006899    0.09  0.92852    }
\CommentTok{#> Insulin                  -0.001192   0.000901   -1.32  0.18607    }
\CommentTok{#> BMI                       0.089701   0.015088    5.95  2.8e-09 ***}
\CommentTok{#> DiabetesPedigreeFunction  0.945180   0.299147    3.16  0.00158 ** }
\CommentTok{#> Age                       0.014869   0.009335    1.59  0.11119    }
\CommentTok{#> ---}
\CommentTok{#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1}
\CommentTok{#> }
\CommentTok{#> (Dispersion parameter for binomial family taken to be 1)}
\CommentTok{#> }
\CommentTok{#>     Null deviance: 993.48  on 767  degrees of freedom}
\CommentTok{#> Residual deviance: 723.45  on 759  degrees of freedom}
\CommentTok{#> AIC: 741.4}
\CommentTok{#> }
\CommentTok{#> Number of Fisher Scoring iterations: 5}
\KeywordTok{require}\NormalTok{(stargazer)}
\CommentTok{#> Loading required package: stargazer}
\CommentTok{#> }
\CommentTok{#> Please cite as:}
\CommentTok{#>  Hlavac, Marek (2018). stargazer: Well-Formatted Regression and Summary Statistics Tables.}
\CommentTok{#>  R package version 5.2.2. https://CRAN.R-project.org/package=stargazer}
\KeywordTok{stargazer}\NormalTok{(glm1,}\DataTypeTok{type=}\StringTok{'text'}\NormalTok{)}
\CommentTok{#> }
\CommentTok{#> ====================================================}
\CommentTok{#>                              Dependent variable:    }
\CommentTok{#>                          ---------------------------}
\CommentTok{#>                                    Outcome          }
\CommentTok{#> ----------------------------------------------------}
\CommentTok{#> Pregnancies                       0.123***          }
\CommentTok{#>                                    (0.032)          }
\CommentTok{#>                                                     }
\CommentTok{#> Glucose                           0.035***          }
\CommentTok{#>                                    (0.004)          }
\CommentTok{#>                                                     }
\CommentTok{#> BloodPressure                     -0.013**          }
\CommentTok{#>                                    (0.005)          }
\CommentTok{#>                                                     }
\CommentTok{#> SkinThickness                       0.001           }
\CommentTok{#>                                    (0.007)          }
\CommentTok{#>                                                     }
\CommentTok{#> Insulin                            -0.001           }
\CommentTok{#>                                    (0.001)          }
\CommentTok{#>                                                     }
\CommentTok{#> BMI                               0.090***          }
\CommentTok{#>                                    (0.015)          }
\CommentTok{#>                                                     }
\CommentTok{#> DiabetesPedigreeFunction          0.945***          }
\CommentTok{#>                                    (0.299)          }
\CommentTok{#>                                                     }
\CommentTok{#> Age                                 0.015           }
\CommentTok{#>                                    (0.009)          }
\CommentTok{#>                                                     }
\CommentTok{#> Constant                          -8.400***         }
\CommentTok{#>                                    (0.717)          }
\CommentTok{#>                                                     }
\CommentTok{#> ----------------------------------------------------}
\CommentTok{#> Observations                         768            }
\CommentTok{#> Log Likelihood                    -362.000          }
\CommentTok{#> Akaike Inf. Crit.                  741.000          }
\CommentTok{#> ====================================================}
\CommentTok{#> Note:                    *p<0.1; **p<0.05; ***p<0.01}
\end{Highlighting}
\end{Shaded}

The overall model is significant. As expected the glucose rate has the lowest p-value of all the variables. However, Age, Insulin and Skin Thickness are not good predictors of Diabetes.

\hypertarget{a-second-model}{%
\section{A second model}\label{a-second-model}}

Since some variables are not significant, removing them is a good way to improve model robustness. In the second model, SkinThickness, Insulin, and Age are removed.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# second model: selected features}
\NormalTok{glm2 =}\StringTok{ }\KeywordTok{glm}\NormalTok{(Outcome}\OperatorTok{~}\NormalTok{., }
         \DataTypeTok{data =}\NormalTok{ DiabetesData[,}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{3}\NormalTok{,}\DecValTok{6}\OperatorTok{:}\DecValTok{7}\NormalTok{,}\DecValTok{9}\NormalTok{), }\DataTypeTok{with=}\NormalTok{F], }
         \DataTypeTok{family =} \KeywordTok{binomial}\NormalTok{(}\DataTypeTok{link=}\StringTok{"logit"}\NormalTok{))}

\KeywordTok{summary}\NormalTok{(glm2)}
\CommentTok{#> }
\CommentTok{#> Call:}
\CommentTok{#> glm(formula = Outcome ~ ., family = binomial(link = "logit"), }
\CommentTok{#>     data = DiabetesData[, c(1:3, 6:7, 9), with = F])}
\CommentTok{#> }
\CommentTok{#> Deviance Residuals: }
\CommentTok{#>    Min      1Q  Median      3Q     Max  }
\CommentTok{#> -2.793  -0.736  -0.419   0.725   2.955  }
\CommentTok{#> }
\CommentTok{#> Coefficients:}
\CommentTok{#>                          Estimate Std. Error z value Pr(>|z|)    }
\CommentTok{#> (Intercept)              -7.95495    0.67582  -11.77  < 2e-16 ***}
\CommentTok{#> Pregnancies               0.15349    0.02784    5.51  3.5e-08 ***}
\CommentTok{#> Glucose                   0.03466    0.00339   10.21  < 2e-16 ***}
\CommentTok{#> BloodPressure            -0.01201    0.00503   -2.39    0.017 *  }
\CommentTok{#> BMI                       0.08483    0.01412    6.01  1.9e-09 ***}
\CommentTok{#> DiabetesPedigreeFunction  0.91063    0.29403    3.10    0.002 ** }
\CommentTok{#> ---}
\CommentTok{#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1}
\CommentTok{#> }
\CommentTok{#> (Dispersion parameter for binomial family taken to be 1)}
\CommentTok{#> }
\CommentTok{#>     Null deviance: 993.48  on 767  degrees of freedom}
\CommentTok{#> Residual deviance: 728.56  on 762  degrees of freedom}
\CommentTok{#> AIC: 740.6}
\CommentTok{#> }
\CommentTok{#> Number of Fisher Scoring iterations: 5}
\end{Highlighting}
\end{Shaded}

\hypertarget{classification-rate-and-confusion-matrix}{%
\section{Classification rate and confusion matrix}\label{classification-rate-and-confusion-matrix}}

Now that we have our model, let's access its performance.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Correctly classified observations}
\KeywordTok{mean}\NormalTok{((glm2}\OperatorTok{$}\NormalTok{fitted.values}\OperatorTok{>}\FloatTok{0.5}\NormalTok{)}\OperatorTok{==}\NormalTok{DiabetesData}\OperatorTok{$}\NormalTok{Outcome)}
\CommentTok{#> [1] 0.775}
\end{Highlighting}
\end{Shaded}

Around 77.4\% of all observations are correctly classified. Due to class imbalance, we need to go further with a confusion matrix.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{###Confusion matrix count}
\NormalTok{RP=}\KeywordTok{sum}\NormalTok{((glm2}\OperatorTok{$}\NormalTok{fitted.values}\OperatorTok{>=}\FloatTok{0.5}\NormalTok{)}\OperatorTok{==}\NormalTok{DiabetesData}\OperatorTok{$}\NormalTok{Outcome }\OperatorTok{&}\StringTok{ }\NormalTok{DiabetesData}\OperatorTok{$}\NormalTok{Outcome}\OperatorTok{==}\DecValTok{1}\NormalTok{)}
\NormalTok{FP=}\KeywordTok{sum}\NormalTok{((glm2}\OperatorTok{$}\NormalTok{fitted.values}\OperatorTok{>=}\FloatTok{0.5}\NormalTok{)}\OperatorTok{!=}\NormalTok{DiabetesData}\OperatorTok{$}\NormalTok{Outcome }\OperatorTok{&}\StringTok{ }\NormalTok{DiabetesData}\OperatorTok{$}\NormalTok{Outcome}\OperatorTok{==}\DecValTok{0}\NormalTok{)}
\NormalTok{RN=}\KeywordTok{sum}\NormalTok{((glm2}\OperatorTok{$}\NormalTok{fitted.values}\OperatorTok{>=}\FloatTok{0.5}\NormalTok{)}\OperatorTok{==}\NormalTok{DiabetesData}\OperatorTok{$}\NormalTok{Outcome }\OperatorTok{&}\StringTok{ }\NormalTok{DiabetesData}\OperatorTok{$}\NormalTok{Outcome}\OperatorTok{==}\DecValTok{0}\NormalTok{)}
\NormalTok{FN=}\KeywordTok{sum}\NormalTok{((glm2}\OperatorTok{$}\NormalTok{fitted.values}\OperatorTok{>=}\FloatTok{0.5}\NormalTok{)}\OperatorTok{!=}\NormalTok{DiabetesData}\OperatorTok{$}\NormalTok{Outcome }\OperatorTok{&}\StringTok{ }\NormalTok{DiabetesData}\OperatorTok{$}\NormalTok{Outcome}\OperatorTok{==}\DecValTok{1}\NormalTok{)}
\NormalTok{confMat<-}\KeywordTok{matrix}\NormalTok{(}\KeywordTok{c}\NormalTok{(RP,FP,FN,RN),}\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{)}
\KeywordTok{colnames}\NormalTok{(confMat)<-}\KeywordTok{c}\NormalTok{(}\StringTok{"Pred Diabetes"}\NormalTok{,}\StringTok{'Pred no diabetes'}\NormalTok{)}
\KeywordTok{rownames}\NormalTok{(confMat)<-}\KeywordTok{c}\NormalTok{(}\StringTok{"Real Diabetes"}\NormalTok{,}\StringTok{'Real no diabetes'}\NormalTok{)}
\NormalTok{confMat}
\CommentTok{#>                  Pred Diabetes Pred no diabetes}
\CommentTok{#> Real Diabetes              154              114}
\CommentTok{#> Real no diabetes            59              441}
\end{Highlighting}
\end{Shaded}

The model is good to detect people who do not have diabetes. However, its performance on ill people is not great (only 154 out of 268 have been correctly classified).

You can also get the percentage of Real/False Positive/Negative:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Confusion matrix proportion}
\NormalTok{RPR=RP}\OperatorTok{/}\KeywordTok{sum}\NormalTok{(DiabetesData}\OperatorTok{$}\NormalTok{Outcome}\OperatorTok{==}\DecValTok{1}\NormalTok{)}\OperatorTok{*}\DecValTok{100}
\NormalTok{FNR=FN}\OperatorTok{/}\KeywordTok{sum}\NormalTok{(DiabetesData}\OperatorTok{$}\NormalTok{Outcome}\OperatorTok{==}\DecValTok{1}\NormalTok{)}\OperatorTok{*}\DecValTok{100}
\NormalTok{FPR=FP}\OperatorTok{/}\KeywordTok{sum}\NormalTok{(DiabetesData}\OperatorTok{$}\NormalTok{Outcome}\OperatorTok{==}\DecValTok{0}\NormalTok{)}\OperatorTok{*}\DecValTok{100}
\NormalTok{RNR=RN}\OperatorTok{/}\KeywordTok{sum}\NormalTok{(DiabetesData}\OperatorTok{$}\NormalTok{Outcome}\OperatorTok{==}\DecValTok{0}\NormalTok{)}\OperatorTok{*}\DecValTok{100}
\NormalTok{confMat<-}\KeywordTok{matrix}\NormalTok{(}\KeywordTok{c}\NormalTok{(RPR,FPR,FNR,RNR),}\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{)}
\KeywordTok{colnames}\NormalTok{(confMat)<-}\KeywordTok{c}\NormalTok{(}\StringTok{"Pred Diabetes"}\NormalTok{,}\StringTok{'Pred no diabetes'}\NormalTok{)}
\KeywordTok{rownames}\NormalTok{(confMat)<-}\KeywordTok{c}\NormalTok{(}\StringTok{"Real Diabetes"}\NormalTok{,}\StringTok{'Real no diabetes'}\NormalTok{)}
\NormalTok{confMat}
\CommentTok{#>                  Pred Diabetes Pred no diabetes}
\CommentTok{#> Real Diabetes             57.5             42.5}
\CommentTok{#> Real no diabetes          11.8             88.2}
\end{Highlighting}
\end{Shaded}

And here is the matrix, 57.5\% of people with diabetes are correctly classified. A way to improve the false negative rate would lower the detection threshold. However, as a consequence, the false positive rate would increase.

\hypertarget{plots-and-decision-boundaries}{%
\section{Plots and decision boundaries}\label{plots-and-decision-boundaries}}

The two strongest predictors of the outcome are Glucose rate and BMI. High glucose rate and high BMI are strong indicators of Diabetes.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Plot and decision boundaries}
\NormalTok{DiabetesData}\OperatorTok{$}\NormalTok{Predicted <-}\StringTok{ }\NormalTok{glm2}\OperatorTok{$}\NormalTok{fitted.values}

\KeywordTok{ggplot}\NormalTok{(DiabetesData, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ BMI, }\DataTypeTok{y =}\NormalTok{ Glucose, }\DataTypeTok{color =}\NormalTok{ Predicted }\OperatorTok{>}\StringTok{ }\FloatTok{0.5}\NormalTok{)) }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{size=}\DecValTok{2}\NormalTok{, }\DataTypeTok{alpha=}\FloatTok{0.5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{meta_137-logistic_regression-diabetes_files/figure-latex/plot-bmi-vs-glucose-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(DiabetesData, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{BMI, }\DataTypeTok{y =}\NormalTok{ Glucose, }\DataTypeTok{color=}\NormalTok{Outcome }\OperatorTok{==}\StringTok{ }\NormalTok{(Predicted }\OperatorTok{>}\StringTok{ }\FloatTok{0.5}\NormalTok{))) }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{size=}\DecValTok{2}\NormalTok{, }\DataTypeTok{alpha=}\FloatTok{0.5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{meta_137-logistic_regression-diabetes_files/figure-latex/unnamed-chunk-3-1} \end{center}

We can also plot both BMI and glucose against the outcomes, the other variables are taken at their mean level.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{range}\NormalTok{(DiabetesData}\OperatorTok{$}\NormalTok{BMI)}
\CommentTok{#> [1]  0.0 67.1}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# BMI vs predicted}
\NormalTok{BMI_plot =}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{BMI =}\NormalTok{ ((}\KeywordTok{min}\NormalTok{(DiabetesData}\OperatorTok{$}\NormalTok{BMI}\DecValTok{-2}\NormalTok{)}\OperatorTok{*}\DecValTok{100}\NormalTok{)}\OperatorTok{:}
\StringTok{                               }\NormalTok{(}\KeywordTok{max}\NormalTok{(DiabetesData}\OperatorTok{$}\NormalTok{BMI}\OperatorTok{+}\DecValTok{2}\NormalTok{)}\OperatorTok{*}\DecValTok{100}\NormalTok{))}\OperatorTok{/}\DecValTok{100}\NormalTok{,}
                    \DataTypeTok{Glucose =} \KeywordTok{mean}\NormalTok{(DiabetesData}\OperatorTok{$}\NormalTok{Glucose),}
                    \DataTypeTok{Pregnancies =} \KeywordTok{mean}\NormalTok{(DiabetesData}\OperatorTok{$}\NormalTok{Pregnancies),}
                    \DataTypeTok{BloodPressure =} \KeywordTok{mean}\NormalTok{(DiabetesData}\OperatorTok{$}\NormalTok{BloodPressure),}
                    \DataTypeTok{DiabetesPedigreeFunction =} \KeywordTok{mean}\NormalTok{(DiabetesData}\OperatorTok{$}\NormalTok{DiabetesPedigreeFunction))}

\NormalTok{BMI_plot}\OperatorTok{$}\NormalTok{Predicted =}\StringTok{ }\KeywordTok{predict}\NormalTok{(glm2, }\DataTypeTok{newdata =}\NormalTok{ BMI_plot, }\DataTypeTok{type =} \StringTok{'response'}\NormalTok{)}
\KeywordTok{ggplot}\NormalTok{(BMI_plot, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ BMI, }\DataTypeTok{y =}\NormalTok{ Predicted)) }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{geom_line}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{meta_137-logistic_regression-diabetes_files/figure-latex/bmi-vs-predicted-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{range}\NormalTok{(BMI_plot}\OperatorTok{$}\NormalTok{BMI)}
\CommentTok{#> [1] -2.0 69.1}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{range}\NormalTok{(DiabetesData}\OperatorTok{$}\NormalTok{Glucose)}
\CommentTok{#> [1]   0 199}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Glucose vs predicted}
\NormalTok{Glucose_plot=}\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{Glucose =} 
\NormalTok{                        ((}\KeywordTok{min}\NormalTok{(DiabetesData}\OperatorTok{$}\NormalTok{Glucose}\DecValTok{-2}\NormalTok{)}\OperatorTok{*}\DecValTok{100}\NormalTok{)}\OperatorTok{:}
\StringTok{                             }\NormalTok{(}\KeywordTok{max}\NormalTok{(DiabetesData}\OperatorTok{$}\NormalTok{Glucose}\OperatorTok{+}\DecValTok{2}\NormalTok{)}\OperatorTok{*}\DecValTok{100}\NormalTok{))}\OperatorTok{/}\DecValTok{100}\NormalTok{,}
                    \DataTypeTok{BMI=}\KeywordTok{mean}\NormalTok{(DiabetesData}\OperatorTok{$}\NormalTok{BMI),}
                    \DataTypeTok{Pregnancies=}\KeywordTok{mean}\NormalTok{(DiabetesData}\OperatorTok{$}\NormalTok{Pregnancies),}
                    \DataTypeTok{BloodPressure=}\KeywordTok{mean}\NormalTok{(DiabetesData}\OperatorTok{$}\NormalTok{BloodPressure),}
                    \DataTypeTok{DiabetesPedigreeFunction=}\KeywordTok{mean}\NormalTok{(DiabetesData}\OperatorTok{$}\NormalTok{DiabetesPedigreeFunction))}

\NormalTok{Glucose_plot}\OperatorTok{$}\NormalTok{Predicted =}\StringTok{ }\KeywordTok{predict}\NormalTok{(glm2, }\DataTypeTok{newdata =}\NormalTok{ Glucose_plot, }\DataTypeTok{type =} \StringTok{'response'}\NormalTok{)}
\KeywordTok{ggplot}\NormalTok{(Glucose_plot, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Glucose, }\DataTypeTok{y =}\NormalTok{ Predicted)) }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{geom_line}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{meta_137-logistic_regression-diabetes_files/figure-latex/glucose-vs-predicted-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{range}\NormalTok{(Glucose_plot}\OperatorTok{$}\NormalTok{Glucose)}
\CommentTok{#> [1]  -2 201}
\end{Highlighting}
\end{Shaded}

\hypertarget{sensitivity-analysis-for-neural-networks}{%
\chapter{Sensitivity analysis for neural networks}\label{sensitivity-analysis-for-neural-networks}}

\hypertarget{introduction-14}{%
\section{Introduction}\label{introduction-14}}

\url{https://beckmw.wordpress.com/tag/nnet/}

I've made quite a few blog posts about neural networks and some of the diagnostic tools that can be used to `demystify' the information contained in these models. Frankly, I'm kind of sick of writing about neural networks but I wanted to share one last tool I've implemented in R. I'm a strong believer that supervised neural networks can be used for much more than prediction, as is the common assumption by most researchers. I hope that my collection of posts, including this one, has shown the versatility of these models to develop inference into causation. To date, I've authored posts on visualizing neural networks, animating neural networks, and determining importance of model inputs. This post will describe a function for a sensitivity analysis of a neural network. Specifically, I will describe an approach to evaluate the form of the relationship of a response variable with the explanatory variables used in the model.

The general goal of a sensitivity analysis is similar to evaluating relative importance of explanatory variables, with a few important distinctions. For both analyses, we are interested in the relationships between explanatory and response variables as described by the model in the hope that the neural network has explained some real-world phenomenon. Using Garson's algorithm,1 we can get an idea of the magnitude and sign of the relationship between variables relative to each other. Conversely, the sensitivity analysis allows us to obtain information about the form of the relationship between variables rather than a categorical description, such as variable x is positively and strongly related to y. For example, how does a response variable change in relation to increasing or decreasing values of a given explanatory variable? Is it a linear response, non-linear, uni-modal, no response, etc.? Furthermore, how does the form of the response change given values of the other explanatory variables in the model? We might expect that the relationship between a response and explanatory variable might differ given the context of the other explanatory variables (i.e., an interaction may be present). The sensitivity analysis can provide this information.

As with most of my posts, I've created the sensitivity analysis function using ideas from other people that are much more clever than me. I've simply converted these ideas into a useful form in R. Ultimate credit for the sensitivity analysis goes to Sovan Lek (and colleagues), who developed the approach in the mid-1990s. The `Lek-profile method' is described briefly in Lek et al.~19962 and in more detail in Gevrey et al.~2003.3 I'll provide a brief summary here since the method is pretty simple. In fact, the profile method can be extended to any statistical model and is not specific to neural networks, although it is one of few methods used to evaluate the latter. For any statistical model where multiple response variables are related to multiple explanatory variables, we choose one response and one explanatory variable. We obtain predictions of the response variable across the range of values for the given explanatory variable. All other explanatory variables are held constant at a given set of respective values (e.g., minimum, 20th percentile, maximum). The final product is a set of response curves for one response variable across the range of values for one explanatory variable, while holding all other explanatory variables constant. This is implemented in R by creating a matrix of values for explanatory variables where the number of rows is the number of observations and the number of columns is the number of explanatory variables. All explanatory variables are held at their mean (or other constant value) while the variable of interest is sequenced from its minimum to maximum value across the range of observations. This matrix (actually a data frame) is then used to predict values of the response variable from a fitted model object. This is repeated for different variables.

I'll illustrate the function using simulated data, as I've done in previous posts. The exception here is that I'll be using two response variables instead of one. The two response variables are linear combinations of eight explanatory variables, with random error components taken from a normal distribution. The relationships between the variables are determined by the arbitrary set of parameters (\texttt{parms1} and \texttt{parms2}). The explanatory variables are partially correlated and taken from a multivariate normal distribution.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{require}\NormalTok{(clusterGeneration)}
\CommentTok{#> Loading required package: clusterGeneration}
\CommentTok{#> Loading required package: MASS}
\KeywordTok{require}\NormalTok{(nnet)}
\CommentTok{#> Loading required package: nnet}
  
\CommentTok{#define number of variables and observations}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{2}\NormalTok{)}
\NormalTok{num.vars<-}\DecValTok{8}
\NormalTok{num.obs<-}\DecValTok{10000}
  
\CommentTok{#define correlation matrix for explanatory variables }
\CommentTok{#define actual parameter values}
\NormalTok{cov.mat<-}\KeywordTok{genPositiveDefMat}\NormalTok{(num.vars,}\DataTypeTok{covMethod=}\KeywordTok{c}\NormalTok{(}\StringTok{"unifcorrmat"}\NormalTok{))}\OperatorTok{$}\NormalTok{Sigma}
\NormalTok{rand.vars<-}\KeywordTok{mvrnorm}\NormalTok{(num.obs,}\KeywordTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{,num.vars),}\DataTypeTok{Sigma=}\NormalTok{cov.mat)}
\NormalTok{parms1<-}\KeywordTok{runif}\NormalTok{(num.vars,}\OperatorTok{-}\DecValTok{10}\NormalTok{,}\DecValTok{10}\NormalTok{)}
\NormalTok{y1<-rand.vars }\OperatorTok{%*%}\StringTok{ }\KeywordTok{matrix}\NormalTok{(parms1) }\OperatorTok{+}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(num.obs,}\DataTypeTok{sd=}\DecValTok{20}\NormalTok{)}
\NormalTok{parms2<-}\KeywordTok{runif}\NormalTok{(num.vars,}\OperatorTok{-}\DecValTok{10}\NormalTok{,}\DecValTok{10}\NormalTok{)}
\NormalTok{y2<-rand.vars }\OperatorTok{%*%}\StringTok{ }\KeywordTok{matrix}\NormalTok{(parms2) }\OperatorTok{+}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(num.obs,}\DataTypeTok{sd=}\DecValTok{20}\NormalTok{)}
 
\CommentTok{#prep data and create neural network}
\NormalTok{rand.vars<-}\KeywordTok{data.frame}\NormalTok{(rand.vars)}
\NormalTok{resp<-}\KeywordTok{apply}\NormalTok{(}\KeywordTok{cbind}\NormalTok{(y1,y2),}\DecValTok{2}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(y) (y}\OperatorTok{-}\KeywordTok{min}\NormalTok{(y))}\OperatorTok{/}\NormalTok{(}\KeywordTok{max}\NormalTok{(y)}\OperatorTok{-}\KeywordTok{min}\NormalTok{(y)))}
\NormalTok{resp<-}\KeywordTok{data.frame}\NormalTok{(resp)}
\KeywordTok{names}\NormalTok{(resp)<-}\KeywordTok{c}\NormalTok{(}\StringTok{'Y1'}\NormalTok{,}\StringTok{'Y2'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mod1 <-}\StringTok{ }\KeywordTok{nnet}\NormalTok{(rand.vars,resp,}\DataTypeTok{size=}\DecValTok{8}\NormalTok{,}\DataTypeTok{linout=}\NormalTok{T)}
\CommentTok{#> # weights:  90}
\CommentTok{#> initial  value 30121.205794 }
\CommentTok{#> iter  10 value 130.537462}
\CommentTok{#> iter  20 value 57.187090}
\CommentTok{#> iter  30 value 47.285919}
\CommentTok{#> iter  40 value 42.778564}
\CommentTok{#> iter  50 value 39.837784}
\CommentTok{#> iter  60 value 36.694632}
\CommentTok{#> iter  70 value 35.140948}
\CommentTok{#> iter  80 value 34.268819}
\CommentTok{#> iter  90 value 33.772282}
\CommentTok{#> iter 100 value 33.472654}
\CommentTok{#> final  value 33.472654 }
\CommentTok{#> stopped after 100 iterations}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#import the function from Github}
\KeywordTok{library}\NormalTok{(devtools)}

\CommentTok{# source_url('https://gist.githubusercontent.com/fawda123/7471137/raw/466c1474d0a505ff044412703516c34f1a4684a5/nnet_plot_update.r')}
\KeywordTok{source}\NormalTok{(}\StringTok{"nnet_plot_update.r"}\NormalTok{)}
 
\CommentTok{#plot each model}
\KeywordTok{plot.nnet}\NormalTok{(mod1)}
\CommentTok{#> Loading required package: scales}
\CommentTok{#> Loading required package: reshape}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{meta_905-regression_-sensitivity_analysis_nn_files/figure-latex/unnamed-chunk-4-1} \end{center}

\hypertarget{the-lek-profile-function}{%
\section{The Lek profile function}\label{the-lek-profile-function}}

We've created a neural network that hopefully describes the relationship of two response variables with eight explanatory variables. The sensitivity analysis lets us visualize these relationships. The Lek profile function can be used once we have a neural network model in our workspace. The function is imported and used as follows:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# source('https://gist.githubusercontent.com/fawda123/6860630/raw/b8bf4a6c88d6b392b1bfa6ef24759ae98f31877c/lek_fun.r')}
\KeywordTok{source}\NormalTok{(}\StringTok{"lek_fun.r"}\NormalTok{)}

\KeywordTok{lek.fun}\NormalTok{(mod1)}
\CommentTok{#> Loading required package: ggplot2}
\CommentTok{#> Registered S3 methods overwritten by 'ggplot2':}
\CommentTok{#>   method         from }
\CommentTok{#>   [.quosures     rlang}
\CommentTok{#>   c.quosures     rlang}
\CommentTok{#>   print.quosures rlang}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{meta_905-regression_-sensitivity_analysis_nn_files/figure-latex/unnamed-chunk-5-1} \end{center}

\begin{quote}
Fig: Sensitivity analysis of the two response variables in the neural network model to individual explanatory variables. Splits represent the quantile values at which the remaining explanatory variables were held constant. The function can be obtained \href{https://gist.githubusercontent.com/fawda123/6860630/raw/b8bf4a6c88d6b392b1bfa6ef24759ae98f31877c/lek_fun.r}{here}
\end{quote}

By default, the function runs a sensitivity analysis for all variables. This creates a busy plot so we may want to look at specific variables of interest. Maybe we want to evaluate different quantile values as well. These options can be changed using the arguments.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{lek.fun}\NormalTok{(mod1,}\DataTypeTok{var.sens=}\KeywordTok{c}\NormalTok{(}\StringTok{'X2'}\NormalTok{,}\StringTok{'X5'}\NormalTok{),}\DataTypeTok{split.vals=}\KeywordTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DataTypeTok{by=}\FloatTok{0.05}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{meta_905-regression_-sensitivity_analysis_nn_files/figure-latex/unnamed-chunk-6-1} \end{center}

\begin{quote}
Fig: Sensitivity analysis of the two response variables in relation to explanatory variables X2 and X5 and different quantile values for the remaining variables.
\end{quote}

The function also returns a ggplot2 object that can be further modified. You may prefer a different theme, color, or line type, for example.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p1<-}\KeywordTok{lek.fun}\NormalTok{(mod1)}
\KeywordTok{class}\NormalTok{(p1)}
\CommentTok{#> [1] "gg"     "ggplot"}
\CommentTok{# [1] "gg"     "ggplot"}
 
\NormalTok{p1 }\OperatorTok{+}\StringTok{ }
\StringTok{   }\KeywordTok{theme_bw}\NormalTok{() }\OperatorTok{+}
\StringTok{   }\KeywordTok{scale_colour_brewer}\NormalTok{(}\DataTypeTok{palette=}\StringTok{"PuBu"}\NormalTok{) }\OperatorTok{+}
\StringTok{   }\KeywordTok{scale_linetype_manual}\NormalTok{(}\DataTypeTok{values=}\KeywordTok{rep}\NormalTok{(}\StringTok{'dashed'}\NormalTok{,}\DecValTok{6}\NormalTok{)) }\OperatorTok{+}
\StringTok{   }\KeywordTok{scale_size_manual}\NormalTok{(}\DataTypeTok{values=}\KeywordTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{6}\NormalTok{))}
\CommentTok{#> Scale for 'linetype' is already present. Adding another scale for}
\CommentTok{#> 'linetype', which will replace the existing scale.}
\CommentTok{#> Scale for 'size' is already present. Adding another scale for 'size',}
\CommentTok{#> which will replace the existing scale.}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{meta_905-regression_-sensitivity_analysis_nn_files/figure-latex/unnamed-chunk-7-1} \end{center}

\hypertarget{getting-a-dataframe-from-lek}{%
\section{\texorpdfstring{Getting a dataframe from \texttt{lek}}{Getting a dataframe from lek}}\label{getting-a-dataframe-from-lek}}

Finally, the actual values from the sensitivity analysis can be returned if you'd prefer that instead. The output is a data frame in long form that was created using melt.list from the reshape package for compatibility with ggplot2. The six columns indicate values for explanatory variables on the x-axes, names of the response variables, predicted values of the response variables, quantiles at which other explanatory variables were held constant, and names of the explanatory variables on the x-axes.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(}\KeywordTok{lek.fun}\NormalTok{(mod1,}\DataTypeTok{val.out =} \OtherTok{TRUE}\NormalTok{))}
\CommentTok{#>   Explanatory resp.name Response Splits exp.name}
\CommentTok{#> 1       -9.58        Y1    0.466      0       X1}
\CommentTok{#> 2       -9.39        Y1    0.466      0       X1}
\CommentTok{#> 3       -9.19        Y1    0.467      0       X1}
\CommentTok{#> 4       -9.00        Y1    0.467      0       X1}
\CommentTok{#> 5       -8.81        Y1    0.468      0       X1}
\CommentTok{#> 6       -8.62        Y1    0.468      0       X1}
\end{Highlighting}
\end{Shaded}

\hypertarget{the-lek-function-works-with-lm}{%
\section{\texorpdfstring{The \texttt{lek} function works with \texttt{lm}}{The lek function works with lm}}\label{the-lek-function-works-with-lm}}

I mentioned earlier that the function is not unique to neural networks and can work with other models created in R. I haven't done an extensive test of the function, but I'm fairly certain that it will work if the model object has a predict method (e.g., predict.lm). Here's an example using the function to evaluate a multiple linear regression for one of the response variables.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mod2 <-}\KeywordTok{lm}\NormalTok{(Y1 }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =} \KeywordTok{cbind}\NormalTok{(resp[,}\StringTok{'Y1'}\NormalTok{, }\DataTypeTok{drop =}\NormalTok{ F], rand.vars))}
\KeywordTok{lek.fun}\NormalTok{(mod2)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{meta_905-regression_-sensitivity_analysis_nn_files/figure-latex/unnamed-chunk-9-1} \end{center}

This function has little relevance for conventional models like linear regression since a wealth of \texttt{diagnostic} tools are already available (e.g., effects plots, add/drop procedures, outlier tests, etc.). The application of the function to neural networks provides insight into the relationships described by the models, insights that to my knowledge, cannot be obtained using current tools in R. This post concludes my contribution of diagnostic tools for neural networks in R and I hope that they have been useful to some of you. I have spent the last year or so working with neural networks and my opinion of their utility is mixed. I see advantages in the use of highly flexible computer-based algorithms, although in most cases similar conclusions can be made using more conventional analyses. I suggest that neural networks only be used \emph{if there is an extremely high sample size} and other methods have proven inconclusive. Feel free to voice your opinions or suggestions in the comments.

\hypertarget{lek-function-works-with-rsnns}{%
\section{\texorpdfstring{\texttt{lek} function works with \texttt{RSNNS}}{lek function works with RSNNS}}\label{lek-function-works-with-rsnns}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{require}\NormalTok{(clusterGeneration)}
\KeywordTok{require}\NormalTok{(RSNNS)}
\CommentTok{#> Loading required package: RSNNS}
\CommentTok{#> Loading required package: Rcpp}
\KeywordTok{require}\NormalTok{(devtools)}
 
\CommentTok{#define number of variables and observations}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{2}\NormalTok{)}
\NormalTok{num.vars<-}\DecValTok{8}
\NormalTok{num.obs<-}\DecValTok{10000}
 
\CommentTok{#define correlation matrix for explanatory variables }
\CommentTok{#define actual parameter values}
\NormalTok{cov.mat <-}\KeywordTok{genPositiveDefMat}\NormalTok{(num.vars,}\DataTypeTok{covMethod=}\KeywordTok{c}\NormalTok{(}\StringTok{"unifcorrmat"}\NormalTok{))}\OperatorTok{$}\NormalTok{Sigma}
\NormalTok{rand.vars <-}\KeywordTok{mvrnorm}\NormalTok{(num.obs,}\KeywordTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{,num.vars),}\DataTypeTok{Sigma=}\NormalTok{cov.mat)}
\NormalTok{parms1 <-}\KeywordTok{runif}\NormalTok{(num.vars,}\OperatorTok{-}\DecValTok{10}\NormalTok{,}\DecValTok{10}\NormalTok{)}
\NormalTok{y1 <-rand.vars }\OperatorTok{%*%}\StringTok{ }\KeywordTok{matrix}\NormalTok{(parms1) }\OperatorTok{+}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(num.obs,}\DataTypeTok{sd=}\DecValTok{20}\NormalTok{)}
\NormalTok{parms2 <-}\KeywordTok{runif}\NormalTok{(num.vars,}\OperatorTok{-}\DecValTok{10}\NormalTok{,}\DecValTok{10}\NormalTok{)}
\NormalTok{y2 <-rand.vars }\OperatorTok{%*%}\StringTok{ }\KeywordTok{matrix}\NormalTok{(parms2) }\OperatorTok{+}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(num.obs,}\DataTypeTok{sd=}\DecValTok{20}\NormalTok{)}
 
\CommentTok{#prep data and create neural network}
\NormalTok{rand.vars <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(rand.vars)}
\NormalTok{resp <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(}\KeywordTok{cbind}\NormalTok{(y1,y2),}\DecValTok{2}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(y) (y}\OperatorTok{-}\KeywordTok{min}\NormalTok{(y))}\OperatorTok{/}\NormalTok{(}\KeywordTok{max}\NormalTok{(y)}\OperatorTok{-}\KeywordTok{min}\NormalTok{(y)))}
\NormalTok{resp <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(resp)}
\KeywordTok{names}\NormalTok{(resp)<-}\KeywordTok{c}\NormalTok{(}\StringTok{'Y1'}\NormalTok{,}\StringTok{'Y2'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tibble}\OperatorTok{::}\KeywordTok{as_tibble}\NormalTok{(rand.vars)}
\CommentTok{#> # A tibble: 10,000 x 8}
\CommentTok{#>        X1     X2     X3     X4     X5    X6    X7     X8}
\CommentTok{#>     <dbl>  <dbl>  <dbl>  <dbl>  <dbl> <dbl> <dbl>  <dbl>}
\CommentTok{#> 1  1.61    2.13   2.13   3.97  -1.34   2.00  3.11 -2.55 }
\CommentTok{#> 2 -1.25    3.07  -0.325  1.61  -0.484  2.28  2.98 -1.71 }
\CommentTok{#> 3 -3.17   -1.29  -1.77  -1.66  -0.549 -3.19  1.07  1.81 }
\CommentTok{#> 4 -2.39    3.28  -3.42  -0.160 -1.52   2.67  7.05 -1.14 }
\CommentTok{#> 5 -1.55   -0.181 -1.14   2.27  -1.68  -1.67  3.08  0.334}
\CommentTok{#> 6  0.0690 -1.54  -2.98   2.84   1.42   1.31  1.82  2.07 }
\CommentTok{#> # ... with 9,994 more rows}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tibble}\OperatorTok{::}\KeywordTok{as_tibble}\NormalTok{(resp)}
\CommentTok{#> # A tibble: 10,000 x 2}
\CommentTok{#>      Y1    Y2}
\CommentTok{#>   <dbl> <dbl>}
\CommentTok{#> 1 0.461 0.500}
\CommentTok{#> 2 0.416 0.509}
\CommentTok{#> 3 0.534 0.675}
\CommentTok{#> 4 0.548 0.619}
\CommentTok{#> 5 0.519 0.659}
\CommentTok{#> 6 0.389 0.622}
\CommentTok{#> # ... with 9,994 more rows}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# create neural network model}
\NormalTok{mod2 <-}\StringTok{ }\KeywordTok{mlp}\NormalTok{(rand.vars, resp, }\DataTypeTok{size =} \DecValTok{8}\NormalTok{, }\DataTypeTok{linOut =}\NormalTok{ T)}
 
\CommentTok{#import sensitivity analysis function}
\KeywordTok{source_url}\NormalTok{(}\StringTok{'https://gist.githubusercontent.com/fawda123/6860630/raw/b8bf4a6c88d6b392b1bfa6ef24759ae98f31877c/lek_fun.r'}\NormalTok{)}
\CommentTok{#> SHA-1 hash of file is 4a2d33b94a08f46a94518207a4ae7cc412845222}
 
\CommentTok{#sensitivity analsyis, note 'exp.in' argument}
\KeywordTok{lek.fun}\NormalTok{(mod2, }\DataTypeTok{exp.in =}\NormalTok{ rand.vars)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{meta_905-regression_-sensitivity_analysis_nn_files/figure-latex/unnamed-chunk-13-1} \end{center}

\hypertarget{references}{%
\chapter{References}\label{references}}

1 Garson GD. 1991. Interpreting neural network connection weights. Artificial Intelligence Expert. 6:46--51.
2 Lek S, Delacoste M, Baran P, Dimopoulos I, Lauga J, Aulagnier S. 1996. Application of neural networks to modelling nonlinear relationships in Ecology. Ecological Modelling. 90:39-52.
3 Gevrey M, Dimopoulos I, Lek S. 2003. Review and comparison of methods to study the contribution of variables in artificial neural network models. Ecological Modelling. 160:249-264.

\hypertarget{what-is-.hat-in-regression-output}{%
\chapter{What is .hat in regression output}\label{what-is-.hat-in-regression-output}}

\url{https://stats.stackexchange.com/a/256364/154908}

\textbf{Q.} The augment() function in the broom package for R creates a dataframe of predicted values from a regression model. Columns created include the fitted values, the standard error of the fit and Cook's distance. They also include something with which I'm not familar and that is the column .hat.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(broom)}
\KeywordTok{data}\NormalTok{(mtcars)}

\NormalTok{m1 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(mpg }\OperatorTok{~}\StringTok{ }\NormalTok{wt, }\DataTypeTok{data =}\NormalTok{ mtcars)}

\KeywordTok{head}\NormalTok{(}\KeywordTok{augment}\NormalTok{(m1))}
\CommentTok{#> # A tibble: 6 x 10}
\CommentTok{#>   .rownames   mpg    wt .fitted .se.fit .resid   .hat .sigma .cooksd}
\CommentTok{#>   <chr>     <dbl> <dbl>   <dbl>   <dbl>  <dbl>  <dbl>  <dbl>   <dbl>}
\CommentTok{#> 1 Mazda RX4  21    2.62    23.3   0.634 -2.28  0.0433   3.07 1.33e-2}
\CommentTok{#> 2 Mazda RX~  21    2.88    21.9   0.571 -0.920 0.0352   3.09 1.72e-3}
\CommentTok{#> 3 Datsun 7~  22.8  2.32    24.9   0.736 -2.09  0.0584   3.07 1.54e-2}
\CommentTok{#> 4 Hornet 4~  21.4  3.22    20.1   0.538  1.30  0.0313   3.09 3.02e-3}
\CommentTok{#> 5 Hornet S~  18.7  3.44    18.9   0.553 -0.200 0.0329   3.10 7.60e-5}
\CommentTok{#> 6 Valiant    18.1  3.46    18.8   0.555 -0.693 0.0332   3.10 9.21e-4}
\CommentTok{#> # ... with 1 more variable: .std.resid <dbl>}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# .hat vector}
\KeywordTok{augment}\NormalTok{(m1)}\OperatorTok{$}\NormalTok{.hat}
\CommentTok{#>  [1] 0.0433 0.0352 0.0584 0.0313 0.0329 0.0332 0.0354 0.0313 0.0314 0.0329}
\CommentTok{#> [11] 0.0329 0.0558 0.0401 0.0419 0.1705 0.1953 0.1838 0.0661 0.1177 0.0956}
\CommentTok{#> [21] 0.0503 0.0343 0.0328 0.0443 0.0445 0.0866 0.0704 0.1291 0.0313 0.0380}
\CommentTok{#> [31] 0.0354 0.0377}
\end{Highlighting}
\end{Shaded}

Can anyone explain what this value is, and is it different between linear regression and logistic regression?

\textbf{A.} Those would be the diagonal elements of the hat-matrix which describe the leverage each point has on its fitted values.

If one fits:

\[\vec{Y} = \mathbf{X} \vec {\beta} + \vec {\epsilon}\]

then:

\[\mathbf{H} = \mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\]
In this example:

\[
\begin{pmatrix}Y_1\\
\vdots\\
Y_{32}\end{pmatrix} = \begin{pmatrix}
1 & 2.620\\
\vdots\\
1 & 2.780
\end{pmatrix} \cdot \begin{pmatrix}
\beta_0\\
\beta_1
\end{pmatrix} + \begin{pmatrix}\epsilon_1\\
\vdots\\
\epsilon_{32}\end{pmatrix}
\]

Then calculating this \(\mathbf{H}\) matrix results in:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(MASS)}

\NormalTok{wt <-}\StringTok{ }\NormalTok{mtcars[, }\DecValTok{6}\NormalTok{]}

\NormalTok{X <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\KeywordTok{cbind}\NormalTok{(}\KeywordTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{, }\KeywordTok{length}\NormalTok{(wt)), wt), }\DataTypeTok{ncol=}\DecValTok{2}\NormalTok{)}

\NormalTok{H <-}\StringTok{ }\NormalTok{X }\OperatorTok{%*%}\StringTok{ }\KeywordTok{ginv}\NormalTok{(}\KeywordTok{t}\NormalTok{(X) }\OperatorTok{%*%}\StringTok{ }\NormalTok{X) }\OperatorTok{%*%}\StringTok{ }\KeywordTok{t}\NormalTok{(X)}
\end{Highlighting}
\end{Shaded}

Where this last matrix is a \(32 \times 32\) matrix and contains these hat values on the diagonal.

\begin{verbatim}
X                           32x2
t(X)                        2x32
X %*% t(X)                  32x32
t(X) %*% X                  2x2
ginv(t(X) %*% X)            2x2
ginv(t(X) %*% X) %*% t(X)   2x32
X %*% ginv(t(X) %*% X)      32x2
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{dim}\NormalTok{(}\KeywordTok{ginv}\NormalTok{(}\KeywordTok{t}\NormalTok{(X) }\OperatorTok{%*%}\StringTok{ }\NormalTok{X) }\OperatorTok{%*%}\StringTok{ }\KeywordTok{t}\NormalTok{(X))}
\CommentTok{#> [1]  2 32}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x1 <-}\StringTok{ }\NormalTok{X }\OperatorTok{%*%}\StringTok{ }\KeywordTok{ginv}\NormalTok{(}\KeywordTok{t}\NormalTok{(X) }\OperatorTok{%*%}\StringTok{ }\NormalTok{X)}
\KeywordTok{dim}\NormalTok{(x1)}
\CommentTok{#> [1] 32  2}
\KeywordTok{dim}\NormalTok{(x1 }\OperatorTok{%*%}\StringTok{ }\KeywordTok{t}\NormalTok{(X))}
\CommentTok{#> [1] 32 32}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x2 <-}\StringTok{ }\KeywordTok{ginv}\NormalTok{(}\KeywordTok{t}\NormalTok{(X) }\OperatorTok{%*%}\StringTok{ }\NormalTok{X) }\OperatorTok{%*%}\StringTok{ }\KeywordTok{t}\NormalTok{(X)}
\KeywordTok{dim}\NormalTok{(x2)}
\CommentTok{#> [1]  2 32}
\KeywordTok{dim}\NormalTok{(X }\OperatorTok{%*%}\StringTok{ }\NormalTok{x2)}
\CommentTok{#> [1] 32 32}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# this last matrix is a 3232 matrix and contains these hat values on the diagonal.}
\KeywordTok{diag}\NormalTok{(H)}
\CommentTok{#>  [1] 0.0433 0.0352 0.0584 0.0313 0.0329 0.0332 0.0354 0.0313 0.0314 0.0329}
\CommentTok{#> [11] 0.0329 0.0558 0.0401 0.0419 0.1705 0.1953 0.1838 0.0661 0.1177 0.0956}
\CommentTok{#> [21] 0.0503 0.0343 0.0328 0.0443 0.0445 0.0866 0.0704 0.1291 0.0313 0.0380}
\CommentTok{#> [31] 0.0354 0.0377}
\end{Highlighting}
\end{Shaded}

\hypertarget{q-q-normal-to-compare-data-to-distributions}{%
\chapter{Q-Q normal to compare data to distributions}\label{q-q-normal-to-compare-data-to-distributions}}

\hypertarget{introduction-15}{%
\section{Introduction}\label{introduction-15}}

\url{https://mgimond.github.io/ES218/Week06a.html}

Thus far, we have used the quantile-quantile plots to compare the distributions between two empirical (i.e.~observational) datasets. This is sometimes referred to as an empirical \textbf{Q-Q} plot. We can also use the q-q plot to compare an \emph{empirical} observation to a \emph{theoretical} observation (i.e.~one defined mathematically). Such a plot is usually referred to as a \textbf{theoretical Q-Q plot}. Examples of popular theoretical observations are the normal distribution (aka the Gaussian distribution), the \textbf{chi-square} distribution, and the exponential distribution just to name a few.

\begin{verbatim}
#> Registered S3 methods overwritten by 'ggplot2':
#>   method         from 
#>   [.quosures     rlang
#>   c.quosures     rlang
#>   print.quosures rlang
\end{verbatim}

\begin{center}\includegraphics[width=0.7\linewidth]{misc_111c-comparing_distributions_with_qq_plot_files/figure-latex/unnamed-chunk-2-1} \end{center}

\hypertarget{why-we-want-to-compare-emprirical-vs-theoretical-distributions}{%
\section{Why we want to compare emprirical vs theoretical distributions}\label{why-we-want-to-compare-emprirical-vs-theoretical-distributions}}

There are many reasons we might want to compare empirical data to theoretical distributions:

\begin{itemize}
\item
  A theoretical distribution is easy to parameterize. For example, if the shape of the distribution of a batch of numbers can be approximated by a normal distribution we can reduce the complexity of our data to just two values: the mean and the standard deviation.
\item
  If data can be approximated by certain theoretical distributions, then many mainstream statistical procedures can be applied to the data.
\item
  In inferential statistics, knowing that a sample was derived from a population whose distribution follows a theoretical distribution allows us to derive certain properties of the population from the sample. For example, if we know that a sample comes from a normally distributed population, we can define confidence intervals for the sample mean using a \textbf{t-distribution}.
\item
  Modeling the distribution of the observed data can provide insight into the underlying process that generated the data.
\end{itemize}

But very few empirical datasets follow any theoretical distributions exactly. So the questions usually ends up being ``how well does theoretical distribution X fit my data?''

The theoretical quantile-quantile plot is a tool to explore how a batch of numbers deviates from a theoretical distribution and to visually assess whether the difference is significant for the purpose of the analysis. In the following examples, we will compare empirical data to the normal distribution using the normal quantile-quantile plot.

\hypertarget{the-normal-q-q-plot}{%
\section{The normal q-q plot}\label{the-normal-q-q-plot}}

The normal q-q plot is just a special case of the empirical q-q plot we've explored so far; the difference being that we assign the normal distribution quantiles to the x-axis.

\hypertarget{drawing-a-normal-q-q-plot-from-scratch}{%
\subsection{Drawing a normal q-q plot from scratch}\label{drawing-a-normal-q-q-plot-from-scratch}}

In the following example, we'll compare the Alto 1 group to a normal distribution. First, we'll extract the Alto 1 height values and save them as an atomic vector object using dplyr's piping operations.

However, dplyr's operations will return a dataframe--even if a single column is selected. To force the output to an atomic vector, we'll pipe the subset to \texttt{pull(height)} which will extract the height column into a plain vector element.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(dplyr)}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'dplyr'}
\CommentTok{#> The following object is masked from 'package:gridExtra':}
\CommentTok{#> }
\CommentTok{#>     combine}
\CommentTok{#> The following objects are masked from 'package:stats':}
\CommentTok{#> }
\CommentTok{#>     filter, lag}
\CommentTok{#> The following objects are masked from 'package:base':}
\CommentTok{#> }
\CommentTok{#>     intersect, setdiff, setequal, union}

\NormalTok{df   <-}\StringTok{ }\NormalTok{lattice}\OperatorTok{::}\NormalTok{singer}
\NormalTok{alto <-}\StringTok{ }\NormalTok{df }\OperatorTok{%>%}\StringTok{  }
\StringTok{    }\KeywordTok{filter}\NormalTok{(voice.part }\OperatorTok{==}\StringTok{ "Alto 1"}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{arrange}\NormalTok{(height) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{pull}\NormalTok{(height) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\NormalTok{print}
\CommentTok{#>  [1] 60 61 61 61 61 62 62 62 63 63 63 63 64 64 64 65 65 65 65 66 66 66 66}
\CommentTok{#> [24] 66 66 66 67 67 67 67 68 68 69 70 72}
\end{Highlighting}
\end{Shaded}

Next, we need to find the matching normal distribution quantiles. We first find the f-values for alto, then use qnorm to find the matching normal distribution values from those same f-values

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{i <-}\StringTok{ }\DecValTok{1}\OperatorTok{:}\KeywordTok{length}\NormalTok{(alto)}
\NormalTok{fi <-}\StringTok{ }\NormalTok{(i }\OperatorTok{-}\StringTok{ }\FloatTok{0.5}\NormalTok{) }\OperatorTok{/}\StringTok{ }\KeywordTok{length}\NormalTok{(alto)}
\NormalTok{fi}
\CommentTok{#>  [1] 0.0143 0.0429 0.0714 0.1000 0.1286 0.1571 0.1857 0.2143 0.2429 0.2714}
\CommentTok{#> [11] 0.3000 0.3286 0.3571 0.3857 0.4143 0.4429 0.4714 0.5000 0.5286 0.5571}
\CommentTok{#> [21] 0.5857 0.6143 0.6429 0.6714 0.7000 0.7286 0.7571 0.7857 0.8143 0.8429}
\CommentTok{#> [31] 0.8714 0.9000 0.9286 0.9571 0.9857}
\NormalTok{x.norm <-}\StringTok{ }\KeywordTok{qnorm}\NormalTok{(fi)}
\NormalTok{x.norm}
\CommentTok{#>  [1] -2.1893 -1.7185 -1.4652 -1.2816 -1.1332 -1.0063 -0.8938 -0.7916}
\CommentTok{#>  [9] -0.6971 -0.6085 -0.5244 -0.4439 -0.3661 -0.2905 -0.2165 -0.1437}
\CommentTok{#> [17] -0.0717  0.0000  0.0717  0.1437  0.2165  0.2905  0.3661  0.4439}
\CommentTok{#> [25]  0.5244  0.6085  0.6971  0.7916  0.8938  1.0063  1.1332  1.2816}
\CommentTok{#> [33]  1.4652  1.7185  2.1893}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(x.norm)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{misc_111c-comparing_distributions_with_qq_plot_files/figure-latex/unnamed-chunk-5-1} \end{center}

Now we can plot the sorted alto values against the normal values.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{( alto }\OperatorTok{~}\StringTok{ }\NormalTok{x.norm, }\DataTypeTok{type=}\StringTok{"p"}\NormalTok{, }\DataTypeTok{xlab=}\StringTok{"Normal quantiles"}\NormalTok{, }\DataTypeTok{pch=}\DecValTok{20}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{misc_111c-comparing_distributions_with_qq_plot_files/figure-latex/unnamed-chunk-6-1} \end{center}

When comparing a batch of numbers to a theoretical distribution on a q-q plot, we are looking for significant deviation from a straight line. To make it easier to judge straightness, we can fit a line to the points. Note that we are not creating a 45 (or x=y) slope; the range of values between both sets of numbers do not match. Here, we are only seeking the straightness of the points.

There are many ways one can fit a line to the data, Cleveland opts to fit a line to the first and third quartile of the q-q plot. The following chunk of code identifies the quantiles for both the alto dataset and the theoretical normal distribution. It then computes the slope and intercept from these coordinates.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Find 1st and 3rd quartile for the Alto 1 data}
\NormalTok{y <-}\StringTok{ }\KeywordTok{quantile}\NormalTok{(alto, }\KeywordTok{c}\NormalTok{(}\FloatTok{0.25}\NormalTok{, }\FloatTok{0.75}\NormalTok{), }\DataTypeTok{type=}\DecValTok{5}\NormalTok{)}
\NormalTok{y}
\CommentTok{#>  25%  75% }
\CommentTok{#> 63.0 66.8}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Find the 1st and 3rd quartile of the normal distribution}
\NormalTok{x <-}\StringTok{ }\KeywordTok{qnorm}\NormalTok{( }\KeywordTok{c}\NormalTok{(}\FloatTok{0.25}\NormalTok{, }\FloatTok{0.75}\NormalTok{))}
\NormalTok{x}
\CommentTok{#> [1] -0.674  0.674}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Now we can compute the intercept and slope of the line that passes}
\CommentTok{# through these points}
\NormalTok{slope <-}\StringTok{ }\KeywordTok{diff}\NormalTok{(y) }\OperatorTok{/}\StringTok{ }\KeywordTok{diff}\NormalTok{(x)}
\NormalTok{int   <-}\StringTok{ }\NormalTok{y[}\DecValTok{1}\NormalTok{] }\OperatorTok{-}\StringTok{ }\NormalTok{slope }\OperatorTok{*}\StringTok{ }\NormalTok{x[}\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

Next, we add the line to the plot.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{( alto }\OperatorTok{~}\StringTok{ }\NormalTok{x.norm, }\DataTypeTok{type=}\StringTok{"p"}\NormalTok{, }\DataTypeTok{xlab=}\StringTok{"Normal quantiles"}\NormalTok{, }\DataTypeTok{pch=}\DecValTok{20}\NormalTok{)}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{a=}\NormalTok{int, }\DataTypeTok{b=}\NormalTok{slope )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{misc_111c-comparing_distributions_with_qq_plot_files/figure-latex/unnamed-chunk-10-1} \end{center}

\hypertarget{using-rs-built-in-functions}{%
\section{Using R's built-in functions}\label{using-rs-built-in-functions}}

R has two built-in functions that facilitate the plot building task when comparing a batch to a normal distribution: \texttt{qqnorm} and \texttt{qqline.} Note that the function \texttt{qqline} allows the user to define the quantile method via the qtype= parameter. Here, we set it to 5 to match our choice of f-value calculation.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{qqnorm}\NormalTok{(alto)           }\CommentTok{# plot the points}
\KeywordTok{qqline}\NormalTok{(alto, }\DataTypeTok{qtype=}\DecValTok{5}\NormalTok{)  }\CommentTok{# plot the line}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{misc_111c-comparing_distributions_with_qq_plot_files/figure-latex/unnamed-chunk-11-1} \end{center}

That's it. Just two lines of code!

\hypertarget{using-the-ggplot2-plotting-environment}{%
\section{Using the ggplot2 plotting environment}\label{using-the-ggplot2-plotting-environment}}

We can take advantage of the \texttt{stat\_qq()} function to plot the points, but the equation for the line must be computed manually (as was done earlier). Those steps will be repeated here.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# normal distribution}
\KeywordTok{library}\NormalTok{(ggplot2)}

\CommentTok{# Find the slope and intercept of the line that passes through the 1st and 3rd}
\CommentTok{# quartile of the normal q-q plot}

\NormalTok{y     <-}\StringTok{ }\KeywordTok{quantile}\NormalTok{(alto, }\KeywordTok{c}\NormalTok{(}\FloatTok{0.25}\NormalTok{, }\FloatTok{0.75}\NormalTok{), }\DataTypeTok{type=}\DecValTok{5}\NormalTok{) }\CommentTok{# Find the 1st and 3rd quartiles}
\NormalTok{x     <-}\StringTok{ }\KeywordTok{qnorm}\NormalTok{( }\KeywordTok{c}\NormalTok{(}\FloatTok{0.25}\NormalTok{, }\FloatTok{0.75}\NormalTok{))                 }\CommentTok{# Find the matching normal values on the x-axis}
\NormalTok{slope <-}\StringTok{ }\KeywordTok{diff}\NormalTok{(y) }\OperatorTok{/}\StringTok{ }\KeywordTok{diff}\NormalTok{(x)                     }\CommentTok{# Compute the line slope}
\NormalTok{int   <-}\StringTok{ }\NormalTok{y[}\DecValTok{1}\NormalTok{] }\OperatorTok{-}\StringTok{ }\NormalTok{slope }\OperatorTok{*}\StringTok{ }\NormalTok{x[}\DecValTok{1}\NormalTok{]                   }\CommentTok{# Compute the line intercept}

\CommentTok{# Generate normal q-q plot}
\KeywordTok{ggplot}\NormalTok{() }\OperatorTok{+}\StringTok{ }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{sample=}\NormalTok{alto) }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{stat_qq}\NormalTok{(}\DataTypeTok{distribution=}\NormalTok{qnorm) }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{geom_abline}\NormalTok{(}\DataTypeTok{intercept=}\NormalTok{int, }\DataTypeTok{slope=}\NormalTok{slope) }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{ylab}\NormalTok{(}\StringTok{"Height"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{misc_111c-comparing_distributions_with_qq_plot_files/figure-latex/unnamed-chunk-12-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\NormalTok{qq_any <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(var, f) \{}
    \CommentTok{# Find the slope and intercept of the line that passes through the 1st and 3rd}
    \CommentTok{# quartile of the normal q-q plot}
    
\NormalTok{    y     <-}\StringTok{ }\KeywordTok{quantile}\NormalTok{(var, }\KeywordTok{c}\NormalTok{(}\FloatTok{0.25}\NormalTok{, }\FloatTok{0.75}\NormalTok{), }\DataTypeTok{type=}\DecValTok{5}\NormalTok{) }\CommentTok{# Find the 1st and 3rd quartiles}
\NormalTok{    x     <-}\StringTok{ }\KeywordTok{f}\NormalTok{( }\KeywordTok{c}\NormalTok{(}\FloatTok{0.25}\NormalTok{, }\FloatTok{0.75}\NormalTok{))                 }\CommentTok{# Find the matching normal values x-axis}
\NormalTok{    slope <-}\StringTok{ }\KeywordTok{diff}\NormalTok{(y) }\OperatorTok{/}\StringTok{ }\KeywordTok{diff}\NormalTok{(x)                     }\CommentTok{# Compute the line slope}
\NormalTok{    int   <-}\StringTok{ }\NormalTok{y[}\DecValTok{1}\NormalTok{] }\OperatorTok{-}\StringTok{ }\NormalTok{slope }\OperatorTok{*}\StringTok{ }\NormalTok{x[}\DecValTok{1}\NormalTok{]                   }\CommentTok{# Compute the line intercept}
    \KeywordTok{ggplot}\NormalTok{() }\OperatorTok{+}\StringTok{ }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{sample =}\NormalTok{ var) }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{stat_qq}\NormalTok{(}\DataTypeTok{distribution =}\NormalTok{ f) }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{geom_abline}\NormalTok{(}\DataTypeTok{intercept=}\NormalTok{int, }\DataTypeTok{slope=}\NormalTok{slope)}
\NormalTok{\}}

\CommentTok{# two function only, for the moment}
\KeywordTok{qq_any}\NormalTok{(alto, qexp)}
\KeywordTok{qq_any}\NormalTok{(alto, qnorm)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{misc_111c-comparing_distributions_with_qq_plot_files/figure-latex/unnamed-chunk-13-1} \includegraphics[width=0.7\linewidth]{misc_111c-comparing_distributions_with_qq_plot_files/figure-latex/unnamed-chunk-13-2} \end{center}

We can, of course, make use of ggplot's faceting function to generate trellised plots. For example, the following plot replicates Cleveland's figure 2.11 (except for the layout which we'll setup as a single row of plots instead). But first, we will need to compute the slopes for each singer group. We'll use dplyr's piping operations to create a new dataframe with singer group name, slope and intercept.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(dplyr)}

\NormalTok{intsl <-}\StringTok{ }\NormalTok{df }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{group_by}\NormalTok{(voice.part) }\OperatorTok{%>%}\StringTok{ }
\StringTok{       }\KeywordTok{summarize}\NormalTok{(}\DataTypeTok{q25    =} \KeywordTok{quantile}\NormalTok{(height,}\FloatTok{0.25}\NormalTok{, }\DataTypeTok{type=}\DecValTok{5}\NormalTok{),}
                 \DataTypeTok{q75    =} \KeywordTok{quantile}\NormalTok{(height,}\FloatTok{0.75}\NormalTok{, }\DataTypeTok{type=}\DecValTok{5}\NormalTok{),}
                 \DataTypeTok{norm25 =} \KeywordTok{qnorm}\NormalTok{( }\FloatTok{0.25}\NormalTok{),}
                 \DataTypeTok{norm75 =} \KeywordTok{qnorm}\NormalTok{( }\FloatTok{0.75}\NormalTok{),}
                 \DataTypeTok{slope  =}\NormalTok{ (q25 }\OperatorTok{-}\StringTok{ }\NormalTok{q75) }\OperatorTok{/}\StringTok{ }\NormalTok{(norm25 }\OperatorTok{-}\StringTok{ }\NormalTok{norm75),}
                 \DataTypeTok{int    =}\NormalTok{ q25 }\OperatorTok{-}\StringTok{ }\NormalTok{slope }\OperatorTok{*}\StringTok{ }\NormalTok{norm25) }\OperatorTok{%>%}
\StringTok{       }\KeywordTok{select}\NormalTok{(voice.part, slope, int) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\NormalTok{print}
\CommentTok{#> # A tibble: 8 x 3}
\CommentTok{#>   voice.part slope   int}
\CommentTok{#>   <fct>      <dbl> <dbl>}
\CommentTok{#> 1 Bass 2      2.97  72  }
\CommentTok{#> 2 Bass 1      2.22  70.5}
\CommentTok{#> 3 Tenor 2     1.48  70  }
\CommentTok{#> 4 Tenor 1     3.89  68.6}
\CommentTok{#> 5 Alto 2      2.22  65.5}
\CommentTok{#> 6 Alto 1      2.78  64.9}
\CommentTok{#> # ... with 2 more rows}
\end{Highlighting}
\end{Shaded}

It's important that the \texttt{voice.part} names match those in \texttt{df} letter-for-letter so that when ggplot is called, it will know which facet to assign the slope and intercept values to via \texttt{geom\_abline}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(df, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{sample =}\NormalTok{ height)) }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{stat_qq}\NormalTok{(}\DataTypeTok{distribution =}\NormalTok{ qnorm) }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{geom_abline}\NormalTok{(}\DataTypeTok{data=}\NormalTok{intsl, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{intercept=}\NormalTok{int, }\DataTypeTok{slope=}\NormalTok{slope), }\DataTypeTok{col=}\StringTok{"blue"}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{facet_wrap}\NormalTok{(}\OperatorTok{~}\NormalTok{voice.part, }\DataTypeTok{nrow=}\DecValTok{1}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{ylab}\NormalTok{(}\StringTok{"Height"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{misc_111c-comparing_distributions_with_qq_plot_files/figure-latex/unnamed-chunk-15-1} \end{center}

\hypertarget{qq-and-pp-plots}{%
\chapter{QQ and PP Plots}\label{qq-and-pp-plots}}

\url{https://homepage.divms.uiowa.edu/~luke/classes/STAT4580/qqpp.html}

\hypertarget{qq-plot}{%
\section{QQ Plot}\label{qq-plot}}

One way to assess how well a particular theoretical model describes a data distribution is to plot data quantiles against theoretical quantiles.

Base graphics provides \texttt{qqnorm}, lattice has \texttt{qqmath}, and ggplot2 has \texttt{geom\_qq}.

The default theoretical distribution used in these is a standard normal, but, except for \texttt{qqnorm}, these allow you to specify an alternative.

For a large sample from the theoretical distribution the plot should be a straight line through the origin with slope 1:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(ggplot2)}
\CommentTok{#> Registered S3 methods overwritten by 'ggplot2':}
\CommentTok{#>   method         from }
\CommentTok{#>   [.quosures     rlang}
\CommentTok{#>   c.quosures     rlang}
\CommentTok{#>   print.quosures rlang}

\NormalTok{n <-}\StringTok{ }\DecValTok{10000}
\KeywordTok{ggplot}\NormalTok{() }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_qq}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{sample =} \KeywordTok{rnorm}\NormalTok{(n)))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{misc_111d-qq_plots_pp_plots_files/figure-latex/unnamed-chunk-2-1} \end{center}

If the plot is a straight line with a different slope or intercept, then the data distribution corresponds to a location-scale transformation of the theoretical distribution.

The slope is the scale and the intercept is the location:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{() }\OperatorTok{+}
\StringTok{    }\KeywordTok{geom_qq}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{sample =} \KeywordTok{rnorm}\NormalTok{(n, }\DecValTok{10}\NormalTok{, }\DecValTok{4}\NormalTok{))) }\OperatorTok{+}
\StringTok{    }\KeywordTok{geom_abline}\NormalTok{(}\DataTypeTok{intercept =} \DecValTok{10}\NormalTok{, }\DataTypeTok{slope =} \DecValTok{4}\NormalTok{,}
                \DataTypeTok{color =} \StringTok{"red"}\NormalTok{, }\DataTypeTok{size =} \FloatTok{1.5}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{0.8}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{misc_111d-qq_plots_pp_plots_files/figure-latex/unnamed-chunk-3-1} \end{center}

The QQ plot can be constructed directly as a scatterplot of the sorted sample \(i = 1, \dots, n\) against quantiles for

\[p_i = \frac{i}{n} - \frac{1}{2n}\]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p <-}\StringTok{ }\NormalTok{(}\DecValTok{1} \OperatorTok{:}\StringTok{ }\NormalTok{n) }\OperatorTok{/}\StringTok{ }\NormalTok{n }\OperatorTok{-}\StringTok{ }\FloatTok{0.5} \OperatorTok{/}\StringTok{ }\NormalTok{n}
\NormalTok{y <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(n, }\DecValTok{10}\NormalTok{, }\DecValTok{4}\NormalTok{)}
\KeywordTok{ggplot}\NormalTok{() }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{qnorm}\NormalTok{(p), }\DataTypeTok{y =} \KeywordTok{sort}\NormalTok{(y)))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{misc_111d-qq_plots_pp_plots_files/figure-latex/unnamed-chunk-4-1} \end{center}

\hypertarget{some-examples}{%
\section{Some Examples}\label{some-examples}}

The histograms and density estimates for the duration variable in the \texttt{geyser} data set showed that the distribution is far from a normal distribution, and the normal QQ plot shows this as well:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(MASS)}
\KeywordTok{ggplot}\NormalTok{(geyser) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_qq}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{sample =}\NormalTok{ duration))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{misc_111d-qq_plots_pp_plots_files/figure-latex/unnamed-chunk-5-1} \end{center}

Except for rounding the parent heights in the Galton data seemed not too fat from normally distributed:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(psych)}

\KeywordTok{ggplot}\NormalTok{(galton) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_qq}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{sample =}\NormalTok{ parent))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{misc_111d-qq_plots_pp_plots_files/figure-latex/psych_galton-1} \end{center}

Rounding interferes more with this visualization than with a histogram or a density plot.

Rounding is more visible with this visualization than with a histogram or a density plot.

Another Gatlton dataset available in the UsingR package with less rounding is father.son:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(UsingR)}
\KeywordTok{ggplot}\NormalTok{(father.son) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_qq}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{sample =}\NormalTok{ fheight))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{misc_111d-qq_plots_pp_plots_files/figure-latex/usingr-father_son-1} \end{center}

The middle seems to be fairly straight, but the ends are somewhat wiggly.

How can you calibrate your judgment?

\hypertarget{calibrating-the-variability}{%
\section{Calibrating the Variability}\label{calibrating-the-variability}}

One approach is to use simulation, sometimes called a graphical bootstrap.

The \texttt{nboot} function will simulate R samples from a normal distribution that match a variable x on sample size, sample mean, and sample SD.

The result is returned in a dataframe suitable for plotting:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nsim <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(n, }\DataTypeTok{m =} \DecValTok{0}\NormalTok{, }\DataTypeTok{s =} \DecValTok{1}\NormalTok{) \{}
\NormalTok{    z <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(n)}
\NormalTok{    m }\OperatorTok{+}\StringTok{ }\NormalTok{s }\OperatorTok{*}\StringTok{ }\NormalTok{((z }\OperatorTok{-}\StringTok{ }\KeywordTok{mean}\NormalTok{(z)) }\OperatorTok{/}\StringTok{ }\KeywordTok{sd}\NormalTok{(z))}
\NormalTok{\}}

\NormalTok{nboot <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x, R) \{}
\NormalTok{    n <-}\StringTok{ }\KeywordTok{length}\NormalTok{(x)}
\NormalTok{    m <-}\StringTok{ }\KeywordTok{mean}\NormalTok{(x)}
\NormalTok{    s <-}\StringTok{ }\KeywordTok{sd}\NormalTok{(x)}
    \KeywordTok{do.call}\NormalTok{(rbind,}
            \KeywordTok{lapply}\NormalTok{(}\DecValTok{1} \OperatorTok{:}\StringTok{ }\NormalTok{R,}
                   \ControlFlowTok{function}\NormalTok{(i) \{}
\NormalTok{                       xx <-}\StringTok{ }\KeywordTok{sort}\NormalTok{(}\KeywordTok{nsim}\NormalTok{(n, m, s))}
\NormalTok{                       p <-}\StringTok{ }\KeywordTok{seq_along}\NormalTok{(x) }\OperatorTok{/}\StringTok{ }\NormalTok{n }\OperatorTok{-}\StringTok{ }\FloatTok{0.5} \OperatorTok{/}\StringTok{ }\NormalTok{n}
                       \KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ xx, }\DataTypeTok{p =}\NormalTok{ p, }\DataTypeTok{sim =}\NormalTok{ i)}
\NormalTok{    \}))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Plotting these as lines shows the variability in shapes we can expect when sampling from the theoretical normal distribution:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gb <-}\StringTok{ }\KeywordTok{nboot}\NormalTok{(father.son}\OperatorTok{$}\NormalTok{fheight, }\DecValTok{50}\NormalTok{)}
\NormalTok{tibble}\OperatorTok{::}\KeywordTok{as_tibble}\NormalTok{(gb)}
\CommentTok{#> # A tibble: 53,900 x 3}
\CommentTok{#>       x        p   sim}
\CommentTok{#>   <dbl>    <dbl> <int>}
\CommentTok{#> 1  59.8 0.000464     1}
\CommentTok{#> 2  59.9 0.00139      1}
\CommentTok{#> 3  59.9 0.00232      1}
\CommentTok{#> 4  60.8 0.00325      1}
\CommentTok{#> 5  60.8 0.00417      1}
\CommentTok{#> 6  60.9 0.00510      1}
\CommentTok{#> # ... with 5.389e+04 more rows}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{() }\OperatorTok{+}
\StringTok{    }\KeywordTok{geom_line}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{qnorm}\NormalTok{(p), }\DataTypeTok{y =}\NormalTok{ x, }\DataTypeTok{group =}\NormalTok{ sim),}
              \DataTypeTok{color =} \StringTok{"gray"}\NormalTok{, }\DataTypeTok{data =}\NormalTok{ gb)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{misc_111d-qq_plots_pp_plots_files/figure-latex/unnamed-chunk-7-1} \end{center}

We can then insert this simulation behind our data to help calibrate the visualization:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(father.son) }\OperatorTok{+}
\StringTok{    }\KeywordTok{geom_line}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{qnorm}\NormalTok{(p), }\DataTypeTok{y =}\NormalTok{ x, }\DataTypeTok{group =}\NormalTok{ sim),}
              \DataTypeTok{color =} \StringTok{"gray"}\NormalTok{, }\DataTypeTok{data =}\NormalTok{ gb) }\OperatorTok{+}
\StringTok{    }\KeywordTok{geom_qq}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{sample =}\NormalTok{ fheight))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{misc_111d-qq_plots_pp_plots_files/figure-latex/unnamed-chunk-8-1} \end{center}

\hypertarget{scalability}{%
\section{Scalability}\label{scalability}}

For large sample sizes overplotting will occur:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(diamonds) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_qq}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{sample =}\NormalTok{ price))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{misc_111d-qq_plots_pp_plots_files/figure-latex/unnamed-chunk-9-1} \end{center}

This can be alleviated by using a grid of quantiles:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nq <-}\StringTok{ }\DecValTok{100}
\NormalTok{p <-}\StringTok{ }\NormalTok{(}\DecValTok{1} \OperatorTok{:}\StringTok{ }\NormalTok{nq) }\OperatorTok{/}\StringTok{ }\NormalTok{nq }\OperatorTok{-}\StringTok{ }\FloatTok{0.5} \OperatorTok{/}\StringTok{ }\NormalTok{nq}
\KeywordTok{ggplot}\NormalTok{() }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{qnorm}\NormalTok{(p), }\DataTypeTok{y =} \KeywordTok{quantile}\NormalTok{(diamonds}\OperatorTok{$}\NormalTok{price, p)))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{misc_111d-qq_plots_pp_plots_files/figure-latex/unnamed-chunk-10-1} \end{center}

A more reasonable model might be an exponential distribution:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{() }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{qexp}\NormalTok{(p), }\DataTypeTok{y =} \KeywordTok{quantile}\NormalTok{(diamonds}\OperatorTok{$}\NormalTok{price, p)))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{misc_111d-qq_plots_pp_plots_files/figure-latex/unnamed-chunk-11-1} \end{center}

\hypertarget{comparing-two-distributions}{%
\section{Comparing Two Distributions}\label{comparing-two-distributions}}

The QQ plot can also be used to compare two distributions based on a sample from each.

If the samples are the same size then this is just a plot of the ordered sample values against each other.

Choosing a fixed set of quantiles allows samples of unequal size to be compared.

Using a small set of quantiles we can compare the distributions of waiting times between eruptions of Old Faithful from the two different data sets we have looked at:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nq <-}\StringTok{ }\DecValTok{31}  \CommentTok{# user defined}
\NormalTok{nq <-}\StringTok{ }\KeywordTok{min}\NormalTok{(}\KeywordTok{length}\NormalTok{(geyser}\OperatorTok{$}\NormalTok{waiting), }\KeywordTok{length}\NormalTok{(faithful}\OperatorTok{$}\NormalTok{waiting)) }\CommentTok{# or take the minimum}
\NormalTok{p <-}\StringTok{ }\NormalTok{(}\DecValTok{1} \OperatorTok{:}\StringTok{ }\NormalTok{nq) }\OperatorTok{/}\StringTok{ }\NormalTok{nq }\OperatorTok{-}\StringTok{ }\FloatTok{0.5} \OperatorTok{/}\StringTok{ }\NormalTok{nq}

\NormalTok{wg <-}\StringTok{ }\NormalTok{geyser}\OperatorTok{$}\NormalTok{waiting}
\NormalTok{wf <-}\StringTok{ }\NormalTok{faithful}\OperatorTok{$}\NormalTok{waiting}

\KeywordTok{ggplot}\NormalTok{() }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{quantile}\NormalTok{(wg, p), }\DataTypeTok{y =} \KeywordTok{quantile}\NormalTok{(wf, p)))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{misc_111d-qq_plots_pp_plots_files/figure-latex/compare_distributions-1} \end{center}

\hypertarget{pp-plots}{%
\section{PP Plots}\label{pp-plots}}

The PP plot for comparing a sample to a theoretical model plots the theoretical proportion less than or equal to each observed value against the actual proportion.

For a theoretical cumulative distribution function F this means plotting

\[F(x(i))pi\]

For the \texttt{fheight} variable in the \texttt{father.son} data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m <-}\StringTok{ }\KeywordTok{mean}\NormalTok{(father.son}\OperatorTok{$}\NormalTok{fheight)}
\NormalTok{s <-}\StringTok{ }\KeywordTok{sd}\NormalTok{(father.son}\OperatorTok{$}\NormalTok{fheight)}
\NormalTok{n <-}\StringTok{ }\KeywordTok{nrow}\NormalTok{(father.son)}
\NormalTok{p <-}\StringTok{ }\NormalTok{(}\DecValTok{1} \OperatorTok{:}\StringTok{ }\NormalTok{n) }\OperatorTok{/}\StringTok{ }\NormalTok{n }\OperatorTok{-}\StringTok{ }\FloatTok{0.5} \OperatorTok{/}\StringTok{ }\NormalTok{n}
\KeywordTok{ggplot}\NormalTok{(father.son) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ p, }\DataTypeTok{y =} \KeywordTok{sort}\NormalTok{(}\KeywordTok{pnorm}\NormalTok{(fheight, m, s))))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{misc_111d-qq_plots_pp_plots_files/figure-latex/unnamed-chunk-12-1} \end{center}

\begin{itemize}
\item
  The values on the vertical axis are the probability integral transform of the data for the theoretical distribution.
\item
  If the data are a sample from the theoretical distribution then these transforms would be uniformly distributed on {[}0,1{]}.
\item
  The PP plot is a QQ plot of these transformed values against a uniform distribution.
\item
  The PP plot goes through the points (0,0) and (1,1) and so is much less variable in the tails:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pp <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{() }\OperatorTok{+}
\KeywordTok{geom_line}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ p, }\DataTypeTok{y =} \KeywordTok{pnorm}\NormalTok{(x, m, s), }\DataTypeTok{group =}\NormalTok{ sim),}
          \DataTypeTok{color =} \StringTok{"gray"}\NormalTok{, }\DataTypeTok{data =}\NormalTok{ gb)}
\NormalTok{pp}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{misc_111d-qq_plots_pp_plots_files/figure-latex/unnamed-chunk-13-1} \end{center}

Adding the data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pp }\OperatorTok{+}\StringTok{ }
\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ p, }\DataTypeTok{y =} \KeywordTok{sort}\NormalTok{(}\KeywordTok{pnorm}\NormalTok{(fheight, m, s))), }\DataTypeTok{data =}\NormalTok{ (father.son))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{misc_111d-qq_plots_pp_plots_files/figure-latex/unnamed-chunk-14-1} \end{center}

The PP plot is also less sensitive to deviations in the tails.

A compromise between the QQ and PP plots uses the arcsine square root variance-stabilizing transformation, which makes the variability approximately constant across the range of the plot:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{vpp <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{() }\OperatorTok{+}
\KeywordTok{geom_line}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{asin}\NormalTok{(}\KeywordTok{sqrt}\NormalTok{(p)), }\DataTypeTok{y =} \KeywordTok{asin}\NormalTok{(}\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{pnorm}\NormalTok{(x, m, s))), }\DataTypeTok{group =}\NormalTok{ sim),}
          \DataTypeTok{color =} \StringTok{"gray"}\NormalTok{, }\DataTypeTok{data =}\NormalTok{ gb)}
\NormalTok{vpp}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{misc_111d-qq_plots_pp_plots_files/figure-latex/unnamed-chunk-15-1} \end{center}

Adding the data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{vpp }\OperatorTok{+}
\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{asin}\NormalTok{(}\KeywordTok{sqrt}\NormalTok{(p)), }\DataTypeTok{y =} \KeywordTok{sort}\NormalTok{(}\KeywordTok{asin}\NormalTok{(}\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{pnorm}\NormalTok{(fheight, m, s))))),}
           \DataTypeTok{data =}\NormalTok{ (father.son))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{misc_111d-qq_plots_pp_plots_files/figure-latex/unnamed-chunk-16-1} \end{center}

\hypertarget{plots-for-assessing-model-fit}{%
\section{Plots For Assessing Model Fit}\label{plots-for-assessing-model-fit}}

\begin{itemize}
\item
  Both QQ and PP plots can be used to asses how well a theoretical family of models fits your data, or your residuals.
\item
  To use a PP plot you have to estimate the parameters first.
\item
  For a location-scale family, like the normal distribution family, you can use a QQ plot with a standard member of the family.
\item
  Some other families can use other transformations that lead to straight lines for family members:
\end{itemize}

The Weibull family is widely used in reliability modeling; its CDF is
\[F(t) = 1 - \exp\left\{-\left(\frac{t}{b}\right)^a\right\}\]

\begin{itemize}
\item
  The logarithms of Weibull random variables form a location-scale family.
\item
  Special paper used to be available for Weibull probability plots.
\end{itemize}

A Weibull QQ plot for price in the diamonds data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n <-}\StringTok{ }\KeywordTok{nrow}\NormalTok{(diamonds)}
\NormalTok{p <-}\StringTok{ }\NormalTok{(}\DecValTok{1} \OperatorTok{:}\StringTok{ }\NormalTok{n) }\OperatorTok{/}\StringTok{ }\NormalTok{n }\OperatorTok{-}\StringTok{ }\FloatTok{0.5} \OperatorTok{/}\StringTok{ }\NormalTok{n}
\KeywordTok{ggplot}\NormalTok{(diamonds) }\OperatorTok{+}
\StringTok{    }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{log10}\NormalTok{(}\KeywordTok{qweibull}\NormalTok{(p, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)), }\DataTypeTok{y =} \KeywordTok{log10}\NormalTok{(}\KeywordTok{sort}\NormalTok{(price))))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{misc_111d-qq_plots_pp_plots_files/figure-latex/unnamed-chunk-17-1} \end{center}

\begin{itemize}
\item
  The lower tail does not match a Weibull distribution.
\item
  Is this important?
\item
  In engineering applications it often is.
\item
  In selecting a reasonable model to capture the shape of this distribution it may not be.
\item
  QQ plots are helpful for understanding departures from a theoretical model.
\item
  No data will fit a theoretical model perfectly.
\item
  Case-specific judgment is needed to decide whether departures are important.
\item
  George Box: All models are wrong but some are useful.
\end{itemize}

\hypertarget{data-visualization-working-with-models}{%
\chapter{Data Visualization: Working with models}\label{data-visualization-working-with-models}}

\hypertarget{introduction-16}{%
\section{Introduction}\label{introduction-16}}

Source: \url{https://socviz.co/modeling.html}

Data visualization is about more than generating figures that display the raw numbers from a table of data. Right from the beginning, it involves summarizing or transforming parts of the data, and then plotting the results. Statistical models are a central part of that process. In this Chapter, we will begin by looking briefly at how \texttt{ggplot} can use various modeling techniques directly within geoms. Then we will see how to use the \texttt{broom} and \texttt{margins} libraries to tidily extract and plot estimates from models that we fit ourselves.\\

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load libraries}
\KeywordTok{library}\NormalTok{(ggplot2)}
\CommentTok{#> Registered S3 methods overwritten by 'ggplot2':}
\CommentTok{#>   method         from }
\CommentTok{#>   [.quosures     rlang}
\CommentTok{#>   c.quosures     rlang}
\CommentTok{#>   print.quosures rlang}
\KeywordTok{library}\NormalTok{(dplyr)}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'dplyr'}
\CommentTok{#> The following objects are masked from 'package:stats':}
\CommentTok{#> }
\CommentTok{#>     filter, lag}
\CommentTok{#> The following objects are masked from 'package:base':}
\CommentTok{#> }
\CommentTok{#>     intersect, setdiff, setequal, union}
\KeywordTok{library}\NormalTok{(tidyr)}
\KeywordTok{library}\NormalTok{(purrr)}
\KeywordTok{library}\NormalTok{(socviz)       }\CommentTok{# devtools::install_github("kjhealy/socviz")}
\KeywordTok{library}\NormalTok{(gapminder)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot two lines}
\NormalTok{p <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ gapminder,}
            \DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{log}\NormalTok{(gdpPercap), }\DataTypeTok{y =}\NormalTok{ lifeExp))}

\NormalTok{p }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{alpha=}\FloatTok{0.1}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{color =} \StringTok{"tomato"}\NormalTok{, }\DataTypeTok{fill=}\StringTok{"tomato"}\NormalTok{, }\DataTypeTok{method =}\NormalTok{ MASS}\OperatorTok{::}\NormalTok{rlm) }\OperatorTok{+}
\StringTok{    }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{color =} \StringTok{"steelblue"}\NormalTok{, }\DataTypeTok{fill=}\StringTok{"steelblue"}\NormalTok{, }\DataTypeTok{method =} \StringTok{"lm"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{misc_140-data_visualization-modeling_files/figure-latex/unnamed-chunk-4-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot spline}
\NormalTok{p }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{alpha=}\FloatTok{0.1}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{color =} \StringTok{"tomato"}\NormalTok{, }\DataTypeTok{method =} \StringTok{"lm"}\NormalTok{, }\DataTypeTok{size =} \FloatTok{1.2}\NormalTok{, }
                \DataTypeTok{formula =}\NormalTok{ y }\OperatorTok{~}\StringTok{ }\NormalTok{splines}\OperatorTok{::}\KeywordTok{bs}\NormalTok{(x, }\DecValTok{3}\NormalTok{), }\DataTypeTok{se =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{misc_140-data_visualization-modeling_files/figure-latex/unnamed-chunk-5-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{alpha=}\FloatTok{0.1}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{geom_quantile}\NormalTok{(}\DataTypeTok{color =} \StringTok{"tomato"}\NormalTok{, }\DataTypeTok{size =} \FloatTok{1.2}\NormalTok{, }\DataTypeTok{method =} \StringTok{"rqss"}\NormalTok{,}
                  \DataTypeTok{lambda =} \DecValTok{1}\NormalTok{, }\DataTypeTok{quantiles =} \KeywordTok{c}\NormalTok{(}\FloatTok{0.20}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\FloatTok{0.85}\NormalTok{))}
\CommentTok{#> Loading required package: SparseM}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'SparseM'}
\CommentTok{#> The following object is masked from 'package:base':}
\CommentTok{#> }
\CommentTok{#>     backsolve}
\CommentTok{#> Smoothing formula not specified. Using: y ~ qss(x, lambda = 1)}
\CommentTok{#> Warning in rq.fit.sfn(x, y, tau = tau, rhs = rhs, control = control, ...): tiny diagonals replaced with Inf when calling blkfct}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{misc_140-data_visualization-modeling_files/figure-latex/unnamed-chunk-6-1} \end{center}

Histograms, density plots, boxplots, and other geoms compute either single numbers or new variables before plotting them. As we saw in Section 4.4, these calculations are done by \texttt{stat\_} functions, each of which works hand-in-hand with its default \texttt{geom\_} function, and vice versa. Moreover, from the smoothing lines we drew from almost the very first plots we made, we have seen that \texttt{stat\_} functions can do a fair amount of calculation and even model estimation on the fly. The \texttt{geom\_smooth()} function can take a range of method arguments to fit LOESS, OLS, and robust regression lines, amongst others.

Both the \texttt{geom\_smooth()} and \texttt{geom\_quantile()} functions can also be instructed to use different formulas to produce their fits. In the top panel of Figure 6.1, we access the \texttt{MASS} library's \texttt{rlm} function to fit a robust regression line. In the second panel, the \texttt{bs} function is invoked directly from the \texttt{splines} library in the same way, to fit a polynominal curve to the data. This is the same approach to directly accessing functions without loading a whole library that we have already used several times when using functions from the \texttt{scales} library. The \texttt{geom\_quantile()} function, meanwhile, is like a specialized version of \texttt{geom\_smooth()} that can fit quantile regression lines using a variety of methods. The \texttt{quantiles} argument takes a vector specifying the quantiles at which to fit the lines.

\hypertarget{show-several-fits-at-once-with-a-legend}{%
\section{Show several fits at once, with a legend}\label{show-several-fits-at-once-with-a-legend}}

As we just saw in the first panel of Figure 6.1, where we plotted both an OLS and a robust regression line, we can look at several fits at once on the same plot by layering on new smoothers with \texttt{geom\_smooth()}. As long as we set the color and fill aesthetics to different values for each fit, we can easily distinguish them visually. However, \texttt{ggplot} will not draw a legend that guides us about which fit is which. This is because the smoothers are not logically connected to one another. They exist as separate layers. What if we are comparing several different fits and want a legend describing them?

As it turns out, \texttt{geom\_smooth()} can do this via the slightly unusual route of mapping the color and fill aesthetics to a string describing the model we are fitting, and then using \texttt{scale\_color\_manual()} and \texttt{scale\_fill\_manual()} to create the legend. First we use \texttt{brewer.pal()} from the \texttt{RColorBrewer} library to extract three qualitatively different colors from a larger palette. The colors are represented as hex values. As before use the \texttt{::} convention to use the function without loading the whole library:\\

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model_colors <-}\StringTok{ }\NormalTok{RColorBrewer}\OperatorTok{::}\KeywordTok{brewer.pal}\NormalTok{(}\DecValTok{3}\NormalTok{, }\StringTok{"Set1"}\NormalTok{)}
\NormalTok{model_colors}
\CommentTok{#> [1] "#E41A1C" "#377EB8" "#4DAF4A"}
\end{Highlighting}
\end{Shaded}

Then we create a plot with three different smoothers, mapping the color and fill within the \texttt{aes()} function as the name of the smoother:\\

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p0 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ gapminder,}
            \DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{log}\NormalTok{(gdpPercap), }\DataTypeTok{y =}\NormalTok{ lifeExp))}

\NormalTok{p1 <-}\StringTok{ }\NormalTok{p0 }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{alpha =} \FloatTok{0.2}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{method =} \StringTok{"lm"}\NormalTok{, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{color =} \StringTok{"OLS"}\NormalTok{, }\DataTypeTok{fill =} \StringTok{"OLS"}\NormalTok{)) }\OperatorTok{+}
\StringTok{    }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{method =} \StringTok{"lm"}\NormalTok{, }\DataTypeTok{formula =}\NormalTok{ y }\OperatorTok{~}\StringTok{ }\NormalTok{splines}\OperatorTok{::}\KeywordTok{bs}\NormalTok{(x, }\DataTypeTok{df =} \DecValTok{3}\NormalTok{),}
                \KeywordTok{aes}\NormalTok{(}\DataTypeTok{color =} \StringTok{"Cubic Spline"}\NormalTok{, }\DataTypeTok{fill =} \StringTok{"Cubic Spline"}\NormalTok{)) }\OperatorTok{+}
\StringTok{    }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{method =} \StringTok{"loess"}\NormalTok{,}
                \KeywordTok{aes}\NormalTok{(}\DataTypeTok{color =} \StringTok{"LOESS"}\NormalTok{, }\DataTypeTok{fill =} \StringTok{"LOESS"}\NormalTok{))}


\NormalTok{p1 }\OperatorTok{+}\StringTok{ }\KeywordTok{scale_color_manual}\NormalTok{(}\DataTypeTok{name =} \StringTok{"Models"}\NormalTok{, }\DataTypeTok{values =}\NormalTok{ model_colors) }\OperatorTok{+}
\StringTok{    }\KeywordTok{scale_fill_manual}\NormalTok{(}\DataTypeTok{name =} \StringTok{"Models"}\NormalTok{, }\DataTypeTok{values =}\NormalTok{ model_colors) }\OperatorTok{+}
\StringTok{    }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{legend.position =} \StringTok{"top"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{misc_140-data_visualization-modeling_files/figure-latex/unnamed-chunk-8-1} \end{center}

In a way we have cheated a little here to make the plot work. Until now, we have always mapped aesthetics to the names of variables, not to strings like ``OLS'' or ``Cubic Splines''. In Chapter 3, when we discussed mapping versus setting aesthetics, we saw what happened when we tried to change the color of the points in a scatterplot by setting them to ``purple'' inside the \texttt{aes()}function. The result was that the points turned red instead, as \texttt{ggplot} in effect created a new variable and labeled it with the word ``purple''. We learned there that the \texttt{aes()} function was for mapping variables to aesthetics.

Here we take advantage of that behavior, creating a new single-value variable for the name of each of our models. Ggplot will properly construct the relevant guide if we call \texttt{scale\_color\_manual()} and \texttt{scale\_fill\_manual()}. Remember that we have to call two scale functions because we have two mappings. The result is a single plot containing not just our three smoothers, but also an appropriate legend to guide the reader.

These model-fitting features make \texttt{ggplot} very useful for exploratory work, and make it straightforward to generate and compare model-based trends and other summaries as part of the process of descriptive data visualization. The various \texttt{stat\_} functions are a flexible way to add summary estimates of various kinds to plots. But we will also want more than this, including presenting results from models we fit ourselves.

\hypertarget{look-inside-model-objects}{%
\section{Look inside model objects}\label{look-inside-model-objects}}

Covering the details of fitting statistical models in R is beyond the scope of this book. For a comprehensive, modern introduction to that topic you should work your way through (Gelman \& Hill, 2018). (Harrell, 2016) is also very good on the many practical connections between modeling and graphing data. Similarly, (Gelman, 2004) provides a detailed discussion of the use of graphics as a tool in model-checking and validation. Here we will discuss some ways to take the models that you fit and extract information that is easy to work with in \texttt{ggplot}. Our goal, as always, is to get from however the object is stored to a tidy table of numbers that we can plot. Most classes of statistical model in R will contain the information we need, or will have a special set of functions, or methods, designed to extract it.

We can start by learning a little more about how the output of models is stored in R. Remember, we are always working with objects, and objects have an internal structure consisting of named pieces. Sometimes these are single numbers, sometimes vectors, and sometimes lists of things like vectors, matrices, or formulas.

We have been working extensively with tibbles and data frames. These store tables of data with named columns, perhaps consisting of different classes of variable, such as integers, characters, dates, or factors. Model objects are a little more complicated again.\\

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gapminder}
\CommentTok{#> # A tibble: 1,704 x 6}
\CommentTok{#>   country     continent  year lifeExp      pop gdpPercap}
\CommentTok{#>   <fct>       <fct>     <int>   <dbl>    <int>     <dbl>}
\CommentTok{#> 1 Afghanistan Asia       1952    28.8  8425333      779.}
\CommentTok{#> 2 Afghanistan Asia       1957    30.3  9240934      821.}
\CommentTok{#> 3 Afghanistan Asia       1962    32.0 10267083      853.}
\CommentTok{#> 4 Afghanistan Asia       1967    34.0 11537966      836.}
\CommentTok{#> 5 Afghanistan Asia       1972    36.1 13079460      740.}
\CommentTok{#> 6 Afghanistan Asia       1977    38.4 14880372      786.}
\CommentTok{#> # ... with 1,698 more rows}
\end{Highlighting}
\end{Shaded}

Remember, we can use the \texttt{str()} function to learn more about the internal structure of any object. For example, we can get some information on what class (or classes) of object \texttt{gapminder} is, how large it is, and what components it has. The output from \texttt{str(gapminder)} is somewhat dense:\\

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str}\NormalTok{(gapminder)}
\CommentTok{#> Classes 'tbl_df', 'tbl' and 'data.frame':    1704 obs. of  6 variables:}
\CommentTok{#>  $ country  : Factor w/ 142 levels "Afghanistan",..: 1 1 1 1 1 1 1 1 1 1 ...}
\CommentTok{#>  $ continent: Factor w/ 5 levels "Africa","Americas",..: 3 3 3 3 3 3 3 3 3 3 ...}
\CommentTok{#>  $ year     : int  1952 1957 1962 1967 1972 1977 1982 1987 1992 1997 ...}
\CommentTok{#>  $ lifeExp  : num  28.8 30.3 32 34 36.1 ...}
\CommentTok{#>  $ pop      : int  8425333 9240934 10267083 11537966 13079460 14880372 12881816 13867957 16317921 22227415 ...}
\CommentTok{#>  $ gdpPercap: num  779 821 853 836 740 ...}
\end{Highlighting}
\end{Shaded}

There is a lot of information here about the object as a whole and each variable in it. In the same way, statistical models in R have an internal structure. But because models are more complex entities than data tables, their structure is correspondingly more complicated. There are more pieces of information, and more kinds of information, that we might want to use. All of this information is generally stored in or is computable from parts of a model object.

We can create a linear model, an ordinary OLS regression, using the \texttt{gapminder} data. This dataset has a country-year structure that makes an OLS specification like this the wrong one to use. But never mind that for now. We use the \texttt{lm()} function to run the model, and store it in an object called \texttt{out}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{out <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(}\DataTypeTok{formula =}\NormalTok{ lifeExp }\OperatorTok{~}\StringTok{ }\NormalTok{gdpPercap }\OperatorTok{+}\StringTok{ }\NormalTok{pop }\OperatorTok{+}\StringTok{ }\NormalTok{continent,}
          \DataTypeTok{data =}\NormalTok{ gapminder)}
\end{Highlighting}
\end{Shaded}

The first argument is the formula for the \texttt{model.} \texttt{lifeExp} is the dependent variable and the tilde \texttt{\textasciitilde{}} operator is used to designate the left- and right-hand sides of a model (including in cases, as we saw with \texttt{facet\_wrap()} where the model just has a right-hand side.)

Let's look at the results by asking R to print a summary of the model.\\

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(out)}
\CommentTok{#> }
\CommentTok{#> Call:}
\CommentTok{#> lm(formula = lifeExp ~ gdpPercap + pop + continent, data = gapminder)}
\CommentTok{#> }
\CommentTok{#> Residuals:}
\CommentTok{#>    Min     1Q Median     3Q    Max }
\CommentTok{#> -49.16  -4.49   0.30   5.11  25.17 }
\CommentTok{#> }
\CommentTok{#> Coefficients:}
\CommentTok{#>                   Estimate Std. Error t value Pr(>|t|)    }
\CommentTok{#> (Intercept)       4.78e+01   3.40e-01  140.82   <2e-16 ***}
\CommentTok{#> gdpPercap         4.50e-04   2.35e-05   19.16   <2e-16 ***}
\CommentTok{#> pop               6.57e-09   1.98e-09    3.33    9e-04 ***}
\CommentTok{#> continentAmericas 1.35e+01   6.00e-01   22.46   <2e-16 ***}
\CommentTok{#> continentAsia     8.19e+00   5.71e-01   14.34   <2e-16 ***}
\CommentTok{#> continentEurope   1.75e+01   6.25e-01   27.97   <2e-16 ***}
\CommentTok{#> continentOceania  1.81e+01   1.78e+00   10.15   <2e-16 ***}
\CommentTok{#> ---}
\CommentTok{#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1}
\CommentTok{#> }
\CommentTok{#> Residual standard error: 8.37 on 1697 degrees of freedom}
\CommentTok{#> Multiple R-squared:  0.582,  Adjusted R-squared:  0.581 }
\CommentTok{#> F-statistic:  394 on 6 and 1697 DF,  p-value: <2e-16}
\end{Highlighting}
\end{Shaded}

When we use the \texttt{summary()} function on out, we are not getting a simple feed of what's in the model object. Instead, like any function, \texttt{summary()} takes its input, performs some actions, and produces output. In this case, what is printed to the console is partly information that is stored inside the model object, and partly information that the \texttt{summary()} function has calculated and formated for display on the screen. Behind the scenes, \texttt{summary()} gets help from other functions. Objects of different classes have default methods associated with them, so that when the generic \texttt{summary()} function is applied to a linear model object, the function knows to pass the work on to a more specialized function that does a bunch of calculations and formatting appropriate to a linear model object. We use the same generic \texttt{summary()} function on data frames, as in \texttt{summary(gapminder)}, but in that case a different default method is applied.

Schematic view of a linear model object.Figure 6.3: Schematic view of a linear model object.

The output from \texttt{summary()} gives a precis of the model, but we can't really do any further analysis with it directly. For example, what if we want to plot something from the model? The information necessary to make plots is inside the out object, but it is not obvious how to use it.

If we take a look at the structure of the model object with \texttt{str(out)} we will find that there is a lot of information in there. Like most complex objects in R, out is organized as a list of components or elements. Several of these elements are themselves lists. Figure 6.3 gives you a schematic view of the contents of a linear model object. In this list of items, elements are single values, some are data frames, and some are additional lists of simpler items. Again, remember our earlier discussion where we said objects could be thought of as being organized like a filing system: cabinets contain drawers, and drawer may contain which may contain pages of information, whole documents, or groups of folders with more documents inside. As an alternative analogy, and sticking with the image of a list, you can think of a master to-do list for a project, where the top-level headings lead to contain additional lists of tasks of different kinds.

The \texttt{out} object created by \texttt{lm} contains several different named elements. Some, like the residual degrees of freedom in the model, are just a single number. Try \texttt{out\$df.residual} at the console. Others are much larger entities, such as the data frame used to fit the model, which is retained by default. Try \texttt{out\$model}, but be prepared for a lot of stuff to be printed at the console. Other elements have been computed by R and then stored, such as the coefficients of the model and other quantities. You can try \texttt{out\$coefficients}, \texttt{out\$residuals}, and \texttt{out\$fitted.values}, for instance. Others are lists themselves (like \texttt{qr}). So you can see that the \texttt{summary()} function is selecting and printing only a small amount of core information, in comparison to what is stored in the model object.

Just like the tables of data we saw earlier in Section A.1.3, the output of \texttt{summary()} is presented in a way that is compact and efficient in terms of getting information across, but also untidy when considered from the point of view of further manipulation. There is a table of coefficients, but the variable names are in the rows. The column names are awkward, and some information (e.g.~at the bottom of the output) has been calculated and printed out, but is not stored in the model object.

\hypertarget{get-model-based-graphics-right}{%
\section{Get model-based graphics right}\label{get-model-based-graphics-right}}

Figures based on statistical models face all the ordinary challenges of effective data visualization, and then some. This is because model results usually carry a considerable extra burden of interpretation and necessary background knowledge. The more complex the model, the trickier it becomes to convey this information effectively, and the easier it becomes to lead one's audience or oneself into error. Within the social sciences, our ability to clearly and honestly present model-based graphics has greatly improved over the past ten or fifteen years. Over the same period, it has become clearer that some kinds of models are quite tricky to understand, even ones that had previously been seen as straightforward elements of the modeling toolkit (Ai \& Norton, 2003; Brambor, Clark, \& Golder, 2006).

Plotting model estimates is closely connected to properly estimating models in the first place. This means there is no substitute for learning the statistics. You should not use graphical methods as a substitute for understanding the model used to produce them. While this book cannot teach you that material, we can make a few general points about what good model-based graphics look like, and work through some examples of how \texttt{ggplot} and some additional libraries can make it easier to get good results.

\hypertarget{present-your-findings-in-substantive-terms}{%
\subsection{Present your findings in substantive terms}\label{present-your-findings-in-substantive-terms}}

Useful model-based plots show results in ways that are substantively meaningful and directly interpretable with respect to the questions the analysis is trying to answer. This means showing results in a context where other variables in the analysis are held at sensible values, such as their means or medians. With continuous variables, it can often be useful to generate predicted values that cover some substantively meaningful move across the distribution, such as from the \textbf{25th to the 75th percentile}, rather than a single-unit increment in the variable of interest. For unordered categorical variables, predicted values might be presented with respect to the modal category in the data, or for a particular category of theoretical interest. Presenting substantively interpretable findings often also means using (and sometimes converting to) a scale that readers can easily understand. If your model reports results in log-odds, for example, converting the estimates to predicted probabilities will make it easier to interpret. All of this advice is quite general. Each of these points applies equally well to the presentation of summary results in a table rather than a graph. There is nothing distinctively graphical about putting the focus on the substantive meaning of your findings.

\hypertarget{show-your-degree-of-confidence}{%
\subsection{Show your degree of confidence}\label{show-your-degree-of-confidence}}

Much the same applies to presenting the degree of uncertainty or confidence you have in your results. Model estimates come with various measures of precision, confidence, credence, or significance. Presenting and interpreting these measures is notoriously prone to misinterpretation, or over-interpretation, as researchers and audiences both demand more from things like confidence intervals and p-values than these statistics can deliver. At a minimum, having decided on an appropriate measure of model fit or the right assessment of confidence, you should show their range when you present your results. A family of related \texttt{ggplot} geoms allow you to show a range or interval defined by position on the x-axis and then a \texttt{ymin} and \texttt{ymax} range on the y-axis. These geoms include \texttt{geom\_pointrange()} and \texttt{geom\_errorbar()}, which we will see in action shortly. A related geom, \texttt{geom\_ribbon()} uses the same arguments to draw filled areas, and is useful for plotting ranges of y-axis values along some continuously varying x-axis.

\hypertarget{show-your-data-when-you-can}{%
\subsection{Show your data when you can}\label{show-your-data-when-you-can}}

Plotting the results from a multivariate model generally means one of two things. First, we can show what is in effect a table of coefficients with associated measures of confidence, perhaps organizing the coefficients into meaningful groups, or by the size of the predicted association, or both. Second, we can show the predicted values of some variables (rather than just a model's coefficients) across some range of interest. The latter approach lets us show the original data points if we wish. The way \texttt{ggplot} builds graphics layer by layer allows us to easily combine model estimates (e.g.~a regression line and an associated range) and the underlying data. In effect these are manually-constructed versions of the automatically-generated plots that we have been producing with \texttt{geom\_smooth()} since the beginning of this book.

\hypertarget{generate-predictions-to-graph}{%
\section{Generate predictions to graph}\label{generate-predictions-to-graph}}

Having fitted a model, then, we might want to get a picture of the estimates it produces over the range of some particular variable, holding other covariates constant at some sensible values. The \texttt{predict()} function is a generic way of using model objects to produce this kind of prediction. In R, ``generic'' functions take their inputs and pass them along to more specific functions behind the scenes, ones that are suited to working with the particular kind of model object we have. The details of getting predicted values from a \textbf{OLS} model, for instance, will be somewhat different from getting predictions out of a logistic regression. But in each case we can use the same \texttt{predict()} function, taking care to check the documentation to see what form the results are returned in for the kind of model we are working with. Many of the most commonly-used functions in R are generic in this way. The \texttt{summary()} function, for example, works on objects of many different classes, from vectors to data frames and statistical models, producing appropriate output in each case by way of a class-specific function in the background.

For \texttt{predict()} to calculate the new values for us, it needs some new data to fit the model to. We will generate a new data frame whose columns have the same names as the variables in the model's original data, but where the rows have new values. A very useful function called \texttt{expand.grid()} will help us do this. We will give it a list of variables, specifying the range of values we want each variable to take. Then \texttt{expand.grid()} will generate the will multiply out the full range of values for all combinations of the values we give it, thus creating a new data frame with the new data we need.

In the following bit of code, we use \texttt{min()} and \texttt{max()} to get the minimum and maximum values for per capita GDP, and then create a vector with one hundred evenly-spaced elements between the minimum and the maximum. We hold population constant at its median, and we let continent take all of its five available values.\\

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{min_gdp <-}\StringTok{ }\KeywordTok{min}\NormalTok{(gapminder}\OperatorTok{$}\NormalTok{gdpPercap)}
\NormalTok{max_gdp <-}\StringTok{ }\KeywordTok{max}\NormalTok{(gapminder}\OperatorTok{$}\NormalTok{gdpPercap)}
\NormalTok{med_pop <-}\StringTok{ }\KeywordTok{median}\NormalTok{(gapminder}\OperatorTok{$}\NormalTok{pop)}

\NormalTok{pred_df <-}\StringTok{ }\KeywordTok{expand.grid}\NormalTok{(}\DataTypeTok{gdpPercap =}\NormalTok{ (}\KeywordTok{seq}\NormalTok{(}\DataTypeTok{from =}\NormalTok{ min_gdp,}
                                        \DataTypeTok{to =}\NormalTok{ max_gdp,}
                                        \DataTypeTok{length.out =} \DecValTok{100}\NormalTok{)),}
                       \DataTypeTok{pop =}\NormalTok{ med_pop,}
                       \DataTypeTok{continent =} \KeywordTok{c}\NormalTok{(}\StringTok{"Africa"}\NormalTok{, }\StringTok{"Americas"}\NormalTok{,}
                                     \StringTok{"Asia"}\NormalTok{, }\StringTok{"Europe"}\NormalTok{, }\StringTok{"Oceania"}\NormalTok{))}

\KeywordTok{dim}\NormalTok{(pred_df)}
\CommentTok{#> [1] 500   3}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(pred_df)}
\CommentTok{#>   gdpPercap     pop continent}
\CommentTok{#> 1       241 7023596    Africa}
\CommentTok{#> 2      1385 7023596    Africa}
\CommentTok{#> 3      2530 7023596    Africa}
\CommentTok{#> 4      3674 7023596    Africa}
\CommentTok{#> 5      4818 7023596    Africa}
\CommentTok{#> 6      5962 7023596    Africa}
\end{Highlighting}
\end{Shaded}

Now we can use \texttt{predict()}. If we give the function our new data and model, without any further argument, it will calculate the fitted values for every row in the data frame. If we specify \texttt{interval\ =\ \textquotesingle{}predict\textquotesingle{}} as an argument, it will calculate 95\% prediction intervals in addition to the point estimate.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pred_out <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(}\DataTypeTok{object =}\NormalTok{ out,}
                    \DataTypeTok{newdata =}\NormalTok{ pred_df,}
                    \DataTypeTok{interval =} \StringTok{"predict"}\NormalTok{)}
\KeywordTok{head}\NormalTok{(pred_out)}
\CommentTok{#>    fit  lwr  upr}
\CommentTok{#> 1 48.0 31.5 64.4}
\CommentTok{#> 2 48.5 32.1 64.9}
\CommentTok{#> 3 49.0 32.6 65.4}
\CommentTok{#> 4 49.5 33.1 65.9}
\CommentTok{#> 5 50.0 33.6 66.4}
\CommentTok{#> 6 50.5 34.1 67.0}
\end{Highlighting}
\end{Shaded}

Because we know that, by construction, the cases in \texttt{pred\_df} and \texttt{pred\_out} correspond row for row, we can bind the two data frames together by column. This method of joining or merging tables is definitely not recommended when you are dealing with data.\\

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pred_df <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(pred_df, pred_out)}
\KeywordTok{head}\NormalTok{(pred_df)}
\CommentTok{#>   gdpPercap     pop continent  fit  lwr  upr}
\CommentTok{#> 1       241 7023596    Africa 48.0 31.5 64.4}
\CommentTok{#> 2      1385 7023596    Africa 48.5 32.1 64.9}
\CommentTok{#> 3      2530 7023596    Africa 49.0 32.6 65.4}
\CommentTok{#> 4      3674 7023596    Africa 49.5 33.1 65.9}
\CommentTok{#> 5      4818 7023596    Africa 50.0 33.6 66.4}
\CommentTok{#> 6      5962 7023596    Africa 50.5 34.1 67.0}
\end{Highlighting}
\end{Shaded}

The end result is a tidy data frame, containing the predicted values from the model for the range of values we specified. Now we can plot the results. Because we produced a full range of predicted values, we can decide whether or not to use all of them. Here we further subset the predictions to just those for Europe and Africa.\\

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \KeywordTok{subset}\NormalTok{(pred_df, continent }\OperatorTok{%in%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Europe"}\NormalTok{, }\StringTok{"Africa"}\NormalTok{)),}
            \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ gdpPercap,}
                \DataTypeTok{y =}\NormalTok{ fit, }\DataTypeTok{ymin =}\NormalTok{ lwr, }\DataTypeTok{ymax =}\NormalTok{ upr,}
                \DataTypeTok{color =}\NormalTok{ continent,}
                \DataTypeTok{fill =}\NormalTok{ continent,}
                \DataTypeTok{group =}\NormalTok{ continent))}

\NormalTok{p }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{data =} \KeywordTok{subset}\NormalTok{(gapminder,}
\NormalTok{                             continent }\OperatorTok{%in%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Europe"}\NormalTok{, }\StringTok{"Africa"}\NormalTok{)),}
               \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ gdpPercap, }\DataTypeTok{y =}\NormalTok{ lifeExp,}
                   \DataTypeTok{color =}\NormalTok{ continent),}
               \DataTypeTok{alpha =} \FloatTok{0.5}\NormalTok{,}
               \DataTypeTok{inherit.aes =} \OtherTok{FALSE}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{geom_line}\NormalTok{() }\OperatorTok{+}
\StringTok{    }\KeywordTok{geom_ribbon}\NormalTok{(}\DataTypeTok{alpha =} \FloatTok{0.2}\NormalTok{, }\DataTypeTok{color =} \OtherTok{FALSE}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{scale_x_log10}\NormalTok{(}\DataTypeTok{labels =}\NormalTok{ scales}\OperatorTok{::}\NormalTok{dollar)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{misc_140-data_visualization-modeling_files/figure-latex/unnamed-chunk-17-1} \end{center}

We use a new geom here to draw the area covered by the prediction intervals: \texttt{geom\_ribbon()}. It takes an x argument like a line, but a ymin and ymax argument as specified in the \texttt{ggplot()} aesthetic mapping. This defines the lower and upper limits of the prediction interval.

In practice, you may not use \texttt{predict()} directly all that often. Instead, you might write code using additional libraries that encapsulate the process of producing predictions and plots from models. These are especially useful when your model is a little more complex and the interpretation of coefficients becomes trickier. This happens, for instance, when you have a binary outcome variable and need to convert the results of a logistic regression into predicted probabilities, or when you have interaction terms amongst your predictions. We will discuss some of these helper libraries in the next few sections. However, bear in mind that \texttt{predict()} and its ability to work safely with different classes of model underpins many of those libraries. So it's useful to see it in action first hand in order to understand what it is doing.

\hypertarget{tidy-model-objects-with-broom}{%
\section{\texorpdfstring{Tidy model objects with \texttt{broom}}{Tidy model objects with broom}}\label{tidy-model-objects-with-broom}}

The \texttt{predict} method is very useful, but there are a lot of other things we might want to do with our model output. We will use David Robinson's \texttt{broom} package to help us out. It is a library of functions that help us get from the model results that R generates to numbers that we can plot. It will take model objects and turn pieces of them into data frames that you can use easily with \texttt{ggplot.}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(broom)}
\end{Highlighting}
\end{Shaded}

Broom takes ggplot's approach to tidy data and extends it to the model objects that R produces. Its methods can tidily extract three kinds of information. First, we can see component-level information about aspects of the model itself, such as coefficients and t-statistics. Second, we can obtain observation-level information about the model's connection to the underlying data. This includes the fitted values and residuals for each observation in the data. And finally we can get model-level information that summarizes the fit as a whole, such as an F-statistic, the model deviance, or the r-squared. There is a \texttt{broom} function for each of these tasks.

\hypertarget{get-component-level-statistics-with-tidy}{%
\subsection{\texorpdfstring{Get component-level statistics with \texttt{tidy()}}{Get component-level statistics with tidy()}}\label{get-component-level-statistics-with-tidy}}

The \texttt{tidy()} function takes a model object and returns a data frame of component-level information. We can work with this to make plots in a familiar way, and much more easily than fishing inside the model object to extract the various terms. Here is an example, using the default results as just returned. For a more convenient display of the results, we will pipe the object we create with \texttt{tidy()} through a function that rounds the numeric columns of the data frame to two decimal places. This doesn't change anything about the object itself, of course.\\

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{out_comp <-}\StringTok{ }\KeywordTok{tidy}\NormalTok{(out)}
\NormalTok{out_comp }\OperatorTok{%>%}\StringTok{ }\KeywordTok{round_df}\NormalTok{()}
\CommentTok{#> # A tibble: 7 x 5}
\CommentTok{#>   term              estimate std.error statistic p.value}
\CommentTok{#>   <chr>                <dbl>     <dbl>     <dbl>   <dbl>}
\CommentTok{#> 1 (Intercept)          47.8      0.34     141.         0}
\CommentTok{#> 2 gdpPercap             0        0         19.2        0}
\CommentTok{#> 3 pop                   0        0          3.33       0}
\CommentTok{#> 4 continentAmericas    13.5      0.6       22.5        0}
\CommentTok{#> 5 continentAsia         8.19     0.570     14.3        0}
\CommentTok{#> 6 continentEurope      17.5      0.62      28.0        0}
\CommentTok{#> # ... with 1 more row}
\end{Highlighting}
\end{Shaded}

We are now able to treat this dataframe just like all the other data that we have seen so far.\\

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(out_comp, }\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ term,}
                                    \DataTypeTok{y =}\NormalTok{ estimate))}

\NormalTok{p }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_point}\NormalTok{() }\OperatorTok{+}\StringTok{ }\KeywordTok{coord_flip}\NormalTok{() }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{misc_140-data_visualization-modeling_files/figure-latex/unnamed-chunk-20-1} \end{center}

We can extend and clean up this plot in a variety of ways. For example, we can tell \texttt{tidy()} to calculate confidence intervals for the estimates, using R's \texttt{confint()} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{out_conf <-}\StringTok{ }\KeywordTok{tidy}\NormalTok{(out, }\DataTypeTok{conf.int =} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{out_conf }\OperatorTok{%>%}\StringTok{ }\KeywordTok{round_df}\NormalTok{()}
\CommentTok{#> # A tibble: 7 x 7}
\CommentTok{#>   term              estimate std.error statistic p.value conf.low conf.high}
\CommentTok{#>   <chr>                <dbl>     <dbl>     <dbl>   <dbl>    <dbl>     <dbl>}
\CommentTok{#> 1 (Intercept)          47.8      0.34     141.         0    47.2      48.5 }
\CommentTok{#> 2 gdpPercap             0        0         19.2        0     0         0   }
\CommentTok{#> 3 pop                   0        0          3.33       0     0         0   }
\CommentTok{#> 4 continentAmericas    13.5      0.6       22.5        0    12.3      14.6 }
\CommentTok{#> 5 continentAsia         8.19     0.570     14.3        0     7.07      9.31}
\CommentTok{#> 6 continentEurope      17.5      0.62      28.0        0    16.2      18.7 }
\CommentTok{#> # ... with 1 more row}
\end{Highlighting}
\end{Shaded}

The convenience ``not in'' operator \texttt{\%nin\%} is available via the \texttt{socviz} library. It does the opposite of \texttt{\%in\%} and selects only the items in a first vector of characters that are not in the second. We'll use it to drop the intercept term from the table. We also want to something about the labels. When fitting a model with categorical variables, R will create coefficient names based on the variable name and the category name, like continentAmericas. Normally we like to clean these up before plotting. Most commonly, we just want to strip away the variable name at the beginning of the coefficient label. For this we can use \texttt{prefix\_strip()}, a convenience function in the \texttt{socviz} library. We tell it which prefixes to drop, using it to create a new column variable in \texttt{out\_conf} that corresponds to the terms column, but that has nicer labels.\\

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{out_conf <-}\StringTok{ }\KeywordTok{subset}\NormalTok{(out_conf, term }\OperatorTok{%nin%}\StringTok{ "(Intercept)"}\NormalTok{)}
\NormalTok{out_conf}\OperatorTok{$}\NormalTok{nicelabs <-}\StringTok{ }\KeywordTok{prefix_strip}\NormalTok{(out_conf}\OperatorTok{$}\NormalTok{term, }\StringTok{"continent"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Now we can use \texttt{geom\_pointrange()}to make a figure that displays some information about our confidence in the variable estimates, as opposed to just the coefficients. As with the boxplots earlier, we use \texttt{reorder()} to sort the names of the model's terms by the estimate variable, thus arranging our plot of effects from largest to smallest in magnitude.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(out_conf, }\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{reorder}\NormalTok{(nicelabs, estimate),}
                                    \DataTypeTok{y =}\NormalTok{ estimate, }\DataTypeTok{ymin =}\NormalTok{ conf.low, }\DataTypeTok{ymax =}\NormalTok{ conf.high))}
\NormalTok{p }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_pointrange}\NormalTok{() }\OperatorTok{+}\StringTok{ }\KeywordTok{coord_flip}\NormalTok{() }\OperatorTok{+}\StringTok{ }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x=}\StringTok{""}\NormalTok{, }\DataTypeTok{y=}\StringTok{"OLS Estimate"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{misc_140-data_visualization-modeling_files/figure-latex/unnamed-chunk-23-1} \end{center}

Dotplots of this kind can be very compact. The vertical axis can often be compressed quite a bit, with no loss in comprehension. In fact, they are often easier to read with much less room between the rows than given by a default square shape.

\hypertarget{get-observation-level-statistics-with-augment}{%
\subsection{\texorpdfstring{Get observation-level statistics with \texttt{augment()}}{Get observation-level statistics with augment()}}\label{get-observation-level-statistics-with-augment}}

The values returned by \texttt{augment()} are all statistics calculated at the level of the original observations. As such, they can be added on to the data frame that the model is based on. Working from a call to \texttt{augment()} will return a data frame with all the original observations used in the estimation of the model, together with columns like the following:

\begin{itemize}
\tightlist
\item
  \textbf{.fitted} --- The fitted values of the model.
\item
  \textbf{.se.fit} --- The standard errors of the fitted values.
\item
  \textbf{.resid} --- The residuals.
\item
  \textbf{.hat} --- The diagonal of the hat matrix.
\item
  \textbf{.sigma} --- An estimate of residual standard deviation when the corresponding observation is dropped from the model.
\item
  \textbf{.cooksd} --- Cook's distance, a common regression diagnostic; and
\item
  \textbf{.std.resid} --- The standardized residuals.
\end{itemize}

Each of these variables is named with a leading \texttt{dot}, for example \texttt{.hat} rather than hat, and so on. This is to guard against accidentally confusing it with (or accidentally overwriting) an existing variable in your data with this name. The columns of values return will differ slightly depending on the class of model being fitted.\\

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{out_aug <-}\StringTok{ }\KeywordTok{augment}\NormalTok{(out)}
\KeywordTok{head}\NormalTok{(out_aug) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{round_df}\NormalTok{()}
\CommentTok{#> # A tibble: 6 x 11}
\CommentTok{#>   lifeExp gdpPercap    pop continent .fitted .se.fit .resid  .hat .sigma}
\CommentTok{#>     <dbl>     <dbl>  <dbl> <fct>       <dbl>   <dbl>  <dbl> <dbl>  <dbl>}
\CommentTok{#> 1    28.8      779. 8.43e6 Asia         56.4    0.47  -27.6     0   8.34}
\CommentTok{#> 2    30.3      821. 9.24e6 Asia         56.4    0.47  -26.1     0   8.34}
\CommentTok{#> 3    32        853. 1.03e7 Asia         56.5    0.47  -24.5     0   8.35}
\CommentTok{#> 4    34.0      836. 1.15e7 Asia         56.5    0.47  -22.4     0   8.35}
\CommentTok{#> 5    36.1      740. 1.31e7 Asia         56.4    0.47  -20.3     0   8.35}
\CommentTok{#> 6    38.4      786. 1.49e7 Asia         56.5    0.47  -18.0     0   8.36}
\CommentTok{#> # ... with 2 more variables: .cooksd <dbl>, .std.resid <dbl>}
\end{Highlighting}
\end{Shaded}

By default, \texttt{augment()} will extract the available data from the model object. This will usually include the variables used in the model itself, but not any additional ones contained in the original data frame. Sometimes it is useful to have these. We can add them by specifying the \texttt{data} argument:\\

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{out_aug <-}\StringTok{ }\KeywordTok{augment}\NormalTok{(out, }\DataTypeTok{data =}\NormalTok{ gapminder)}
\KeywordTok{head}\NormalTok{(out_aug) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{round_df}\NormalTok{()}
\CommentTok{#> # A tibble: 6 x 13}
\CommentTok{#>   country continent  year lifeExp    pop gdpPercap .fitted .se.fit .resid}
\CommentTok{#>   <fct>   <fct>     <dbl>   <dbl>  <dbl>     <dbl>   <dbl>   <dbl>  <dbl>}
\CommentTok{#> 1 Afghan~ Asia       1952    28.8 8.43e6      779.    56.4    0.47  -27.6}
\CommentTok{#> 2 Afghan~ Asia       1957    30.3 9.24e6      821.    56.4    0.47  -26.1}
\CommentTok{#> 3 Afghan~ Asia       1962    32   1.03e7      853.    56.5    0.47  -24.5}
\CommentTok{#> 4 Afghan~ Asia       1967    34.0 1.15e7      836.    56.5    0.47  -22.4}
\CommentTok{#> 5 Afghan~ Asia       1972    36.1 1.31e7      740.    56.4    0.47  -20.3}
\CommentTok{#> 6 Afghan~ Asia       1977    38.4 1.49e7      786.    56.5    0.47  -18.0}
\CommentTok{#> # ... with 4 more variables: .hat <dbl>, .sigma <dbl>, .cooksd <dbl>,}
\CommentTok{#> #   .std.resid <dbl>}
\end{Highlighting}
\end{Shaded}

If some rows containing missing data were dropped to fit the model, then these will not be carried over to the augmented dataframe.

The new columns created by \texttt{augment()} can be used to create some standard regression plots. For example, we can plot the residuals versus the fitted values. Figure 6.7 suggests, unsurprisingly, that our country-year data has rather more structure than is captured by our \texttt{OLS} model.\\

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ out_aug,}
            \DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ .fitted, }\DataTypeTok{y =}\NormalTok{ .resid))}
\NormalTok{p }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{misc_140-data_visualization-modeling_files/figure-latex/fig-67-1} \end{center}

\hypertarget{get-model-level-statistics-with-glance}{%
\subsection{\texorpdfstring{Get model-level statistics with \texttt{glance()}}{Get model-level statistics with glance()}}\label{get-model-level-statistics-with-glance}}

This function organizes the information typically presented at the bottom of a model's \texttt{summary()} output. By itself, it usually just returns a table with a single row in it. But as we shall see in a moment, the real power of broom's approach is the way that it can scale up to cases where we are grouping or subsampling our data.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{glance}\NormalTok{(out) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{round_df}\NormalTok{()}
\CommentTok{#> # A tibble: 1 x 11}
\CommentTok{#>   r.squared adj.r.squared sigma statistic p.value    df logLik    AIC}
\CommentTok{#>       <dbl>         <dbl> <dbl>     <dbl>   <dbl> <dbl>  <dbl>  <dbl>}
\CommentTok{#> 1     0.580         0.580  8.37      394.       0     7 -6034. 12084.}
\CommentTok{#> # ... with 3 more variables: BIC <dbl>, deviance <dbl>, df.residual <dbl>}
\end{Highlighting}
\end{Shaded}

Broom is able to \texttt{tidy} (and \texttt{augment}, and \texttt{glance} at) a wide range of model types. Not all functions are available for all classes of model. Consult broom's documentation for more details on what is available. For example, here is a plot created from the tidied output of an event-history analysis. First we generate a Cox proportional hazards model of some \texttt{survival} data.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(survival)}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'survival'}
\CommentTok{#> The following object is masked from 'package:quantreg':}
\CommentTok{#> }
\CommentTok{#>     untangle.specials}

\NormalTok{out_cph <-}\StringTok{ }\KeywordTok{coxph}\NormalTok{(}\KeywordTok{Surv}\NormalTok{(time, status) }\OperatorTok{~}\StringTok{ }\NormalTok{age }\OperatorTok{+}\StringTok{ }\NormalTok{sex, }\DataTypeTok{data =}\NormalTok{ lung)}
\NormalTok{out_surv <-}\StringTok{ }\KeywordTok{survfit}\NormalTok{(out_cph)}
\end{Highlighting}
\end{Shaded}

The details of the fit are not important here, but in the first step the \texttt{Surv()} function creates the response or outcome variable for the proportional hazards model that is then fitted by the \texttt{coxph()}function. Then the \texttt{survfit()} function creates the survival curve from the model, much like we used \texttt{predict()} to generate predicted values earlier. Try \texttt{summary(out\_cph)} to see the model, and \texttt{summary(out\_surv)} to see the table of predicted values that will form the basis for our plot. Next we tidy \texttt{out\_surv} to get a data frame, and plot it.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Figure 6.8: A Kaplan-Meier plot.}
\NormalTok{out_tidy <-}\StringTok{ }\KeywordTok{tidy}\NormalTok{(out_surv)}

\NormalTok{p <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ out_tidy, }\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(time, estimate))}
\NormalTok{p }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_line}\NormalTok{() }\OperatorTok{+}
\StringTok{    }\KeywordTok{geom_ribbon}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{ymin =}\NormalTok{ conf.low, }\DataTypeTok{ymax =}\NormalTok{ conf.high), }\DataTypeTok{alpha =} \FloatTok{.2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{misc_140-data_visualization-modeling_files/figure-latex/unnamed-chunk-28-1} \end{center}

\hypertarget{grouped-analysis-and-list-columns}{%
\section{Grouped analysis and list-columns}\label{grouped-analysis-and-list-columns}}

Broom makes it possible to quickly fit models to different subsets of your data and get consistent and usable tables of results out the other end. For example, let's say we wanted to look at the gapminder data by examining the relationship between life expectancy and GDP by continent, for each year in the data.

The \texttt{gapminder} data is at bottom organized by country-years. That is the unit of observation in the rows. If we wanted, we could take a slice of the data manually, such as ``all countries observed in Asia, in 1962'' or ``all in Africa, 2002''. Here is ``Europe, 1977'':\\

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{eu77 <-}\StringTok{ }\NormalTok{gapminder }\OperatorTok{%>%}\StringTok{ }\KeywordTok{filter}\NormalTok{(continent }\OperatorTok{==}\StringTok{ "Europe"}\NormalTok{, year }\OperatorTok{==}\StringTok{ }\DecValTok{1977}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We could then see what the relationship between life expectancy and GDP looked like for that continent-year group:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(lifeExp }\OperatorTok{~}\StringTok{ }\KeywordTok{log}\NormalTok{(gdpPercap), }\DataTypeTok{data =}\NormalTok{ eu77)}
\KeywordTok{summary}\NormalTok{(fit)}
\CommentTok{#> }
\CommentTok{#> Call:}
\CommentTok{#> lm(formula = lifeExp ~ log(gdpPercap), data = eu77)}
\CommentTok{#> }
\CommentTok{#> Residuals:}
\CommentTok{#>    Min     1Q Median     3Q    Max }
\CommentTok{#> -7.496 -1.031  0.093  1.176  3.712 }
\CommentTok{#> }
\CommentTok{#> Coefficients:}
\CommentTok{#>                Estimate Std. Error t value Pr(>|t|)    }
\CommentTok{#> (Intercept)      29.489      7.161    4.12  0.00031 ***}
\CommentTok{#> log(gdpPercap)    4.488      0.756    5.94  2.2e-06 ***}
\CommentTok{#> ---}
\CommentTok{#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1}
\CommentTok{#> }
\CommentTok{#> Residual standard error: 2.11 on 28 degrees of freedom}
\CommentTok{#> Multiple R-squared:  0.557,  Adjusted R-squared:  0.541 }
\CommentTok{#> F-statistic: 35.2 on 1 and 28 DF,  p-value: 2.17e-06}
\end{Highlighting}
\end{Shaded}

With \texttt{dplyr} and \texttt{broom} we can do this for every continent-year slice of the data in a compact and tidy way. We start with our table of data, and then (\texttt{\%\textgreater{}\%)} group the countries by continent and year using the \texttt{group\_by()} function. We introduced this grouping operation in Chapter 4. Our data is reorganized first by continent, and within continent by year. Here we will take one further step and nest the data that make up each group:\\

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{out_le <-}\StringTok{ }\NormalTok{gapminder }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{group_by}\NormalTok{(continent, year) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{nest}\NormalTok{()}

\NormalTok{out_le}
\CommentTok{#> # A tibble: 60 x 3}
\CommentTok{#>   continent  year data             }
\CommentTok{#>   <fct>     <int> <list>           }
\CommentTok{#> 1 Asia       1952 <tibble [33 x 4]>}
\CommentTok{#> 2 Asia       1957 <tibble [33 x 4]>}
\CommentTok{#> 3 Asia       1962 <tibble [33 x 4]>}
\CommentTok{#> 4 Asia       1967 <tibble [33 x 4]>}
\CommentTok{#> 5 Asia       1972 <tibble [33 x 4]>}
\CommentTok{#> 6 Asia       1977 <tibble [33 x 4]>}
\CommentTok{#> # ... with 54 more rows}
\end{Highlighting}
\end{Shaded}

Think of what \texttt{nest()} does as a more intensive version what \texttt{group\_by()} does. The resulting object is has the tabular form we expect (it is a tibble) but it looks a little unusual. The first two columns are the familiar continent and year. But we now also have a new column, data, that contains a small table of data corresponding to each continent-year group. This is a list-column, something we have not seen before. It turns out to be very useful for bundling together complex objects (structured, in this case, as a list of tibbles, each being a 33x4 table of data) within the rows of our data (which remains tabular). Our ``Europe 1977'' fit is in there. We can look at it, if we like, by filtering the data and then unnesting the list column.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{out_le }\OperatorTok{%>%}\StringTok{ }\KeywordTok{filter}\NormalTok{(continent }\OperatorTok{==}\StringTok{ "Europe"} \OperatorTok{&}\StringTok{ }\NormalTok{year }\OperatorTok{==}\StringTok{ }\DecValTok{1977}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{unnest}\NormalTok{()}
\CommentTok{#> # A tibble: 30 x 6}
\CommentTok{#>   continent  year country                lifeExp     pop gdpPercap}
\CommentTok{#>   <fct>     <int> <fct>                    <dbl>   <int>     <dbl>}
\CommentTok{#> 1 Europe     1977 Albania                   68.9 2509048     3533.}
\CommentTok{#> 2 Europe     1977 Austria                   72.2 7568430    19749.}
\CommentTok{#> 3 Europe     1977 Belgium                   72.8 9821800    19118.}
\CommentTok{#> 4 Europe     1977 Bosnia and Herzegovina    69.9 4086000     3528.}
\CommentTok{#> 5 Europe     1977 Bulgaria                  70.8 8797022     7612.}
\CommentTok{#> 6 Europe     1977 Croatia                   70.6 4318673    11305.}
\CommentTok{#> # ... with 24 more rows}
\end{Highlighting}
\end{Shaded}

List-columns are useful because we can act on them in a compact and tidy way. In particular, we can pass functions along to each row of the list-column and make something happen. For example, a moment ago we ran a regression of life expectancy and logged GDP for European countries in 1977. We can do that for every continent-year combination in the data. We first create a convenience function called \texttt{fit\_ols()} that takes a single argument, \texttt{df} (for data frame) and that fits the linear model we are interested in. Then we map that function to each of our list-column rows in turn. Recall from Chapter 4 that mutate creates new variables or columns on the fly within a pipeline.

The \texttt{map} action is an important idea in functional programming. If you have written code in other, more imperative languages you can think of it as a compact alternative to writing \texttt{for\ \ldots{}\ next} loops. You can of course write loops like this in R. Computationally they are often not any less efficient than their functional alternatives. But mapping functions to arrays is more easily integrated into a sequence of data transformations.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit_ols <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(df) \{}
    \KeywordTok{lm}\NormalTok{(lifeExp }\OperatorTok{~}\StringTok{ }\KeywordTok{log}\NormalTok{(gdpPercap), }\DataTypeTok{data =}\NormalTok{ df)}
\NormalTok{\}}

\NormalTok{out_le <-}\StringTok{ }\NormalTok{gapminder }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{group_by}\NormalTok{(continent, year) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{nest}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{model =} \KeywordTok{map}\NormalTok{(data, fit_ols)) }

\NormalTok{out_le}
\CommentTok{#> # A tibble: 60 x 4}
\CommentTok{#>   continent  year data              model }
\CommentTok{#>   <fct>     <int> <list>            <list>}
\CommentTok{#> 1 Asia       1952 <tibble [33 x 4]> <lm>  }
\CommentTok{#> 2 Asia       1957 <tibble [33 x 4]> <lm>  }
\CommentTok{#> 3 Asia       1962 <tibble [33 x 4]> <lm>  }
\CommentTok{#> 4 Asia       1967 <tibble [33 x 4]> <lm>  }
\CommentTok{#> 5 Asia       1972 <tibble [33 x 4]> <lm>  }
\CommentTok{#> 6 Asia       1977 <tibble [33 x 4]> <lm>  }
\CommentTok{#> # ... with 54 more rows}
\end{Highlighting}
\end{Shaded}

Before starting the pipeline we create a new function: It is a convenience function whose only job is to estimate a particular OLS model on some data. Like almost everything in R, functions are a kind of object. To make a new one, we use the slightly special \texttt{function()} function. (Nerds love that sort of thing.) There is a little more detail on creating functions in the Appendix. To see what fit\_ols() looks like once it is created, type \texttt{fit\_ols} without parentheses at the Console. To see what it does, try \texttt{fit\_ols(df\ =\ gapminder)}, or \texttt{summary(fit\_ols(gapminder))}.

Now we have two list-columns: \texttt{data}, and \texttt{model.} The latter was created by mapping the \texttt{fit\_ols()} function to each row of data. Inside each element of model is a linear model for that continent-year. So we now have sixty OLS fits, one for every continent-year grouping. Having the models inside the list column is not much use to us in and of itself. But we can extract the information we want while keeping things in a tidy tabular form. For clarity we will run the pipeline from the beginning again, this time adding a few new steps.

First we extract summary statistics from each model by mapping the \texttt{tidy()} function from \texttt{broom} to the model list column. Then we unnest the result, dropping the other columns in the process. Finally, we filter out all the Intercept terms, and also drop all observations from Oceania. In the case of the Intercepts we do this just out of convenience. Oceania we drop just because there are so few observations. We put the results in an object called \texttt{out\_tidy.}\\

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit_ols <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(df) \{}
    \KeywordTok{lm}\NormalTok{(lifeExp }\OperatorTok{~}\StringTok{ }\KeywordTok{log}\NormalTok{(gdpPercap), }\DataTypeTok{data =}\NormalTok{ df)}
\NormalTok{\}}

\NormalTok{out_tidy <-}\StringTok{ }\NormalTok{gapminder }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{group_by}\NormalTok{(continent, year) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{nest}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{model =} \KeywordTok{map}\NormalTok{(data, fit_ols),}
           \DataTypeTok{tidied =} \KeywordTok{map}\NormalTok{(model, tidy)) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{unnest}\NormalTok{(tidied, }\DataTypeTok{.drop =} \OtherTok{TRUE}\NormalTok{) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{filter}\NormalTok{(term }\OperatorTok{%nin%}\StringTok{ "(Intercept)"} \OperatorTok{&}
\StringTok{           }\NormalTok{continent }\OperatorTok{%nin%}\StringTok{ "Oceania"}\NormalTok{)}

\NormalTok{out_tidy }\OperatorTok{%>%}\StringTok{ }\KeywordTok{sample_n}\NormalTok{(}\DecValTok{5}\NormalTok{)}
\CommentTok{#> # A tibble: 5 x 7}
\CommentTok{#>   continent  year term           estimate std.error statistic       p.value}
\CommentTok{#>   <fct>     <int> <chr>             <dbl>     <dbl>     <dbl>         <dbl>}
\CommentTok{#> 1 Americas   1992 log(gdpPercap)     6.06     0.895      6.77 0.000000664  }
\CommentTok{#> 2 Europe     2002 log(gdpPercap)     3.74     0.445      8.40 0.00000000391}
\CommentTok{#> 3 Asia       2007 log(gdpPercap)     5.16     0.694      7.43 0.0000000226 }
\CommentTok{#> 4 Americas   1952 log(gdpPercap)    10.4      2.72       3.84 0.000827     }
\CommentTok{#> 5 Americas   1957 log(gdpPercap)    10.3      2.40       4.31 0.000261}
\end{Highlighting}
\end{Shaded}

We now have tidy regression output with an estimate of the association between \texttt{log} GDP per capita and life expectancy for each year, within continents. We can plot these estimates in a way that takes advantage of their groupiness.\\

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Figure 6.9: Yearly estimates of the association between GDP and Life Expectancy, pooled by continent.}
\NormalTok{p <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ out_tidy,}
            \DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ year, }\DataTypeTok{y =}\NormalTok{ estimate,}
                          \DataTypeTok{ymin =}\NormalTok{ estimate }\OperatorTok{-}\StringTok{ }\DecValTok{2}\OperatorTok{*}\NormalTok{std.error,}
                          \DataTypeTok{ymax =}\NormalTok{ estimate }\OperatorTok{+}\StringTok{ }\DecValTok{2}\OperatorTok{*}\NormalTok{std.error,}
                          \DataTypeTok{group =}\NormalTok{ continent, }\DataTypeTok{color =}\NormalTok{ continent))}

\NormalTok{p }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_pointrange}\NormalTok{(}\DataTypeTok{position =} \KeywordTok{position_dodge}\NormalTok{(}\DataTypeTok{width =} \DecValTok{1}\NormalTok{)) }\OperatorTok{+}
\StringTok{    }\KeywordTok{scale_x_continuous}\NormalTok{(}\DataTypeTok{breaks =} \KeywordTok{unique}\NormalTok{(gapminder}\OperatorTok{$}\NormalTok{year)) }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{legend.position =} \StringTok{"top"}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"Year"}\NormalTok{, }\DataTypeTok{y =} \StringTok{"Estimate"}\NormalTok{, }\DataTypeTok{color =} \StringTok{"Continent"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{misc_140-data_visualization-modeling_files/figure-latex/unnamed-chunk-35-1} \end{center}

The call to \texttt{position\_dodge()} within \texttt{geom\_pointrange()} allows the point ranges for each continent to be near each other within years, instead of being plotted right on top of one another. We could have faceted the results by continent, but doing it this way lets us see differences in the yearly estimates much more easily. This technique is very useful not just for cases like this, but also when you want to compare the coefficients given by different kinds of statistical model. This sometimes happens when we're interested in seeing how, say, OLS performs against some other model specification.

\hypertarget{plot-marginal-effects}{%
\section{Plot marginal effects}\label{plot-marginal-effects}}

Our earlier discussion of \texttt{predict()} was about obtaining estimates of the average effect of some coefficient, net of the other terms in the model. Over the past decade, estimating and plotting partial or marginal effects from a model has become an increasingly common way of presenting accurate and interpretively useful predictions. Interest in marginal effects plots was stimulated by the realization that the interpretation of terms in logistic regression models, in particular, was trickier than it seemed---especially when there were interaction terms in the model (Ai \& Norton, 2003). Thomas Leeper's \texttt{margins} package can make these plots for us.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(margins)}
\end{Highlighting}
\end{Shaded}

To see it in action, we'll take another look at the General Social Survey data in \texttt{gss\_sm}, this time focusing on the binary variable, obama.As is common with retrospective questions on elections, rather more people claim to have voted for Obama than is consistent with the vote share he received in the election. It is coded 1 if the respondent said they voted for Barack Obama in the 2012 presidential election, and 0 otherwise. In this case, mostly for convenience here, the zero code includes all other answers to the question, including those who said they voted for Mitt Romney, those who said they did not vote, those who refused to answer, and those who said they didn't know who they voted for. We will fit a logistic regression on obama, with \texttt{age}, \texttt{polviews}, \texttt{race}, and \texttt{sex} as the predictors. The \texttt{age} variable is the respondent's age in years. The \texttt{sex} variable is coded as ``Male'' or ``Female'' with ``Male'' as the reference category. The \texttt{race} variable is coded as ``White'', ``Black'', or ``Other'' with ``White'' as the reference category. The polviews measure is a self-reported scale of the respondent's political orientation from ``Extremely Conservative'' through ``Extremely Liberal'', with ``Moderate'' in the middle. We take \texttt{polviews} and create a new variable, \texttt{polviews\_m}, using the \texttt{relevel()} function to recode ``Moderate'' to be the reference category. We fit the model with the \texttt{glm()} function, and specify an interaction between race and sex.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gss_sm}\OperatorTok{$}\NormalTok{polviews_m <-}\StringTok{ }\KeywordTok{relevel}\NormalTok{(gss_sm}\OperatorTok{$}\NormalTok{polviews, }\DataTypeTok{ref =} \StringTok{"Moderate"}\NormalTok{)}

\NormalTok{out_bo <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(obama }\OperatorTok{~}\StringTok{ }\NormalTok{polviews_m }\OperatorTok{+}\StringTok{ }\NormalTok{sex}\OperatorTok{*}\NormalTok{race,}
              \DataTypeTok{family =} \StringTok{"binomial"}\NormalTok{, }\DataTypeTok{data =}\NormalTok{ gss_sm)}
\KeywordTok{summary}\NormalTok{(out_bo)}
\CommentTok{#> }
\CommentTok{#> Call:}
\CommentTok{#> glm(formula = obama ~ polviews_m + sex * race, family = "binomial", }
\CommentTok{#>     data = gss_sm)}
\CommentTok{#> }
\CommentTok{#> Deviance Residuals: }
\CommentTok{#>    Min      1Q  Median      3Q     Max  }
\CommentTok{#> -2.905  -0.554   0.177   0.542   2.244  }
\CommentTok{#> }
\CommentTok{#> Coefficients:}
\CommentTok{#>                                  Estimate Std. Error z value Pr(>|z|)    }
\CommentTok{#> (Intercept)                       0.29649    0.13409    2.21   0.0270 *  }
\CommentTok{#> polviews_mExtremely Liberal       2.37295    0.52504    4.52  6.2e-06 ***}
\CommentTok{#> polviews_mLiberal                 2.60003    0.35667    7.29  3.1e-13 ***}
\CommentTok{#> polviews_mSlightly Liberal        1.29317    0.24843    5.21  1.9e-07 ***}
\CommentTok{#> polviews_mSlightly Conservative  -1.35528    0.18129   -7.48  7.7e-14 ***}
\CommentTok{#> polviews_mConservative           -2.34746    0.20038  -11.71  < 2e-16 ***}
\CommentTok{#> polviews_mExtremely Conservative -2.72738    0.38721   -7.04  1.9e-12 ***}
\CommentTok{#> sexFemale                         0.25487    0.14537    1.75   0.0796 .  }
\CommentTok{#> raceBlack                         3.84953    0.50132    7.68  1.6e-14 ***}
\CommentTok{#> raceOther                        -0.00214    0.43576    0.00   0.9961    }
\CommentTok{#> sexFemale:raceBlack              -0.19751    0.66007   -0.30   0.7648    }
\CommentTok{#> sexFemale:raceOther               1.57483    0.58766    2.68   0.0074 ** }
\CommentTok{#> ---}
\CommentTok{#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1}
\CommentTok{#> }
\CommentTok{#> (Dispersion parameter for binomial family taken to be 1)}
\CommentTok{#> }
\CommentTok{#>     Null deviance: 2247.9  on 1697  degrees of freedom}
\CommentTok{#> Residual deviance: 1345.9  on 1686  degrees of freedom}
\CommentTok{#>   (1169 observations deleted due to missingness)}
\CommentTok{#> AIC: 1370}
\CommentTok{#> }
\CommentTok{#> Number of Fisher Scoring iterations: 6}
\end{Highlighting}
\end{Shaded}

The summary reports the coefficients and other information. We can now graph the data in any one of several ways. Using \texttt{margins()} we calculate the marginal effects for each variable:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bo_m <-}\StringTok{ }\KeywordTok{margins}\NormalTok{(out_bo)}
\KeywordTok{summary}\NormalTok{(bo_m)}
\CommentTok{#>                            factor     AME     SE        z      p   lower}
\CommentTok{#>            polviews_mConservative -0.4119 0.0283 -14.5394 0.0000 -0.4674}
\CommentTok{#>  polviews_mExtremely Conservative -0.4538 0.0420 -10.7971 0.0000 -0.5361}
\CommentTok{#>       polviews_mExtremely Liberal  0.2681 0.0295   9.0996 0.0000  0.2103}
\CommentTok{#>                 polviews_mLiberal  0.2768 0.0229  12.0736 0.0000  0.2319}
\CommentTok{#>   polviews_mSlightly Conservative -0.2658 0.0330  -8.0596 0.0000 -0.3304}
\CommentTok{#>        polviews_mSlightly Liberal  0.1933 0.0303   6.3896 0.0000  0.1340}
\CommentTok{#>                         raceBlack  0.4032 0.0173  23.3568 0.0000  0.3694}
\CommentTok{#>                         raceOther  0.1247 0.0386   3.2297 0.0012  0.0490}
\CommentTok{#>                         sexFemale  0.0443 0.0177   2.5073 0.0122  0.0097}
\CommentTok{#>    upper}
\CommentTok{#>  -0.3564}
\CommentTok{#>  -0.3714}
\CommentTok{#>   0.3258}
\CommentTok{#>   0.3218}
\CommentTok{#>  -0.2011}
\CommentTok{#>   0.2526}
\CommentTok{#>   0.4371}
\CommentTok{#>   0.2005}
\CommentTok{#>   0.0789}
\end{Highlighting}
\end{Shaded}

The \texttt{margins} library comes with several plot methods of its own. If you wish, at this point you can just try \texttt{plot(bo\_m)} to see a plot of the average marginal effects, produced with the general look of a Stata graphic. Other plot methods in the margins library include \texttt{cplot()}, which visualizes marginal effects conditional on a second variable, and \texttt{image()}, which shows predictions or marginal effects as a filled heatmap or contour plot.

Alternatively, we can take results from \texttt{margins()} and plot them ourselves. To clean up the summary a little a little, we convert it to a tibble, then use \texttt{prefix\_strip()} and \texttt{prefix\_replace()} to tidy the labels. We want to strip the polviews\_m and sex prefixes, and (to avoid ambiguity about ``Other''), adjust the race prefix.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bo_gg <-}\StringTok{ }\KeywordTok{as_tibble}\NormalTok{(}\KeywordTok{summary}\NormalTok{(bo_m))}
\NormalTok{prefixes <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"polviews_m"}\NormalTok{, }\StringTok{"sex"}\NormalTok{)}
\NormalTok{bo_gg}\OperatorTok{$}\NormalTok{factor <-}\StringTok{ }\KeywordTok{prefix_strip}\NormalTok{(bo_gg}\OperatorTok{$}\NormalTok{factor, prefixes)}
\NormalTok{bo_gg}\OperatorTok{$}\NormalTok{factor <-}\StringTok{ }\KeywordTok{prefix_replace}\NormalTok{(bo_gg}\OperatorTok{$}\NormalTok{factor, }\StringTok{"race"}\NormalTok{, }\StringTok{"Race: "}\NormalTok{)}

\NormalTok{bo_gg }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(factor, AME, lower, upper) }
\CommentTok{#> # A tibble: 9 x 4}
\CommentTok{#>   factor                    AME  lower  upper}
\CommentTok{#>   <chr>                   <dbl>  <dbl>  <dbl>}
\CommentTok{#> 1 Conservative           -0.412 -0.467 -0.356}
\CommentTok{#> 2 Extremely Conservative -0.454 -0.536 -0.371}
\CommentTok{#> 3 Extremely Liberal       0.268  0.210  0.326}
\CommentTok{#> 4 Liberal                 0.277  0.232  0.322}
\CommentTok{#> 5 Slightly Conservative  -0.266 -0.330 -0.201}
\CommentTok{#> 6 Slightly Liberal        0.193  0.134  0.253}
\CommentTok{#> # ... with 3 more rows}
\end{Highlighting}
\end{Shaded}

Now we have a table that we can plot as we have learned:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ bo_gg, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{reorder}\NormalTok{(factor, AME),}
                              \DataTypeTok{y =}\NormalTok{ AME, }\DataTypeTok{ymin =}\NormalTok{ lower, }\DataTypeTok{ymax =}\NormalTok{ upper))}

\NormalTok{p }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_hline}\NormalTok{(}\DataTypeTok{yintercept =} \DecValTok{0}\NormalTok{, }\DataTypeTok{color =} \StringTok{"gray80"}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{geom_pointrange}\NormalTok{() }\OperatorTok{+}\StringTok{ }\KeywordTok{coord_flip}\NormalTok{() }\OperatorTok{+}
\StringTok{    }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \OtherTok{NULL}\NormalTok{, }\DataTypeTok{y =} \StringTok{"Average Marginal Effect"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{misc_140-data_visualization-modeling_files/figure-latex/unnamed-chunk-40-1} \end{center}

If we are just interested in getting conditional effects for a particular variable, then conveniently we can ask the plot methods in the margins library to do the work calculating effects for us but without drawing their plot. Instead, they can return the results in a format we can easily use in ggplot, and with less need for clean up, for the clean-up. For example, with \texttt{cplot()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pv_cp <-}\StringTok{ }\KeywordTok{cplot}\NormalTok{(out_bo, }\DataTypeTok{x =} \StringTok{"sex"}\NormalTok{, }\DataTypeTok{draw =} \OtherTok{FALSE}\NormalTok{)}
\CommentTok{#>    xvals yvals upper lower}
\CommentTok{#> 1   Male 0.574 0.638 0.509}
\CommentTok{#> 2 Female 0.634 0.689 0.580}

\NormalTok{p <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ pv_cp, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{reorder}\NormalTok{(xvals, yvals),}
                              \DataTypeTok{y =}\NormalTok{ yvals, }\DataTypeTok{ymin =}\NormalTok{ lower, }\DataTypeTok{ymax =}\NormalTok{ upper))}

\NormalTok{p }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_hline}\NormalTok{(}\DataTypeTok{yintercept =} \DecValTok{0}\NormalTok{, }\DataTypeTok{color =} \StringTok{"gray80"}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{geom_pointrange}\NormalTok{() }\OperatorTok{+}\StringTok{ }\KeywordTok{coord_flip}\NormalTok{() }\OperatorTok{+}
\StringTok{    }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \OtherTok{NULL}\NormalTok{, }\DataTypeTok{y =} \StringTok{"Conditional Effect"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{misc_140-data_visualization-modeling_files/figure-latex/unnamed-chunk-41-1} \end{center}

The \texttt{margins} package is under active development. It can do much more than described here. The vignettes that come with the package provide more extensive discussion and numerous examples.

\hypertarget{plots-from-complex-surveys}{%
\section{Plots from complex surveys}\label{plots-from-complex-surveys}}

Social scientists often work with data collected using a complex survey design. Survey instruments may be stratified by region or some other characteristic, contain replicate weights to make them comparable to a reference population, have a clustered structure, and so on. In Chapter 4 we learned how calculate and then plot frequency tables of categorical variables, using some data from the General Social Survey (GSS). However, if we want accurate estimates of US households from the GSS, we will need to take the survey's design into account, and use the survey weights provided in the dataset. Thomas Lumley's \texttt{survey} library provides a comprehensive set of tools for addressing these issues. The tools and the theory behind them are discussed in detail in Lumley (2010), and an overview of the package is provided in Lumley (2004). While the functions in the \texttt{survey} package are straightforward to use and return results in a generally tidy form, the package predates the tidyverse and its conventions by several years. This means we cannot use survey functions directly with \texttt{dplyr}. However, Greg Freedman Ellis has written a helper package, \texttt{srvyr}, that solves this problem for us, and lets us use the \texttt{survey} library's functions within a data analysis pipeline in a familiar way.

For example, the \texttt{gss\_lon} data contains a small subset of measures from every wave of the GSS since its inception in 1972. It also contains several variables that describe the design of the survey and provide replicate weights for observations in various years. These technical details are described in the GSS documentation. Similar information is typically provided by other complex surveys. Here we will use this design information to calculate weighted estimates of the distribution of educational attainment by race, for selected survey years from 1976 to 2016.

To begin, we load the \texttt{survey} and \texttt{srvyr} libraries.\\

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(survey)}
\CommentTok{#> Loading required package: grid}
\CommentTok{#> Loading required package: Matrix}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'Matrix'}
\CommentTok{#> The following object is masked from 'package:tidyr':}
\CommentTok{#> }
\CommentTok{#>     expand}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'survey'}
\CommentTok{#> The following object is masked from 'package:graphics':}
\CommentTok{#> }
\CommentTok{#>     dotchart}
\KeywordTok{library}\NormalTok{(srvyr)}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'srvyr'}
\CommentTok{#> The following object is masked from 'package:stats':}
\CommentTok{#> }
\CommentTok{#>     filter}
\end{Highlighting}
\end{Shaded}

Next, we take our \texttt{gss\_lon} dataset and use the survey tools to create a new object that contains the data, as before, but with some additional information about the survey's design:\\

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{options}\NormalTok{(}\DataTypeTok{survey.lonely.psu =} \StringTok{"adjust"}\NormalTok{)}
\KeywordTok{options}\NormalTok{(}\DataTypeTok{na.action=}\StringTok{"na.pass"}\NormalTok{)}

\NormalTok{gss_wt <-}\StringTok{ }\KeywordTok{subset}\NormalTok{(gss_lon, year }\OperatorTok{>}\StringTok{ }\DecValTok{1974}\NormalTok{) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{stratvar =} \KeywordTok{interaction}\NormalTok{(year, vstrat)) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{as_survey_design}\NormalTok{(}\DataTypeTok{ids =}\NormalTok{ vpsu,}
                     \DataTypeTok{strata =}\NormalTok{ stratvar,}
                     \DataTypeTok{weights =}\NormalTok{ wtssall,}
                     \DataTypeTok{nest =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The two options set at the beginning provide some information to the survey library about how to behave. You should consult Lumley (2010) and the survey package documentation for details. The subsequent operations create \texttt{gss\_wt}, an object with one additional column (\texttt{stratvar}), describing the yearly sampling strata. We use the \texttt{interaction()} function to do this. It multiplies the \texttt{vstrat} variable by the \texttt{year} variable to get a vector of stratum information for each year. We have to do this because of the way the GSS codes its stratum information. In the next step, we use the \texttt{as\_survey\_design()} function to add the key pieces of information about the survey design. It adds information about the sampling identifiers (\texttt{ids}), the strata (\texttt{strata}), and the replicate weights (\texttt{weights}). With those in place we can take advantage of a large number of specialized functions in the survey library that allow us to calculate properly weighted survey means or estimate models with the correct sampling specification. For example, we can easily calculate the distribution of education by race for a series of years from 1976 to 2016. We use \texttt{survey\_mean()} to do this:\\

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{out_grp <-}\StringTok{ }\NormalTok{gss_wt }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{filter}\NormalTok{(year }\OperatorTok{%in%}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\DecValTok{1976}\NormalTok{, }\DecValTok{2016}\NormalTok{, }\DataTypeTok{by =} \DecValTok{4}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{group_by}\NormalTok{(year, race, degree) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{summarize}\NormalTok{(}\DataTypeTok{prop =} \KeywordTok{survey_mean}\NormalTok{(}\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{))}
\CommentTok{#> Warning: Factor `degree` contains implicit NA, consider using}
\CommentTok{#> `forcats::fct_explicit_na`}

\NormalTok{out_grp}
\CommentTok{#> # A tibble: 150 x 5}
\CommentTok{#>    year race  degree           prop prop_se}
\CommentTok{#>   <dbl> <fct> <fct>           <dbl>   <dbl>}
\CommentTok{#> 1  1976 White Lt High School 0.328  0.0160 }
\CommentTok{#> 2  1976 White High School    0.518  0.0162 }
\CommentTok{#> 3  1976 White Junior College 0.0129 0.00298}
\CommentTok{#> 4  1976 White Bachelor       0.101  0.00960}
\CommentTok{#> 5  1976 White Graduate       0.0393 0.00644}
\CommentTok{#> 6  1976 Black Lt High School 0.562  0.0611 }
\CommentTok{#> # ... with 144 more rows}
\end{Highlighting}
\end{Shaded}

The results returned in \texttt{out\_grp} include standard errors. We can also ask \texttt{survey\_mean()} to calculate confidence intervals for us, if we wish.

Grouping with \texttt{group\_by()} lets us calculate counts or means for the innermost variable, grouped by the next variable ``up'' or ``out'', in this case, degree by race, such that the proportions for degree will sum to one for each group in race, and this will be done separately for each value of year. If we want the marginal frequencies, such that the values for all combinations of race and degree sum to one within each year, we first have to interact the variables we are cross-classifying. Then we group by the new interacted variable and do the calculation as before:\\

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{out_mrg <-}\StringTok{ }\NormalTok{gss_wt }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{filter}\NormalTok{(year }\OperatorTok{%in%}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\DecValTok{1976}\NormalTok{, }\DecValTok{2016}\NormalTok{, }\DataTypeTok{by =} \DecValTok{4}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{racedeg =} \KeywordTok{interaction}\NormalTok{(race, degree)) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{group_by}\NormalTok{(year, racedeg) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{summarize}\NormalTok{(}\DataTypeTok{prop =} \KeywordTok{survey_mean}\NormalTok{(}\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{))}
\CommentTok{#> Warning: Factor `racedeg` contains implicit NA, consider using}
\CommentTok{#> `forcats::fct_explicit_na`}

\NormalTok{out_mrg}
\CommentTok{#> # A tibble: 150 x 4}
\CommentTok{#>    year racedeg                 prop prop_se}
\CommentTok{#>   <dbl> <fct>                  <dbl>   <dbl>}
\CommentTok{#> 1  1976 White.Lt High School 0.298   0.0146 }
\CommentTok{#> 2  1976 Black.Lt High School 0.0471  0.00840}
\CommentTok{#> 3  1976 Other.Lt High School 0.00195 0.00138}
\CommentTok{#> 4  1976 White.High School    0.471   0.0160 }
\CommentTok{#> 5  1976 Black.High School    0.0283  0.00594}
\CommentTok{#> 6  1976 Other.High School    0.00325 0.00166}
\CommentTok{#> # ... with 144 more rows}
\end{Highlighting}
\end{Shaded}

This gives us the numbers that we want and returns them in a tidy data frame. The \texttt{interaction()} function produces variable labels that are a compound of the two variables we interacted, with each combination of categories separated by a period, (such as White.Graduate. However, perhaps we would like to see these categories as two separate columns, one for race and one for education, as before. Because the variable labels are organized in a predictable way, we can use one of the convenient functions in the tidyverse's \texttt{tidyr} library to separate the single variable into two columns while correctly preserving the row values. Appropriately, this function is called \texttt{separate()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{out_mrg <-}\StringTok{ }\NormalTok{gss_wt }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{filter}\NormalTok{(year }\OperatorTok{%in%}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\DecValTok{1976}\NormalTok{, }\DecValTok{2016}\NormalTok{, }\DataTypeTok{by =} \DecValTok{4}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{racedeg =} \KeywordTok{interaction}\NormalTok{(race, degree)) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{group_by}\NormalTok{(year, racedeg) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{summarize}\NormalTok{(}\DataTypeTok{prop =} \KeywordTok{survey_mean}\NormalTok{(}\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{separate}\NormalTok{(racedeg, }\DataTypeTok{sep =} \StringTok{"}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{."}\NormalTok{, }\DataTypeTok{into =} \KeywordTok{c}\NormalTok{(}\StringTok{"race"}\NormalTok{, }\StringTok{"degree"}\NormalTok{))}
\CommentTok{#> Warning: Factor `racedeg` contains implicit NA, consider using}
\CommentTok{#> `forcats::fct_explicit_na`}

\NormalTok{out_mrg}
\CommentTok{#> # A tibble: 150 x 5}
\CommentTok{#>    year race  degree            prop prop_se}
\CommentTok{#>   <dbl> <chr> <chr>            <dbl>   <dbl>}
\CommentTok{#> 1  1976 White Lt High School 0.298   0.0146 }
\CommentTok{#> 2  1976 Black Lt High School 0.0471  0.00840}
\CommentTok{#> 3  1976 Other Lt High School 0.00195 0.00138}
\CommentTok{#> 4  1976 White High School    0.471   0.0160 }
\CommentTok{#> 5  1976 Black High School    0.0283  0.00594}
\CommentTok{#> 6  1976 Other High School    0.00325 0.00166}
\CommentTok{#> # ... with 144 more rows}
\end{Highlighting}
\end{Shaded}

The call to \texttt{separate()} says to take the \texttt{racedeg} column, split each value when it sees a period, and reorganize the results into two columns, race and degree. This gives us a tidy table much like \texttt{out\_grp}, but for the marginal frequencies.

Reasonable people can disagree over how best to plot a small multiple of a frequency table while faceting by year, especially when there is some measure of uncertainty attached. A barplot is the obvious approach for a single case, but when there are many years it can become difficult to compare bars across panels. This is especially the case when standard errors or confidence intervals are used in conjunction with bars.Sometimes it may be preferable to show that the underlying variable is categorical, as a bar chart makes clear, and not continuous, as a line graph suggests. Here the trade-off is in favor of the line graphs as the bars are very hard to compare across facets. This is sometimes called a ``dynamite plot'', not because it looks amazing but because the t-shaped error bars on the tops of the columns make them look like cartoon dynamite plungers. An alternative is to use a line graph to join up the time observations, faceting on educational categories instead of year. Figure 6.12 shows the results for our GSS data in \emph{dynamite-plot} form, where the error bars are defined as twice the standard error in either direction around the point estimate.\\

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \KeywordTok{subset}\NormalTok{(out_grp, race }\OperatorTok{%nin%}\StringTok{ "Other"}\NormalTok{),}
            \DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ degree, }\DataTypeTok{y =}\NormalTok{ prop,}
                          \DataTypeTok{ymin =}\NormalTok{ prop }\OperatorTok{-}\StringTok{ }\DecValTok{2}\OperatorTok{*}\NormalTok{prop_se,}
                          \DataTypeTok{ymax =}\NormalTok{ prop }\OperatorTok{+}\StringTok{ }\DecValTok{2}\OperatorTok{*}\NormalTok{prop_se,}
                          \DataTypeTok{fill =}\NormalTok{ race,}
                          \DataTypeTok{color =}\NormalTok{ race,}
                          \DataTypeTok{group =}\NormalTok{ race))}

\NormalTok{dodge <-}\StringTok{ }\KeywordTok{position_dodge}\NormalTok{(}\DataTypeTok{width=}\FloatTok{0.9}\NormalTok{)}

\NormalTok{p }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_col}\NormalTok{(}\DataTypeTok{position =}\NormalTok{ dodge, }\DataTypeTok{alpha =} \FloatTok{0.2}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{geom_errorbar}\NormalTok{(}\DataTypeTok{position =}\NormalTok{ dodge, }\DataTypeTok{width =} \FloatTok{0.2}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{scale_x_discrete}\NormalTok{(}\DataTypeTok{labels =}\NormalTok{ scales}\OperatorTok{::}\KeywordTok{wrap_format}\NormalTok{(}\DecValTok{10}\NormalTok{)) }\OperatorTok{+}
\StringTok{    }\KeywordTok{scale_y_continuous}\NormalTok{(}\DataTypeTok{labels =}\NormalTok{ scales}\OperatorTok{::}\NormalTok{percent) }\OperatorTok{+}
\StringTok{    }\KeywordTok{scale_color_brewer}\NormalTok{(}\DataTypeTok{type =} \StringTok{"qual"}\NormalTok{, }\DataTypeTok{palette =} \StringTok{"Dark2"}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{scale_fill_brewer}\NormalTok{(}\DataTypeTok{type =} \StringTok{"qual"}\NormalTok{, }\DataTypeTok{palette =} \StringTok{"Dark2"}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{title =} \StringTok{"Educational Attainment by Race"}\NormalTok{,}
         \DataTypeTok{subtitle =} \StringTok{"GSS 1976-2016"}\NormalTok{,}
         \DataTypeTok{fill =} \StringTok{"Race"}\NormalTok{,}
         \DataTypeTok{color =} \StringTok{"Race"}\NormalTok{,}
         \DataTypeTok{x =} \OtherTok{NULL}\NormalTok{, }\DataTypeTok{y =} \StringTok{"Percent"}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{facet_wrap}\NormalTok{(}\OperatorTok{~}\StringTok{ }\NormalTok{year, }\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{legend.position =} \StringTok{"top"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{misc_140-data_visualization-modeling_files/figure-latex/unnamed-chunk-47-1} \end{center}

This plot has a few cosmetic details and adjustments that we will learn more about in Chapter 8. As before, I encourage you to peel back the plot from the bottom, one instruction at a time, to see what changes. One useful adjustment to notice is the new call to the \texttt{scales} library to adjust the labels on the x-axis. The adjustment on the y-axis is familiar, \texttt{scales::percent} to convert the proportion to a percentage. On the x-axis, the issue is that several of the labels are rather long. If we do not adjust them they will print over one another. The \texttt{scales::wrap\_format()} function will break long labels into lines. It takes a single numerical argument (here 10) that is the maximum length a string can be before it is wrapped onto a new line.

Faceting by education instead.Figure 6.13: Faceting by education instead.
A graph like this is true to the categorical nature of the data, while showing the breakdown of groups within each year. But you should experiment with some alternatives. For example, we might decide that it is better to facet by degree category instead, and put the year on the x-axis within each panel. If we do that, then we can use \texttt{geom\_line()} to show a time trend, which is more natural, and \texttt{geom\_ribbon()} to show the error range. This is perhaps a better way to show the data, especially as it brings out the time trends within each degree category, and allows us to see the similarities and differences by racial classification at the same time.\\

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \KeywordTok{subset}\NormalTok{(out_grp, race }\OperatorTok{%nin%}\StringTok{ "Other"}\NormalTok{),}
            \DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ year, }\DataTypeTok{y =}\NormalTok{ prop, }\DataTypeTok{ymin =}\NormalTok{ prop }\OperatorTok{-}\StringTok{ }\DecValTok{2}\OperatorTok{*}\NormalTok{prop_se,}
                          \DataTypeTok{ymax =}\NormalTok{ prop }\OperatorTok{+}\StringTok{ }\DecValTok{2}\OperatorTok{*}\NormalTok{prop_se, }\DataTypeTok{fill =}\NormalTok{ race, }\DataTypeTok{color =}\NormalTok{ race,}
                          \DataTypeTok{group =}\NormalTok{ race))}

\NormalTok{p }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_ribbon}\NormalTok{(}\DataTypeTok{alpha =} \FloatTok{0.3}\NormalTok{, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{color =} \OtherTok{NULL}\NormalTok{)) }\OperatorTok{+}
\StringTok{    }\KeywordTok{geom_line}\NormalTok{() }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{facet_wrap}\NormalTok{(}\OperatorTok{~}\StringTok{ }\NormalTok{degree, }\DataTypeTok{ncol =} \DecValTok{1}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{scale_y_continuous}\NormalTok{(}\DataTypeTok{labels =}\NormalTok{ scales}\OperatorTok{::}\NormalTok{percent) }\OperatorTok{+}
\StringTok{    }\KeywordTok{scale_color_brewer}\NormalTok{(}\DataTypeTok{type =} \StringTok{"qual"}\NormalTok{, }\DataTypeTok{palette =} \StringTok{"Dark2"}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{scale_fill_brewer}\NormalTok{(}\DataTypeTok{type =} \StringTok{"qual"}\NormalTok{, }\DataTypeTok{palette =} \StringTok{"Dark2"}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{title =} \StringTok{"Educational Attainment}\CharTok{\textbackslash{}n}\StringTok{by Race"}\NormalTok{,}
         \DataTypeTok{subtitle =} \StringTok{"GSS 1976-2016"}\NormalTok{, }\DataTypeTok{fill =} \StringTok{"Race"}\NormalTok{,}
         \DataTypeTok{color =} \StringTok{"Race"}\NormalTok{, }\DataTypeTok{x =} \OtherTok{NULL}\NormalTok{, }\DataTypeTok{y =} \StringTok{"Percent"}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{legend.position =} \StringTok{"top"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{misc_140-data_visualization-modeling_files/figure-latex/unnamed-chunk-48-1} \end{center}

\hypertarget{where-to-go-next}{%
\section{Where to go next}\label{where-to-go-next}}

In general, when you estimate models and want to plot the results, the difficult step is not the plotting but rather calculating and extracting the right numbers. Generating predicted values and measures of confidence or uncertainty from models requires that you understand the model you are fitting, and the function you use to fit it, especially when it involves interactions, cross-level effects, or transformations of the predictor or response scales. The details can vary substantially from model type to model type, and also with the goals of any particular analysis. It is unwise to approach them mechanically. That said, several tools exist to help you work with model objects and produce a default set of plots from them.

\hypertarget{default-plots-for-models}{%
\subsection{Default plots for models}\label{default-plots-for-models}}

Just as model objects in R usually have a default \texttt{summary()} method, printing out an overview tailored to the type of model it is, they will usually have a default \texttt{plot()} method, too. Figures produced by \texttt{plot()} are typically not generated via \texttt{ggplot}, but it is usually worth exploring them. They typically make use of either R's base graphics or the \texttt{lattice} library (Sarkar, 2008). These are two plotting systems that we do not cover in this book. Default plot methods are easy to examine. Let's take a look again at our simple OLS model.\\

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{out <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(}\DataTypeTok{formula =}\NormalTok{ lifeExp }\OperatorTok{~}\StringTok{ }\KeywordTok{log}\NormalTok{(gdpPercap) }\OperatorTok{+}\StringTok{ }\NormalTok{pop }\OperatorTok{+}\StringTok{ }\NormalTok{continent, }\DataTypeTok{data =}\NormalTok{ gapminder)}
\end{Highlighting}
\end{Shaded}

To look at some of R's default plots for this model, use the \texttt{plot()} function.\\

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Plot not shown}
\KeywordTok{plot}\NormalTok{(out, }\DataTypeTok{which =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{), }\DataTypeTok{ask=}\OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{misc_140-data_visualization-modeling_files/figure-latex/unnamed-chunk-50-1} \includegraphics[width=0.7\linewidth]{misc_140-data_visualization-modeling_files/figure-latex/unnamed-chunk-50-2} \end{center}

The \texttt{which()} statement here selects the first two of four default plots for this kind of model. If you want to easily reproduce base R's default model graphics using \texttt{ggplot}, the \texttt{ggfortify} library is worth examining. It is in some ways similar to \texttt{broom}, in that it tidies the output of model objects, but it focuses on producing a standard plot (or group of plots) for a wide variety of model types. It does this by defining a function called \texttt{autoplot()}. The idea is to be able to use \texttt{autoplot()} with the output of many different kinds of model.

A second option worth looking at is the \texttt{coefplot} library. It provides a quick way to produce good-quality plots of point estimates and confidence intervals. It has the advantage of managing the estimation of interaction effects and other occasionally tricky calculations.\\

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(coefplot)}
\NormalTok{out <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(}\DataTypeTok{formula =}\NormalTok{ lifeExp }\OperatorTok{~}\StringTok{ }\KeywordTok{log}\NormalTok{(gdpPercap) }\OperatorTok{+}\StringTok{ }\KeywordTok{log}\NormalTok{(pop) }\OperatorTok{+}\StringTok{ }\NormalTok{continent, }\DataTypeTok{data =}\NormalTok{ gapminder)}

\KeywordTok{coefplot}\NormalTok{(out, }\DataTypeTok{sort =} \StringTok{"magnitude"}\NormalTok{, }\DataTypeTok{intercept =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{misc_140-data_visualization-modeling_files/figure-latex/unnamed-chunk-51-1} \end{center}

\hypertarget{tools-in-development}{%
\subsection{Tools in development}\label{tools-in-development}}

Tidyverse tools for modeling and model exploration are being actively developed. The \texttt{broom} and \texttt{margins} libraries continue to get more and more useful. There are also other projects worth paying attention to. The \texttt{infer} packageinfer.netlify.com is in its early stages but can already do useful things in a pipeline-friendly way. You can install it from CRAN with \texttt{install.packages("infer")}.

\hypertarget{extensions-to-ggplot}{%
\subsection{Extensions to ggplot}\label{extensions-to-ggplot}}

The \texttt{GGally} package provides a suite of functions designed to make producing standard but somewhat complex plots a little easier. For instance, it can produce generalized pairs plots, a useful way of quickly examining possible relationships between several different variables at once. This sort of plot is like the visual version of a correlation matrix. It shows a bivariate plot for all pairs of variables in the data. This is relatively straightforward when all the variables are continuous measures. Things get more complex when, as is often the case in the social sciences, some or all variables are categorical or otherwise limited in the range of values they can take. A generalized pairs plot can handle these cases. For example, Figure ?? shows a generalized pairs plot for five variables from the \texttt{organdata} dataset.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(GGally)}

\NormalTok{organdata_sm <-}\StringTok{ }\NormalTok{organdata }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{select}\NormalTok{(donors, pop_dens, pubhealth,}
\NormalTok{           roads, consent_law)}

\KeywordTok{ggpairs}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ organdata_sm,}
        \DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{color =}\NormalTok{ consent_law),}
        \DataTypeTok{upper =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{continuous =} \KeywordTok{wrap}\NormalTok{(}\StringTok{"density"}\NormalTok{), }\DataTypeTok{combo =} \StringTok{"box_no_facet"}\NormalTok{),}
        \DataTypeTok{lower =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{continuous =} \KeywordTok{wrap}\NormalTok{(}\StringTok{"points"}\NormalTok{), }\DataTypeTok{combo =} \KeywordTok{wrap}\NormalTok{(}\StringTok{"dot_no_facet"}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{misc_140-data_visualization-modeling_files/figure-latex/unnamed-chunk-52-1} \end{center}

Multi-panel plots like this are intrinsically very rich in information. When combined with several within-panel types of representation, or any more than a modest number of variables, they can become quite complex. They should be used less for the presentation of finished work, although it is possible. More often they are a useful tool for the working researcher to quickly investigate aspects of a dataset. The goal is not to pithily summarize a single point one already knows, but to open things up for further exploration.

\hypertarget{visualizing-residuals}{%
\chapter{Visualizing residuals}\label{visualizing-residuals}}

Source: \url{https://www.r-bloggers.com/visualising-residuals/}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(mpg }\OperatorTok{~}\StringTok{ }\NormalTok{hp, }\DataTypeTok{data =}\NormalTok{ mtcars)  }\CommentTok{# Fit the model}
\KeywordTok{summary}\NormalTok{(fit)  }\CommentTok{# Report the results}
\CommentTok{#> }
\CommentTok{#> Call:}
\CommentTok{#> lm(formula = mpg ~ hp, data = mtcars)}
\CommentTok{#> }
\CommentTok{#> Residuals:}
\CommentTok{#>    Min     1Q Median     3Q    Max }
\CommentTok{#> -5.712 -2.112 -0.885  1.582  8.236 }
\CommentTok{#> }
\CommentTok{#> Coefficients:}
\CommentTok{#>             Estimate Std. Error t value Pr(>|t|)    }
\CommentTok{#> (Intercept)  30.0989     1.6339   18.42  < 2e-16 ***}
\CommentTok{#> hp           -0.0682     0.0101   -6.74  1.8e-07 ***}
\CommentTok{#> ---}
\CommentTok{#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1}
\CommentTok{#> }
\CommentTok{#> Residual standard error: 3.86 on 30 degrees of freedom}
\CommentTok{#> Multiple R-squared:  0.602,  Adjusted R-squared:  0.589 }
\CommentTok{#> F-statistic: 45.5 on 1 and 30 DF,  p-value: 1.79e-07}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow =} \KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{))  }\CommentTok{# Split the plotting panel into a 2 x 2 grid}
\KeywordTok{plot}\NormalTok{(fit)  }\CommentTok{# Plot the model information}

\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{))  }\CommentTok{# Return plotting panel to 1 section}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{misc_906-visualizing_residuals_files/figure-latex/unnamed-chunk-3-1} \end{center}

\hypertarget{simple-linear-regression}{%
\section{Simple Linear Regression}\label{simple-linear-regression}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{d <-}\StringTok{ }\NormalTok{mtcars}
\NormalTok{fit <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(mpg }\OperatorTok{~}\StringTok{ }\NormalTok{hp, }\DataTypeTok{data =}\NormalTok{ d)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{d}\OperatorTok{$}\NormalTok{predicted <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(fit)   }\CommentTok{# Save the predicted values}
\NormalTok{d}\OperatorTok{$}\NormalTok{residuals <-}\StringTok{ }\KeywordTok{residuals}\NormalTok{(fit) }\CommentTok{# Save the residual values}

\CommentTok{# Quick look at the actual, predicted, and residual values}
\KeywordTok{library}\NormalTok{(dplyr)}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'dplyr'}
\CommentTok{#> The following objects are masked from 'package:stats':}
\CommentTok{#> }
\CommentTok{#>     filter, lag}
\CommentTok{#> The following objects are masked from 'package:base':}
\CommentTok{#> }
\CommentTok{#>     intersect, setdiff, setequal, union}
\NormalTok{d }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(mpg, predicted, residuals) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{head}\NormalTok{()}
\CommentTok{#>                    mpg predicted residuals}
\CommentTok{#> Mazda RX4         21.0      22.6    -1.594}
\CommentTok{#> Mazda RX4 Wag     21.0      22.6    -1.594}
\CommentTok{#> Datsun 710        22.8      23.8    -0.954}
\CommentTok{#> Hornet 4 Drive    21.4      22.6    -1.194}
\CommentTok{#> Hornet Sportabout 18.7      18.2     0.541}
\CommentTok{#> Valiant           18.1      22.9    -4.835}
\end{Highlighting}
\end{Shaded}

\hypertarget{step-3-plot-the-actual-and-predicted-values}{%
\subsection{Step 3: plot the actual and predicted values}\label{step-3-plot-the-actual-and-predicted-values}}

\begin{quote}
plot first the actual data
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(ggplot2)}
\CommentTok{#> Registered S3 methods overwritten by 'ggplot2':}
\CommentTok{#>   method         from }
\CommentTok{#>   [.quosures     rlang}
\CommentTok{#>   c.quosures     rlang}
\CommentTok{#>   print.quosures rlang}
\KeywordTok{ggplot}\NormalTok{(d, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ hp, }\DataTypeTok{y =}\NormalTok{ mpg)) }\OperatorTok{+}\StringTok{  }\CommentTok{# Set up canvas with outcome variable on y-axis}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{()  }\CommentTok{# Plot the actual points}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{misc_906-visualizing_residuals_files/figure-latex/unnamed-chunk-6-1} \end{center}

\begin{quote}
Next, we plot the predicted values in a way that they're distinguishable from the actual values. For example, let's change their shape:
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(d, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ hp, }\DataTypeTok{y =}\NormalTok{ mpg)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{y =}\NormalTok{ predicted), }\DataTypeTok{shape =} \DecValTok{1}\NormalTok{)  }\CommentTok{# Add the predicted values}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{misc_906-visualizing_residuals_files/figure-latex/unnamed-chunk-7-1} \end{center}

\begin{quote}
This is on track, but it's difficult to see how our actual and predicted values are related. Let's connect the actual data points with their corresponding predicted value using geom\_segment():
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(d, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ hp, }\DataTypeTok{y =}\NormalTok{ mpg)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_segment}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{xend =}\NormalTok{ hp, }\DataTypeTok{yend =}\NormalTok{ predicted)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{y =}\NormalTok{ predicted), }\DataTypeTok{shape =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{misc_906-visualizing_residuals_files/figure-latex/unnamed-chunk-8-1} \end{center}

\begin{quote}
We'll make a few final adjustments:
* Clean up the overall look with theme\_bw().
* Fade out connection lines by adjusting their alpha.
* Add the regression slope with geom\_smooth():
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]

\KeywordTok{library}\NormalTok{(ggplot2)}
\KeywordTok{ggplot}\NormalTok{(d, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ hp, }\DataTypeTok{y =}\NormalTok{ mpg)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{method =} \StringTok{"lm"}\NormalTok{, }\DataTypeTok{se =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{color =} \StringTok{"lightgrey"}\NormalTok{) }\OperatorTok{+}\StringTok{  }\CommentTok{# Plot regression slope}
\StringTok{  }\KeywordTok{geom_segment}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{xend =}\NormalTok{ hp, }\DataTypeTok{yend =}\NormalTok{ predicted), }\DataTypeTok{alpha =} \FloatTok{.2}\NormalTok{) }\OperatorTok{+}\StringTok{  }\CommentTok{# alpha to fade lines}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{y =}\NormalTok{ predicted), }\DataTypeTok{shape =} \DecValTok{1}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_bw}\NormalTok{()  }\CommentTok{# Add theme for cleaner look}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{misc_906-visualizing_residuals_files/figure-latex/unnamed-chunk-9-1} \end{center}

\hypertarget{step-4-use-residuals-to-adjust}{%
\section{Step 4: use residuals to adjust}\label{step-4-use-residuals-to-adjust}}

\begin{quote}
Finally, we want to make an adjustment to highlight the size of the residual. There are MANY options. To make comparisons easy, I'll make adjustments to the actual values, but you could just as easily apply these, or other changes, to the predicted values. Here are a few examples building on the previous plot:
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]


\CommentTok{# ALPHA}
\CommentTok{# Changing alpha of actual values based on absolute value of residuals}
\KeywordTok{ggplot}\NormalTok{(d, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ hp, }\DataTypeTok{y =}\NormalTok{ mpg)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{method =} \StringTok{"lm"}\NormalTok{, }\DataTypeTok{se =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{color =} \StringTok{"lightgrey"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_segment}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{xend =}\NormalTok{ hp, }\DataTypeTok{yend =}\NormalTok{ predicted), }\DataTypeTok{alpha =} \FloatTok{.2}\NormalTok{) }\OperatorTok{+}

\StringTok{  }\CommentTok{# > Alpha adjustments made here...}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{alpha =} \KeywordTok{abs}\NormalTok{(residuals))) }\OperatorTok{+}\StringTok{  }\CommentTok{# Alpha mapped to abs(residuals)}
\StringTok{  }\KeywordTok{guides}\NormalTok{(}\DataTypeTok{alpha =} \OtherTok{FALSE}\NormalTok{) }\OperatorTok{+}\StringTok{  }\CommentTok{# Alpha legend removed}
\StringTok{  }\CommentTok{# <}

\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{y =}\NormalTok{ predicted), }\DataTypeTok{shape =} \DecValTok{1}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{misc_906-visualizing_residuals_files/figure-latex/unnamed-chunk-10-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# COLOR}
\CommentTok{# High residuals (in abolsute terms) made more red on actual values.}
\KeywordTok{ggplot}\NormalTok{(d, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ hp, }\DataTypeTok{y =}\NormalTok{ mpg)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{method =} \StringTok{"lm"}\NormalTok{, }\DataTypeTok{se =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{color =} \StringTok{"lightgrey"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_segment}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{xend =}\NormalTok{ hp, }\DataTypeTok{yend =}\NormalTok{ predicted), }\DataTypeTok{alpha =} \FloatTok{.2}\NormalTok{) }\OperatorTok{+}

\StringTok{  }\CommentTok{# > Color adjustments made here...}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{color =} \KeywordTok{abs}\NormalTok{(residuals))) }\OperatorTok{+}\StringTok{ }\CommentTok{# Color mapped to abs(residuals)}
\StringTok{  }\KeywordTok{scale_color_continuous}\NormalTok{(}\DataTypeTok{low =} \StringTok{"black"}\NormalTok{, }\DataTypeTok{high =} \StringTok{"red"}\NormalTok{) }\OperatorTok{+}\StringTok{  }\CommentTok{# Colors to use here}
\StringTok{  }\KeywordTok{guides}\NormalTok{(}\DataTypeTok{color =} \OtherTok{FALSE}\NormalTok{) }\OperatorTok{+}\StringTok{  }\CommentTok{# Color legend removed}
\StringTok{  }\CommentTok{# <}

\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{y =}\NormalTok{ predicted), }\DataTypeTok{shape =} \DecValTok{1}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{misc_906-visualizing_residuals_files/figure-latex/unnamed-chunk-11-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# SIZE AND COLOR}
\CommentTok{# Same coloring as above, size corresponding as well}
\KeywordTok{ggplot}\NormalTok{(d, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ hp, }\DataTypeTok{y =}\NormalTok{ mpg)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{method =} \StringTok{"lm"}\NormalTok{, }\DataTypeTok{se =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{color =} \StringTok{"lightgrey"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_segment}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{xend =}\NormalTok{ hp, }\DataTypeTok{yend =}\NormalTok{ predicted), }\DataTypeTok{alpha =} \FloatTok{.2}\NormalTok{) }\OperatorTok{+}

\StringTok{  }\CommentTok{# > Color AND size adjustments made here...}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{color =} \KeywordTok{abs}\NormalTok{(residuals), }\DataTypeTok{size =} \KeywordTok{abs}\NormalTok{(residuals))) }\OperatorTok{+}\StringTok{ }\CommentTok{# size also mapped}
\StringTok{  }\KeywordTok{scale_color_continuous}\NormalTok{(}\DataTypeTok{low =} \StringTok{"black"}\NormalTok{, }\DataTypeTok{high =} \StringTok{"red"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{guides}\NormalTok{(}\DataTypeTok{color =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{size =} \OtherTok{FALSE}\NormalTok{) }\OperatorTok{+}\StringTok{  }\CommentTok{# Size legend also removed}
\StringTok{  }\CommentTok{# <}

\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{y =}\NormalTok{ predicted), }\DataTypeTok{shape =} \DecValTok{1}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{misc_906-visualizing_residuals_files/figure-latex/unnamed-chunk-12-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# COLOR UNDER/OVER}
\CommentTok{# Color mapped to residual with sign taken into account.}
\CommentTok{# i.e., whether actual value is greater or less than predicted}
\KeywordTok{ggplot}\NormalTok{(d, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ hp, }\DataTypeTok{y =}\NormalTok{ mpg)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{method =} \StringTok{"lm"}\NormalTok{, }\DataTypeTok{se =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{color =} \StringTok{"lightgrey"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_segment}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{xend =}\NormalTok{ hp, }\DataTypeTok{yend =}\NormalTok{ predicted), }\DataTypeTok{alpha =} \FloatTok{.2}\NormalTok{) }\OperatorTok{+}

\StringTok{  }\CommentTok{# > Color adjustments made here...}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{color =}\NormalTok{ residuals)) }\OperatorTok{+}\StringTok{  }\CommentTok{# Color mapped here}
\StringTok{  }\KeywordTok{scale_color_gradient2}\NormalTok{(}\DataTypeTok{low =} \StringTok{"blue"}\NormalTok{, }\DataTypeTok{mid =} \StringTok{"white"}\NormalTok{, }\DataTypeTok{high =} \StringTok{"red"}\NormalTok{) }\OperatorTok{+}\StringTok{  }\CommentTok{# Colors to use here}
\StringTok{  }\KeywordTok{guides}\NormalTok{(}\DataTypeTok{color =} \OtherTok{FALSE}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\CommentTok{# <}

\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{y =}\NormalTok{ predicted), }\DataTypeTok{shape =} \DecValTok{1}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{misc_906-visualizing_residuals_files/figure-latex/unnamed-chunk-13-1} \end{center}

I particularly like this last example, because the colours nicely help to identify non-linearity in the data. For example, we can see that there is more red for extreme values of hp where the actual values are greater than what is being predicted. There is more blue in the centre, however, indicating that the actual values are less than what is being predicted. Together, this suggests that the relationship between the variables is non-linear, and might be better modelled by including a quadratic term in the regression equation.

\hypertarget{part-neural-networks}{%
\part{Neural Networks}\label{part-neural-networks}}

\hypertarget{building-deep-neural-nets-with-h2o-that-predict-arrhythmia-of-the-heart}{%
\chapter{Building deep neural nets with h2o that predict arrhythmia of the heart}\label{building-deep-neural-nets-with-h2o-that-predict-arrhythmia-of-the-heart}}

\hypertarget{introduction-17}{%
\section{Introduction}\label{introduction-17}}

27 February 2017

This week, I am showing how to build feed-forward deep neural networks or multilayer perceptrons. The models in this example are built to classify ECG data into being either from healthy hearts or from someone suffering from arrhythmia. I will show how to prepare a dataset for modeling, setting weights and other modeling parameters, and finally, how to evaluate model performance with the \texttt{h2o} package.

\hypertarget{deep-learning-with-neural-networks}{%
\subsection{Deep learning with neural networks}\label{deep-learning-with-neural-networks}}

Deep learning with neural networks is arguably one of the most rapidly growing applications of machine learning and AI today. They allow building complex models that consist of multiple hidden layers within artificial networks and are able to find non-linear patterns in unstructured data. Deep neural networks are usually feed-forward, which means that each layer feeds its output to subsequent layers, but recurrent or feed-back neural networks can also be built. Feed-forward neural networks are also called multilayer perceptrons (MLPs).

\hypertarget{h2o}{%
\subsection{H2O}\label{h2o}}

The R package h2o provides a convenient interface to \textbf{H2O}, which is an open-source machine learning and deep learning platform. H2O distributes a wide range of common machine learning algorithms for classification, regression and deep learning.

\hypertarget{preparing-the-r-session}{%
\subsection{Preparing the R session}\label{preparing-the-r-session}}

First, we need to load the packages.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(dplyr)}
\KeywordTok{library}\NormalTok{(h2o)}
\KeywordTok{library}\NormalTok{(ggplot2)}
\KeywordTok{library}\NormalTok{(ggrepel)}
\KeywordTok{library}\NormalTok{(h2o)}

\KeywordTok{h2o.init}\NormalTok{()}
\CommentTok{#> }
\CommentTok{#> H2O is not running yet, starting it now...}
\CommentTok{#> }
\CommentTok{#> Note:  In case of errors look at the following log files:}
\CommentTok{#>     /tmp/RtmpQvWp1Q/h2o_datascience_started_from_r.out}
\CommentTok{#>     /tmp/RtmpQvWp1Q/h2o_datascience_started_from_r.err}
\CommentTok{#> }
\CommentTok{#> }
\CommentTok{#> Starting H2O JVM and connecting: . Connection successful!}
\CommentTok{#> }
\CommentTok{#> R is connected to the H2O cluster: }
\CommentTok{#>     H2O cluster uptime:         1 seconds 256 milliseconds }
\CommentTok{#>     H2O cluster timezone:       America/Chicago }
\CommentTok{#>     H2O data parsing timezone:  UTC }
\CommentTok{#>     H2O cluster version:        3.22.1.1 }
\CommentTok{#>     H2O cluster version age:    8 months and 23 days !!! }
\CommentTok{#>     H2O cluster name:           H2O_started_from_R_datascience_mwl453 }
\CommentTok{#>     H2O cluster total nodes:    1 }
\CommentTok{#>     H2O cluster total memory:   6.96 GB }
\CommentTok{#>     H2O cluster total cores:    8 }
\CommentTok{#>     H2O cluster allowed cores:  8 }
\CommentTok{#>     H2O cluster healthy:        TRUE }
\CommentTok{#>     H2O Connection ip:          localhost }
\CommentTok{#>     H2O Connection port:        54321 }
\CommentTok{#>     H2O Connection proxy:       NA }
\CommentTok{#>     H2O Internal Security:      FALSE }
\CommentTok{#>     H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4 }
\CommentTok{#>     R Version:                  R version 3.6.0 (2019-04-26)}
\CommentTok{#> Warning in h2o.clusterInfo(): }
\CommentTok{#> Your H2O cluster version is too old (8 months and 23 days)!}
\CommentTok{#> Please download and install the latest version from http://h2o.ai/download/}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my_theme <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(}\DataTypeTok{base_size =} \DecValTok{12}\NormalTok{, }\DataTypeTok{base_family =} \StringTok{"sans"}\NormalTok{)\{}
  \KeywordTok{theme_minimal}\NormalTok{(}\DataTypeTok{base_size =}\NormalTok{ base_size, }\DataTypeTok{base_family =}\NormalTok{ base_family) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}
    \DataTypeTok{axis.text =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{size =} \DecValTok{12}\NormalTok{),}
    \DataTypeTok{axis.title =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{size =} \DecValTok{14}\NormalTok{),}
    \DataTypeTok{panel.grid.major =} \KeywordTok{element_line}\NormalTok{(}\DataTypeTok{color =} \StringTok{"grey"}\NormalTok{),}
    \DataTypeTok{panel.grid.minor =} \KeywordTok{element_blank}\NormalTok{(),}
    \DataTypeTok{panel.background =} \KeywordTok{element_rect}\NormalTok{(}\DataTypeTok{fill =} \StringTok{"aliceblue"}\NormalTok{),}
    \DataTypeTok{strip.background =} \KeywordTok{element_rect}\NormalTok{(}\DataTypeTok{fill =} \StringTok{"darkgrey"}\NormalTok{, }\DataTypeTok{color =} \StringTok{"grey"}\NormalTok{, }\DataTypeTok{size =} \DecValTok{1}\NormalTok{),}
    \DataTypeTok{strip.text =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{face =} \StringTok{"bold"}\NormalTok{, }\DataTypeTok{size =} \DecValTok{12}\NormalTok{, }\DataTypeTok{color =} \StringTok{"white"}\NormalTok{),}
    \DataTypeTok{legend.position =} \StringTok{"right"}\NormalTok{,}
    \DataTypeTok{legend.justification =} \StringTok{"top"}\NormalTok{, }
    \DataTypeTok{panel.border =} \KeywordTok{element_rect}\NormalTok{(}\DataTypeTok{color =} \StringTok{"grey"}\NormalTok{, }\DataTypeTok{fill =} \OtherTok{NA}\NormalTok{, }\DataTypeTok{size =} \FloatTok{0.5}\NormalTok{)}
\NormalTok{  )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\hypertarget{arrhythmia-data}{%
\section{Arrhythmia data}\label{arrhythmia-data}}

The data I am using to demonstrate the building of neural nets is the arrhythmia dataset from \href{https://archive.ics.uci.edu/ml/datasets/Arrhythmia}{UC Irvine's machine learning database}. It contains 279 features from ECG heart rhythm diagnostics and one output column. I am not going to rename the feature columns because they are too many and the descriptions are too complex. Also, we don't need to know specifically which features we are looking at for building the models.

\begin{quote}
For a description of each feature, see \url{https://archive.ics.uci.edu/ml/machine-learning-databases/arrhythmia/arrhythmia.names}.
\end{quote}

The output column defines 16 classes: \emph{class 1} samples are from healthy ECGs, the remaining classes belong to different types of arrhythmia, with \emph{class 16} being all remaining arrhythmia cases that didn't fit into distinct classes.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{arrhythmia <-}\StringTok{ }\KeywordTok{read.table}\NormalTok{(}\KeywordTok{file.path}\NormalTok{(data_raw_dir, }\StringTok{"arrhythmia.data.txt"}\NormalTok{), }\DataTypeTok{sep =} \StringTok{","}\NormalTok{)}
\NormalTok{arrhythmia[arrhythmia }\OperatorTok{==}\StringTok{ "?"}\NormalTok{] <-}\StringTok{ }\OtherTok{NA}

\CommentTok{# making sure, that all feature columns are numeric}
\NormalTok{arrhythmia[}\OperatorTok{-}\DecValTok{280}\NormalTok{] <-}\StringTok{ }\KeywordTok{lapply}\NormalTok{(arrhythmia[}\OperatorTok{-}\DecValTok{280}\NormalTok{], as.character)}
\NormalTok{arrhythmia[}\OperatorTok{-}\DecValTok{280}\NormalTok{] <-}\StringTok{ }\KeywordTok{lapply}\NormalTok{(arrhythmia[}\OperatorTok{-}\DecValTok{280}\NormalTok{], as.numeric)}

\CommentTok{#  renaming output column and converting to factor}
\KeywordTok{colnames}\NormalTok{(arrhythmia)[}\DecValTok{280}\NormalTok{] <-}\StringTok{ "class"}
\NormalTok{arrhythmia}\OperatorTok{$}\NormalTok{class <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(arrhythmia}\OperatorTok{$}\NormalTok{class)}
\end{Highlighting}
\end{Shaded}

As usual, I want to get acquainted with the data and explore it's properties before I am building any model. So, I am first going to look at the distribution of classes and of healthy and arrhythmia samples.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p1 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(arrhythmia, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ class)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{fill =} \StringTok{"navy"}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{0.7}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{my_theme}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

Because I am interested in distinguishing healthy from arrhythmia ECGs, I am converting the output to binary format by combining all arrhythmia cases into one class.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#  all arrhythmia cases into one class}
\NormalTok{arrhythmia}\OperatorTok{$}\NormalTok{diagnosis <-}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(arrhythmia}\OperatorTok{$}\NormalTok{class }\OperatorTok{==}\StringTok{ }\DecValTok{1}\NormalTok{, }\StringTok{"healthy"}\NormalTok{, }\StringTok{"arrhythmia"}\NormalTok{)}
\NormalTok{arrhythmia}\OperatorTok{$}\NormalTok{diagnosis <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(arrhythmia}\OperatorTok{$}\NormalTok{diagnosis)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p2 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(arrhythmia, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ diagnosis)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{fill =} \StringTok{"navy"}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{0.7}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{my_theme}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(gridExtra)}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'gridExtra'}
\CommentTok{#> The following object is masked from 'package:dplyr':}
\CommentTok{#> }
\CommentTok{#>     combine}
\KeywordTok{library}\NormalTok{(grid)}

\KeywordTok{grid.arrange}\NormalTok{(p1, p2, }\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{nn-classification_239b-deeplearning_h2o_arrhythmia-sglander_files/figure-latex/plot-cases-1} \end{center}

With binary classification, we have almost the same numbers of healthy and arrhythmia cases in our dataset.

I am also interested in how much the normal and arrhythmia cases cluster in a Principal Component Analysis (PCA). I am first preparing the PCA plotting function and then run it on the feature data.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(pcaGoPromoter)}
\CommentTok{#> Warning: replacing previous import 'BiocGenerics::boxplot' by}
\CommentTok{#> 'graphics::boxplot' when loading 'Biostrings'}
\CommentTok{#> Warning: replacing previous import 'BiocGenerics::image' by}
\CommentTok{#> 'graphics::image' when loading 'Biostrings'}
\CommentTok{#> Warning: replacing previous import 'S4Vectors::na.exclude' by}
\CommentTok{#> 'stats::na.exclude' when loading 'Biostrings'}
\CommentTok{#> Warning: replacing previous import 'IRanges::smoothEnds' by}
\CommentTok{#> 'stats::smoothEnds' when loading 'Biostrings'}
\CommentTok{#> Warning: replacing previous import 'BiocGenerics::density' by}
\CommentTok{#> 'stats::density' when loading 'Biostrings'}
\CommentTok{#> Warning: replacing previous import 'IRanges::mad' by 'stats::mad' when}
\CommentTok{#> loading 'Biostrings'}
\CommentTok{#> Warning: replacing previous import 'S4Vectors::na.omit' by 'stats::na.omit'}
\CommentTok{#> when loading 'Biostrings'}
\CommentTok{#> Warning: replacing previous import 'S4Vectors::complete.cases' by}
\CommentTok{#> 'stats::complete.cases' when loading 'Biostrings'}
\CommentTok{#> Warning: replacing previous import 'IRanges::runmed' by 'stats::runmed'}
\CommentTok{#> when loading 'Biostrings'}
\CommentTok{#> Warning: replacing previous import 'IRanges::start' by 'stats::start' when}
\CommentTok{#> loading 'Biostrings'}
\CommentTok{#> Warning: replacing previous import 'IRanges::window<-' by 'stats::window<-'}
\CommentTok{#> when loading 'Biostrings'}
\CommentTok{#> Warning: replacing previous import 'S4Vectors::window' by 'stats::window'}
\CommentTok{#> when loading 'Biostrings'}
\CommentTok{#> Warning: replacing previous import 'S4Vectors::aggregate' by}
\CommentTok{#> 'stats::aggregate' when loading 'Biostrings'}
\CommentTok{#> Warning: replacing previous import 'BiocGenerics::weights' by}
\CommentTok{#> 'stats::weights' when loading 'Biostrings'}
\CommentTok{#> Warning: replacing previous import 'IRanges::cor' by 'stats::cor' when}
\CommentTok{#> loading 'Biostrings'}
\CommentTok{#> Warning: replacing previous import 'IRanges::cov' by 'stats::cov' when}
\CommentTok{#> loading 'Biostrings'}
\CommentTok{#> Warning: replacing previous import 'IRanges::quantile' by 'stats::quantile'}
\CommentTok{#> when loading 'Biostrings'}
\CommentTok{#> Warning: replacing previous import 'IRanges::end' by 'stats::end' when}
\CommentTok{#> loading 'Biostrings'}
\CommentTok{#> Warning: replacing previous import 'BiocGenerics::residuals' by}
\CommentTok{#> 'stats::residuals' when loading 'Biostrings'}
\CommentTok{#> Warning: replacing previous import 'IRanges::median' by 'stats::median'}
\CommentTok{#> when loading 'Biostrings'}
\CommentTok{#> Warning: replacing previous import 'IRanges::sd' by 'stats::sd' when}
\CommentTok{#> loading 'Biostrings'}
\CommentTok{#> Warning: replacing previous import 'IRanges::var' by 'stats::var' when}
\CommentTok{#> loading 'Biostrings'}
\CommentTok{#> Warning: replacing previous import 'S4Vectors::xtabs' by 'stats::xtabs'}
\CommentTok{#> when loading 'Biostrings'}
\CommentTok{#> Warning: replacing previous import 'IRanges::IQR' by 'stats::IQR' when}
\CommentTok{#> loading 'Biostrings'}
\CommentTok{#> Warning: replacing previous import 'S4Vectors::tail' by 'utils::tail' when}
\CommentTok{#> loading 'Biostrings'}
\CommentTok{#> Warning: replacing previous import 'IRanges::stack' by 'utils::stack' when}
\CommentTok{#> loading 'Biostrings'}
\CommentTok{#> Warning: replacing previous import 'XVector::relist' by 'utils::relist'}
\CommentTok{#> when loading 'Biostrings'}
\CommentTok{#> Warning: replacing previous import 'S4Vectors::head' by 'utils::head' when}
\CommentTok{#> loading 'Biostrings'}

\NormalTok{pca_func <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(pcaOutput2, group_name)\{}
\NormalTok{    centroids <-}\StringTok{ }\KeywordTok{aggregate}\NormalTok{(}\KeywordTok{cbind}\NormalTok{(PC1, PC2) }\OperatorTok{~}\StringTok{ }\NormalTok{groups, pcaOutput2, mean)}
\NormalTok{    conf.rgn  <-}\StringTok{ }\KeywordTok{do.call}\NormalTok{(rbind, }\KeywordTok{lapply}\NormalTok{(}\KeywordTok{unique}\NormalTok{(pcaOutput2}\OperatorTok{$}\NormalTok{groups), }\ControlFlowTok{function}\NormalTok{(t)}
          \KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{groups =} \KeywordTok{as.character}\NormalTok{(t),}
                     \KeywordTok{ellipse}\NormalTok{(}\KeywordTok{cov}\NormalTok{(pcaOutput2[pcaOutput2}\OperatorTok{$}\NormalTok{groups }\OperatorTok{==}\StringTok{ }\NormalTok{t, }\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{]),}
                           \DataTypeTok{centre =} \KeywordTok{as.matrix}\NormalTok{(centroids[centroids}\OperatorTok{$}\NormalTok{groups }\OperatorTok{==}\StringTok{ }\NormalTok{t, }\DecValTok{2}\OperatorTok{:}\DecValTok{3}\NormalTok{]),}
                           \DataTypeTok{level =} \FloatTok{0.95}\NormalTok{),}
                     \DataTypeTok{stringsAsFactors =} \OtherTok{FALSE}\NormalTok{)))}
        
\NormalTok{    plot <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ pcaOutput2, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ PC1, }\DataTypeTok{y =}\NormalTok{ PC2, }\DataTypeTok{group =}\NormalTok{ groups, }
                                          \DataTypeTok{color =}\NormalTok{ groups)) }\OperatorTok{+}\StringTok{ }
\StringTok{      }\KeywordTok{geom_polygon}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ conf.rgn, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{fill =}\NormalTok{ groups), }\DataTypeTok{alpha =} \FloatTok{0.2}\NormalTok{) }\OperatorTok{+}
\StringTok{      }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{size =} \DecValTok{2}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{0.5}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{      }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{color =} \KeywordTok{paste}\NormalTok{(group_name),}
           \DataTypeTok{fill =} \KeywordTok{paste}\NormalTok{(group_name),}
           \DataTypeTok{x =} \KeywordTok{paste0}\NormalTok{(}\StringTok{"PC1: "}\NormalTok{, }\KeywordTok{round}\NormalTok{(pcaOutput}\OperatorTok{$}\NormalTok{pov[}\DecValTok{1}\NormalTok{], }\DataTypeTok{digits =} \DecValTok{2}\NormalTok{) }\OperatorTok{*}\StringTok{ }\DecValTok{100}\NormalTok{, }\StringTok{"% variance"}\NormalTok{),}
           \DataTypeTok{y =} \KeywordTok{paste0}\NormalTok{(}\StringTok{"PC2: "}\NormalTok{, }\KeywordTok{round}\NormalTok{(pcaOutput}\OperatorTok{$}\NormalTok{pov[}\DecValTok{2}\NormalTok{], }\DataTypeTok{digits =} \DecValTok{2}\NormalTok{) }\OperatorTok{*}\StringTok{ }\DecValTok{100}\NormalTok{, }\StringTok{"% variance"}\NormalTok{)) }\OperatorTok{+}
\StringTok{      }\KeywordTok{my_theme}\NormalTok{()}
    
    \KeywordTok{return}\NormalTok{(plot)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Find what columns have NAs and the quantity}
\ControlFlowTok{for}\NormalTok{ (col }\ControlFlowTok{in} \KeywordTok{names}\NormalTok{(arrhythmia)) \{}
\NormalTok{    n_nas <-}\StringTok{ }\KeywordTok{length}\NormalTok{(}\KeywordTok{which}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(arrhythmia[, col])))}
    \ControlFlowTok{if}\NormalTok{ (n_nas }\OperatorTok{>}\StringTok{ }\DecValTok{0}\NormalTok{) }\KeywordTok{cat}\NormalTok{(col, n_nas, }\StringTok{"}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\NormalTok{\}}
\CommentTok{#> V11 8 }
\CommentTok{#> V12 22 }
\CommentTok{#> V13 1 }
\CommentTok{#> V14 376 }
\CommentTok{#> V15 1}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Replace NAs with zeros}
\NormalTok{arrhythmia[}\KeywordTok{is.na}\NormalTok{(arrhythmia)] <-}\StringTok{ }\DecValTok{0}
\end{Highlighting}
\end{Shaded}

Find and plot the PCAs.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pcaOutput <-}\StringTok{ }\KeywordTok{pca}\NormalTok{(}\KeywordTok{t}\NormalTok{(arrhythmia[}\OperatorTok{-}\KeywordTok{c}\NormalTok{(}\DecValTok{280}\NormalTok{, }\DecValTok{281}\NormalTok{)]), }\DataTypeTok{printDropped=}\OtherTok{FALSE}\NormalTok{, }
                 \DataTypeTok{scale=}\OtherTok{TRUE}\NormalTok{, }
                 \DataTypeTok{center =} \OtherTok{TRUE}\NormalTok{)}

\NormalTok{pcaOutput2 <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(pcaOutput}\OperatorTok{$}\NormalTok{scores)}

\NormalTok{pcaOutput2}\OperatorTok{$}\NormalTok{groups <-}\StringTok{ }\NormalTok{arrhythmia}\OperatorTok{$}\NormalTok{class}
\NormalTok{p1 <-}\StringTok{ }\KeywordTok{pca_func}\NormalTok{(pcaOutput2, }\DataTypeTok{group_name =} \StringTok{"class"}\NormalTok{)}

\NormalTok{pcaOutput2}\OperatorTok{$}\NormalTok{groups <-}\StringTok{ }\NormalTok{arrhythmia}\OperatorTok{$}\NormalTok{diagnosis}
\NormalTok{p2 <-}\StringTok{ }\KeywordTok{pca_func}\NormalTok{(pcaOutput2, }\DataTypeTok{group_name =} \StringTok{"diagnosis"}\NormalTok{)}

\KeywordTok{grid.arrange}\NormalTok{(p1, p2, }\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{nn-classification_239b-deeplearning_h2o_arrhythmia-sglander_files/figure-latex/plot-pca-1} \end{center}

The PCA shows that there is a big overlap between healthy and arrhythmia samples, i.e.~there does not seem to be major global differences in all features. The class that is most distinct from all others seems to be class 9.

I want to give the arrhythmia cases that are very different from the rest a stronger weight in the neural network, so I define a \textbf{weight column} where every sample outside the central PCA cluster will get a ``2'', they will in effect be used twice in the model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{weights <-}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(pcaOutput2}\OperatorTok{$}\NormalTok{PC1 }\OperatorTok{<}\StringTok{ }\DecValTok{-5} \OperatorTok{&}\StringTok{ }\KeywordTok{abs}\NormalTok{(pcaOutput2}\OperatorTok{$}\NormalTok{PC2) }\OperatorTok{>}\StringTok{ }\DecValTok{10}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

I also want to know what the variance is within features.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(matrixStats)}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'matrixStats'}
\CommentTok{#> The following object is masked from 'package:dplyr':}
\CommentTok{#> }
\CommentTok{#>     count}

\NormalTok{colvars <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{feature =} \KeywordTok{colnames}\NormalTok{(arrhythmia[}\OperatorTok{-}\KeywordTok{c}\NormalTok{(}\DecValTok{280}\NormalTok{, }\DecValTok{281}\NormalTok{)]),}
                      \DataTypeTok{variance =} \KeywordTok{colVars}\NormalTok{(}\KeywordTok{as.matrix}\NormalTok{(arrhythmia[}\OperatorTok{-}\KeywordTok{c}\NormalTok{(}\DecValTok{280}\NormalTok{, }\DecValTok{281}\NormalTok{)])))}

\KeywordTok{subset}\NormalTok{(colvars, variance }\OperatorTok{>}\StringTok{ }\DecValTok{50}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{feature =} \KeywordTok{factor}\NormalTok{(feature, }\DataTypeTok{levels =} \KeywordTok{colnames}\NormalTok{(arrhythmia[}\OperatorTok{-}\KeywordTok{c}\NormalTok{(}\DecValTok{280}\NormalTok{, }\DecValTok{281}\NormalTok{)]))) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ feature, }\DataTypeTok{y =}\NormalTok{ variance)) }\OperatorTok{+}
\StringTok{    }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{stat =} \StringTok{"identity"}\NormalTok{, }\DataTypeTok{fill =} \StringTok{"navy"}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{0.7}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{my_theme}\NormalTok{() }\OperatorTok{+}
\StringTok{    }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{axis.text.x =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{angle =} \DecValTok{90}\NormalTok{, }\DataTypeTok{vjust =} \FloatTok{0.5}\NormalTok{, }\DataTypeTok{hjust =} \DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{nn-classification_239b-deeplearning_h2o_arrhythmia-sglander_files/figure-latex/plot-feature-variance-1} \end{center}

Features with low variance are less likely to strongly contribute to a differentiation between healthy and arrhythmia cases, so I am going to remove them. I am also concatenating the weights column:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{arrhythmia_subset <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(weights, }
\NormalTok{                           arrhythmia[, }\KeywordTok{c}\NormalTok{(}\DecValTok{281}\NormalTok{, }\DecValTok{280}\NormalTok{, }\KeywordTok{which}\NormalTok{(colvars}\OperatorTok{$}\NormalTok{variance }\OperatorTok{>}\StringTok{ }\DecValTok{50}\NormalTok{))])}
\end{Highlighting}
\end{Shaded}

\hypertarget{converting-the-dataframe-to-a-h2o-object}{%
\section{\texorpdfstring{Converting the dataframe to a \texttt{h2o} object}{Converting the dataframe to a h2o object}}\label{converting-the-dataframe-to-a-h2o-object}}

Now that I have my final data frame for modeling, for working with h2o functions, the data needs to be converted from a \textbf{DataFrame} to an \textbf{H2O Frame}. This is done with the \texttt{as\_h2o\_frame()} function.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#as_h2o_frame(arrhythmia_subset)}
\NormalTok{arrhythmia_hf <-}\StringTok{ }\KeywordTok{as.h2o}\NormalTok{(arrhythmia_subset, }\DataTypeTok{key=}\StringTok{"arrhtythmia.hex"}\NormalTok{)}
\CommentTok{#> }
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|}\StringTok{                                                                 }\ErrorTok{|}\StringTok{   }\DecValTok{0}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=================================================================|}\StringTok{ }\DecValTok{100}\NormalTok{%}
\end{Highlighting}
\end{Shaded}

We can now access all functions from the h2o package that are built to work on \texttt{h2o} Frames. A useful such function is \texttt{h2o.describe()}. It is similar to base R's \texttt{summary()} function but outputs many more descriptive measures for our data. To get a good overview about these measures, I am going to plot them.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyr) }\CommentTok{# for gathering}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'tidyr'}
\CommentTok{#> The following object is masked from 'package:S4Vectors':}
\CommentTok{#> }
\CommentTok{#>     expand}
\KeywordTok{h2o.describe}\NormalTok{(arrhythmia_hf[, }\DecValTok{-1}\NormalTok{]) }\OperatorTok{%>%}\StringTok{ }\CommentTok{# excluding the weights column}
\StringTok{  }\KeywordTok{gather}\NormalTok{(x, y, Zeros}\OperatorTok{:}\NormalTok{Sigma) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{group =} \KeywordTok{ifelse}\NormalTok{(}
\NormalTok{    x }\OperatorTok{%in%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Min"}\NormalTok{, }\StringTok{"Max"}\NormalTok{, }\StringTok{"Mean"}\NormalTok{), }\StringTok{"min, mean, max"}\NormalTok{, }
    \KeywordTok{ifelse}\NormalTok{(x }\OperatorTok{%in%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"NegInf"}\NormalTok{, }\StringTok{"PosInf"}\NormalTok{), }\StringTok{"Inf"}\NormalTok{, }\StringTok{"sigma, zeros"}\NormalTok{))) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\CommentTok{# separating them into facets makes them easier to see}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{Label =} \KeywordTok{factor}\NormalTok{(Label, }\DataTypeTok{levels =} \KeywordTok{colnames}\NormalTok{(arrhythmia_hf[, }\DecValTok{-1}\NormalTok{]))) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Label, }\DataTypeTok{y =} \KeywordTok{as.numeric}\NormalTok{(y), }\DataTypeTok{color =}\NormalTok{ x)) }\OperatorTok{+}
\StringTok{    }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{size =} \DecValTok{4}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{0.6}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{scale_color_brewer}\NormalTok{(}\DataTypeTok{palette =} \StringTok{"Set1"}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{my_theme}\NormalTok{() }\OperatorTok{+}
\StringTok{    }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{axis.text.x =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{angle =} \DecValTok{90}\NormalTok{, }\DataTypeTok{vjust =} \FloatTok{0.5}\NormalTok{, }\DataTypeTok{hjust =} \DecValTok{1}\NormalTok{)) }\OperatorTok{+}
\StringTok{    }\KeywordTok{facet_grid}\NormalTok{(group }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{scales =} \StringTok{"free"}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"Feature"}\NormalTok{,}
         \DataTypeTok{y =} \StringTok{"Value"}\NormalTok{,}
         \DataTypeTok{color =} \StringTok{""}\NormalTok{)}
\CommentTok{#> Warning: Removed 2 rows containing missing values (geom_point).}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{nn-classification_239b-deeplearning_h2o_arrhythmia-sglander_files/figure-latex/plot-h2o-describe-1} \end{center}

I am also interested in the correlation between features and the output. We can use the \texttt{h2o.cor()} function to calculate the correlation matrix. It is again much easier to understand the data when we visualize it, so I am going to create another plot.\\

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(reshape2) }\CommentTok{# for melting}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'reshape2'}
\CommentTok{#> The following object is masked from 'package:tidyr':}
\CommentTok{#> }
\CommentTok{#>     smiths}

\CommentTok{# diagnosis is now a characer column and we need to convert it again}
\NormalTok{arrhythmia_hf[, }\DecValTok{2}\NormalTok{] <-}\StringTok{ }\KeywordTok{h2o.asfactor}\NormalTok{(arrhythmia_hf[, }\DecValTok{2}\NormalTok{]) }
\NormalTok{arrhythmia_hf[, }\DecValTok{3}\NormalTok{] <-}\StringTok{ }\KeywordTok{h2o.asfactor}\NormalTok{(arrhythmia_hf[, }\DecValTok{3}\NormalTok{]) }\CommentTok{# same for class}

\NormalTok{cor <-}\StringTok{ }\KeywordTok{h2o.cor}\NormalTok{(arrhythmia_hf[, }\OperatorTok{-}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{)])}
\KeywordTok{rownames}\NormalTok{(cor) <-}\StringTok{ }\KeywordTok{colnames}\NormalTok{(cor)}

\KeywordTok{melt}\NormalTok{(cor) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{Var2 =} \KeywordTok{rep}\NormalTok{(}\KeywordTok{rownames}\NormalTok{(cor), }\KeywordTok{nrow}\NormalTok{(cor))) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{Var2 =} \KeywordTok{factor}\NormalTok{(Var2, }\DataTypeTok{levels =} \KeywordTok{colnames}\NormalTok{(cor))) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{variable =} \KeywordTok{factor}\NormalTok{(variable, }\DataTypeTok{levels =} \KeywordTok{colnames}\NormalTok{(cor))) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ variable, }\DataTypeTok{y =}\NormalTok{ Var2, }\DataTypeTok{fill =}\NormalTok{ value)) }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{geom_tile}\NormalTok{(}\DataTypeTok{width =} \FloatTok{0.9}\NormalTok{, }\DataTypeTok{height =} \FloatTok{0.9}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{scale_fill_gradient2}\NormalTok{(}\DataTypeTok{low =} \StringTok{"white"}\NormalTok{, }\DataTypeTok{high =} \StringTok{"red"}\NormalTok{, }\DataTypeTok{name =} \StringTok{"Cor."}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{my_theme}\NormalTok{() }\OperatorTok{+}
\StringTok{    }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{axis.text.x =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{angle =} \DecValTok{90}\NormalTok{, }\DataTypeTok{vjust =} \FloatTok{0.5}\NormalTok{, }\DataTypeTok{hjust =} \DecValTok{1}\NormalTok{)) }\OperatorTok{+}
\StringTok{    }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{""}\NormalTok{, }
         \DataTypeTok{y =} \StringTok{""}\NormalTok{)}
\CommentTok{#> No id variables; using all as measure variables}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{nn-classification_239b-deeplearning_h2o_arrhythmia-sglander_files/figure-latex/plot-correlation-1} \end{center}

\hypertarget{training-test-and-validation-data}{%
\section{Training, test and validation data}\label{training-test-and-validation-data}}

Now we can use the \texttt{h2o.splitFrame()} function to split the data into training, validation and test data.

Here, I am using 70\% for training and 15\% each for validation and testing. We could also just split the data into two sections, a training and test set but when we have sufficient samples, it is a good idea to evaluate model performance on an independent test set on top of training with a validation set. Because we can easily overfit a model, we want to get an idea about how generalizable it is - this we can only assess by looking at how well it works on previously unknown data.

I am also defining \texttt{response}, \texttt{features} and \texttt{weights} column names now.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{splits <-}\StringTok{ }\KeywordTok{h2o.splitFrame}\NormalTok{(arrhythmia_hf, }
                         \DataTypeTok{ratios =} \KeywordTok{c}\NormalTok{(}\FloatTok{0.7}\NormalTok{, }\FloatTok{0.15}\NormalTok{), }
                         \DataTypeTok{seed =} \DecValTok{1}\NormalTok{)}

\NormalTok{train <-}\StringTok{ }\NormalTok{splits[[}\DecValTok{1}\NormalTok{]]}
\NormalTok{valid <-}\StringTok{ }\NormalTok{splits[[}\DecValTok{2}\NormalTok{]]}
\NormalTok{test <-}\StringTok{ }\NormalTok{splits[[}\DecValTok{3}\NormalTok{]]}

\NormalTok{response <-}\StringTok{ "diagnosis"}
\NormalTok{weights <-}\StringTok{ "weights"}
\NormalTok{features <-}\StringTok{ }\KeywordTok{setdiff}\NormalTok{(}\KeywordTok{colnames}\NormalTok{(train), }\KeywordTok{c}\NormalTok{(response, weights, }\StringTok{"class"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(train}\OperatorTok{$}\NormalTok{diagnosis, }\DataTypeTok{exact_quantiles =} \OtherTok{TRUE}\NormalTok{)}
\CommentTok{#>  diagnosis      }
\CommentTok{#>  healthy   :163 }
\CommentTok{#>  arrhythmia:155}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(valid}\OperatorTok{$}\NormalTok{diagnosis, }\DataTypeTok{exact_quantiles =} \OtherTok{TRUE}\NormalTok{)}
\CommentTok{#>  diagnosis     }
\CommentTok{#>  healthy   :43 }
\CommentTok{#>  arrhythmia:25}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(test}\OperatorTok{$}\NormalTok{diagnosis, }\DataTypeTok{exact_quantiles =} \OtherTok{TRUE}\NormalTok{)}
\CommentTok{#>  diagnosis     }
\CommentTok{#>  healthy   :39 }
\CommentTok{#>  arrhythmia:27}
\end{Highlighting}
\end{Shaded}

If we had more categorical features, we could use the \texttt{h2o.interaction()} function to define interaction terms, but since we only have numeric features here, we don't need this.

We can also run a PCA on the training data, using the \texttt{h2o.prcomp()} function to calculate the singular value decomposition of the Gram matrix with the power method.\\

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pca <-}\StringTok{ }\KeywordTok{h2o.prcomp}\NormalTok{(}\DataTypeTok{training_frame =}\NormalTok{ train,}
           \DataTypeTok{x =}\NormalTok{ features,}
           \DataTypeTok{validation_frame =}\NormalTok{ valid,}
           \DataTypeTok{transform =} \StringTok{"NORMALIZE"}\NormalTok{,}
           \DataTypeTok{k =} \DecValTok{3}\NormalTok{,}
           \DataTypeTok{seed =} \DecValTok{42}\NormalTok{)}
\CommentTok{#> }
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|}\StringTok{                                                                 }\ErrorTok{|}\StringTok{   }\DecValTok{0}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=============}\StringTok{                                                    }\ErrorTok{|}\StringTok{  }\DecValTok{20}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=================================================================|}\StringTok{ }\DecValTok{100}\NormalTok{%}
\CommentTok{#> Warning in doTryCatch(return(expr), name, parentenv, handler): _train:}
\CommentTok{#> Dataset used may contain fewer number of rows due to removal of rows with}
\CommentTok{#> NA/missing values. If this is not desirable, set impute_missing argument in}
\CommentTok{#> pca call to TRUE/True/true/... depending on the client language.}
\NormalTok{pca}
\CommentTok{#> Model Details:}
\CommentTok{#> ==============}
\CommentTok{#> }
\CommentTok{#> H2ODimReductionModel: pca}
\CommentTok{#> Model ID:  PCA_model_R_1569010079835_1 }
\CommentTok{#> Importance of components: }
\CommentTok{#>                             pc1      pc2      pc3}
\CommentTok{#> Standard deviation     0.582620 0.507796 0.421869}
\CommentTok{#> Proportion of Variance 0.164697 0.125110 0.086351}
\CommentTok{#> Cumulative Proportion  0.164697 0.289808 0.376159}
\CommentTok{#> }
\CommentTok{#> }
\CommentTok{#> H2ODimReductionMetrics: pca}
\CommentTok{#> }
\CommentTok{#> No model metrics available for PCA}
\CommentTok{#> H2ODimReductionMetrics: pca}
\CommentTok{#> }
\CommentTok{#> No model metrics available for PCA}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{eigenvec <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(pca}\OperatorTok{@}\NormalTok{model}\OperatorTok{$}\NormalTok{eigenvectors)}
\NormalTok{eigenvec}\OperatorTok{$}\NormalTok{label <-}\StringTok{ }\NormalTok{features}

\KeywordTok{ggplot}\NormalTok{(eigenvec, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ pc1, }\DataTypeTok{y =}\NormalTok{ pc2, }\DataTypeTok{label =}\NormalTok{ label)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{color =} \StringTok{"navy"}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{0.7}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_text_repel}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{my_theme}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{nn-classification_239b-deeplearning_h2o_arrhythmia-sglander_files/figure-latex/plot-eigenvectors-1} \end{center}

\hypertarget{modeling}{%
\section{Modeling}\label{modeling}}

Now, we can build a deep neural network model. We can specify quite a few parameters, like

\begin{itemize}
\item
  \textbf{Cross-validation}: Cross validation can tell us the training and validation errors for each model. The final model will be overwritten with the best model, if we don't specify otherwise.
\item
  \textbf{Adaptive learning rate}: For deep learning with h2o, we by default use stochastic gradient descent optimization with an an adaptive learning rate. The two corresponding parameters rho and epsilon help us find global (or near enough) optima.
\item
  \textbf{Activation function}: The activation function defines the node output relative to a given set of inputs. We want our activation function to be non-linear and continuously differentiable.
\item
  \textbf{Hidden nodes}: Defines the number of hidden layers and the number of nodes per layer.
\item
  \textbf{Epochs}: Increasing the number of epochs (one full training cycle on all training samples) can increase model performance, but we also run the risk of overfitting. To determine the optimal number of epochs, we need to use early stopping.
\item
  \textbf{Early stopping}: By default, early stopping is enabled. This means that training will be stopped when we reach a certain validation error to prevent overfitting.
\end{itemize}

Of course, you need quite a bit of experience and intuition to hit on a good combination of parameters. That's why it usually makes sense to do a grid search for hyper-parameter tuning. Here, I want to focus on building and evaluating deep learning models, though. I will cover grid search in next week's post.\\

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# this will take some time and all CPUs}
\NormalTok{dl_model <-}\StringTok{ }\KeywordTok{h2o.deeplearning}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ features,}
                             \DataTypeTok{y =}\NormalTok{ response,}
                             \DataTypeTok{weights_column =}\NormalTok{ weights,}
                             \DataTypeTok{model_id =} \StringTok{"dl_model"}\NormalTok{,}
                             \DataTypeTok{training_frame =}\NormalTok{ train,}
                             \DataTypeTok{validation_frame =}\NormalTok{ valid,}
                             \DataTypeTok{nfolds =} \DecValTok{15}\NormalTok{,                                   }\CommentTok{# 10x cross validation}
                             \DataTypeTok{keep_cross_validation_fold_assignment =} \OtherTok{TRUE}\NormalTok{,}
                             \DataTypeTok{fold_assignment =} \StringTok{"Stratified"}\NormalTok{,}
                             \DataTypeTok{activation =} \StringTok{"RectifierWithDropout"}\NormalTok{,}
                             \DataTypeTok{score_each_iteration =} \OtherTok{TRUE}\NormalTok{,}
                             \DataTypeTok{hidden =} \KeywordTok{c}\NormalTok{(}\DecValTok{200}\NormalTok{, }\DecValTok{200}\NormalTok{, }\DecValTok{200}\NormalTok{, }\DecValTok{200}\NormalTok{, }\DecValTok{200}\NormalTok{),           }\CommentTok{# 5 hidden layers, each of 200 neurons}
                             \DataTypeTok{epochs =} \DecValTok{100}\NormalTok{,}
                             \DataTypeTok{variable_importances =} \OtherTok{TRUE}\NormalTok{,}
                             \DataTypeTok{export_weights_and_biases =} \OtherTok{TRUE}\NormalTok{,}
                             \DataTypeTok{seed =} \DecValTok{42}\NormalTok{)}
\CommentTok{#> }
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|}\StringTok{                                                                 }\ErrorTok{|}\StringTok{   }\DecValTok{0}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=}\StringTok{                                                                }\ErrorTok{|}\StringTok{   }\DecValTok{2}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|======}\StringTok{                                                           }\ErrorTok{|}\StringTok{   }\DecValTok{9}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|======}\StringTok{                                                           }\ErrorTok{|}\StringTok{  }\DecValTok{10}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=======}\StringTok{                                                          }\ErrorTok{|}\StringTok{  }\DecValTok{11}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|===========}\StringTok{                                                      }\ErrorTok{|}\StringTok{  }\DecValTok{16}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|===========}\StringTok{                                                      }\ErrorTok{|}\StringTok{  }\DecValTok{17}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|============}\StringTok{                                                     }\ErrorTok{|}\StringTok{  }\DecValTok{19}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=============}\StringTok{                                                    }\ErrorTok{|}\StringTok{  }\DecValTok{20}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=============}\StringTok{                                                    }\ErrorTok{|}\StringTok{  }\DecValTok{21}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==============}\StringTok{                                                   }\ErrorTok{|}\StringTok{  }\DecValTok{22}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=================}\StringTok{                                                }\ErrorTok{|}\StringTok{  }\DecValTok{26}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==================}\StringTok{                                               }\ErrorTok{|}\StringTok{  }\DecValTok{28}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|===================}\StringTok{                                              }\ErrorTok{|}\StringTok{  }\DecValTok{29}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|====================}\StringTok{                                             }\ErrorTok{|}\StringTok{  }\DecValTok{30}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|====================}\StringTok{                                             }\ErrorTok{|}\StringTok{  }\DecValTok{31}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|======================}\StringTok{                                           }\ErrorTok{|}\StringTok{  }\DecValTok{33}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|========================}\StringTok{                                         }\ErrorTok{|}\StringTok{  }\DecValTok{37}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|========================}\StringTok{                                         }\ErrorTok{|}\StringTok{  }\DecValTok{38}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=========================}\StringTok{                                        }\ErrorTok{|}\StringTok{  }\DecValTok{38}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=========================}\StringTok{                                        }\ErrorTok{|}\StringTok{  }\DecValTok{39}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==========================}\StringTok{                                       }\ErrorTok{|}\StringTok{  }\DecValTok{40}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|===========================}\StringTok{                                      }\ErrorTok{|}\StringTok{  }\DecValTok{42}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|============================}\StringTok{                                     }\ErrorTok{|}\StringTok{  }\DecValTok{43}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=============================}\StringTok{                                    }\ErrorTok{|}\StringTok{  }\DecValTok{45}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==============================}\StringTok{                                   }\ErrorTok{|}\StringTok{  }\DecValTok{47}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|===============================}\StringTok{                                  }\ErrorTok{|}\StringTok{  }\DecValTok{48}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==================================}\StringTok{                               }\ErrorTok{|}\StringTok{  }\DecValTok{52}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|===================================}\StringTok{                              }\ErrorTok{|}\StringTok{  }\DecValTok{54}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|====================================}\StringTok{                             }\ErrorTok{|}\StringTok{  }\DecValTok{56}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=======================================}\StringTok{                          }\ErrorTok{|}\StringTok{  }\DecValTok{59}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=======================================}\StringTok{                          }\ErrorTok{|}\StringTok{  }\DecValTok{61}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|========================================}\StringTok{                         }\ErrorTok{|}\StringTok{  }\DecValTok{62}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=========================================}\StringTok{                        }\ErrorTok{|}\StringTok{  }\DecValTok{63}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==========================================}\StringTok{                       }\ErrorTok{|}\StringTok{  }\DecValTok{64}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|===========================================}\StringTok{                      }\ErrorTok{|}\StringTok{  }\DecValTok{66}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=============================================}\StringTok{                    }\ErrorTok{|}\StringTok{  }\DecValTok{69}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|===============================================}\StringTok{                  }\ErrorTok{|}\StringTok{  }\DecValTok{72}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|================================================}\StringTok{                 }\ErrorTok{|}\StringTok{  }\DecValTok{73}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=================================================}\StringTok{                }\ErrorTok{|}\StringTok{  }\DecValTok{75}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=================================================}\StringTok{                }\ErrorTok{|}\StringTok{  }\DecValTok{76}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|========================================================}\StringTok{         }\ErrorTok{|}\StringTok{  }\DecValTok{86}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=========================================================}\StringTok{        }\ErrorTok{|}\StringTok{  }\DecValTok{88}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==========================================================}\StringTok{       }\ErrorTok{|}\StringTok{  }\DecValTok{89}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==========================================================}\StringTok{       }\ErrorTok{|}\StringTok{  }\DecValTok{90}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=================================================================|}\StringTok{ }\DecValTok{100}\NormalTok{%}
\end{Highlighting}
\end{Shaded}

Because training can take a while, depending on how many samples, features, nodes and hidden layers you are training on, it is a good idea to save your model.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# if file exists, overwrite it}
\KeywordTok{h2o.saveModel}\NormalTok{(dl_model, }\DataTypeTok{path =} \KeywordTok{file.path}\NormalTok{(data_out_dir, }\StringTok{"dl_model"}\NormalTok{), }\DataTypeTok{force =} \OtherTok{TRUE}\NormalTok{)}
\CommentTok{#> [1] "/home/datascience/repos/machine-learning-rsuite/export/dl_model/dl_model"}
\end{Highlighting}
\end{Shaded}

We can then re-load the model again any time to check the model quality and make predictions on new data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dl_model <-}\StringTok{ }\KeywordTok{h2o.loadModel}\NormalTok{(}\KeywordTok{file.path}\NormalTok{(data_out_dir, }\StringTok{"dl_model/dl_model"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\hypertarget{model-performance}{%
\section{Model performance}\label{model-performance}}

We now want to know how our model performed on the validation data. The summary() function will give us a detailed overview of our model. I am not showing the output here, because it is quite extensive.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sum_model <-}\StringTok{ }\KeywordTok{summary}\NormalTok{(dl_model)}
\CommentTok{#> Model Details:}
\CommentTok{#> ==============}
\CommentTok{#> }
\CommentTok{#> H2OBinomialModel: deeplearning}
\CommentTok{#> Model Key:  dl_model }
\CommentTok{#> Status of Neuron Layers: predicting diagnosis, 2-class classification, bernoulli distribution, CrossEntropy loss, 179,402 weights/biases, 2.1 MB, 34,090 training samples, mini-batch size 1}
\CommentTok{#>   layer units             type dropout       l1       l2 mean_rate}
\CommentTok{#> 1     1    90            Input  0.00 %       NA       NA        NA}
\CommentTok{#> 2     2   200 RectifierDropout 50.00 % 0.000000 0.000000  0.003946}
\CommentTok{#> 3     3   200 RectifierDropout 50.00 % 0.000000 0.000000  0.006433}
\CommentTok{#> 4     4   200 RectifierDropout 50.00 % 0.000000 0.000000  0.009130}
\CommentTok{#> 5     5   200 RectifierDropout 50.00 % 0.000000 0.000000  0.007841}
\CommentTok{#> 6     6   200 RectifierDropout 50.00 % 0.000000 0.000000  0.023682}
\CommentTok{#> 7     7     2          Softmax      NA 0.000000 0.000000  0.002243}
\CommentTok{#>   rate_rms momentum mean_weight weight_rms mean_bias bias_rms}
\CommentTok{#> 1       NA       NA          NA         NA        NA       NA}
\CommentTok{#> 2 0.003634 0.000000    0.002825   0.096114  0.429671 0.054778}
\CommentTok{#> 3 0.003863 0.000000   -0.008409   0.074646  0.949195 0.053149}
\CommentTok{#> 4 0.004991 0.000000   -0.007090   0.072220  0.966390 0.029603}
\CommentTok{#> 5 0.003807 0.000000   -0.006317   0.071143  0.973594 0.036020}
\CommentTok{#> 6 0.056406 0.000000   -0.009330   0.070480  0.952516 0.032818}
\CommentTok{#> 7 0.001142 0.000000   -0.041027   0.377082  0.000224 0.053261}
\CommentTok{#> }
\CommentTok{#> H2OBinomialMetrics: deeplearning}
\CommentTok{#> ** Reported on training data. **}
\CommentTok{#> ** Metrics reported on full training frame **}
\CommentTok{#> }
\CommentTok{#> MSE:  0.0321}
\CommentTok{#> RMSE:  0.179}
\CommentTok{#> LogLoss:  0.117}
\CommentTok{#> Mean Per-Class Error:  0.0336}
\CommentTok{#> AUC:  0.984}
\CommentTok{#> pr_auc:  0.964}
\CommentTok{#> Gini:  0.967}
\CommentTok{#> }
\CommentTok{#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:}
\CommentTok{#>            arrhythmia healthy    Error     Rate}
\CommentTok{#> arrhythmia        155       9 0.054878   =9/164}
\CommentTok{#> healthy             2     161 0.012270   =2/163}
\CommentTok{#> Totals            157     170 0.033639  =11/327}
\CommentTok{#> }
\CommentTok{#> Maximum Metrics: Maximum metrics at their respective thresholds}
\CommentTok{#>                         metric threshold    value idx}
\CommentTok{#> 1                       max f1  0.684745 0.966967 169}
\CommentTok{#> 2                       max f2  0.684745 0.979319 169}
\CommentTok{#> 3                 max f0point5  0.922844 0.972046 155}
\CommentTok{#> 4                 max accuracy  0.845474 0.966361 163}
\CommentTok{#> 5                max precision  0.991717 1.000000   0}
\CommentTok{#> 6                   max recall  0.137131 1.000000 181}
\CommentTok{#> 7              max specificity  0.991717 1.000000   0}
\CommentTok{#> 8             max absolute_mcc  0.684745 0.933586 169}
\CommentTok{#> 9   max min_per_class_accuracy  0.845474 0.963415 163}
\CommentTok{#> 10 max mean_per_class_accuracy  0.684745 0.966426 169}
\CommentTok{#> }
\CommentTok{#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`}
\CommentTok{#> H2OBinomialMetrics: deeplearning}
\CommentTok{#> ** Reported on validation data. **}
\CommentTok{#> ** Metrics reported on full validation frame **}
\CommentTok{#> }
\CommentTok{#> MSE:  0.182}
\CommentTok{#> RMSE:  0.427}
\CommentTok{#> LogLoss:  0.976}
\CommentTok{#> Mean Per-Class Error:  0.192}
\CommentTok{#> AUC:  0.884}
\CommentTok{#> pr_auc:  0.902}
\CommentTok{#> Gini:  0.767}
\CommentTok{#> }
\CommentTok{#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:}
\CommentTok{#>            arrhythmia healthy    Error    Rate}
\CommentTok{#> arrhythmia         16       9 0.360000   =9/25}
\CommentTok{#> healthy             1      42 0.023256   =1/43}
\CommentTok{#> Totals             17      51 0.147059  =10/68}
\CommentTok{#> }
\CommentTok{#> Maximum Metrics: Maximum metrics at their respective thresholds}
\CommentTok{#>                         metric threshold    value idx}
\CommentTok{#> 1                       max f1  0.000686 0.893617  50}
\CommentTok{#> 2                       max f2  0.000686 0.941704  50}
\CommentTok{#> 3                 max f0point5  0.786678 0.871795  37}
\CommentTok{#> 4                 max accuracy  0.000686 0.852941  50}
\CommentTok{#> 5                max precision  0.992411 1.000000   0}
\CommentTok{#> 6                   max recall  0.000001 1.000000  57}
\CommentTok{#> 7              max specificity  0.992411 1.000000   0}
\CommentTok{#> 8             max absolute_mcc  0.000686 0.686752  50}
\CommentTok{#> 9   max min_per_class_accuracy  0.786678 0.790698  37}
\CommentTok{#> 10 max mean_per_class_accuracy  0.786678 0.815349  37}
\CommentTok{#> }
\CommentTok{#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`}
\CommentTok{#> H2OBinomialMetrics: deeplearning}
\CommentTok{#> ** Reported on cross-validation data. **}
\CommentTok{#> ** 15-fold cross-validation on training data (Metrics computed for combined holdout predictions) **}
\CommentTok{#> }
\CommentTok{#> MSE:  0.168}
\CommentTok{#> RMSE:  0.41}
\CommentTok{#> LogLoss:  0.557}
\CommentTok{#> Mean Per-Class Error:  0.208}
\CommentTok{#> AUC:  0.848}
\CommentTok{#> pr_auc:  0.81}
\CommentTok{#> Gini:  0.697}
\CommentTok{#> }
\CommentTok{#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:}
\CommentTok{#>            arrhythmia healthy    Error     Rate}
\CommentTok{#> arrhythmia        120      44 0.268293  =44/164}
\CommentTok{#> healthy            24     139 0.147239  =24/163}
\CommentTok{#> Totals            144     183 0.207951  =68/327}
\CommentTok{#> }
\CommentTok{#> Maximum Metrics: Maximum metrics at their respective thresholds}
\CommentTok{#>                         metric threshold    value idx}
\CommentTok{#> 1                       max f1  0.442932 0.803468 182}
\CommentTok{#> 2                       max f2  0.012515 0.884956 251}
\CommentTok{#> 3                 max f0point5  0.624456 0.792453 157}
\CommentTok{#> 4                 max accuracy  0.467967 0.792049 178}
\CommentTok{#> 5                max precision  0.990572 1.000000   0}
\CommentTok{#> 6                   max recall  0.001848 1.000000 280}
\CommentTok{#> 7              max specificity  0.990572 1.000000   0}
\CommentTok{#> 8             max absolute_mcc  0.442932 0.588667 182}
\CommentTok{#> 9   max min_per_class_accuracy  0.607830 0.785276 161}
\CommentTok{#> 10 max mean_per_class_accuracy  0.442932 0.792234 182}
\CommentTok{#> }
\CommentTok{#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`}
\CommentTok{#> Cross-Validation Metrics Summary: }
\CommentTok{#>                               mean          sd cv_1_valid   cv_2_valid}
\CommentTok{#> accuracy                 0.8385412 0.061696008 0.64285713    0.8235294}
\CommentTok{#> auc                     0.85863346  0.06217786 0.69518715   0.73333335}
\CommentTok{#> err                     0.16145879 0.061696008 0.35714287    0.1764706}
\CommentTok{#> err_count                3.8666666   1.9298819       10.0          3.0}
\CommentTok{#> f0point5                0.82062894  0.08774206 0.57894737    0.6896552}
\CommentTok{#> f1                      0.84875447 0.057496537     0.6875   0.72727275}
\CommentTok{#> f2                       0.8901033  0.04788143 0.84615386    0.7692308}
\CommentTok{#> lift_top_group           1.7281693  0.54166335  2.5454545          0.0}
\CommentTok{#> logloss                  0.5452315   0.1306371  0.6586832    0.7813473}
\CommentTok{#> max_per_class_error     0.30671087 0.113830075  0.5882353          0.2}
\CommentTok{#> mcc                      0.7049009  0.10066028 0.46442038   0.60385966}
\CommentTok{#> mean_per_class_accuracy  0.8367473  0.05778133  0.7058824   0.81666666}
\CommentTok{#> mean_per_class_error    0.16325273  0.05778133 0.29411766   0.18333334}
\CommentTok{#> mse                     0.16131651  0.03845805 0.23015584   0.21473385}
\CommentTok{#> precision               0.80784994 0.110800184 0.52380955    0.6666667}
\CommentTok{#> r2                      0.32223144  0.16513458 0.03506859 -0.034301396}
\CommentTok{#> recall                   0.9275503  0.06717654        1.0          0.8}
\CommentTok{#> rmse                     0.3942398 0.054274667  0.4797456   0.46339384}
\CommentTok{#> specificity             0.74594426  0.14763153  0.4117647    0.8333333}
\CommentTok{#>                          cv_3_valid cv_4_valid  cv_5_valid  cv_6_valid}
\CommentTok{#> accuracy                 0.93333334 0.82608694   0.9444444         1.0}
\CommentTok{#> auc                        0.962963 0.85714287  0.96103895         1.0}
\CommentTok{#> err                      0.06666667 0.17391305 0.055555556         0.0}
\CommentTok{#> err_count                       1.0        4.0         1.0         0.0}
\CommentTok{#> f0point5                  0.9183673 0.81395346  0.98039216         1.0}
\CommentTok{#> f1                       0.94736844      0.875  0.95238096         1.0}
\CommentTok{#> f2                        0.9782609  0.9459459   0.9259259         1.0}
\CommentTok{#> lift_top_group            1.6666666  1.6428572   1.6363636         2.0}
\CommentTok{#> logloss                   0.3135496  0.4296336   0.3291604  0.11736608}
\CommentTok{#> max_per_class_error      0.16666667 0.44444445  0.09090909         0.0}
\CommentTok{#> mcc                       0.8660254  0.6573422   0.8918826         1.0}
\CommentTok{#> mean_per_class_accuracy   0.9166667  0.7777778  0.95454544         1.0}
\CommentTok{#> mean_per_class_error    0.083333336 0.22222222 0.045454547         0.0}
\CommentTok{#> mse                     0.104235224  0.1389879 0.090775445 0.034521867}
\CommentTok{#> precision                       0.9  0.7777778         1.0         1.0}
\CommentTok{#> r2                        0.5656866 0.41647142   0.6180358  0.86191255}
\CommentTok{#> recall                          1.0        1.0  0.90909094         1.0}
\CommentTok{#> rmse                     0.32285482  0.3728108  0.30128965  0.18580061}
\CommentTok{#> specificity               0.8333333  0.5555556         1.0         1.0}
\CommentTok{#>                         cv_7_valid  cv_8_valid cv_9_valid cv_10_valid}
\CommentTok{#> accuracy                0.90909094  0.90909094  0.8076923         0.8}
\CommentTok{#> auc                     0.90178573   0.9285714 0.86928105  0.83035713}
\CommentTok{#> err                     0.09090909  0.09090909  0.1923077         0.2}
\CommentTok{#> err_count                      2.0         1.0        5.0         6.0}
\CommentTok{#> f0point5                    0.9375   0.9677419 0.82474226  0.75581396}
\CommentTok{#> f1                      0.85714287   0.9230769  0.8648649      0.8125}
\CommentTok{#> f2                       0.7894737  0.88235295 0.90909094   0.8783784}
\CommentTok{#> lift_top_group                2.75   1.5714285  1.5294118    2.142857}
\CommentTok{#> logloss                 0.52325845  0.74918216 0.63733953   0.6056329}
\CommentTok{#> max_per_class_error           0.25  0.14285715 0.44444445      0.3125}
\CommentTok{#> mcc                     0.81009257   0.8280787  0.5608894  0.62737644}
\CommentTok{#> mean_per_class_accuracy      0.875   0.9285714   0.748366  0.80803573}
\CommentTok{#> mean_per_class_error         0.125 0.071428575   0.251634  0.19196428}
\CommentTok{#> mse                     0.14090821  0.16433945 0.16959226  0.18447527}
\CommentTok{#> precision                      1.0         1.0        0.8   0.7222222}
\CommentTok{#> r2                      0.39107522   0.2898188 0.25069037  0.25880474}
\CommentTok{#> recall                        0.75  0.85714287  0.9411765   0.9285714}
\CommentTok{#> rmse                    0.37537742  0.40538803 0.41181582  0.42950583}
\CommentTok{#> specificity                    1.0         1.0  0.5555556      0.6875}
\CommentTok{#>                         cv_11_valid cv_12_valid cv_13_valid cv_14_valid}
\CommentTok{#> accuracy                  0.8636364   0.7619048        0.75   0.8064516}
\CommentTok{#> auc                      0.90082645   0.7181818      0.8125       0.875}
\CommentTok{#> err                      0.13636364  0.23809524        0.25  0.19354838}
\CommentTok{#> err_count                       3.0         5.0         8.0         6.0}
\CommentTok{#> f0point5                  0.9302326  0.71428573  0.71428573   0.7692308}
\CommentTok{#> f1                       0.84210527         0.8         0.8  0.84210527}
\CommentTok{#> f2                        0.7692308  0.90909094  0.90909094   0.9302326}
\CommentTok{#> lift_top_group                  2.0         0.0         2.0      1.9375}
\CommentTok{#> logloss                  0.52656233   0.8181485  0.66666746  0.49635586}
\CommentTok{#> max_per_class_error      0.27272728  0.45454547         0.5         0.4}
\CommentTok{#> mcc                      0.75592893   0.6030227  0.57735026  0.66057825}
\CommentTok{#> mean_per_class_accuracy   0.8636364  0.77272725        0.75         0.8}
\CommentTok{#> mean_per_class_error     0.13636364  0.22727273        0.25         0.2}
\CommentTok{#> mse                      0.15577565  0.24097086  0.22684109  0.15012316}
\CommentTok{#> precision                       1.0   0.6666667   0.6666667  0.72727275}
\CommentTok{#> r2                        0.3768974 0.033925887 0.092635654  0.39888185}
\CommentTok{#> recall                   0.72727275         1.0         1.0         1.0}
\CommentTok{#> rmse                     0.39468426  0.49088785  0.47627836   0.3874573}
\CommentTok{#> specificity                     1.0  0.54545456         0.5         0.6}
\CommentTok{#>                         cv_15_valid}
\CommentTok{#> accuracy                        0.8}
\CommentTok{#> auc                       0.8333333}
\CommentTok{#> err                             0.2}
\CommentTok{#> err_count                       3.0}
\CommentTok{#> f0point5                 0.71428573}
\CommentTok{#> f1                              0.8}
\CommentTok{#> f2                       0.90909094}
\CommentTok{#> lift_top_group                  2.5}
\CommentTok{#> logloss                   0.5255857}
\CommentTok{#> max_per_class_error      0.33333334}
\CommentTok{#> mcc                       0.6666667}
\CommentTok{#> mean_per_class_accuracy   0.8333333}
\CommentTok{#> mean_per_class_error     0.16666667}
\CommentTok{#> mse                      0.17331165}
\CommentTok{#> precision                 0.6666667}
\CommentTok{#> r2                        0.2778681}
\CommentTok{#> recall                          1.0}
\CommentTok{#> rmse                     0.41630718}
\CommentTok{#> specificity               0.6666667}
\CommentTok{#> }
\CommentTok{#> Scoring History: }
\CommentTok{#>              timestamp          duration training_speed    epochs}
\CommentTok{#> 1  2019-09-20 15:09:41         0.000 sec             NA   0.00000}
\CommentTok{#> 2  2019-09-20 15:09:42  1 min 26.356 sec   5597 obs/sec  10.72013}
\CommentTok{#> 3  2019-09-20 15:09:42  1 min 26.981 sec   5715 obs/sec  21.44025}
\CommentTok{#> 4  2019-09-20 15:09:43  1 min 27.544 sec   5977 obs/sec  32.16038}
\CommentTok{#> 5  2019-09-20 15:09:44  1 min 28.145 sec   6017 obs/sec  42.88050}
\CommentTok{#> 6  2019-09-20 15:09:44  1 min 28.698 sec   6140 obs/sec  53.60063}
\CommentTok{#> 7  2019-09-20 15:09:45  1 min 29.245 sec   6234 obs/sec  64.32075}
\CommentTok{#> 8  2019-09-20 15:09:45  1 min 29.790 sec   6319 obs/sec  75.04088}
\CommentTok{#> 9  2019-09-20 15:09:46  1 min 30.319 sec   6401 obs/sec  85.76101}
\CommentTok{#> 10 2019-09-20 15:09:46  1 min 30.852 sec   6460 obs/sec  96.48113}
\CommentTok{#> 11 2019-09-20 15:09:47  1 min 31.392 sec   6503 obs/sec 107.20126}
\CommentTok{#>    iterations      samples training_rmse training_logloss training_r2}
\CommentTok{#> 1           0     0.000000            NA               NA          NA}
\CommentTok{#> 2           1  3409.000000       0.37355          0.46858     0.44184}
\CommentTok{#> 3           2  6818.000000       0.35218          0.47369     0.50387}
\CommentTok{#> 4           3 10227.000000       0.31384          0.35018     0.60602}
\CommentTok{#> 5           4 13636.000000       0.29469          0.30311     0.65263}
\CommentTok{#> 6           5 17045.000000       0.28485          0.29392     0.67544}
\CommentTok{#> 7           6 20454.000000       0.25468          0.25312     0.74055}
\CommentTok{#> 8           7 23863.000000       0.21535          0.16877     0.81450}
\CommentTok{#> 9           8 27272.000000       0.24487          0.21845     0.76014}
\CommentTok{#> 10          9 30681.000000       0.18801          0.13523     0.85861}
\CommentTok{#> 11         10 34090.000000       0.17923          0.11736     0.87151}
\CommentTok{#>    training_auc training_pr_auc training_lift}
\CommentTok{#> 1            NA              NA            NA}
\CommentTok{#> 2       0.88695         0.85448       2.00613}
\CommentTok{#> 3       0.91179         0.87339       1.50460}
\CommentTok{#> 4       0.93835         0.90497       2.00613}
\CommentTok{#> 5       0.94692         0.91555       2.00613}
\CommentTok{#> 6       0.95432         0.92980       2.00613}
\CommentTok{#> 7       0.96832         0.94387       2.00613}
\CommentTok{#> 8       0.97366         0.95149       2.00613}
\CommentTok{#> 9       0.98077         0.96509       2.00613}
\CommentTok{#> 10      0.98227         0.96737       2.00613}
\CommentTok{#> 11      0.98365         0.96430       2.00613}
\CommentTok{#>    training_classification_error validation_rmse validation_logloss}
\CommentTok{#> 1                             NA              NA                 NA}
\CommentTok{#> 2                        0.18043         0.39644            0.58228}
\CommentTok{#> 3                        0.15291         0.40302            0.65206}
\CommentTok{#> 4                        0.11621         0.41669            0.71722}
\CommentTok{#> 5                        0.09480         0.40738            0.63853}
\CommentTok{#> 6                        0.09480         0.40913            0.80101}
\CommentTok{#> 7                        0.06728         0.41996            0.89227}
\CommentTok{#> 8                        0.05505         0.42863            0.98097}
\CommentTok{#> 9                        0.04587         0.46062            1.26800}
\CommentTok{#> 10                       0.03364         0.44414            1.13089}
\CommentTok{#> 11                       0.03364         0.42686            0.97588}
\CommentTok{#>    validation_r2 validation_auc validation_pr_auc validation_lift}
\CommentTok{#> 1             NA             NA                NA              NA}
\CommentTok{#> 2        0.32398        0.83907           0.84912         1.58140}
\CommentTok{#> 3        0.30136        0.87535           0.88555         1.58140}
\CommentTok{#> 4        0.25316        0.86884           0.88149         1.58140}
\CommentTok{#> 5        0.28613        0.86791           0.88102         1.58140}
\CommentTok{#> 6        0.28001        0.87628           0.88782         1.58140}
\CommentTok{#> 7        0.24137        0.85674           0.86697         1.58140}
\CommentTok{#> 8        0.20974        0.85953           0.87294         1.58140}
\CommentTok{#> 9        0.08735        0.84744           0.85773         1.58140}
\CommentTok{#> 10       0.15151        0.83442           0.85225         1.58140}
\CommentTok{#> 11       0.21624        0.88372           0.90172         1.58140}
\CommentTok{#>    validation_classification_error}
\CommentTok{#> 1                               NA}
\CommentTok{#> 2                          0.19118}
\CommentTok{#> 3                          0.16176}
\CommentTok{#> 4                          0.17647}
\CommentTok{#> 5                          0.16176}
\CommentTok{#> 6                          0.17647}
\CommentTok{#> 7                          0.16176}
\CommentTok{#> 8                          0.17647}
\CommentTok{#> 9                          0.16176}
\CommentTok{#> 10                         0.16176}
\CommentTok{#> 11                         0.14706}
\CommentTok{#> }
\CommentTok{#> Variable Importances: (Extract with `h2o.varimp`) }
\CommentTok{#> =================================================}
\CommentTok{#> }
\CommentTok{#> Variable Importances: }
\CommentTok{#>   variable relative_importance scaled_importance percentage}
\CommentTok{#> 1     V169            1.000000          1.000000   0.014302}
\CommentTok{#> 2     V136            0.922611          0.922611   0.013195}
\CommentTok{#> 3     V239            0.913344          0.913344   0.013062}
\CommentTok{#> 4     V103            0.910889          0.910889   0.013027}
\CommentTok{#> 5     V229            0.893744          0.893744   0.012782}
\CommentTok{#> }
\CommentTok{#> ---}
\CommentTok{#>    variable relative_importance scaled_importance percentage}
\CommentTok{#> 85     V259            0.700921          0.700921   0.010024}
\CommentTok{#> 86     V168            0.698275          0.698275   0.009987}
\CommentTok{#> 87     V179            0.695452          0.695452   0.009946}
\CommentTok{#> 88     V269            0.695200          0.695200   0.009943}
\CommentTok{#> 89      V45            0.683744          0.683744   0.009779}
\CommentTok{#> 90      V33            0.664106          0.664106   0.009498}
\end{Highlighting}
\end{Shaded}

One performance metric we are usually interested in is the mean per class error for training and validation data.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{h2o.mean_per_class_error}\NormalTok{(dl_model, }\DataTypeTok{train =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{valid =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{xval =} \OtherTok{TRUE}\NormalTok{)}
\CommentTok{#>  train  valid   xval }
\CommentTok{#> 0.0336 0.1916 0.2078}
\end{Highlighting}
\end{Shaded}

The confusion matrix tells us, how many classes have been predicted correctly and how many predictions were accurate. Here, we see the errors in predictions on validation data.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{h2o.confusionMatrix}\NormalTok{(dl_model, }\DataTypeTok{valid =} \OtherTok{TRUE}\NormalTok{)}
\CommentTok{#> Confusion Matrix (vertical: actual; across: predicted)  for max f1 @ threshold = 0.000686170136461276:}
\CommentTok{#>            arrhythmia healthy    Error    Rate}
\CommentTok{#> arrhythmia         16       9 0.360000   =9/25}
\CommentTok{#> healthy             1      42 0.023256   =1/43}
\CommentTok{#> Totals             17      51 0.147059  =10/68}
\end{Highlighting}
\end{Shaded}

We can also plot the classification error over all epochs or samples.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(dl_model,}
     \DataTypeTok{timestep =} \StringTok{"epochs"}\NormalTok{,}
     \DataTypeTok{metric =} \StringTok{"classification_error"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{nn-classification_239b-deeplearning_h2o_arrhythmia-sglander_files/figure-latex/unnamed-chunk-19-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(dl_model,}
     \DataTypeTok{timestep =} \StringTok{"samples"}\NormalTok{,}
     \DataTypeTok{metric =} \StringTok{"classification_error"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{nn-classification_239b-deeplearning_h2o_arrhythmia-sglander_files/figure-latex/unnamed-chunk-20-1} \end{center}

Next to the classification error, we are usually interested in the logistic loss (negative log-likelihood or log loss). It describes the sum of errors for each sample in the training or validation data or the negative logarithm of the likelihood of error for a given prediction/ classification. Simply put, the lower the loss, the better the model (if we ignore potential overfitting).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(dl_model,}
     \DataTypeTok{timestep =} \StringTok{"epochs"}\NormalTok{,}
     \DataTypeTok{metric =} \StringTok{"logloss"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{nn-classification_239b-deeplearning_h2o_arrhythmia-sglander_files/figure-latex/unnamed-chunk-21-1} \end{center}

We can also plot the mean squared error (MSE). The \textbf{MSE} tells us the average of the prediction errors squared, i.e.~the estimator's variance and bias. The closer to zero, the better a model.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(dl_model,}
     \DataTypeTok{timestep =} \StringTok{"epochs"}\NormalTok{,}
     \DataTypeTok{metric =} \StringTok{"rmse"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{nn-classification_239b-deeplearning_h2o_arrhythmia-sglander_files/figure-latex/unnamed-chunk-22-1} \end{center}

Next, we want to know the area under the curve (AUC). \textbf{AUC} is an important metric for measuring binary classification model performances. It gives the area under the curve, i.e.~the integral, of true positive vs false positive rates. The closer to 1, the better a model.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{h2o.auc}\NormalTok{(dl_model, }\DataTypeTok{train =} \OtherTok{TRUE}\NormalTok{)}
\CommentTok{#> [1] 0.984}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{h2o.auc}\NormalTok{(dl_model, }\DataTypeTok{valid =} \OtherTok{TRUE}\NormalTok{)}
\CommentTok{#> [1] 0.884}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{h2o.auc}\NormalTok{(dl_model, }\DataTypeTok{xval =} \OtherTok{TRUE}\NormalTok{)}
\CommentTok{#> [1] 0.848}
\end{Highlighting}
\end{Shaded}

The weights for connecting two adjacent layers and per-neuron biases that we specified the model to save, can be accessed with:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{w <-}\StringTok{ }\KeywordTok{h2o.weights}\NormalTok{(dl_model, }\DataTypeTok{matrix_id =} \DecValTok{1}\NormalTok{)}
\NormalTok{b <-}\StringTok{ }\KeywordTok{h2o.biases}\NormalTok{(dl_model, }\DataTypeTok{vector_id =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Variable importance can be extracted as well (but keep in mind, that variable importance in deep neural networks is difficult to assess and should be considered only as rough estimates).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{h2o.varimp}\NormalTok{(dl_model)}
\CommentTok{#> Variable Importances: }
\CommentTok{#>   variable relative_importance scaled_importance percentage}
\CommentTok{#> 1     V169            1.000000          1.000000   0.014302}
\CommentTok{#> 2     V136            0.922611          0.922611   0.013195}
\CommentTok{#> 3     V239            0.913344          0.913344   0.013062}
\CommentTok{#> 4     V103            0.910889          0.910889   0.013027}
\CommentTok{#> 5     V229            0.893744          0.893744   0.012782}
\CommentTok{#> }
\CommentTok{#> ---}
\CommentTok{#>    variable relative_importance scaled_importance percentage}
\CommentTok{#> 85     V259            0.700921          0.700921   0.010024}
\CommentTok{#> 86     V168            0.698275          0.698275   0.009987}
\CommentTok{#> 87     V179            0.695452          0.695452   0.009946}
\CommentTok{#> 88     V269            0.695200          0.695200   0.009943}
\CommentTok{#> 89      V45            0.683744          0.683744   0.009779}
\CommentTok{#> 90      V33            0.664106          0.664106   0.009498}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{h2o.varimp_plot}\NormalTok{(dl_model)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{nn-classification_239b-deeplearning_h2o_arrhythmia-sglander_files/figure-latex/unnamed-chunk-28-1} \end{center}

\hypertarget{test-data}{%
\section{Test data}\label{test-data}}

Now that we have a good idea about model performance on validation data, we want to know how it performed on unseen test data. A good model should find an optimal balance between accuracy on training and test data. A model that has 0\% error on the training data but 40\% error on the test data is in effect useless. It overfit on the training data and is thus not able to generalize to unknown data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{perf <-}\StringTok{ }\KeywordTok{h2o.performance}\NormalTok{(dl_model, test)}
\NormalTok{perf}
\CommentTok{#> H2OBinomialMetrics: deeplearning}
\CommentTok{#> }
\CommentTok{#> MSE:  0.257}
\CommentTok{#> RMSE:  0.507}
\CommentTok{#> LogLoss:  1.75}
\CommentTok{#> Mean Per-Class Error:  0.303}
\CommentTok{#> AUC:  0.788}
\CommentTok{#> pr_auc:  0.783}
\CommentTok{#> Gini:  0.576}
\CommentTok{#> }
\CommentTok{#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:}
\CommentTok{#>            arrhythmia healthy    Error    Rate}
\CommentTok{#> arrhythmia         12      15 0.555556  =15/27}
\CommentTok{#> healthy             2      37 0.051282   =2/39}
\CommentTok{#> Totals             14      52 0.257576  =17/66}
\CommentTok{#> }
\CommentTok{#> Maximum Metrics: Maximum metrics at their respective thresholds}
\CommentTok{#>                         metric threshold    value idx}
\CommentTok{#> 1                       max f1  0.000002 0.813187  51}
\CommentTok{#> 2                       max f2  0.000000 0.906977  58}
\CommentTok{#> 3                 max f0point5  0.977116 0.804196  25}
\CommentTok{#> 4                 max accuracy  0.400678 0.742424  37}
\CommentTok{#> 5                max precision  0.990653 1.000000   0}
\CommentTok{#> 6                   max recall  0.000000 1.000000  58}
\CommentTok{#> 7              max specificity  0.990653 1.000000   0}
\CommentTok{#> 8             max absolute_mcc  0.977116 0.481615  25}
\CommentTok{#> 9   max min_per_class_accuracy  0.824797 0.703704  35}
\CommentTok{#> 10 max mean_per_class_accuracy  0.965774 0.740741  30}
\CommentTok{#> }
\CommentTok{#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`}
\end{Highlighting}
\end{Shaded}

Plotting the test performance's AUC plot shows us approximately how good the predictions are.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(perf)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{nn-classification_239b-deeplearning_h2o_arrhythmia-sglander_files/figure-latex/unnamed-chunk-30-1} \end{center}

We also want to know the log loss, MSE and AUC values, as well as other model metrics for the test data:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{h2o.logloss}\NormalTok{(perf)}
\CommentTok{#> [1] 1.75}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{h2o.mse}\NormalTok{(perf)}
\CommentTok{#> [1] 0.257}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{h2o.auc}\NormalTok{(perf)}
\CommentTok{#> [1] 0.788}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(}\KeywordTok{h2o.metric}\NormalTok{(perf))}
\CommentTok{#> Metrics for Thresholds: Binomial metrics as a function of classification thresholds}
\CommentTok{#>   threshold       f1       f2 f0point5 accuracy precision   recall}
\CommentTok{#> 1  0.990653 0.050000 0.031847 0.116279 0.424242  1.000000 0.025641}
\CommentTok{#> 2  0.990156 0.097561 0.063291 0.212766 0.439394  1.000000 0.051282}
\CommentTok{#> 3  0.989233 0.142857 0.094340 0.294118 0.454545  1.000000 0.076923}
\CommentTok{#> 4  0.989156 0.186047 0.125000 0.363636 0.469697  1.000000 0.102564}
\CommentTok{#> 5  0.988960 0.181818 0.124224 0.338983 0.454545  0.800000 0.102564}
\CommentTok{#> 6  0.988743 0.222222 0.154321 0.396825 0.469697  0.833333 0.128205}
\CommentTok{#>   specificity absolute_mcc min_per_class_accuracy mean_per_class_accuracy}
\CommentTok{#> 1    1.000000     0.103203               0.025641                0.512821}
\CommentTok{#> 2    1.000000     0.147087               0.051282                0.525641}
\CommentTok{#> 3    1.000000     0.181568               0.076923                0.538462}
\CommentTok{#> 4    1.000000     0.211341               0.102564                0.551282}
\CommentTok{#> 5    0.962963     0.121754               0.102564                0.532764}
\CommentTok{#> 6    0.962963     0.155921               0.128205                0.545584}
\CommentTok{#>   tns fns fps tps      tnr      fnr      fpr      tpr idx}
\CommentTok{#> 1  27  38   0   1 1.000000 0.974359 0.000000 0.025641   0}
\CommentTok{#> 2  27  37   0   2 1.000000 0.948718 0.000000 0.051282   1}
\CommentTok{#> 3  27  36   0   3 1.000000 0.923077 0.000000 0.076923   2}
\CommentTok{#> 4  27  35   0   4 1.000000 0.897436 0.000000 0.102564   3}
\CommentTok{#> 5  26  35   1   4 0.962963 0.897436 0.037037 0.102564   4}
\CommentTok{#> 6  26  34   1   5 0.962963 0.871795 0.037037 0.128205   5}
\end{Highlighting}
\end{Shaded}

The confusion matrix alone can be seen with the \texttt{h2o.confusionMatrix()} function, but is is also part of the performance summary.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{h2o.confusionMatrix}\NormalTok{(dl_model, test)}
\CommentTok{#> Confusion Matrix (vertical: actual; across: predicted)  for max f1 @ threshold = 2.48957253598693e-06:}
\CommentTok{#>            arrhythmia healthy    Error    Rate}
\CommentTok{#> arrhythmia         12      15 0.555556  =15/27}
\CommentTok{#> healthy             2      37 0.051282   =2/39}
\CommentTok{#> Totals             14      52 0.257576  =17/66}
\end{Highlighting}
\end{Shaded}

The final predictions with probabilities can be extracted with the \texttt{h2o.predict()} function. Beware though, that the number of correct and wrong classifications can be slightly different from the confusion matrix above.

Here, I combine the predictions with the actual test diagnoses and classes into a data frame. For plotting I also want to have a column, that tells me whether the predictions were correct. By default, a prediction probability above 0.5 will get scored as a prediction for the respective category. I find it often makes sense to be more stringent with this, though and set a higher threshold. Therefore, I am creating another column with stringent predictions, where I only count predictions that were made with more than 80\% probability. Everything that does not fall within this range gets scored as ``uncertain''. For these stringent predictions, I am also creating a column that tells me whether they were accurate.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{finalRf_predictions <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{class =} \KeywordTok{as.vector}\NormalTok{(test}\OperatorTok{$}\NormalTok{class), }
                                  \DataTypeTok{actual =} \KeywordTok{as.vector}\NormalTok{(test}\OperatorTok{$}\NormalTok{diagnosis), }
                                  \KeywordTok{as.data.frame}\NormalTok{(}\KeywordTok{h2o.predict}\NormalTok{(}\DataTypeTok{object =}\NormalTok{ dl_model, }
                                                            \DataTypeTok{newdata =}\NormalTok{ test)))}
\CommentTok{#> }
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|}\StringTok{                                                                 }\ErrorTok{|}\StringTok{   }\DecValTok{0}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=================================================================|}\StringTok{ }\DecValTok{100}\NormalTok{%}

\NormalTok{finalRf_predictions}\OperatorTok{$}\NormalTok{accurate <-}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(}
\NormalTok{  finalRf_predictions}\OperatorTok{$}\NormalTok{actual }\OperatorTok{==}\StringTok{ }\NormalTok{finalRf_predictions}\OperatorTok{$}\NormalTok{predict, }\StringTok{"yes"}\NormalTok{, }\StringTok{"no"}\NormalTok{)}

\NormalTok{finalRf_predictions}\OperatorTok{$}\NormalTok{predict_stringent <-}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(}
\NormalTok{  finalRf_predictions}\OperatorTok{$}\NormalTok{arrhythmia }\OperatorTok{>}\StringTok{ }\FloatTok{0.8}\NormalTok{, }\StringTok{"arrhythmia"}\NormalTok{,           }
  \KeywordTok{ifelse}\NormalTok{(finalRf_predictions}\OperatorTok{$}\NormalTok{healthy }\OperatorTok{>}\StringTok{ }\FloatTok{0.8}\NormalTok{, }\StringTok{"healthy"}\NormalTok{, }\StringTok{"uncertain"}\NormalTok{))}

\NormalTok{finalRf_predictions}\OperatorTok{$}\NormalTok{accurate_stringent <-}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(}
\NormalTok{  finalRf_predictions}\OperatorTok{$}\NormalTok{actual }\OperatorTok{==}\StringTok{ }\NormalTok{finalRf_predictions}\OperatorTok{$}\NormalTok{predict_stringent, }\StringTok{"yes"}\NormalTok{, }
  \KeywordTok{ifelse}\NormalTok{(finalRf_predictions}\OperatorTok{$}\NormalTok{predict_stringent }\OperatorTok{==}\StringTok{ "uncertain"}\NormalTok{, }\StringTok{"na"}\NormalTok{, }\StringTok{"no"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{finalRf_predictions }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(actual, predict) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{n =} \KeywordTok{n}\NormalTok{())}
\CommentTok{#> # A tibble: 4 x 3}
\CommentTok{#> # Groups:   actual [2]}
\CommentTok{#>   actual     predict        n}
\CommentTok{#>   <fct>      <fct>      <int>}
\CommentTok{#> 1 arrhythmia arrhythmia    15}
\CommentTok{#> 2 arrhythmia healthy       12}
\CommentTok{#> 3 healthy    arrhythmia     6}
\CommentTok{#> 4 healthy    healthy       33}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{finalRf_predictions }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(actual, predict_stringent) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{n =} \KeywordTok{n}\NormalTok{())}
\CommentTok{#> # A tibble: 5 x 3}
\CommentTok{#> # Groups:   actual [2]}
\CommentTok{#>   actual     predict_stringent     n}
\CommentTok{#>   <fct>      <chr>             <int>}
\CommentTok{#> 1 arrhythmia arrhythmia           19}
\CommentTok{#> 2 arrhythmia healthy               8}
\CommentTok{#> 3 healthy    arrhythmia            9}
\CommentTok{#> 4 healthy    healthy              28}
\CommentTok{#> 5 healthy    uncertain             2}
\end{Highlighting}
\end{Shaded}

To get a better overview, I am going to plot the predictions (default and stringent):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p1 <-}\StringTok{ }\NormalTok{finalRf_predictions }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ actual, }\DataTypeTok{fill =}\NormalTok{ accurate)) }\OperatorTok{+}
\StringTok{    }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{position =} \StringTok{"dodge"}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{scale_fill_brewer}\NormalTok{(}\DataTypeTok{palette =} \StringTok{"Set1"}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{my_theme}\NormalTok{() }\OperatorTok{+}
\StringTok{    }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{fill =} \StringTok{"Were}\CharTok{\textbackslash{}n}\StringTok{predictions}\CharTok{\textbackslash{}n}\StringTok{accurate?"}\NormalTok{,}
         \DataTypeTok{title =} \StringTok{"Default predictions"}\NormalTok{)}

\NormalTok{p2 <-}\StringTok{ }\NormalTok{finalRf_predictions }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{subset}\NormalTok{(accurate_stringent }\OperatorTok{!=}\StringTok{ "na"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ actual, }\DataTypeTok{fill =}\NormalTok{ accurate_stringent)) }\OperatorTok{+}
\StringTok{    }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{position =} \StringTok{"dodge"}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{scale_fill_brewer}\NormalTok{(}\DataTypeTok{palette =} \StringTok{"Set1"}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{my_theme}\NormalTok{() }\OperatorTok{+}
\StringTok{    }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{fill =} \StringTok{"Were}\CharTok{\textbackslash{}n}\StringTok{predictions}\CharTok{\textbackslash{}n}\StringTok{accurate?"}\NormalTok{,}
         \DataTypeTok{title =} \StringTok{"Stringent predictions"}\NormalTok{)}

\KeywordTok{grid.arrange}\NormalTok{(p1, p2, }\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{nn-classification_239b-deeplearning_h2o_arrhythmia-sglander_files/figure-latex/unnamed-chunk-39-1} \end{center}

Being more stringent with the prediction threshold slightly reduced the number of errors but not by much.

I also want to know whether there are certain classes of arrhythmia that are especially prone to being misclassified:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p1 <-}\StringTok{ }\KeywordTok{subset}\NormalTok{(finalRf_predictions, actual }\OperatorTok{==}\StringTok{ "arrhythmia"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ predict, }\DataTypeTok{fill =}\NormalTok{ class)) }\OperatorTok{+}
\StringTok{    }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{position =} \StringTok{"dodge"}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{my_theme}\NormalTok{() }\OperatorTok{+}
\StringTok{    }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{title =} \StringTok{"Prediction accuracy of arrhythmia cases"}\NormalTok{,}
         \DataTypeTok{subtitle =} \StringTok{"Default predictions"}\NormalTok{,}
         \DataTypeTok{x =} \StringTok{"predicted to be"}\NormalTok{)}

\NormalTok{p2 <-}\StringTok{ }\KeywordTok{subset}\NormalTok{(finalRf_predictions, actual }\OperatorTok{==}\StringTok{ "arrhythmia"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ predict_stringent, }\DataTypeTok{fill =}\NormalTok{ class)) }\OperatorTok{+}
\StringTok{    }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{position =} \StringTok{"dodge"}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{my_theme}\NormalTok{() }\OperatorTok{+}
\StringTok{    }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{title =} \StringTok{"Prediction accuracy of arrhythmia cases"}\NormalTok{,}
         \DataTypeTok{subtitle =} \StringTok{"Stringent predictions"}\NormalTok{,}
         \DataTypeTok{x =} \StringTok{"predicted to be"}\NormalTok{)}

\KeywordTok{grid.arrange}\NormalTok{(p1, p2, }\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{nn-classification_239b-deeplearning_h2o_arrhythmia-sglander_files/figure-latex/unnamed-chunk-40-1} \end{center}

There are no obvious biases towards some classes but with the small number of samples for most classes, this is difficult to assess.

\hypertarget{final-conclusions-how-useful-is-the-model}{%
\section{Final conclusions: How useful is the model?}\label{final-conclusions-how-useful-is-the-model}}

Most samples were classified correctly, but the total error was not particularly good. Moreover, when evaluating the usefulness of a specific model, we need to keep in mind what we want to achieve with it and which questions we want to answer. If we wanted to deploy this model in a clinical setting, it should assist with diagnosing patients. So, we need to think about what the consequences of wrong classifications would be. Would it be better to optimize for high sensitivity, in this example as many arrhythmia cases as possible get detected - with the drawback that we probably also diagnose a few healthy people? Or do we want to maximize precision, meaning that we could be confident that a patient who got predicted to have arrhythmia does indeed have it, while accepting that a few arrhythmia cases would remain undiagnosed? When we consider stringent predictions, this model correctly classified 19 out of 27 arrhythmia cases, but 6 were misdiagnosed. This would mean that some patients who were actually sick, wouldn't have gotten the correct treatment (if decided solely based on this model). For real-life application, this is obviously not sufficient!

Next week, I'll be trying to improve the model by doing a grid search for hyper-parameter tuning.

So, stay tuned\ldots{} (sorry, couldn't resist ;-))

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sessionInfo}\NormalTok{()}
\CommentTok{#> R version 3.6.0 (2019-04-26)}
\CommentTok{#> Platform: x86_64-pc-linux-gnu (64-bit)}
\CommentTok{#> Running under: Ubuntu 18.04.3 LTS}
\CommentTok{#> }
\CommentTok{#> Matrix products: default}
\CommentTok{#> BLAS/LAPACK: /usr/lib/x86_64-linux-gnu/libopenblasp-r0.2.20.so}
\CommentTok{#> }
\CommentTok{#> locale:}
\CommentTok{#>  [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              }
\CommentTok{#>  [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    }
\CommentTok{#>  [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   }
\CommentTok{#>  [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                 }
\CommentTok{#>  [9] LC_ADDRESS=C               LC_TELEPHONE=C            }
\CommentTok{#> [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       }
\CommentTok{#> }
\CommentTok{#> attached base packages:}
\CommentTok{#>  [1] stats4    parallel  grid      stats     graphics  grDevices utils    }
\CommentTok{#>  [8] datasets  methods   base     }
\CommentTok{#> }
\CommentTok{#> other attached packages:}
\CommentTok{#>  [1] reshape2_1.4.3       tidyr_0.8.3          matrixStats_0.54.0  }
\CommentTok{#>  [4] pcaGoPromoter_1.28.0 Biostrings_2.52.0    XVector_0.24.0      }
\CommentTok{#>  [7] IRanges_2.18.0       S4Vectors_0.22.0     BiocGenerics_0.30.0 }
\CommentTok{#> [10] ellipse_0.4.1        gridExtra_2.3        ggrepel_0.8.1       }
\CommentTok{#> [13] ggplot2_3.1.1        h2o_3.22.1.1         dplyr_0.8.0.1       }
\CommentTok{#> [16] logging_0.9-107     }
\CommentTok{#> }
\CommentTok{#> loaded via a namespace (and not attached):}
\CommentTok{#>  [1] Rcpp_1.0.1           assertthat_0.2.1     zeallot_0.1.0       }
\CommentTok{#>  [4] rprojroot_1.3-2      digest_0.6.18        utf8_1.1.4          }
\CommentTok{#>  [7] R6_2.4.0             plyr_1.8.4           backports_1.1.4     }
\CommentTok{#> [10] RSQLite_2.1.1        evaluate_0.13        pillar_1.4.0        }
\CommentTok{#> [13] zlibbioc_1.30.0      rlang_0.3.4          lazyeval_0.2.2      }
\CommentTok{#> [16] rstudioapi_0.10      blob_1.1.1           rmarkdown_1.12      }
\CommentTok{#> [19] labeling_0.3         stringr_1.4.0        RCurl_1.95-4.12     }
\CommentTok{#> [22] bit_1.1-14           munsell_0.5.0        compiler_3.6.0      }
\CommentTok{#> [25] xfun_0.6             pkgconfig_2.0.2      htmltools_0.3.6     }
\CommentTok{#> [28] tidyselect_0.2.5     tibble_2.1.1         bookdown_0.10       }
\CommentTok{#> [31] fansi_0.4.0          crayon_1.3.4         withr_2.1.2         }
\CommentTok{#> [34] bitops_1.0-6         jsonlite_1.6         gtable_0.3.0        }
\CommentTok{#> [37] DBI_1.0.0            magrittr_1.5         scales_1.0.0        }
\CommentTok{#> [40] cli_1.1.0            stringi_1.4.3        vctrs_0.1.0         }
\CommentTok{#> [43] RColorBrewer_1.1-2   tools_3.6.0          bit64_0.9-7         }
\CommentTok{#> [46] Biobase_2.44.0       glue_1.3.1           purrr_0.3.2         }
\CommentTok{#> [49] yaml_2.2.0           AnnotationDbi_1.46.0 colorspace_1.4-1    }
\CommentTok{#> [52] memoise_1.1.0        knitr_1.22}
\end{Highlighting}
\end{Shaded}

\hypertarget{credit-scoring}{%
\chapter{Credit Scoring}\label{credit-scoring}}

\hypertarget{introduction-18}{%
\section{Introduction}\label{introduction-18}}

Source: \url{https://www.r-bloggers.com/using-neural-networks-for-credit-scoring-a-simple-example/}

\hypertarget{motivation}{%
\section{Motivation}\label{motivation}}

Credit scoring is the practice of analysing a persons background and credit application in order to assess the creditworthiness of the person. One can take numerous approaches on analysing this creditworthiness. In the end it basically comes down to first selecting the correct independent variables (e.g.~income, age, gender) that lead to a given level of creditworthiness.

In other words: \texttt{creditworthiness\ =\ f(income,\ age,\ gender,\ \ldots{})}.

A creditscoring system can be represented by linear regression, logistic regression, machine learning or a combination of these. Neural networks are situated in the domain of machine learning. The following is an strongly simplified example. The actual procedure of building a credit scoring system is much more complex and the resulting model will most likely not consist of solely or even a neural network.

If you're unsure on what a neural network exactly is, I find this a good place to start.

For this example the R package \texttt{neuralnet} is used, for a more in-depth view on the exact workings of the package see neuralnet: \texttt{Training\ of\ Neural\ Networks} by F. Gnther and S. Fritsch.

\hypertarget{load-the-data}{%
\section{load the data}\label{load-the-data}}

Dataset downloaded: \url{https://gist.github.com/Bart6114/8675941\#file-creditset-csv}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1234567890}\NormalTok{)}

\KeywordTok{library}\NormalTok{(neuralnet)}

\NormalTok{dataset <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\KeywordTok{file.path}\NormalTok{(data_raw_dir, }\StringTok{"creditset.csv"}\NormalTok{))}
\KeywordTok{head}\NormalTok{(dataset)}
\CommentTok{#>   clientid income  age   loan      LTI default10yr}
\CommentTok{#> 1        1  66156 59.0 8106.5 0.122537           0}
\CommentTok{#> 2        2  34415 48.1 6564.7 0.190752           0}
\CommentTok{#> 3        3  57317 63.1 8021.0 0.139940           0}
\CommentTok{#> 4        4  42710 45.8 6103.6 0.142911           0}
\CommentTok{#> 5        5  66953 18.6 8770.1 0.130989           1}
\CommentTok{#> 6        6  24904 57.5   15.5 0.000622           0}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{names}\NormalTok{(dataset)}
\CommentTok{#> [1] "clientid"    "income"      "age"         "loan"        "LTI"        }
\CommentTok{#> [6] "default10yr"}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(dataset)}
\CommentTok{#>     clientid        income           age            loan      }
\CommentTok{#>  Min.   :   1   Min.   :20014   Min.   :18.1   Min.   :    1  }
\CommentTok{#>  1st Qu.: 501   1st Qu.:32796   1st Qu.:29.1   1st Qu.: 1940  }
\CommentTok{#>  Median :1000   Median :45789   Median :41.4   Median : 3975  }
\CommentTok{#>  Mean   :1000   Mean   :45332   Mean   :40.9   Mean   : 4444  }
\CommentTok{#>  3rd Qu.:1500   3rd Qu.:57791   3rd Qu.:52.6   3rd Qu.: 6432  }
\CommentTok{#>  Max.   :2000   Max.   :69996   Max.   :64.0   Max.   :13766  }
\CommentTok{#>       LTI          default10yr   }
\CommentTok{#>  Min.   :0.0000   Min.   :0.000  }
\CommentTok{#>  1st Qu.:0.0479   1st Qu.:0.000  }
\CommentTok{#>  Median :0.0994   Median :0.000  }
\CommentTok{#>  Mean   :0.0984   Mean   :0.142  }
\CommentTok{#>  3rd Qu.:0.1476   3rd Qu.:0.000  }
\CommentTok{#>  Max.   :0.1999   Max.   :1.000}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# distribution of defaults}
\KeywordTok{table}\NormalTok{(dataset}\OperatorTok{$}\NormalTok{default10yr)}
\CommentTok{#> }
\CommentTok{#>    0    1 }
\CommentTok{#> 1717  283}
\KeywordTok{min}\NormalTok{(dataset}\OperatorTok{$}\NormalTok{LTI)}
\CommentTok{#> [1] 4.91e-05}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{jitter}\NormalTok{(dataset}\OperatorTok{$}\NormalTok{default10yr, }\DecValTok{1}\NormalTok{) }\OperatorTok{~}\StringTok{ }\KeywordTok{jitter}\NormalTok{(dataset}\OperatorTok{$}\NormalTok{LTI, }\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{nn-classification_900-credit_neuralnet_files/figure-latex/unnamed-chunk-5-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# convert LTI continuous variable to categorical}
\NormalTok{dataset}\OperatorTok{$}\NormalTok{LTIrng <-}\StringTok{ }\KeywordTok{cut}\NormalTok{(dataset}\OperatorTok{$}\NormalTok{LTI, }\DataTypeTok{breaks =} \DecValTok{10}\NormalTok{)}
\KeywordTok{unique}\NormalTok{(dataset}\OperatorTok{$}\NormalTok{LTIrng)}
\CommentTok{#>  [1] (0.12,0.14]      (0.18,0.2]       (0.14,0.16]      (-0.000151,0.02]}
\CommentTok{#>  [5] (0.1,0.12]       (0.04,0.06]      (0.06,0.08]      (0.08,0.1]      }
\CommentTok{#>  [9] (0.16,0.18]      (0.02,0.04]     }
\CommentTok{#> 10 Levels: (-0.000151,0.02] (0.02,0.04] (0.04,0.06] ... (0.18,0.2]}
\KeywordTok{plot}\NormalTok{(dataset}\OperatorTok{$}\NormalTok{LTIrng, dataset}\OperatorTok{$}\NormalTok{default10yr)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{nn-classification_900-credit_neuralnet_files/figure-latex/unnamed-chunk-6-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# what age and LTI is more likely to default}
\KeywordTok{library}\NormalTok{(ggplot2)}
\CommentTok{#> Registered S3 methods overwritten by 'ggplot2':}
\CommentTok{#>   method         from }
\CommentTok{#>   [.quosures     rlang}
\CommentTok{#>   c.quosures     rlang}
\CommentTok{#>   print.quosures rlang}

\KeywordTok{ggplot}\NormalTok{(dataset, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ age, }\DataTypeTok{y =}\NormalTok{ LTI, }\DataTypeTok{col =}\NormalTok{ default10yr)) }\OperatorTok{+}
\StringTok{    }\KeywordTok{geom_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{nn-classification_900-credit_neuralnet_files/figure-latex/unnamed-chunk-7-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# what age and loan size is more likely to default}
\KeywordTok{library}\NormalTok{(ggplot2)}

\KeywordTok{ggplot}\NormalTok{(dataset, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ age, }\DataTypeTok{y =}\NormalTok{ loan, }\DataTypeTok{col =}\NormalTok{ default10yr)) }\OperatorTok{+}
\StringTok{    }\KeywordTok{geom_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{nn-classification_900-credit_neuralnet_files/figure-latex/unnamed-chunk-8-1} \end{center}

\hypertarget{objective}{%
\section{Objective}\label{objective}}

The dataset contains information on different clients who received a loan at least 10 years ago. The variables income (yearly), age, loan (size in euros) and LTI (the loan to yearly income ratio) are available. Our goal is to devise a model which predicts, based on the input variables LTI and age, whether or not a default will occur within 10 years.

\hypertarget{steps}{%
\section{Steps}\label{steps}}

The dataset will be split up in a subset used for training the neural network and another set used for testing. As the ordering of the dataset is completely random, we do not have to extract random rows and can just take the first x rows.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## extract a set to train the NN}
\NormalTok{trainset <-}\StringTok{ }\NormalTok{dataset[}\DecValTok{1}\OperatorTok{:}\DecValTok{800}\NormalTok{, ]}

\CommentTok{## select the test set}
\NormalTok{testset <-}\StringTok{ }\NormalTok{dataset[}\DecValTok{801}\OperatorTok{:}\DecValTok{2000}\NormalTok{, ]}
\end{Highlighting}
\end{Shaded}

\hypertarget{build-the-neural-network}{%
\subsection{Build the neural network}\label{build-the-neural-network}}

Now we'll build a neural network with 4 hidden nodes (a neural network is comprised of an input, hidden and output nodes). The number of nodes is chosen here without a clear method, however there are some rules of thumb. The \texttt{lifesign} option refers to the verbosity. The \texttt{ouput} is not linear and we will use a \texttt{threshold} value of 10\%. The \texttt{neuralnet} package uses resilient backpropagation with weight backtracking as its standard algorithm.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## build the neural network (NN)}
\NormalTok{creditnet <-}\StringTok{ }\KeywordTok{neuralnet}\NormalTok{(default10yr }\OperatorTok{~}\StringTok{ }\NormalTok{LTI }\OperatorTok{+}\StringTok{ }\NormalTok{age, trainset, }
                       \DataTypeTok{hidden =} \DecValTok{4}\NormalTok{, }
                       \DataTypeTok{lifesign =} \StringTok{"minimal"}\NormalTok{, }
                       \DataTypeTok{linear.output =} \OtherTok{FALSE}\NormalTok{, }
                       \DataTypeTok{threshold =} \FloatTok{0.1}\NormalTok{)}
\CommentTok{#> hidden: 4    thresh: 0.1    rep: 1/1    steps:   44487   error: 0.20554  time: 9.49 secs}
\end{Highlighting}
\end{Shaded}

The neuralnet package also has the possibility to visualize the generated model and show the found weights.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## plot the NN}
\KeywordTok{plot}\NormalTok{(creditnet, }\DataTypeTok{rep =} \StringTok{"best"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{nn-classification_900-credit_neuralnet_files/figure-latex/unnamed-chunk-11-1} \end{center}

\hypertarget{test-the-neural-network}{%
\section{Test the neural network}\label{test-the-neural-network}}

Once we've trained the neural network we are ready to test it. We use the testset subset for this. The \texttt{compute} function is applied for computing the outputs based on the LTI and age inputs from the testset.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## test the resulting output}
\NormalTok{temp_test <-}\StringTok{ }\KeywordTok{subset}\NormalTok{(testset, }\DataTypeTok{select =} \KeywordTok{c}\NormalTok{(}\StringTok{"LTI"}\NormalTok{, }\StringTok{"age"}\NormalTok{))}

\NormalTok{creditnet.results <-}\StringTok{ }\KeywordTok{compute}\NormalTok{(creditnet, temp_test)}
\end{Highlighting}
\end{Shaded}

The temp dataset contains only the columns LTI and age of the train set. Only these variables are used for input. The set looks as follows:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(temp_test)}
\CommentTok{#>        LTI  age}
\CommentTok{#> 801 0.0231 25.9}
\CommentTok{#> 802 0.1373 40.8}
\CommentTok{#> 803 0.1046 32.5}
\CommentTok{#> 804 0.1599 53.2}
\CommentTok{#> 805 0.1116 46.5}
\CommentTok{#> 806 0.1149 47.1}
\end{Highlighting}
\end{Shaded}

Let's have a look at what the neural network produced:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{actual =}\NormalTok{ testset}\OperatorTok{$}\NormalTok{default10yr, }\DataTypeTok{prediction =}\NormalTok{ creditnet.results}\OperatorTok{$}\NormalTok{net.result)}
\NormalTok{results[}\DecValTok{100}\OperatorTok{:}\DecValTok{115}\NormalTok{, ]}
\CommentTok{#>     actual prediction}
\CommentTok{#> 900      0   7.29e-32}
\CommentTok{#> 901      0   8.17e-11}
\CommentTok{#> 902      0   4.33e-45}
\CommentTok{#> 903      1   1.00e+00}
\CommentTok{#> 904      0   8.06e-04}
\CommentTok{#> 905      0   3.54e-40}
\CommentTok{#> 906      0   1.48e-24}
\CommentTok{#> 907      1   1.00e+00}
\CommentTok{#> 908      0   1.11e-02}
\CommentTok{#> 909      0   8.05e-44}
\CommentTok{#> 910      0   6.72e-07}
\CommentTok{#> 911      1   1.00e+00}
\CommentTok{#> 912      0   9.97e-59}
\CommentTok{#> 913      1   1.00e+00}
\CommentTok{#> 914      0   3.39e-37}
\CommentTok{#> 915      0   1.18e-07}
\end{Highlighting}
\end{Shaded}

We can round to the nearest integer to improve readability:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results}\OperatorTok{$}\NormalTok{prediction <-}\StringTok{ }\KeywordTok{round}\NormalTok{(results}\OperatorTok{$}\NormalTok{prediction)}
\NormalTok{results[}\DecValTok{100}\OperatorTok{:}\DecValTok{115}\NormalTok{, ]}
\CommentTok{#>     actual prediction}
\CommentTok{#> 900      0          0}
\CommentTok{#> 901      0          0}
\CommentTok{#> 902      0          0}
\CommentTok{#> 903      1          1}
\CommentTok{#> 904      0          0}
\CommentTok{#> 905      0          0}
\CommentTok{#> 906      0          0}
\CommentTok{#> 907      1          1}
\CommentTok{#> 908      0          0}
\CommentTok{#> 909      0          0}
\CommentTok{#> 910      0          0}
\CommentTok{#> 911      1          1}
\CommentTok{#> 912      0          0}
\CommentTok{#> 913      1          1}
\CommentTok{#> 914      0          0}
\CommentTok{#> 915      0          0}
\end{Highlighting}
\end{Shaded}

As you can see it is pretty close! As already stated, this is a strongly simplified example. But it might serve as a basis for you to play around with your first neural network.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# how many predictions were wrong}
\NormalTok{indices <-}\StringTok{ }\KeywordTok{which}\NormalTok{(results}\OperatorTok{$}\NormalTok{actual }\OperatorTok{!=}\StringTok{ }\NormalTok{results}\OperatorTok{$}\NormalTok{prediction)}
\NormalTok{indices}
\CommentTok{#> [1]  330 1008}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# what are the predictions that failed}
\NormalTok{results[indices,]}
\CommentTok{#>      actual prediction}
\CommentTok{#> 1130      0          1}
\CommentTok{#> 1808      1          0}
\end{Highlighting}
\end{Shaded}

\hypertarget{build-a-fully-connected-neural-network-from-scratch}{%
\chapter{Build a fully connected neural network from scratch}\label{build-a-fully-connected-neural-network-from-scratch}}

\hypertarget{introduction-19}{%
\section{Introduction}\label{introduction-19}}

\url{http://www.parallelr.com/r-deep-neural-network-from-scratch/}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(neuralnet)}

\CommentTok{# Copyright 2016: www.ParallelR.com}
\CommentTok{# Parallel Blog : R For Deep Learning (I): Build Fully Connected Neural Network From Scratch}
\CommentTok{# Classification by 2-layers DNN and tested by iris dataset}
\CommentTok{# Author: Peng Zhao, patric.zhao@gmail.com}

\CommentTok{# Prediction}
\NormalTok{predict.dnn <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(model, }\DataTypeTok{data =}\NormalTok{ X.test) \{}
  \CommentTok{# new data, transfer to matrix}
\NormalTok{  new.data <-}\StringTok{ }\KeywordTok{data.matrix}\NormalTok{(data)}
  
  \CommentTok{# Feed Forwad}
\NormalTok{  hidden.layer <-}\StringTok{ }\KeywordTok{sweep}\NormalTok{(new.data }\OperatorTok{%*%}\StringTok{ }\NormalTok{model}\OperatorTok{$}\NormalTok{W1 ,}\DecValTok{2}\NormalTok{, model}\OperatorTok{$}\NormalTok{b1, }\StringTok{'+'}\NormalTok{)}
  \CommentTok{# neurons : Rectified Linear}
\NormalTok{  hidden.layer <-}\StringTok{ }\KeywordTok{pmax}\NormalTok{(hidden.layer, }\DecValTok{0}\NormalTok{)}
\NormalTok{  score <-}\StringTok{ }\KeywordTok{sweep}\NormalTok{(hidden.layer }\OperatorTok{%*%}\StringTok{ }\NormalTok{model}\OperatorTok{$}\NormalTok{W2, }\DecValTok{2}\NormalTok{, model}\OperatorTok{$}\NormalTok{b2, }\StringTok{'+'}\NormalTok{)}
  
  \CommentTok{# Loss Function: softmax}
\NormalTok{  score.exp <-}\StringTok{ }\KeywordTok{exp}\NormalTok{(score)}
\NormalTok{  probs <-}\KeywordTok{sweep}\NormalTok{(score.exp, }\DecValTok{1}\NormalTok{, }\KeywordTok{rowSums}\NormalTok{(score.exp), }\StringTok{'/'}\NormalTok{) }
  
  \CommentTok{# select max possiblity}
\NormalTok{  labels.predicted <-}\StringTok{ }\KeywordTok{max.col}\NormalTok{(probs)}
  \KeywordTok{return}\NormalTok{(labels.predicted)}
\NormalTok{\}}

\CommentTok{# Train: build and train a 2-layers neural network }
\NormalTok{train.dnn <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x, y, }\DataTypeTok{traindata=}\NormalTok{data, }\DataTypeTok{testdata=}\OtherTok{NULL}\NormalTok{,}
                  \DataTypeTok{model =} \OtherTok{NULL}\NormalTok{,}
                  \CommentTok{# set hidden layers and neurons}
                  \CommentTok{# currently, only support 1 hidden layer}
                  \DataTypeTok{hidden=}\KeywordTok{c}\NormalTok{(}\DecValTok{6}\NormalTok{), }
                  \CommentTok{# max iteration steps}
                  \DataTypeTok{maxit=}\DecValTok{2000}\NormalTok{,}
                  \CommentTok{# delta loss }
                  \DataTypeTok{abstol=}\FloatTok{1e-2}\NormalTok{,}
                  \CommentTok{# learning rate}
                  \DataTypeTok{lr =} \FloatTok{1e-2}\NormalTok{,}
                  \CommentTok{# regularization rate}
                  \DataTypeTok{reg =} \FloatTok{1e-3}\NormalTok{,}
                  \CommentTok{# show results every 'display' step}
                  \DataTypeTok{display =} \DecValTok{100}\NormalTok{,}
                  \DataTypeTok{random.seed =} \DecValTok{1}\NormalTok{)}
\NormalTok{\{}
  \CommentTok{# to make the case reproducible.}
  \KeywordTok{set.seed}\NormalTok{(random.seed)}
  
  \CommentTok{# total number of training set}
\NormalTok{  N <-}\StringTok{ }\KeywordTok{nrow}\NormalTok{(traindata)}
  
  \CommentTok{# extract the data and label}
  \CommentTok{# don't need atribute }
\NormalTok{  X <-}\StringTok{ }\KeywordTok{unname}\NormalTok{(}\KeywordTok{data.matrix}\NormalTok{(traindata[,x]))}
  \CommentTok{# correct categories represented by integer }
\NormalTok{  Y <-}\StringTok{ }\NormalTok{traindata[,y]}
  \ControlFlowTok{if}\NormalTok{(}\KeywordTok{is.factor}\NormalTok{(Y)) \{ Y <-}\StringTok{ }\KeywordTok{as.integer}\NormalTok{(Y) \}}
  \CommentTok{# create index for both row and col}
  \CommentTok{# create index for both row and col}
\NormalTok{  Y.len   <-}\StringTok{ }\KeywordTok{length}\NormalTok{(}\KeywordTok{unique}\NormalTok{(Y))}
\NormalTok{  Y.set   <-}\StringTok{ }\KeywordTok{sort}\NormalTok{(}\KeywordTok{unique}\NormalTok{(Y))}
\NormalTok{  Y.index <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\NormalTok{N, }\KeywordTok{match}\NormalTok{(Y, Y.set))}

  \CommentTok{# create model or get model from parameter}
  \ControlFlowTok{if}\NormalTok{(}\KeywordTok{is.null}\NormalTok{(model)) \{}
       \CommentTok{# number of input features}
\NormalTok{       D <-}\StringTok{ }\KeywordTok{ncol}\NormalTok{(X)}
       \CommentTok{# number of categories for classification}
\NormalTok{       K <-}\StringTok{ }\KeywordTok{length}\NormalTok{(}\KeywordTok{unique}\NormalTok{(Y))}
\NormalTok{       H <-}\StringTok{  }\NormalTok{hidden}
  
       \CommentTok{# create and init weights and bias }
\NormalTok{       W1 <-}\StringTok{ }\FloatTok{0.01}\OperatorTok{*}\KeywordTok{matrix}\NormalTok{(}\KeywordTok{rnorm}\NormalTok{(D}\OperatorTok{*}\NormalTok{H), }\DataTypeTok{nrow=}\NormalTok{D, }\DataTypeTok{ncol=}\NormalTok{H)}
\NormalTok{       b1 <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DataTypeTok{nrow=}\DecValTok{1}\NormalTok{, }\DataTypeTok{ncol=}\NormalTok{H)}
  
\NormalTok{       W2 <-}\StringTok{ }\FloatTok{0.01}\OperatorTok{*}\KeywordTok{matrix}\NormalTok{(}\KeywordTok{rnorm}\NormalTok{(H}\OperatorTok{*}\NormalTok{K), }\DataTypeTok{nrow=}\NormalTok{H, }\DataTypeTok{ncol=}\NormalTok{K)}
\NormalTok{       b2 <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DataTypeTok{nrow=}\DecValTok{1}\NormalTok{, }\DataTypeTok{ncol=}\NormalTok{K)}
\NormalTok{  \} }\ControlFlowTok{else}\NormalTok{ \{}
\NormalTok{       D  <-}\StringTok{ }\NormalTok{model}\OperatorTok{$}\NormalTok{D}
\NormalTok{       K  <-}\StringTok{ }\NormalTok{model}\OperatorTok{$}\NormalTok{K}
\NormalTok{       H  <-}\StringTok{ }\NormalTok{model}\OperatorTok{$}\NormalTok{H}
\NormalTok{       W1 <-}\StringTok{ }\NormalTok{model}\OperatorTok{$}\NormalTok{W1}
\NormalTok{       b1 <-}\StringTok{ }\NormalTok{model}\OperatorTok{$}\NormalTok{b1}
\NormalTok{       W2 <-}\StringTok{ }\NormalTok{model}\OperatorTok{$}\NormalTok{W2}
\NormalTok{       b2 <-}\StringTok{ }\NormalTok{model}\OperatorTok{$}\NormalTok{b2}
\NormalTok{  \}}
  
  \CommentTok{# use all train data to update weights since it's a small dataset}
\NormalTok{  batchsize <-}\StringTok{ }\NormalTok{N}
  \CommentTok{# init loss to a very big value}
\NormalTok{  loss <-}\StringTok{ }\DecValTok{100000}
  
  \CommentTok{# Training the network}
\NormalTok{  i <-}\StringTok{ }\DecValTok{0}
  \ControlFlowTok{while}\NormalTok{(i }\OperatorTok{<}\StringTok{ }\NormalTok{maxit }\OperatorTok{&&}\StringTok{ }\NormalTok{loss }\OperatorTok{>}\StringTok{ }\NormalTok{abstol ) \{}
    
    \CommentTok{# iteration index}
\NormalTok{    i <-}\StringTok{ }\NormalTok{i }\OperatorTok{+}\DecValTok{1}
    
    \CommentTok{# forward ....}
    \CommentTok{# 1 indicate row, 2 indicate col}
\NormalTok{    hidden.layer <-}\StringTok{ }\KeywordTok{sweep}\NormalTok{(X }\OperatorTok{%*%}\StringTok{ }\NormalTok{W1 ,}\DecValTok{2}\NormalTok{, b1, }\StringTok{'+'}\NormalTok{)}
    \CommentTok{# neurons : ReLU}
\NormalTok{    hidden.layer <-}\StringTok{ }\KeywordTok{pmax}\NormalTok{(hidden.layer, }\DecValTok{0}\NormalTok{)}
\NormalTok{    score <-}\StringTok{ }\KeywordTok{sweep}\NormalTok{(hidden.layer }\OperatorTok{%*%}\StringTok{ }\NormalTok{W2, }\DecValTok{2}\NormalTok{, b2, }\StringTok{'+'}\NormalTok{)}
    
    \CommentTok{# softmax}
\NormalTok{    score.exp <-}\StringTok{ }\KeywordTok{exp}\NormalTok{(score)}
    \CommentTok{# debug}
\NormalTok{    probs <-}\StringTok{ }\NormalTok{score.exp}\OperatorTok{/}\KeywordTok{rowSums}\NormalTok{(score.exp)}
    
    \CommentTok{# compute the loss}
\NormalTok{    corect.logprobs <-}\StringTok{ }\OperatorTok{-}\KeywordTok{log}\NormalTok{(probs[Y.index])}
\NormalTok{    data.loss  <-}\StringTok{ }\KeywordTok{sum}\NormalTok{(corect.logprobs)}\OperatorTok{/}\NormalTok{batchsize}
\NormalTok{    reg.loss   <-}\StringTok{ }\FloatTok{0.5}\OperatorTok{*}\NormalTok{reg}\OperatorTok{*}\StringTok{ }\NormalTok{(}\KeywordTok{sum}\NormalTok{(W1}\OperatorTok{*}\NormalTok{W1) }\OperatorTok{+}\StringTok{ }\KeywordTok{sum}\NormalTok{(W2}\OperatorTok{*}\NormalTok{W2))}
\NormalTok{    loss <-}\StringTok{ }\NormalTok{data.loss }\OperatorTok{+}\StringTok{ }\NormalTok{reg.loss}
    
    \CommentTok{# display results and update model}
    \ControlFlowTok{if}\NormalTok{( i }\OperatorTok{%%}\StringTok{ }\NormalTok{display }\OperatorTok{==}\StringTok{ }\DecValTok{0}\NormalTok{) \{}
        \ControlFlowTok{if}\NormalTok{(}\OperatorTok{!}\KeywordTok{is.null}\NormalTok{(testdata)) \{}
\NormalTok{            model <-}\StringTok{ }\KeywordTok{list}\NormalTok{( }\DataTypeTok{D =}\NormalTok{ D,}
                           \DataTypeTok{H =}\NormalTok{ H,}
                           \DataTypeTok{K =}\NormalTok{ K,}
                           \CommentTok{# weights and bias}
                           \DataTypeTok{W1 =}\NormalTok{ W1, }
                           \DataTypeTok{b1 =}\NormalTok{ b1, }
                           \DataTypeTok{W2 =}\NormalTok{ W2, }
                           \DataTypeTok{b2 =}\NormalTok{ b2)}
\NormalTok{            labs <-}\StringTok{ }\KeywordTok{predict.dnn}\NormalTok{(model, testdata[,}\OperatorTok{-}\NormalTok{y])}
\NormalTok{            accuracy <-}\StringTok{ }\KeywordTok{mean}\NormalTok{(}\KeywordTok{as.integer}\NormalTok{(testdata[,y]) }\OperatorTok{==}\StringTok{ }\NormalTok{Y.set[labs])}
            \KeywordTok{cat}\NormalTok{(i, loss, accuracy, }\StringTok{"}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\NormalTok{        \} }\ControlFlowTok{else}\NormalTok{ \{}
            \KeywordTok{cat}\NormalTok{(i, loss, }\StringTok{"}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\NormalTok{        \}}
\NormalTok{    \}}
    
    \CommentTok{# backward ....}
\NormalTok{    dscores <-}\StringTok{ }\NormalTok{probs}
\NormalTok{    dscores[Y.index] <-}\StringTok{ }\NormalTok{dscores[Y.index] }\DecValTok{-1}
\NormalTok{    dscores <-}\StringTok{ }\NormalTok{dscores }\OperatorTok{/}\StringTok{ }\NormalTok{batchsize}
    
    
\NormalTok{    dW2 <-}\StringTok{ }\KeywordTok{t}\NormalTok{(hidden.layer) }\OperatorTok{%*%}\StringTok{ }\NormalTok{dscores }
\NormalTok{    db2 <-}\StringTok{ }\KeywordTok{colSums}\NormalTok{(dscores)}
    
\NormalTok{    dhidden <-}\StringTok{ }\NormalTok{dscores }\OperatorTok{%*%}\StringTok{ }\KeywordTok{t}\NormalTok{(W2)}
\NormalTok{    dhidden[hidden.layer }\OperatorTok{<=}\StringTok{ }\DecValTok{0}\NormalTok{] <-}\StringTok{ }\DecValTok{0}
    
\NormalTok{    dW1 <-}\StringTok{ }\KeywordTok{t}\NormalTok{(X) }\OperatorTok{%*%}\StringTok{ }\NormalTok{dhidden}
\NormalTok{    db1 <-}\StringTok{ }\KeywordTok{colSums}\NormalTok{(dhidden) }
    
    \CommentTok{# update ....}
\NormalTok{    dW2 <-}\StringTok{ }\NormalTok{dW2 }\OperatorTok{+}\StringTok{ }\NormalTok{reg}\OperatorTok{*}\NormalTok{W2}
\NormalTok{    dW1 <-}\StringTok{ }\NormalTok{dW1  }\OperatorTok{+}\StringTok{ }\NormalTok{reg}\OperatorTok{*}\NormalTok{W1}
    
\NormalTok{    W1 <-}\StringTok{ }\NormalTok{W1 }\OperatorTok{-}\StringTok{ }\NormalTok{lr }\OperatorTok{*}\StringTok{ }\NormalTok{dW1}
\NormalTok{    b1 <-}\StringTok{ }\NormalTok{b1 }\OperatorTok{-}\StringTok{ }\NormalTok{lr }\OperatorTok{*}\StringTok{ }\NormalTok{db1}
    
\NormalTok{    W2 <-}\StringTok{ }\NormalTok{W2 }\OperatorTok{-}\StringTok{ }\NormalTok{lr }\OperatorTok{*}\StringTok{ }\NormalTok{dW2}
\NormalTok{    b2 <-}\StringTok{ }\NormalTok{b2 }\OperatorTok{-}\StringTok{ }\NormalTok{lr }\OperatorTok{*}\StringTok{ }\NormalTok{db2}
    
   
    
\NormalTok{  \}}
  
  \CommentTok{# final results}
  \CommentTok{# creat list to store learned parameters}
  \CommentTok{# you can add more parameters for debug and visualization}
  \CommentTok{# such as residuals, fitted.values ...}
\NormalTok{  model <-}\StringTok{ }\KeywordTok{list}\NormalTok{( }\DataTypeTok{D =}\NormalTok{ D,}
                 \DataTypeTok{H =}\NormalTok{ H,}
                 \DataTypeTok{K =}\NormalTok{ K,}
                 \CommentTok{# weights and bias}
                 \DataTypeTok{W1=}\NormalTok{ W1, }
                 \DataTypeTok{b1=}\NormalTok{ b1, }
                 \DataTypeTok{W2=}\NormalTok{ W2, }
                 \DataTypeTok{b2=}\NormalTok{ b2)}
    
  \KeywordTok{return}\NormalTok{(model)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{########################################################################}
\CommentTok{# testing}
\CommentTok{#######################################################################}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}

\CommentTok{# 0. EDA}
\KeywordTok{summary}\NormalTok{(iris)}
\CommentTok{#>   Sepal.Length   Sepal.Width    Petal.Length   Petal.Width }
\CommentTok{#>  Min.   :4.30   Min.   :2.00   Min.   :1.00   Min.   :0.1  }
\CommentTok{#>  1st Qu.:5.10   1st Qu.:2.80   1st Qu.:1.60   1st Qu.:0.3  }
\CommentTok{#>  Median :5.80   Median :3.00   Median :4.35   Median :1.3  }
\CommentTok{#>  Mean   :5.84   Mean   :3.06   Mean   :3.76   Mean   :1.2  }
\CommentTok{#>  3rd Qu.:6.40   3rd Qu.:3.30   3rd Qu.:5.10   3rd Qu.:1.8  }
\CommentTok{#>  Max.   :7.90   Max.   :4.40   Max.   :6.90   Max.   :2.5  }
\CommentTok{#>        Species  }
\CommentTok{#>  setosa    :50  }
\CommentTok{#>  versicolor:50  }
\CommentTok{#>  virginica :50  }
\CommentTok{#>                 }
\CommentTok{#>                 }
\CommentTok{#> }
\KeywordTok{plot}\NormalTok{(iris)}

\CommentTok{# 1. split data into test/train}
\NormalTok{samp <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{50}\NormalTok{,}\DecValTok{25}\NormalTok{), }\KeywordTok{sample}\NormalTok{(}\DecValTok{51}\OperatorTok{:}\DecValTok{100}\NormalTok{,}\DecValTok{25}\NormalTok{), }\KeywordTok{sample}\NormalTok{(}\DecValTok{101}\OperatorTok{:}\DecValTok{150}\NormalTok{,}\DecValTok{25}\NormalTok{))}

\CommentTok{# 2. train model}
\NormalTok{ ir.model <-}\StringTok{ }\KeywordTok{train.dnn}\NormalTok{(}\DataTypeTok{x=}\DecValTok{1}\OperatorTok{:}\DecValTok{4}\NormalTok{, }\DataTypeTok{y=}\DecValTok{5}\NormalTok{, }\DataTypeTok{traindata=}\NormalTok{iris[samp,], }\DataTypeTok{testdata=}\NormalTok{iris[}\OperatorTok{-}\NormalTok{samp,], }\DataTypeTok{hidden=}\DecValTok{10}\NormalTok{, }\DataTypeTok{maxit=}\DecValTok{2000}\NormalTok{, }\DataTypeTok{display=}\DecValTok{50}\NormalTok{)}
\CommentTok{#> 50 1.1 0.333 }
\CommentTok{#> 100 1.1 0.333 }
\CommentTok{#> 150 1.09 0.333 }
\CommentTok{#> 200 1.08 0.333 }
\CommentTok{#> 250 1.05 0.333 }
\CommentTok{#> 300 1 0.333 }
\CommentTok{#> 350 0.933 0.667 }
\CommentTok{#> 400 0.855 0.667 }
\CommentTok{#> 450 0.775 0.667 }
\CommentTok{#> 500 0.689 0.667 }
\CommentTok{#> 550 0.611 0.68 }
\CommentTok{#> 600 0.552 0.693 }
\CommentTok{#> 650 0.507 0.747 }
\CommentTok{#> 700 0.473 0.84 }
\CommentTok{#> 750 0.445 0.88 }
\CommentTok{#> 800 0.421 0.92 }
\CommentTok{#> 850 0.399 0.947 }
\CommentTok{#> 900 0.379 0.96 }
\CommentTok{#> 950 0.36 0.96 }
\CommentTok{#> 1000 0.341 0.973 }
\CommentTok{#> 1050 0.324 0.973 }
\CommentTok{#> 1100 0.307 0.973 }
\CommentTok{#> 1150 0.292 0.973 }
\CommentTok{#> 1200 0.277 0.973 }
\CommentTok{#> 1250 0.263 0.973 }
\CommentTok{#> 1300 0.25 0.973 }
\CommentTok{#> 1350 0.238 0.973 }
\CommentTok{#> 1400 0.227 0.973 }
\CommentTok{#> 1450 0.216 0.973 }
\CommentTok{#> 1500 0.207 0.973 }
\CommentTok{#> 1550 0.198 0.973 }
\CommentTok{#> 1600 0.19 0.973 }
\CommentTok{#> 1650 0.183 0.973 }
\CommentTok{#> 1700 0.176 0.973 }
\CommentTok{#> 1750 0.17 0.973 }
\CommentTok{#> 1800 0.164 0.973 }
\CommentTok{#> 1850 0.158 0.973 }
\CommentTok{#> 1900 0.153 0.973 }
\CommentTok{#> 1950 0.149 0.973 }
\CommentTok{#> 2000 0.144 0.973}
\CommentTok{# ir.model <- train.dnn(x=1:4, y=5, traindata=iris[samp,], hidden=6, maxit=2000, display=50)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{nn-classification_901-build_fully_connected_nn_from_scratch-nnet_files/figure-latex/unnamed-chunk-3-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# 3. prediction}
\CommentTok{# }\AlertTok{NOTE}\CommentTok{: if the predict is factor, we need to transfer the number into class manually.}
\CommentTok{#       To make the code clear, I don't write this change into predict.dnn function.}
\NormalTok{labels.dnn <-}\StringTok{ }\KeywordTok{predict.dnn}\NormalTok{(ir.model, iris[}\OperatorTok{-}\NormalTok{samp, }\DecValTok{-5}\NormalTok{])}

\CommentTok{# 4. verify the results}
\KeywordTok{table}\NormalTok{(iris[}\OperatorTok{-}\NormalTok{samp,}\DecValTok{5}\NormalTok{], labels.dnn)}
\CommentTok{#>             labels.dnn}
\CommentTok{#>               1  2  3}
\CommentTok{#>   setosa     25  0  0}
\CommentTok{#>   versicolor  0 23  2}
\CommentTok{#>   virginica   0  0 25}
\CommentTok{#          labels.dnn}
\CommentTok{#            1  2  3}
\CommentTok{#setosa     25  0  0}
\CommentTok{#versicolor  0 24  1}
\CommentTok{#virginica   0  0 25}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#accuracy}
\KeywordTok{mean}\NormalTok{(}\KeywordTok{as.integer}\NormalTok{(iris[}\OperatorTok{-}\NormalTok{samp, }\DecValTok{5}\NormalTok{]) }\OperatorTok{==}\StringTok{ }\NormalTok{labels.dnn)}
\CommentTok{#> [1] 0.973}
\CommentTok{# 0.98}

\CommentTok{# 5. compare with nnet}
\KeywordTok{library}\NormalTok{(nnet)}
\NormalTok{ird <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\KeywordTok{rbind}\NormalTok{(iris3[,,}\DecValTok{1}\NormalTok{], iris3[,,}\DecValTok{2}\NormalTok{], iris3[,,}\DecValTok{3}\NormalTok{]),}
                  \DataTypeTok{species =} \KeywordTok{factor}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\KeywordTok{rep}\NormalTok{(}\StringTok{"s"}\NormalTok{,}\DecValTok{50}\NormalTok{), }\KeywordTok{rep}\NormalTok{(}\StringTok{"c"}\NormalTok{, }\DecValTok{50}\NormalTok{), }\KeywordTok{rep}\NormalTok{(}\StringTok{"v"}\NormalTok{, }\DecValTok{50}\NormalTok{))))}
\NormalTok{ir.nn2 <-}\StringTok{ }\KeywordTok{nnet}\NormalTok{(species }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ ird, }\DataTypeTok{subset =}\NormalTok{ samp, }\DataTypeTok{size =} \DecValTok{6}\NormalTok{, }\DataTypeTok{rang =} \FloatTok{0.1}\NormalTok{,}
               \DataTypeTok{decay =} \FloatTok{1e-2}\NormalTok{, }\DataTypeTok{maxit =} \DecValTok{2000}\NormalTok{)}
\CommentTok{#> # weights:  51}
\CommentTok{#> initial  value 82.293110 }
\CommentTok{#> iter  10 value 29.196376}
\CommentTok{#> iter  20 value 5.446284}
\CommentTok{#> iter  30 value 4.782022}
\CommentTok{#> iter  40 value 4.379729}
\CommentTok{#> iter  50 value 4.188725}
\CommentTok{#> iter  60 value 4.120587}
\CommentTok{#> iter  70 value 4.091706}
\CommentTok{#> iter  80 value 4.086017}
\CommentTok{#> iter  90 value 4.081664}
\CommentTok{#> iter 100 value 4.074111}
\CommentTok{#> iter 110 value 4.072894}
\CommentTok{#> iter 120 value 4.069011}
\CommentTok{#> iter 130 value 4.067690}
\CommentTok{#> iter 140 value 4.067633}
\CommentTok{#> final  value 4.067633 }
\CommentTok{#> converged}

           
\NormalTok{labels.nnet <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(ir.nn2, ird[}\OperatorTok{-}\NormalTok{samp,], }\DataTypeTok{type=}\StringTok{"class"}\NormalTok{)}
\KeywordTok{table}\NormalTok{(ird}\OperatorTok{$}\NormalTok{species[}\OperatorTok{-}\NormalTok{samp], labels.nnet)}
\CommentTok{#>    labels.nnet}
\CommentTok{#>      c  s  v}
\CommentTok{#>   c 23  0  2}
\CommentTok{#>   s  0 25  0}
\CommentTok{#>   v  0  0 25}
\CommentTok{#  labels.nnet}
\CommentTok{#   c  s  v}
\CommentTok{#c 22  0  3}
\CommentTok{#s  0 25  0}
\CommentTok{#v  3  0 22}

\CommentTok{# accuracy}
\KeywordTok{mean}\NormalTok{(ird}\OperatorTok{$}\NormalTok{species[}\OperatorTok{-}\NormalTok{samp] }\OperatorTok{==}\StringTok{ }\NormalTok{labels.nnet)}
\CommentTok{#> [1] 0.973}
\CommentTok{# 0.96}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{# Visualization}
\CommentTok{# the output from screen, copy and paste here.}
\NormalTok{data1 <-}\StringTok{ }\NormalTok{(}\StringTok{"i loss accuracy}
\StringTok{50 1.098421 0.3333333 }
\StringTok{100 1.098021 0.3333333 }
\StringTok{150 1.096843 0.3333333 }
\StringTok{200 1.093393 0.3333333 }
\StringTok{250 1.084069 0.3333333 }
\StringTok{300 1.063278 0.3333333 }
\StringTok{350 1.027273 0.3333333 }
\StringTok{400 0.9707605 0.64 }
\StringTok{450 0.8996356 0.6666667 }
\StringTok{500 0.8335469 0.6666667 }
\StringTok{550 0.7662386 0.6666667 }
\StringTok{600 0.6914156 0.6666667 }
\StringTok{650 0.6195753 0.68 }
\StringTok{700 0.5620381 0.68 }
\StringTok{750 0.5184008 0.7333333 }
\StringTok{800 0.4844815 0.84 }
\StringTok{850 0.4568258 0.8933333 }
\StringTok{900 0.4331083 0.92 }
\StringTok{950 0.4118948 0.9333333 }
\StringTok{1000 0.392368 0.96 }
\StringTok{1050 0.3740457 0.96 }
\StringTok{1100 0.3566594 0.96 }
\StringTok{1150 0.3400993 0.9866667 }
\StringTok{1200 0.3243276 0.9866667 }
\StringTok{1250 0.3093422 0.9866667 }
\StringTok{1300 0.2951787 0.9866667 }
\StringTok{1350 0.2818472 0.9866667 }
\StringTok{1400 0.2693641 0.9866667 }
\StringTok{1450 0.2577245 0.9866667 }
\StringTok{1500 0.2469068 0.9866667 }
\StringTok{1550 0.2368819 0.9866667 }
\StringTok{1600 0.2276124 0.9866667 }
\StringTok{1650 0.2190535 0.9866667 }
\StringTok{1700 0.2111565 0.9866667 }
\StringTok{1750 0.2038719 0.9866667 }
\StringTok{1800 0.1971507 0.9866667 }
\StringTok{1850 0.1909452 0.9866667 }
\StringTok{1900 0.1852105 0.9866667 }
\StringTok{1950 0.1799045 0.9866667 }
\StringTok{2000 0.1749881 0.9866667  "}\NormalTok{)}

\NormalTok{data.v <-}\StringTok{ }\KeywordTok{read.table}\NormalTok{(}\DataTypeTok{text=}\NormalTok{data1, }\DataTypeTok{header=}\NormalTok{T)}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mar=}\KeywordTok{c}\NormalTok{(}\FloatTok{5.1}\NormalTok{, }\FloatTok{4.1}\NormalTok{, }\FloatTok{4.1}\NormalTok{, }\FloatTok{4.1}\NormalTok{))}
\KeywordTok{plot}\NormalTok{(}\DataTypeTok{x=}\NormalTok{data.v}\OperatorTok{$}\NormalTok{i, }\DataTypeTok{y=}\NormalTok{data.v}\OperatorTok{$}\NormalTok{loss, }\DataTypeTok{type=}\StringTok{"o"}\NormalTok{, }\DataTypeTok{col=}\StringTok{"blue"}\NormalTok{, }\DataTypeTok{pch=}\DecValTok{16}\NormalTok{, }
     \DataTypeTok{main=}\StringTok{"IRIS loss and accuracy by 2-layers DNN"}\NormalTok{,}
     \DataTypeTok{ylim=}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{1.2}\NormalTok{),}
     \DataTypeTok{xlab=}\StringTok{""}\NormalTok{,}
     \DataTypeTok{ylab=}\StringTok{""}\NormalTok{,}
     \DataTypeTok{axe =}\NormalTok{F)}
\KeywordTok{lines}\NormalTok{(}\DataTypeTok{x=}\NormalTok{data.v}\OperatorTok{$}\NormalTok{i, }\DataTypeTok{y=}\NormalTok{data.v}\OperatorTok{$}\NormalTok{accuracy, }\DataTypeTok{type=}\StringTok{"o"}\NormalTok{, }\DataTypeTok{col=}\StringTok{"red"}\NormalTok{, }\DataTypeTok{pch=}\DecValTok{1}\NormalTok{)}
\KeywordTok{box}\NormalTok{()}
\KeywordTok{axis}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DataTypeTok{at=}\KeywordTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{2000}\NormalTok{,}\DataTypeTok{by=}\DecValTok{200}\NormalTok{))}
\KeywordTok{axis}\NormalTok{(}\DecValTok{4}\NormalTok{, }\DataTypeTok{at=}\KeywordTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{,}\FloatTok{1.0}\NormalTok{,}\DataTypeTok{by=}\FloatTok{0.1}\NormalTok{))}
\KeywordTok{axis}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DataTypeTok{at=}\KeywordTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{,}\FloatTok{1.2}\NormalTok{,}\DataTypeTok{by=}\FloatTok{0.1}\NormalTok{))}
\KeywordTok{mtext}\NormalTok{(}\StringTok{"training step"}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DataTypeTok{line=}\DecValTok{3}\NormalTok{)}
\KeywordTok{mtext}\NormalTok{(}\StringTok{"loss of training set"}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DataTypeTok{line=}\FloatTok{2.5}\NormalTok{)}
\KeywordTok{mtext}\NormalTok{(}\StringTok{"accuracy of testing set"}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DataTypeTok{line=}\DecValTok{2}\NormalTok{)}

\KeywordTok{legend}\NormalTok{(}\StringTok{"bottomleft"}\NormalTok{, }
       \DataTypeTok{legend =} \KeywordTok{c}\NormalTok{(}\StringTok{"loss"}\NormalTok{, }\StringTok{"accuracy"}\NormalTok{),}
       \DataTypeTok{pch =} \KeywordTok{c}\NormalTok{(}\DecValTok{16}\NormalTok{,}\DecValTok{1}\NormalTok{),}
       \DataTypeTok{col =} \KeywordTok{c}\NormalTok{(}\StringTok{"blue"}\NormalTok{,}\StringTok{"red"}\NormalTok{),}
       \DataTypeTok{lwd=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{nn-classification_901-build_fully_connected_nn_from_scratch-nnet_files/figure-latex/unnamed-chunk-6-1} \end{center}

\hypertarget{wine-with-neuralnet}{%
\chapter{Wine with neuralnet}\label{wine-with-neuralnet}}

Source: \url{https://www.r-bloggers.com/multilabel-classification-with-neuralnet-package/}

The neuralnet package is perhaps not the best option in R for using neural networks. If you ask why, for starters it does not recognize the typical formula y\textasciitilde{}., it does not support factors, it does not provide a lot of models other than a standard MLP, and it has great competitors in the nnet package that seems to be better integrated in R and can be used with the caret package, and in the MXnet package that is a high level deep learning library which provides a wide variety of neural networks.

But still, I think there is some value in the ease of use of the neuralnet package, especially for a beginner, therefore I'll be using it.

I'm going to be using both the neuralnet and, curiously enough, the nnet package. Let's load them:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load libs}
\KeywordTok{require}\NormalTok{(neuralnet)}
\CommentTok{#> Loading required package: neuralnet}
\KeywordTok{require}\NormalTok{(nnet)}
\CommentTok{#> Loading required package: nnet}
\KeywordTok{require}\NormalTok{(ggplot2)}
\CommentTok{#> Loading required package: ggplot2}
\CommentTok{#> Registered S3 methods overwritten by 'ggplot2':}
\CommentTok{#>   method         from }
\CommentTok{#>   [.quosures     rlang}
\CommentTok{#>   c.quosures     rlang}
\CommentTok{#>   print.quosures rlang}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{the-dataset}{%
\section{The dataset}\label{the-dataset}}

I looked in the UCI Machine Learning Repository1 and found the wine dataset.

This dataset contains the results of a chemical analysis on 3 different kind of wines. The target variable is the label of the wine which is a factor with 3 (unordered) levels. The predictors are all continuous and represent 13 variables obtained as a result of chemical measurements.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# get the data file from the package location}
\NormalTok{wine_dataset_path <-}\StringTok{ }\KeywordTok{file.path}\NormalTok{(data_raw_dir, }\StringTok{"wine.data"}\NormalTok{)}
\NormalTok{wine_dataset_path}
\CommentTok{#> [1] "/home/datascience/repos/machine-learning-rsuite/import/wine.data"}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{wines <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(wine_dataset_path)}
\NormalTok{wines}
\CommentTok{#>     X1 X14.23 X1.71 X2.43 X15.6 X127 X2.8 X3.06 X.28 X2.29 X5.64 X1.04}
\CommentTok{#> 1    1   13.2  1.78  2.14  11.2  100 2.65  2.76 0.26  1.28  4.38 1.050}
\CommentTok{#> 2    1   13.2  2.36  2.67  18.6  101 2.80  3.24 0.30  2.81  5.68 1.030}
\CommentTok{#> 3    1   14.4  1.95  2.50  16.8  113 3.85  3.49 0.24  2.18  7.80 0.860}
\CommentTok{#> 4    1   13.2  2.59  2.87  21.0  118 2.80  2.69 0.39  1.82  4.32 1.040}
\CommentTok{#> 5    1   14.2  1.76  2.45  15.2  112 3.27  3.39 0.34  1.97  6.75 1.050}
\CommentTok{#> 6    1   14.4  1.87  2.45  14.6   96 2.50  2.52 0.30  1.98  5.25 1.020}
\CommentTok{#> 7    1   14.1  2.15  2.61  17.6  121 2.60  2.51 0.31  1.25  5.05 1.060}
\CommentTok{#> 8    1   14.8  1.64  2.17  14.0   97 2.80  2.98 0.29  1.98  5.20 1.080}
\CommentTok{#> 9    1   13.9  1.35  2.27  16.0   98 2.98  3.15 0.22  1.85  7.22 1.010}
\CommentTok{#> 10   1   14.1  2.16  2.30  18.0  105 2.95  3.32 0.22  2.38  5.75 1.250}
\CommentTok{#> 11   1   14.1  1.48  2.32  16.8   95 2.20  2.43 0.26  1.57  5.00 1.170}
\CommentTok{#> 12   1   13.8  1.73  2.41  16.0   89 2.60  2.76 0.29  1.81  5.60 1.150}
\CommentTok{#> 13   1   14.8  1.73  2.39  11.4   91 3.10  3.69 0.43  2.81  5.40 1.250}
\CommentTok{#> 14   1   14.4  1.87  2.38  12.0  102 3.30  3.64 0.29  2.96  7.50 1.200}
\CommentTok{#> 15   1   13.6  1.81  2.70  17.2  112 2.85  2.91 0.30  1.46  7.30 1.280}
\CommentTok{#> 16   1   14.3  1.92  2.72  20.0  120 2.80  3.14 0.33  1.97  6.20 1.070}
\CommentTok{#> 17   1   13.8  1.57  2.62  20.0  115 2.95  3.40 0.40  1.72  6.60 1.130}
\CommentTok{#> 18   1   14.2  1.59  2.48  16.5  108 3.30  3.93 0.32  1.86  8.70 1.230}
\CommentTok{#> 19   1   13.6  3.10  2.56  15.2  116 2.70  3.03 0.17  1.66  5.10 0.960}
\CommentTok{#> 20   1   14.1  1.63  2.28  16.0  126 3.00  3.17 0.24  2.10  5.65 1.090}
\CommentTok{#> 21   1   12.9  3.80  2.65  18.6  102 2.41  2.41 0.25  1.98  4.50 1.030}
\CommentTok{#> 22   1   13.7  1.86  2.36  16.6  101 2.61  2.88 0.27  1.69  3.80 1.110}
\CommentTok{#> 23   1   12.8  1.60  2.52  17.8   95 2.48  2.37 0.26  1.46  3.93 1.090}
\CommentTok{#> 24   1   13.5  1.81  2.61  20.0   96 2.53  2.61 0.28  1.66  3.52 1.120}
\CommentTok{#> 25   1   13.1  2.05  3.22  25.0  124 2.63  2.68 0.47  1.92  3.58 1.130}
\CommentTok{#> 26   1   13.4  1.77  2.62  16.1   93 2.85  2.94 0.34  1.45  4.80 0.920}
\CommentTok{#> 27   1   13.3  1.72  2.14  17.0   94 2.40  2.19 0.27  1.35  3.95 1.020}
\CommentTok{#> 28   1   13.9  1.90  2.80  19.4  107 2.95  2.97 0.37  1.76  4.50 1.250}
\CommentTok{#> 29   1   14.0  1.68  2.21  16.0   96 2.65  2.33 0.26  1.98  4.70 1.040}
\CommentTok{#> 30   1   13.7  1.50  2.70  22.5  101 3.00  3.25 0.29  2.38  5.70 1.190}
\CommentTok{#> 31   1   13.6  1.66  2.36  19.1  106 2.86  3.19 0.22  1.95  6.90 1.090}
\CommentTok{#> 32   1   13.7  1.83  2.36  17.2  104 2.42  2.69 0.42  1.97  3.84 1.230}
\CommentTok{#> 33   1   13.8  1.53  2.70  19.5  132 2.95  2.74 0.50  1.35  5.40 1.250}
\CommentTok{#> 34   1   13.5  1.80  2.65  19.0  110 2.35  2.53 0.29  1.54  4.20 1.100}
\CommentTok{#> 35   1   13.5  1.81  2.41  20.5  100 2.70  2.98 0.26  1.86  5.10 1.040}
\CommentTok{#> 36   1   13.3  1.64  2.84  15.5  110 2.60  2.68 0.34  1.36  4.60 1.090}
\CommentTok{#> 37   1   13.1  1.65  2.55  18.0   98 2.45  2.43 0.29  1.44  4.25 1.120}
\CommentTok{#> 38   1   13.1  1.50  2.10  15.5   98 2.40  2.64 0.28  1.37  3.70 1.180}
\CommentTok{#> 39   1   14.2  3.99  2.51  13.2  128 3.00  3.04 0.20  2.08  5.10 0.890}
\CommentTok{#> 40   1   13.6  1.71  2.31  16.2  117 3.15  3.29 0.34  2.34  6.13 0.950}
\CommentTok{#> 41   1   13.4  3.84  2.12  18.8   90 2.45  2.68 0.27  1.48  4.28 0.910}
\CommentTok{#> 42   1   13.9  1.89  2.59  15.0  101 3.25  3.56 0.17  1.70  5.43 0.880}
\CommentTok{#> 43   1   13.2  3.98  2.29  17.5  103 2.64  2.63 0.32  1.66  4.36 0.820}
\CommentTok{#> 44   1   13.1  1.77  2.10  17.0  107 3.00  3.00 0.28  2.03  5.04 0.880}
\CommentTok{#> 45   1   14.2  4.04  2.44  18.9  111 2.85  2.65 0.30  1.25  5.24 0.870}
\CommentTok{#> 46   1   14.4  3.59  2.28  16.0  102 3.25  3.17 0.27  2.19  4.90 1.040}
\CommentTok{#> 47   1   13.9  1.68  2.12  16.0  101 3.10  3.39 0.21  2.14  6.10 0.910}
\CommentTok{#> 48   1   14.1  2.02  2.40  18.8  103 2.75  2.92 0.32  2.38  6.20 1.070}
\CommentTok{#> 49   1   13.9  1.73  2.27  17.4  108 2.88  3.54 0.32  2.08  8.90 1.120}
\CommentTok{#> 50   1   13.1  1.73  2.04  12.4   92 2.72  3.27 0.17  2.91  7.20 1.120}
\CommentTok{#> 51   1   13.8  1.65  2.60  17.2   94 2.45  2.99 0.22  2.29  5.60 1.240}
\CommentTok{#> 52   1   13.8  1.75  2.42  14.0  111 3.88  3.74 0.32  1.87  7.05 1.010}
\CommentTok{#> 53   1   13.8  1.90  2.68  17.1  115 3.00  2.79 0.39  1.68  6.30 1.130}
\CommentTok{#> 54   1   13.7  1.67  2.25  16.4  118 2.60  2.90 0.21  1.62  5.85 0.920}
\CommentTok{#> 55   1   13.6  1.73  2.46  20.5  116 2.96  2.78 0.20  2.45  6.25 0.980}
\CommentTok{#> 56   1   14.2  1.70  2.30  16.3  118 3.20  3.00 0.26  2.03  6.38 0.940}
\CommentTok{#> 57   1   13.3  1.97  2.68  16.8  102 3.00  3.23 0.31  1.66  6.00 1.070}
\CommentTok{#> 58   1   13.7  1.43  2.50  16.7  108 3.40  3.67 0.19  2.04  6.80 0.890}
\CommentTok{#> 59   2   12.4  0.94  1.36  10.6   88 1.98  0.57 0.28  0.42  1.95 1.050}
\CommentTok{#> 60   2   12.3  1.10  2.28  16.0  101 2.05  1.09 0.63  0.41  3.27 1.250}
\CommentTok{#> 61   2   12.6  1.36  2.02  16.8  100 2.02  1.41 0.53  0.62  5.75 0.980}
\CommentTok{#> 62   2   13.7  1.25  1.92  18.0   94 2.10  1.79 0.32  0.73  3.80 1.230}
\CommentTok{#> 63   2   12.4  1.13  2.16  19.0   87 3.50  3.10 0.19  1.87  4.45 1.220}
\CommentTok{#> 64   2   12.2  1.45  2.53  19.0  104 1.89  1.75 0.45  1.03  2.95 1.450}
\CommentTok{#> 65   2   12.4  1.21  2.56  18.1   98 2.42  2.65 0.37  2.08  4.60 1.190}
\CommentTok{#> 66   2   13.1  1.01  1.70  15.0   78 2.98  3.18 0.26  2.28  5.30 1.120}
\CommentTok{#> 67   2   12.4  1.17  1.92  19.6   78 2.11  2.00 0.27  1.04  4.68 1.120}
\CommentTok{#> 68   2   13.3  0.94  2.36  17.0  110 2.53  1.30 0.55  0.42  3.17 1.020}
\CommentTok{#> 69   2   12.2  1.19  1.75  16.8  151 1.85  1.28 0.14  2.50  2.85 1.280}
\CommentTok{#> 70   2   12.3  1.61  2.21  20.4  103 1.10  1.02 0.37  1.46  3.05 0.906}
\CommentTok{#> 71   2   13.9  1.51  2.67  25.0   86 2.95  2.86 0.21  1.87  3.38 1.360}
\CommentTok{#> 72   2   13.5  1.66  2.24  24.0   87 1.88  1.84 0.27  1.03  3.74 0.980}
\CommentTok{#> 73   2   13.0  1.67  2.60  30.0  139 3.30  2.89 0.21  1.96  3.35 1.310}
\CommentTok{#> 74   2   12.0  1.09  2.30  21.0  101 3.38  2.14 0.13  1.65  3.21 0.990}
\CommentTok{#> 75   2   11.7  1.88  1.92  16.0   97 1.61  1.57 0.34  1.15  3.80 1.230}
\CommentTok{#> 76   2   13.0  0.90  1.71  16.0   86 1.95  2.03 0.24  1.46  4.60 1.190}
\CommentTok{#> 77   2   11.8  2.89  2.23  18.0  112 1.72  1.32 0.43  0.95  2.65 0.960}
\CommentTok{#> 78   2   12.3  0.99  1.95  14.8  136 1.90  1.85 0.35  2.76  3.40 1.060}
\CommentTok{#> 79   2   12.7  3.87  2.40  23.0  101 2.83  2.55 0.43  1.95  2.57 1.190}
\CommentTok{#> 80   2   12.0  0.92  2.00  19.0   86 2.42  2.26 0.30  1.43  2.50 1.380}
\CommentTok{#> 81   2   12.7  1.81  2.20  18.8   86 2.20  2.53 0.26  1.77  3.90 1.160}
\CommentTok{#> 82   2   12.1  1.13  2.51  24.0   78 2.00  1.58 0.40  1.40  2.20 1.310}
\CommentTok{#> 83   2   13.1  3.86  2.32  22.5   85 1.65  1.59 0.61  1.62  4.80 0.840}
\CommentTok{#> 84   2   11.8  0.89  2.58  18.0   94 2.20  2.21 0.22  2.35  3.05 0.790}
\CommentTok{#> 85   2   12.7  0.98  2.24  18.0   99 2.20  1.94 0.30  1.46  2.62 1.230}
\CommentTok{#> 86   2   12.2  1.61  2.31  22.8   90 1.78  1.69 0.43  1.56  2.45 1.330}
\CommentTok{#> 87   2   11.7  1.67  2.62  26.0   88 1.92  1.61 0.40  1.34  2.60 1.360}
\CommentTok{#> 88   2   11.6  2.06  2.46  21.6   84 1.95  1.69 0.48  1.35  2.80 1.000}
\CommentTok{#> 89   2   12.1  1.33  2.30  23.6   70 2.20  1.59 0.42  1.38  1.74 1.070}
\CommentTok{#> 90   2   12.1  1.83  2.32  18.5   81 1.60  1.50 0.52  1.64  2.40 1.080}
\CommentTok{#> 91   2   12.0  1.51  2.42  22.0   86 1.45  1.25 0.50  1.63  3.60 1.050}
\CommentTok{#> 92   2   12.7  1.53  2.26  20.7   80 1.38  1.46 0.58  1.62  3.05 0.960}
\CommentTok{#> 93   2   12.3  2.83  2.22  18.0   88 2.45  2.25 0.25  1.99  2.15 1.150}
\CommentTok{#> 94   2   11.6  1.99  2.28  18.0   98 3.02  2.26 0.17  1.35  3.25 1.160}
\CommentTok{#> 95   2   12.5  1.52  2.20  19.0  162 2.50  2.27 0.32  3.28  2.60 1.160}
\CommentTok{#> 96   2   11.8  2.12  2.74  21.5  134 1.60  0.99 0.14  1.56  2.50 0.950}
\CommentTok{#> 97   2   12.3  1.41  1.98  16.0   85 2.55  2.50 0.29  1.77  2.90 1.230}
\CommentTok{#> 98   2   12.4  1.07  2.10  18.5   88 3.52  3.75 0.24  1.95  4.50 1.040}
\CommentTok{#> 99   2   12.3  3.17  2.21  18.0   88 2.85  2.99 0.45  2.81  2.30 1.420}
\CommentTok{#> 100  2   12.1  2.08  1.70  17.5   97 2.23  2.17 0.26  1.40  3.30 1.270}
\CommentTok{#> 101  2   12.6  1.34  1.90  18.5   88 1.45  1.36 0.29  1.35  2.45 1.040}
\CommentTok{#> 102  2   12.3  2.45  2.46  21.0   98 2.56  2.11 0.34  1.31  2.80 0.800}
\CommentTok{#> 103  2   11.8  1.72  1.88  19.5   86 2.50  1.64 0.37  1.42  2.06 0.940}
\CommentTok{#> 104  2   12.5  1.73  1.98  20.5   85 2.20  1.92 0.32  1.48  2.94 1.040}
\CommentTok{#> 105  2   12.4  2.55  2.27  22.0   90 1.68  1.84 0.66  1.42  2.70 0.860}
\CommentTok{#> 106  2   12.2  1.73  2.12  19.0   80 1.65  2.03 0.37  1.63  3.40 1.000}
\CommentTok{#> 107  2   12.7  1.75  2.28  22.5   84 1.38  1.76 0.48  1.63  3.30 0.880}
\CommentTok{#> 108  2   12.2  1.29  1.94  19.0   92 2.36  2.04 0.39  2.08  2.70 0.860}
\CommentTok{#> 109  2   11.6  1.35  2.70  20.0   94 2.74  2.92 0.29  2.49  2.65 0.960}
\CommentTok{#> 110  2   11.5  3.74  1.82  19.5  107 3.18  2.58 0.24  3.58  2.90 0.750}
\CommentTok{#> 111  2   12.5  2.43  2.17  21.0   88 2.55  2.27 0.26  1.22  2.00 0.900}
\CommentTok{#> 112  2   11.8  2.68  2.92  20.0  103 1.75  2.03 0.60  1.05  3.80 1.230}
\CommentTok{#> 113  2   11.4  0.74  2.50  21.0   88 2.48  2.01 0.42  1.44  3.08 1.100}
\CommentTok{#> 114  2   12.1  1.39  2.50  22.5   84 2.56  2.29 0.43  1.04  2.90 0.930}
\CommentTok{#> 115  2   11.0  1.51  2.20  21.5   85 2.46  2.17 0.52  2.01  1.90 1.710}
\CommentTok{#> 116  2   11.8  1.47  1.99  20.8   86 1.98  1.60 0.30  1.53  1.95 0.950}
\CommentTok{#> 117  2   12.4  1.61  2.19  22.5  108 2.00  2.09 0.34  1.61  2.06 1.060}
\CommentTok{#> 118  2   12.8  3.43  1.98  16.0   80 1.63  1.25 0.43  0.83  3.40 0.700}
\CommentTok{#> 119  2   12.0  3.43  2.00  19.0   87 2.00  1.64 0.37  1.87  1.28 0.930}
\CommentTok{#> 120  2   11.4  2.40  2.42  20.0   96 2.90  2.79 0.32  1.83  3.25 0.800}
\CommentTok{#> 121  2   11.6  2.05  3.23  28.5  119 3.18  5.08 0.47  1.87  6.00 0.930}
\CommentTok{#> 122  2   12.4  4.43  2.73  26.5  102 2.20  2.13 0.43  1.71  2.08 0.920}
\CommentTok{#> 123  2   13.1  5.80  2.13  21.5   86 2.62  2.65 0.30  2.01  2.60 0.730}
\CommentTok{#> 124  2   11.9  4.31  2.39  21.0   82 2.86  3.03 0.21  2.91  2.80 0.750}
\CommentTok{#> 125  2   12.1  2.16  2.17  21.0   85 2.60  2.65 0.37  1.35  2.76 0.860}
\CommentTok{#> 126  2   12.4  1.53  2.29  21.5   86 2.74  3.15 0.39  1.77  3.94 0.690}
\CommentTok{#> 127  2   11.8  2.13  2.78  28.5   92 2.13  2.24 0.58  1.76  3.00 0.970}
\CommentTok{#> 128  2   12.4  1.63  2.30  24.5   88 2.22  2.45 0.40  1.90  2.12 0.890}
\CommentTok{#> 129  2   12.0  4.30  2.38  22.0   80 2.10  1.75 0.42  1.35  2.60 0.790}
\CommentTok{#> 130  3   12.9  1.35  2.32  18.0  122 1.51  1.25 0.21  0.94  4.10 0.760}
\CommentTok{#> 131  3   12.9  2.99  2.40  20.0  104 1.30  1.22 0.24  0.83  5.40 0.740}
\CommentTok{#> 132  3   12.8  2.31  2.40  24.0   98 1.15  1.09 0.27  0.83  5.70 0.660}
\CommentTok{#> 133  3   12.7  3.55  2.36  21.5  106 1.70  1.20 0.17  0.84  5.00 0.780}
\CommentTok{#> 134  3   12.5  1.24  2.25  17.5   85 2.00  0.58 0.60  1.25  5.45 0.750}
\CommentTok{#> 135  3   12.6  2.46  2.20  18.5   94 1.62  0.66 0.63  0.94  7.10 0.730}
\CommentTok{#> 136  3   12.2  4.72  2.54  21.0   89 1.38  0.47 0.53  0.80  3.85 0.750}
\CommentTok{#> 137  3   12.5  5.51  2.64  25.0   96 1.79  0.60 0.63  1.10  5.00 0.820}
\CommentTok{#> 138  3   13.5  3.59  2.19  19.5   88 1.62  0.48 0.58  0.88  5.70 0.810}
\CommentTok{#> 139  3   12.8  2.96  2.61  24.0  101 2.32  0.60 0.53  0.81  4.92 0.890}
\CommentTok{#> 140  3   12.9  2.81  2.70  21.0   96 1.54  0.50 0.53  0.75  4.60 0.770}
\CommentTok{#> 141  3   13.4  2.56  2.35  20.0   89 1.40  0.50 0.37  0.64  5.60 0.700}
\CommentTok{#> 142  3   13.5  3.17  2.72  23.5   97 1.55  0.52 0.50  0.55  4.35 0.890}
\CommentTok{#> 143  3   13.6  4.95  2.35  20.0   92 2.00  0.80 0.47  1.02  4.40 0.910}
\CommentTok{#> 144  3   12.2  3.88  2.20  18.5  112 1.38  0.78 0.29  1.14  8.21 0.650}
\CommentTok{#> 145  3   13.2  3.57  2.15  21.0  102 1.50  0.55 0.43  1.30  4.00 0.600}
\CommentTok{#> 146  3   13.9  5.04  2.23  20.0   80 0.98  0.34 0.40  0.68  4.90 0.580}
\CommentTok{#> 147  3   12.9  4.61  2.48  21.5   86 1.70  0.65 0.47  0.86  7.65 0.540}
\CommentTok{#> 148  3   13.3  3.24  2.38  21.5   92 1.93  0.76 0.45  1.25  8.42 0.550}
\CommentTok{#> 149  3   13.1  3.90  2.36  21.5  113 1.41  1.39 0.34  1.14  9.40 0.570}
\CommentTok{#> 150  3   13.5  3.12  2.62  24.0  123 1.40  1.57 0.22  1.25  8.60 0.590}
\CommentTok{#> 151  3   12.8  2.67  2.48  22.0  112 1.48  1.36 0.24  1.26 10.80 0.480}
\CommentTok{#> 152  3   13.1  1.90  2.75  25.5  116 2.20  1.28 0.26  1.56  7.10 0.610}
\CommentTok{#> 153  3   13.2  3.30  2.28  18.5   98 1.80  0.83 0.61  1.87 10.52 0.560}
\CommentTok{#> 154  3   12.6  1.29  2.10  20.0  103 1.48  0.58 0.53  1.40  7.60 0.580}
\CommentTok{#> 155  3   13.2  5.19  2.32  22.0   93 1.74  0.63 0.61  1.55  7.90 0.600}
\CommentTok{#> 156  3   13.8  4.12  2.38  19.5   89 1.80  0.83 0.48  1.56  9.01 0.570}
\CommentTok{#> 157  3   12.4  3.03  2.64  27.0   97 1.90  0.58 0.63  1.14  7.50 0.670}
\CommentTok{#> 158  3   14.3  1.68  2.70  25.0   98 2.80  1.31 0.53  2.70 13.00 0.570}
\CommentTok{#> 159  3   13.5  1.67  2.64  22.5   89 2.60  1.10 0.52  2.29 11.75 0.570}
\CommentTok{#> 160  3   12.4  3.83  2.38  21.0   88 2.30  0.92 0.50  1.04  7.65 0.560}
\CommentTok{#> 161  3   13.7  3.26  2.54  20.0  107 1.83  0.56 0.50  0.80  5.88 0.960}
\CommentTok{#> 162  3   12.8  3.27  2.58  22.0  106 1.65  0.60 0.60  0.96  5.58 0.870}
\CommentTok{#> 163  3   13.0  3.45  2.35  18.5  106 1.39  0.70 0.40  0.94  5.28 0.680}
\CommentTok{#> 164  3   13.8  2.76  2.30  22.0   90 1.35  0.68 0.41  1.03  9.58 0.700}
\CommentTok{#> 165  3   13.7  4.36  2.26  22.5   88 1.28  0.47 0.52  1.15  6.62 0.780}
\CommentTok{#> 166  3   13.4  3.70  2.60  23.0  111 1.70  0.92 0.43  1.46 10.68 0.850}
\CommentTok{#> 167  3   12.8  3.37  2.30  19.5   88 1.48  0.66 0.40  0.97 10.26 0.720}
\CommentTok{#> 168  3   13.6  2.58  2.69  24.5  105 1.55  0.84 0.39  1.54  8.66 0.740}
\CommentTok{#> 169  3   13.4  4.60  2.86  25.0  112 1.98  0.96 0.27  1.11  8.50 0.670}
\CommentTok{#> 170  3   12.2  3.03  2.32  19.0   96 1.25  0.49 0.40  0.73  5.50 0.660}
\CommentTok{#> 171  3   12.8  2.39  2.28  19.5   86 1.39  0.51 0.48  0.64  9.90 0.570}
\CommentTok{#> 172  3   14.2  2.51  2.48  20.0   91 1.68  0.70 0.44  1.24  9.70 0.620}
\CommentTok{#> 173  3   13.7  5.65  2.45  20.5   95 1.68  0.61 0.52  1.06  7.70 0.640}
\CommentTok{#> 174  3   13.4  3.91  2.48  23.0  102 1.80  0.75 0.43  1.41  7.30 0.700}
\CommentTok{#> 175  3   13.3  4.28  2.26  20.0  120 1.59  0.69 0.43  1.35 10.20 0.590}
\CommentTok{#> 176  3   13.2  2.59  2.37  20.0  120 1.65  0.68 0.53  1.46  9.30 0.600}
\CommentTok{#> 177  3   14.1  4.10  2.74  24.5   96 2.05  0.76 0.56  1.35  9.20 0.610}
\CommentTok{#>     X3.92 X1065}
\CommentTok{#> 1    3.40  1050}
\CommentTok{#> 2    3.17  1185}
\CommentTok{#> 3    3.45  1480}
\CommentTok{#> 4    2.93   735}
\CommentTok{#> 5    2.85  1450}
\CommentTok{#> 6    3.58  1290}
\CommentTok{#> 7    3.58  1295}
\CommentTok{#> 8    2.85  1045}
\CommentTok{#> 9    3.55  1045}
\CommentTok{#> 10   3.17  1510}
\CommentTok{#> 11   2.82  1280}
\CommentTok{#> 12   2.90  1320}
\CommentTok{#> 13   2.73  1150}
\CommentTok{#> 14   3.00  1547}
\CommentTok{#> 15   2.88  1310}
\CommentTok{#> 16   2.65  1280}
\CommentTok{#> 17   2.57  1130}
\CommentTok{#> 18   2.82  1680}
\CommentTok{#> 19   3.36   845}
\CommentTok{#> 20   3.71   780}
\CommentTok{#> 21   3.52   770}
\CommentTok{#> 22   4.00  1035}
\CommentTok{#> 23   3.63  1015}
\CommentTok{#> 24   3.82   845}
\CommentTok{#> 25   3.20   830}
\CommentTok{#> 26   3.22  1195}
\CommentTok{#> 27   2.77  1285}
\CommentTok{#> 28   3.40   915}
\CommentTok{#> 29   3.59  1035}
\CommentTok{#> 30   2.71  1285}
\CommentTok{#> 31   2.88  1515}
\CommentTok{#> 32   2.87   990}
\CommentTok{#> 33   3.00  1235}
\CommentTok{#> 34   2.87  1095}
\CommentTok{#> 35   3.47   920}
\CommentTok{#> 36   2.78   880}
\CommentTok{#> 37   2.51  1105}
\CommentTok{#> 38   2.69  1020}
\CommentTok{#> 39   3.53   760}
\CommentTok{#> 40   3.38   795}
\CommentTok{#> 41   3.00  1035}
\CommentTok{#> 42   3.56  1095}
\CommentTok{#> 43   3.00   680}
\CommentTok{#> 44   3.35   885}
\CommentTok{#> 45   3.33  1080}
\CommentTok{#> 46   3.44  1065}
\CommentTok{#> 47   3.33   985}
\CommentTok{#> 48   2.75  1060}
\CommentTok{#> 49   3.10  1260}
\CommentTok{#> 50   2.91  1150}
\CommentTok{#> 51   3.37  1265}
\CommentTok{#> 52   3.26  1190}
\CommentTok{#> 53   2.93  1375}
\CommentTok{#> 54   3.20  1060}
\CommentTok{#> 55   3.03  1120}
\CommentTok{#> 56   3.31   970}
\CommentTok{#> 57   2.84  1270}
\CommentTok{#> 58   2.87  1285}
\CommentTok{#> 59   1.82   520}
\CommentTok{#> 60   1.67   680}
\CommentTok{#> 61   1.59   450}
\CommentTok{#> 62   2.46   630}
\CommentTok{#> 63   2.87   420}
\CommentTok{#> 64   2.23   355}
\CommentTok{#> 65   2.30   678}
\CommentTok{#> 66   3.18   502}
\CommentTok{#> 67   3.48   510}
\CommentTok{#> 68   1.93   750}
\CommentTok{#> 69   3.07   718}
\CommentTok{#> 70   1.82   870}
\CommentTok{#> 71   3.16   410}
\CommentTok{#> 72   2.78   472}
\CommentTok{#> 73   3.50   985}
\CommentTok{#> 74   3.13   886}
\CommentTok{#> 75   2.14   428}
\CommentTok{#> 76   2.48   392}
\CommentTok{#> 77   2.52   500}
\CommentTok{#> 78   2.31   750}
\CommentTok{#> 79   3.13   463}
\CommentTok{#> 80   3.12   278}
\CommentTok{#> 81   3.14   714}
\CommentTok{#> 82   2.72   630}
\CommentTok{#> 83   2.01   515}
\CommentTok{#> 84   3.08   520}
\CommentTok{#> 85   3.16   450}
\CommentTok{#> 86   2.26   495}
\CommentTok{#> 87   3.21   562}
\CommentTok{#> 88   2.75   680}
\CommentTok{#> 89   3.21   625}
\CommentTok{#> 90   2.27   480}
\CommentTok{#> 91   2.65   450}
\CommentTok{#> 92   2.06   495}
\CommentTok{#> 93   3.30   290}
\CommentTok{#> 94   2.96   345}
\CommentTok{#> 95   2.63   937}
\CommentTok{#> 96   2.26   625}
\CommentTok{#> 97   2.74   428}
\CommentTok{#> 98   2.77   660}
\CommentTok{#> 99   2.83   406}
\CommentTok{#> 100  2.96   710}
\CommentTok{#> 101  2.77   562}
\CommentTok{#> 102  3.38   438}
\CommentTok{#> 103  2.44   415}
\CommentTok{#> 104  3.57   672}
\CommentTok{#> 105  3.30   315}
\CommentTok{#> 106  3.17   510}
\CommentTok{#> 107  2.42   488}
\CommentTok{#> 108  3.02   312}
\CommentTok{#> 109  3.26   680}
\CommentTok{#> 110  2.81   562}
\CommentTok{#> 111  2.78   325}
\CommentTok{#> 112  2.50   607}
\CommentTok{#> 113  2.31   434}
\CommentTok{#> 114  3.19   385}
\CommentTok{#> 115  2.87   407}
\CommentTok{#> 116  3.33   495}
\CommentTok{#> 117  2.96   345}
\CommentTok{#> 118  2.12   372}
\CommentTok{#> 119  3.05   564}
\CommentTok{#> 120  3.39   625}
\CommentTok{#> 121  3.69   465}
\CommentTok{#> 122  3.12   365}
\CommentTok{#> 123  3.10   380}
\CommentTok{#> 124  3.64   380}
\CommentTok{#> 125  3.28   378}
\CommentTok{#> 126  2.84   352}
\CommentTok{#> 127  2.44   466}
\CommentTok{#> 128  2.78   342}
\CommentTok{#> 129  2.57   580}
\CommentTok{#> 130  1.29   630}
\CommentTok{#> 131  1.42   530}
\CommentTok{#> 132  1.36   560}
\CommentTok{#> 133  1.29   600}
\CommentTok{#> 134  1.51   650}
\CommentTok{#> 135  1.58   695}
\CommentTok{#> 136  1.27   720}
\CommentTok{#> 137  1.69   515}
\CommentTok{#> 138  1.82   580}
\CommentTok{#> 139  2.15   590}
\CommentTok{#> 140  2.31   600}
\CommentTok{#> 141  2.47   780}
\CommentTok{#> 142  2.06   520}
\CommentTok{#> 143  2.05   550}
\CommentTok{#> 144  2.00   855}
\CommentTok{#> 145  1.68   830}
\CommentTok{#> 146  1.33   415}
\CommentTok{#> 147  1.86   625}
\CommentTok{#> 148  1.62   650}
\CommentTok{#> 149  1.33   550}
\CommentTok{#> 150  1.30   500}
\CommentTok{#> 151  1.47   480}
\CommentTok{#> 152  1.33   425}
\CommentTok{#> 153  1.51   675}
\CommentTok{#> 154  1.55   640}
\CommentTok{#> 155  1.48   725}
\CommentTok{#> 156  1.64   480}
\CommentTok{#> 157  1.73   880}
\CommentTok{#> 158  1.96   660}
\CommentTok{#> 159  1.78   620}
\CommentTok{#> 160  1.58   520}
\CommentTok{#> 161  1.82   680}
\CommentTok{#> 162  2.11   570}
\CommentTok{#> 163  1.75   675}
\CommentTok{#> 164  1.68   615}
\CommentTok{#> 165  1.75   520}
\CommentTok{#> 166  1.56   695}
\CommentTok{#> 167  1.75   685}
\CommentTok{#> 168  1.80   750}
\CommentTok{#> 169  1.92   630}
\CommentTok{#> 170  1.83   510}
\CommentTok{#> 171  1.63   470}
\CommentTok{#> 172  1.71   660}
\CommentTok{#> 173  1.74   740}
\CommentTok{#> 174  1.56   750}
\CommentTok{#> 175  1.56   835}
\CommentTok{#> 176  1.62   840}
\CommentTok{#> 177  1.60   560}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{names}\NormalTok{(wines) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"label"}\NormalTok{,}
                  \StringTok{"Alcohol"}\NormalTok{,}
                  \StringTok{"Malic_acid"}\NormalTok{,}
                  \StringTok{"Ash"}\NormalTok{,}
                  \StringTok{"Alcalinity_of_ash"}\NormalTok{,}
                  \StringTok{"Magnesium"}\NormalTok{,}
                  \StringTok{"Total_phenols"}\NormalTok{,}
                  \StringTok{"Flavanoids"}\NormalTok{,}
                  \StringTok{"Nonflavanoid_phenols"}\NormalTok{,}
                  \StringTok{"Proanthocyanins"}\NormalTok{,}
                  \StringTok{"Color_intensity"}\NormalTok{,}
                  \StringTok{"Hue"}\NormalTok{,}
                  \StringTok{"OD280_OD315_of_diluted_wines"}\NormalTok{,}
                  \StringTok{"Proline"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(wines)}
\CommentTok{#>   label Alcohol Malic_acid  Ash Alcalinity_of_ash Magnesium Total_phenols}
\CommentTok{#> 1     1    13.2       1.78 2.14              11.2       100          2.65}
\CommentTok{#> 2     1    13.2       2.36 2.67              18.6       101          2.80}
\CommentTok{#> 3     1    14.4       1.95 2.50              16.8       113          3.85}
\CommentTok{#> 4     1    13.2       2.59 2.87              21.0       118          2.80}
\CommentTok{#> 5     1    14.2       1.76 2.45              15.2       112          3.27}
\CommentTok{#> 6     1    14.4       1.87 2.45              14.6        96          2.50}
\CommentTok{#>   Flavanoids Nonflavanoid_phenols Proanthocyanins Color_intensity  Hue}
\CommentTok{#> 1       2.76                 0.26            1.28            4.38 1.05}
\CommentTok{#> 2       3.24                 0.30            2.81            5.68 1.03}
\CommentTok{#> 3       3.49                 0.24            2.18            7.80 0.86}
\CommentTok{#> 4       2.69                 0.39            1.82            4.32 1.04}
\CommentTok{#> 5       3.39                 0.34            1.97            6.75 1.05}
\CommentTok{#> 6       2.52                 0.30            1.98            5.25 1.02}
\CommentTok{#>   OD280_OD315_of_diluted_wines Proline}
\CommentTok{#> 1                         3.40    1050}
\CommentTok{#> 2                         3.17    1185}
\CommentTok{#> 3                         3.45    1480}
\CommentTok{#> 4                         2.93     735}
\CommentTok{#> 5                         2.85    1450}
\CommentTok{#> 6                         3.58    1290}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plt1 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(wines, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Alcohol, }\DataTypeTok{y =}\NormalTok{ Magnesium, }\DataTypeTok{colour =} \KeywordTok{as.factor}\NormalTok{(label))) }\OperatorTok{+}
\StringTok{    }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{size=}\DecValTok{3}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"Wines"}\NormalTok{)}

\NormalTok{plt1}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{nn-classification_904-wine_selection_nn_files/figure-latex/unnamed-chunk-7-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plt2 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(wines, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Alcohol, }\DataTypeTok{y =}\NormalTok{ Proline, }\DataTypeTok{colour =} \KeywordTok{as.factor}\NormalTok{(label))) }\OperatorTok{+}
\StringTok{    }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{size=}\DecValTok{3}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"Wines"}\NormalTok{)}
\NormalTok{plt2}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{nn-classification_904-wine_selection_nn_files/figure-latex/unnamed-chunk-8-1} \end{center}

\hypertarget{preprocessing}{%
\section{Preprocessing}\label{preprocessing}}

During the preprocessing phase, I have to do at least the following two things:

Encode the categorical variables.
Standardize the predictors.
First of all, let's encode our target variable. The encoding of the categorical variables is needed when using neuralnet since it does not like factors at all. It will shout at you if you try to feed in a factor (I am told nnet likes factors though).

In the wine dataset the variable label contains three different labels: 1,2 and 3.

The usual practice, as far as I know, is to encode categorical variables as a ``one hot'' vector. For instance, if I had three classes, like in this case, I'd need to replace the label variable with three variables like these:

\begin{verbatim}
#   l1,l2,l3
#   1,0,0
#   0,0,1
#   ...
\end{verbatim}

In this case the first observation would be labelled as a 1, the second would be labelled as a 2, and so on. Ironically, the \texttt{nnet} package provides a function to perform this encoding in a painless way:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Encode as a one hot vector multilabel data}
\NormalTok{train <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(wines[, }\DecValTok{2}\OperatorTok{:}\DecValTok{14}\NormalTok{], }\KeywordTok{class.ind}\NormalTok{(}\KeywordTok{as.factor}\NormalTok{(wines}\OperatorTok{$}\NormalTok{label)))}

\CommentTok{# Set labels name}
\KeywordTok{names}\NormalTok{(train) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\KeywordTok{names}\NormalTok{(wines)[}\DecValTok{2}\OperatorTok{:}\DecValTok{14}\NormalTok{],}\StringTok{"l1"}\NormalTok{,}\StringTok{"l2"}\NormalTok{,}\StringTok{"l3"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

By the way, since the predictors are all continuous, you do not need to encode any of them, however, in case you needed to, you could apply the same strategy applied above to all the categorical predictors. Unless of course you'd like to try some other kind of custom encoding.

Now let's standardize the predictors in the {[}01{]}"\textgreater{}{[}01{]} interval by leveraging the lapply function:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Scale data}
\NormalTok{scl <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x) \{ (x }\OperatorTok{-}\StringTok{ }\KeywordTok{min}\NormalTok{(x))}\OperatorTok{/}\NormalTok{(}\KeywordTok{max}\NormalTok{(x) }\OperatorTok{-}\StringTok{ }\KeywordTok{min}\NormalTok{(x)) \}}
\NormalTok{train[, }\DecValTok{1}\OperatorTok{:}\DecValTok{13}\NormalTok{] <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\KeywordTok{lapply}\NormalTok{(train[, }\DecValTok{1}\OperatorTok{:}\DecValTok{13}\NormalTok{], scl))}
\KeywordTok{head}\NormalTok{(train)}
\CommentTok{#>   Alcohol Malic_acid   Ash Alcalinity_of_ash Magnesium Total_phenols}
\CommentTok{#> 1   0.571      0.206 0.417            0.0309     0.326         0.576}
\CommentTok{#> 2   0.561      0.320 0.701            0.4124     0.337         0.628}
\CommentTok{#> 3   0.879      0.239 0.610            0.3196     0.467         0.990}
\CommentTok{#> 4   0.582      0.366 0.807            0.5361     0.522         0.628}
\CommentTok{#> 5   0.834      0.202 0.583            0.2371     0.457         0.790}
\CommentTok{#> 6   0.884      0.223 0.583            0.2062     0.283         0.524}
\CommentTok{#>   Flavanoids Nonflavanoid_phenols Proanthocyanins Color_intensity   Hue}
\CommentTok{#> 1      0.511                0.245           0.274           0.265 0.463}
\CommentTok{#> 2      0.612                0.321           0.757           0.375 0.447}
\CommentTok{#> 3      0.665                0.208           0.558           0.556 0.309}
\CommentTok{#> 4      0.496                0.491           0.445           0.259 0.455}
\CommentTok{#> 5      0.643                0.396           0.492           0.467 0.463}
\CommentTok{#> 6      0.460                0.321           0.495           0.339 0.439}
\CommentTok{#>   OD280_OD315_of_diluted_wines Proline l1 l2 l3}
\CommentTok{#> 1                        0.780   0.551  1  0  0}
\CommentTok{#> 2                        0.696   0.647  1  0  0}
\CommentTok{#> 3                        0.799   0.857  1  0  0}
\CommentTok{#> 4                        0.608   0.326  1  0  0}
\CommentTok{#> 5                        0.579   0.836  1  0  0}
\CommentTok{#> 6                        0.846   0.722  1  0  0}
\end{Highlighting}
\end{Shaded}

\hypertarget{fitting-the-model-with-neuralnet}{%
\section{Fitting the model with neuralnet}\label{fitting-the-model-with-neuralnet}}

Now it is finally time to fit the model.

As you might remember from the old post I wrote, \texttt{neuralnet} does not like the formula y\textasciitilde{}.. Fear not, you can build the formula to be used in a simple step:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Set up formula}
\NormalTok{n <-}\StringTok{ }\KeywordTok{names}\NormalTok{(train)}
\NormalTok{f <-}\StringTok{ }\KeywordTok{as.formula}\NormalTok{(}\KeywordTok{paste}\NormalTok{(}\StringTok{"l1 + l2 + l3 ~"}\NormalTok{, }\KeywordTok{paste}\NormalTok{(n[}\OperatorTok{!}\NormalTok{n }\OperatorTok{%in%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"l1"}\NormalTok{,}\StringTok{"l2"}\NormalTok{,}\StringTok{"l3"}\NormalTok{)], }\DataTypeTok{collapse =} \StringTok{" + "}\NormalTok{)))}
\NormalTok{f}
\CommentTok{#> l1 + l2 + l3 ~ Alcohol + Malic_acid + Ash + Alcalinity_of_ash + }
\CommentTok{#>     Magnesium + Total_phenols + Flavanoids + Nonflavanoid_phenols + }
\CommentTok{#>     Proanthocyanins + Color_intensity + Hue + OD280_OD315_of_diluted_wines + }
\CommentTok{#>     Proline}
\end{Highlighting}
\end{Shaded}

Note that the characters in the vector are not pasted to the right of the ``\textasciitilde{}'' symbol.

Just remember to check that the formula is indeed correct and then you are good to go.

Let's train the neural network with the full dataset. It should take very little time to converge. If you did not standardize the predictors it could take a lot more though.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nn <-}\StringTok{ }\KeywordTok{neuralnet}\NormalTok{(f,}
                \DataTypeTok{data =}\NormalTok{ train,}
                \DataTypeTok{hidden =} \KeywordTok{c}\NormalTok{(}\DecValTok{13}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{3}\NormalTok{),}
                \DataTypeTok{act.fct =} \StringTok{"logistic"}\NormalTok{,}
                \DataTypeTok{linear.output =} \OtherTok{FALSE}\NormalTok{,}
                \DataTypeTok{lifesign =} \StringTok{"minimal"}\NormalTok{)}
\CommentTok{#> hidden: 13, 10, 3    thresh: 0.01    rep: 1/1    steps:      88  error: 0.03039  time: 0.06 secs}
\end{Highlighting}
\end{Shaded}

Note that I set the argument linear.output to FALSE in order to tell the model that I want to apply the activation function act.fct and that I am not doing a regression task. Then I set the activation function to logistic (which by the way is the default option) in order to apply the logistic function. The other available option is tanh but the model seems to perform a little worse with it so I opted for the default option. As far as I know these two are the only two available options, there is no ``relu'' function available although it seems to be a common activation function in other packages.

As far as the number of hidden neurons, I tried some combination and the one used seems to perform slightly better than the others (around 1\% of accuracy difference in cross validation score).

By using the in-built plot method you can get a visual take on what is actually happening inside the model, however the plot is not that helpful I think

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(nn)}
\end{Highlighting}
\end{Shaded}

Let's have a look at the accuracy on the training set:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Compute predictions}
\NormalTok{pr.nn <-}\StringTok{ }\KeywordTok{compute}\NormalTok{(nn, train[, }\DecValTok{1}\OperatorTok{:}\DecValTok{13}\NormalTok{])}

\CommentTok{# Extract results}
\NormalTok{pr.nn_ <-}\StringTok{ }\NormalTok{pr.nn}\OperatorTok{$}\NormalTok{net.result}
\KeywordTok{head}\NormalTok{(pr.nn_)}
\CommentTok{#>       [,1]    [,2]     [,3]}
\CommentTok{#> [1,] 0.990 0.00317 6.99e-06}
\CommentTok{#> [2,] 0.991 0.00233 8.69e-06}
\CommentTok{#> [3,] 0.991 0.00210 8.65e-06}
\CommentTok{#> [4,] 0.986 0.00442 8.74e-06}
\CommentTok{#> [5,] 0.992 0.00212 8.32e-06}
\CommentTok{#> [6,] 0.992 0.00214 8.34e-06}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Accuracy (training set)}
\NormalTok{original_values <-}\StringTok{ }\KeywordTok{max.col}\NormalTok{(train[, }\DecValTok{14}\OperatorTok{:}\DecValTok{16}\NormalTok{])}
\NormalTok{pr.nn_}\DecValTok{2}\NormalTok{ <-}\StringTok{ }\KeywordTok{max.col}\NormalTok{(pr.nn_)}
\KeywordTok{mean}\NormalTok{(pr.nn_}\DecValTok{2} \OperatorTok{==}\StringTok{ }\NormalTok{original_values)}
\CommentTok{#> [1] 1}
\end{Highlighting}
\end{Shaded}

100\% not bad! But wait, this may be because our model over fitted the data, furthermore evaluating accuracy on the training set is kind of cheating since the model already ``knows'' (or should know) the answers. In order to assess the ``true accuracy'' of the model you need to perform some kind of cross validation.

\hypertarget{cross-validating-the-classifier}{%
\section{Cross validating the classifier}\label{cross-validating-the-classifier}}

Let's crossvalidate the model using the evergreen 10 fold cross validation with the following train and test split: 95\% of the dataset will be used as training set while the remaining 5\% as test set.

Just out of curiosity I decided to run a LOOCV round too. In case you'd like to run this cross validation technique, just set the proportion variable to 0.995: this will select just one observation for as test set and leave all the other observations as training set. Running LOOCV you should get similar results to the 10 fold cross validation.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Set seed for reproducibility purposes}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{500}\NormalTok{)}
\CommentTok{# 10 fold cross validation}
\NormalTok{k <-}\StringTok{ }\DecValTok{10}
\CommentTok{# Results from cv}
\NormalTok{outs <-}\StringTok{ }\OtherTok{NULL}
\CommentTok{# Train test split proportions}
\NormalTok{proportion <-}\StringTok{ }\FloatTok{0.95} \CommentTok{# Set to 0.995 for LOOCV}

\CommentTok{# Crossvalidate, go!}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{k)}
\NormalTok{\{}
\NormalTok{    index <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\KeywordTok{nrow}\NormalTok{(train), }\KeywordTok{round}\NormalTok{(proportion}\OperatorTok{*}\KeywordTok{nrow}\NormalTok{(train)))}
\NormalTok{    train_cv <-}\StringTok{ }\NormalTok{train[index, ]}
\NormalTok{    test_cv <-}\StringTok{ }\NormalTok{train[}\OperatorTok{-}\NormalTok{index, ]}
\NormalTok{    nn_cv <-}\StringTok{ }\KeywordTok{neuralnet}\NormalTok{(f,}
                        \DataTypeTok{data =}\NormalTok{ train_cv,}
                        \DataTypeTok{hidden =} \KeywordTok{c}\NormalTok{(}\DecValTok{13}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{3}\NormalTok{),}
                        \DataTypeTok{act.fct =} \StringTok{"logistic"}\NormalTok{,}
                        \DataTypeTok{linear.output =} \OtherTok{FALSE}\NormalTok{)}
    
    \CommentTok{# Compute predictions}
\NormalTok{    pr.nn <-}\StringTok{ }\KeywordTok{compute}\NormalTok{(nn_cv, test_cv[, }\DecValTok{1}\OperatorTok{:}\DecValTok{13}\NormalTok{])}
    \CommentTok{# Extract results}
\NormalTok{    pr.nn_ <-}\StringTok{ }\NormalTok{pr.nn}\OperatorTok{$}\NormalTok{net.result}
    \CommentTok{# Accuracy (test set)}
\NormalTok{    original_values <-}\StringTok{ }\KeywordTok{max.col}\NormalTok{(test_cv[, }\DecValTok{14}\OperatorTok{:}\DecValTok{16}\NormalTok{])}
\NormalTok{    pr.nn_}\DecValTok{2}\NormalTok{ <-}\StringTok{ }\KeywordTok{max.col}\NormalTok{(pr.nn_)}
\NormalTok{    outs[i] <-}\StringTok{ }\KeywordTok{mean}\NormalTok{(pr.nn_}\DecValTok{2} \OperatorTok{==}\StringTok{ }\NormalTok{original_values)}
\NormalTok{\}}

\KeywordTok{mean}\NormalTok{(outs)}
\CommentTok{#> [1] 0.978}
\end{Highlighting}
\end{Shaded}

98.8\%, awesome! Next time when you are invited to a relaxing evening that includes a wine tasting competition I think you should definitely bring your laptop as a contestant!

Aside from that poor taste joke, (I made it again!), indeed this dataset is not the most challenging, I think with some more tweaking a better cross validation score could be achieved. Nevertheless I hope you found this tutorial useful. A gist with the entire code for this tutorial can be found here.

Thank you for reading this article, please feel free to leave a comment if you have any questions or suggestions and share the post with others if you find it useful.

Notes:

\hypertarget{classification-and-regression-with-h2o-deep-learning}{%
\chapter{Classification and Regression with H2O Deep Learning}\label{classification-and-regression-with-h2o-deep-learning}}

\hypertarget{introduction-20}{%
\section{Introduction}\label{introduction-20}}

Source: \url{http://docs.h2o.ai/h2o-tutorials/latest-stable/tutorials/deeplearning/index.html}

Repo: \url{https://github.com/h2oai/h2o-tutorials}

This tutorial shows how a H2O Deep Learning model can be used to do supervised classification and regression. A great tutorial about Deep Learning is given by Quoc Le here and here. This tutorial covers usage of H2O from R. A python version of this tutorial will be available as well in a separate document. This file is available in plain R, R markdown and regular markdown formats, and the plots are available as PDF files. All documents are available on Github.

If run from plain R, execute R in the directory of this script. If run from RStudio, be sure to setwd() to the location of this script.\texttt{h2o.init()} starts H2O in R's current working directory. \texttt{h2o.importFile()} looks for files from the perspective of where H2O was started.

More examples and explanations can be found in our H2O Deep Learning booklet and on our H2O Github Repository. The PDF slide deck can be found on Github.

\hypertarget{h2o-r-package}{%
\section{H2O R Package}\label{h2o-r-package}}

Load the H2O R package:

Source: \url{http://docs.h2o.ai/h2o-tutorials/latest-stable/tutorials/deeplearning/index.html}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## R installation instructions are at http://h2o.ai/download}
\KeywordTok{library}\NormalTok{(h2o)}
\CommentTok{#> }
\CommentTok{#> ----------------------------------------------------------------------}
\CommentTok{#> }
\CommentTok{#> Your next step is to start H2O:}
\CommentTok{#>     > h2o.init()}
\CommentTok{#> }
\CommentTok{#> For H2O package documentation, ask for help:}
\CommentTok{#>     > ??h2o}
\CommentTok{#> }
\CommentTok{#> After starting H2O, you can use the Web UI at http://localhost:54321}
\CommentTok{#> For more information visit http://docs.h2o.ai}
\CommentTok{#> }
\CommentTok{#> ----------------------------------------------------------------------}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'h2o'}
\CommentTok{#> The following objects are masked from 'package:stats':}
\CommentTok{#> }
\CommentTok{#>     cor, sd, var}
\CommentTok{#> The following objects are masked from 'package:base':}
\CommentTok{#> }
\CommentTok{#>     &&, %*%, %in%, ||, apply, as.factor, as.numeric, colnames,}
\CommentTok{#>     colnames<-, ifelse, is.character, is.factor, is.numeric, log,}
\CommentTok{#>     log10, log1p, log2, round, signif, trunc}
\end{Highlighting}
\end{Shaded}

\hypertarget{start-h2o}{%
\section{Start H2O}\label{start-h2o}}

Start up a 1-node H2O server on your local machine, and allow it to use all CPU cores and up to 2GB of memory:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{h2o.init}\NormalTok{(}\DataTypeTok{nthreads=}\OperatorTok{-}\DecValTok{1}\NormalTok{, }\DataTypeTok{max_mem_size=}\StringTok{"2G"}\NormalTok{)}
\CommentTok{#> }
\CommentTok{#> H2O is not running yet, starting it now...}
\CommentTok{#> }
\CommentTok{#> Note:  In case of errors look at the following log files:}
\CommentTok{#>     /tmp/RtmpaoH2ki/h2o_datascience_started_from_r.out}
\CommentTok{#>     /tmp/RtmpaoH2ki/h2o_datascience_started_from_r.err}
\CommentTok{#> }
\CommentTok{#> }
\CommentTok{#> Starting H2O JVM and connecting: . Connection successful!}
\CommentTok{#> }
\CommentTok{#> R is connected to the H2O cluster: }
\CommentTok{#>     H2O cluster uptime:         1 seconds 270 milliseconds }
\CommentTok{#>     H2O cluster timezone:       America/Chicago }
\CommentTok{#>     H2O data parsing timezone:  UTC }
\CommentTok{#>     H2O cluster version:        3.22.1.1 }
\CommentTok{#>     H2O cluster version age:    8 months and 23 days !!! }
\CommentTok{#>     H2O cluster name:           H2O_started_from_R_datascience_mwl453 }
\CommentTok{#>     H2O cluster total nodes:    1 }
\CommentTok{#>     H2O cluster total memory:   1.78 GB }
\CommentTok{#>     H2O cluster total cores:    8 }
\CommentTok{#>     H2O cluster allowed cores:  8 }
\CommentTok{#>     H2O cluster healthy:        TRUE }
\CommentTok{#>     H2O Connection ip:          localhost }
\CommentTok{#>     H2O Connection port:        54321 }
\CommentTok{#>     H2O Connection proxy:       NA }
\CommentTok{#>     H2O Internal Security:      FALSE }
\CommentTok{#>     H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4 }
\CommentTok{#>     R Version:                  R version 3.6.0 (2019-04-26)}
\CommentTok{#> Warning in h2o.clusterInfo(): }
\CommentTok{#> Your H2O cluster version is too old (8 months and 23 days)!}
\CommentTok{#> Please download and install the latest version from http://h2o.ai/download/}
\KeywordTok{h2o.removeAll}\NormalTok{() }\CommentTok{## clean slate - just in case the cluster was already running}
\CommentTok{#> [1] 0}
\end{Highlighting}
\end{Shaded}

The \texttt{h2o.deeplearning} function fits H2O's Deep Learning models from within R. We can run the example from the man page using the example function, or run a longer demonstration from the h2o package using the demo function::

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{args}\NormalTok{(h2o.deeplearning)}
\CommentTok{#> function (x, y, training_frame, model_id = NULL, validation_frame = NULL, }
\CommentTok{#>     nfolds = 0, keep_cross_validation_models = TRUE, keep_cross_validation_predictions = FALSE, }
\CommentTok{#>     keep_cross_validation_fold_assignment = FALSE, fold_assignment = c("AUTO", }
\CommentTok{#>         "Random", "Modulo", "Stratified"), fold_column = NULL, }
\CommentTok{#>     ignore_const_cols = TRUE, score_each_iteration = FALSE, weights_column = NULL, }
\CommentTok{#>     offset_column = NULL, balance_classes = FALSE, class_sampling_factors = NULL, }
\CommentTok{#>     max_after_balance_size = 5, max_hit_ratio_k = 0, checkpoint = NULL, }
\CommentTok{#>     pretrained_autoencoder = NULL, overwrite_with_best_model = TRUE, }
\CommentTok{#>     use_all_factor_levels = TRUE, standardize = TRUE, activation = c("Tanh", }
\CommentTok{#>         "TanhWithDropout", "Rectifier", "RectifierWithDropout", }
\CommentTok{#>         "Maxout", "MaxoutWithDropout"), hidden = c(200, 200), }
\CommentTok{#>     epochs = 10, train_samples_per_iteration = -2, target_ratio_comm_to_comp = 0.05, }
\CommentTok{#>     seed = -1, adaptive_rate = TRUE, rho = 0.99, epsilon = 1e-08, }
\CommentTok{#>     rate = 0.005, rate_annealing = 1e-06, rate_decay = 1, momentum_start = 0, }
\CommentTok{#>     momentum_ramp = 1e+06, momentum_stable = 0, nesterov_accelerated_gradient = TRUE, }
\CommentTok{#>     input_dropout_ratio = 0, hidden_dropout_ratios = NULL, l1 = 0, }
\CommentTok{#>     l2 = 0, max_w2 = 3.4028235e+38, initial_weight_distribution = c("UniformAdaptive", }
\CommentTok{#>         "Uniform", "Normal"), initial_weight_scale = 1, initial_weights = NULL, }
\CommentTok{#>     initial_biases = NULL, loss = c("Automatic", "CrossEntropy", }
\CommentTok{#>         "Quadratic", "Huber", "Absolute", "Quantile"), distribution = c("AUTO", }
\CommentTok{#>         "bernoulli", "multinomial", "gaussian", "poisson", "gamma", }
\CommentTok{#>         "tweedie", "laplace", "quantile", "huber"), quantile_alpha = 0.5, }
\CommentTok{#>     tweedie_power = 1.5, huber_alpha = 0.9, score_interval = 5, }
\CommentTok{#>     score_training_samples = 10000, score_validation_samples = 0, }
\CommentTok{#>     score_duty_cycle = 0.1, classification_stop = 0, regression_stop = 1e-06, }
\CommentTok{#>     stopping_rounds = 5, stopping_metric = c("AUTO", "deviance", }
\CommentTok{#>         "logloss", "MSE", "RMSE", "MAE", "RMSLE", "AUC", "lift_top_group", }
\CommentTok{#>         "misclassification", "mean_per_class_error", "custom", }
\CommentTok{#>         "custom_increasing"), stopping_tolerance = 0, max_runtime_secs = 0, }
\CommentTok{#>     score_validation_sampling = c("Uniform", "Stratified"), diagnostics = TRUE, }
\CommentTok{#>     fast_mode = TRUE, force_load_balance = TRUE, variable_importances = TRUE, }
\CommentTok{#>     replicate_training_data = TRUE, single_node_mode = FALSE, }
\CommentTok{#>     shuffle_training_data = FALSE, missing_values_handling = c("MeanImputation", }
\CommentTok{#>         "Skip"), quiet_mode = FALSE, autoencoder = FALSE, sparse = FALSE, }
\CommentTok{#>     col_major = FALSE, average_activation = 0, sparsity_beta = 0, }
\CommentTok{#>     max_categorical_features = 2147483647, reproducible = FALSE, }
\CommentTok{#>     export_weights_and_biases = FALSE, mini_batch_size = 1, categorical_encoding = c("AUTO", }
\CommentTok{#>         "Enum", "OneHotInternal", "OneHotExplicit", "Binary", }
\CommentTok{#>         "Eigen", "LabelEncoder", "SortByResponse", "EnumLimited"), }
\CommentTok{#>     elastic_averaging = FALSE, elastic_averaging_moving_rate = 0.9, }
\CommentTok{#>     elastic_averaging_regularization = 0.001, export_checkpoints_dir = NULL, }
\CommentTok{#>     verbose = FALSE) }
\CommentTok{#> NULL}
\ControlFlowTok{if}\NormalTok{ (}\KeywordTok{interactive}\NormalTok{()) }\KeywordTok{help}\NormalTok{(h2o.deeplearning)}
\KeywordTok{example}\NormalTok{(h2o.deeplearning)}
\CommentTok{#> }
\CommentTok{#> h2.dpl> ## No test: }
\CommentTok{#> h2.dpl> ##D library(h2o)}
\CommentTok{#> h2.dpl> ##D h2o.init()}
\CommentTok{#> h2.dpl> ##D iris_hf <- as.h2o(iris)}
\CommentTok{#> h2.dpl> ##D iris_dl <- h2o.deeplearning(x = 1:4, y = 5, training_frame = iris_hf, seed=123456)}
\CommentTok{#> h2.dpl> ##D }
\CommentTok{#> h2.dpl> ##D # now make a prediction}
\CommentTok{#> h2.dpl> ##D predictions <- h2o.predict(iris_dl, iris_hf)}
\CommentTok{#> h2.dpl> ## End(No test)}
\CommentTok{#> h2.dpl> }
\CommentTok{#> h2.dpl> }
\CommentTok{#> h2.dpl>}
\ControlFlowTok{if}\NormalTok{ (}\KeywordTok{interactive}\NormalTok{()) }\KeywordTok{demo}\NormalTok{(h2o.deeplearning)  }\CommentTok{#requires user interaction}
\end{Highlighting}
\end{Shaded}

While \textbf{H2O Deep Learning} has many parameters, it was designed to be just as easy to use as the other supervised training methods in H2O. Early stopping, automatic data standardization and handling of categorical variables and missing values and adaptive learning rates (per weight) reduce the amount of parameters the user has to specify. Often, it's just the number and sizes of hidden layers, the number of epochs and the activation function and maybe some regularization techniques.

\hypertarget{lets-have-some-fun-first-decision-boundaries}{%
\section{Let's have some fun first: Decision Boundaries}\label{lets-have-some-fun-first-decision-boundaries}}

We start with a small dataset representing red and black dots on a plane, arranged in the shape of two nested spirals. Then we task H2O's machine learning methods to separate the red and black dots, i.e., recognize each spiral as such by assigning each point in the plane to one of the two spirals.

We visualize the nature of H2O Deep Learning (DL), H2O's tree methods (GBM/DRF) and H2O's generalized linear modeling (GLM) by plotting the decision boundary between the red and black spirals:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# setwd("~/h2o-tutorials/tutorials/deeplearning") ##For RStudio}
\NormalTok{spiral <-}\StringTok{ }\KeywordTok{h2o.importFile}\NormalTok{(}\DataTypeTok{path =} \KeywordTok{normalizePath}\NormalTok{(}\KeywordTok{file.path}\NormalTok{(data_raw_dir, }\StringTok{"spiral.csv"}\NormalTok{)))}
\CommentTok{#> }
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|}\StringTok{                                                                 }\ErrorTok{|}\StringTok{   }\DecValTok{0}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=================================================================|}\StringTok{ }\DecValTok{100}\NormalTok{%}
\NormalTok{grid   <-}\StringTok{ }\KeywordTok{h2o.importFile}\NormalTok{(}\DataTypeTok{path =} \KeywordTok{normalizePath}\NormalTok{(}\KeywordTok{file.path}\NormalTok{(data_raw_dir, }\StringTok{"grid.csv"}\NormalTok{)))}
\CommentTok{#> }
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|}\StringTok{                                                                 }\ErrorTok{|}\StringTok{   }\DecValTok{0}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==========}\StringTok{                                                       }\ErrorTok{|}\StringTok{  }\DecValTok{16}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=================================================================|}\StringTok{ }\DecValTok{100}\NormalTok{%}

\CommentTok{# Define helper to plot contours}
\NormalTok{plotC <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(name, model, }\DataTypeTok{data=}\NormalTok{spiral, }\DataTypeTok{g=}\NormalTok{grid) \{}
\NormalTok{  data <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(data) }\CommentTok{#get data from into R}
\NormalTok{  pred <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(}\KeywordTok{h2o.predict}\NormalTok{(model, g))}
\NormalTok{  n=}\FloatTok{0.5}\OperatorTok{*}\NormalTok{(}\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{nrow}\NormalTok{(g))}\OperatorTok{-}\DecValTok{1}\NormalTok{); d <-}\StringTok{ }\FloatTok{1.5}\NormalTok{; h <-}\StringTok{ }\NormalTok{d}\OperatorTok{*}\NormalTok{(}\OperatorTok{-}\NormalTok{n}\OperatorTok{:}\NormalTok{n)}\OperatorTok{/}\NormalTok{n}
  \KeywordTok{plot}\NormalTok{(data[,}\OperatorTok{-}\DecValTok{3}\NormalTok{],}\DataTypeTok{pch=}\DecValTok{19}\NormalTok{,}\DataTypeTok{col=}\NormalTok{data[,}\DecValTok{3}\NormalTok{],}\DataTypeTok{cex=}\FloatTok{0.5}\NormalTok{,}
       \DataTypeTok{xlim=}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\NormalTok{d,d),}\DataTypeTok{ylim=}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\NormalTok{d,d),}\DataTypeTok{main=}\NormalTok{name)}
  \KeywordTok{contour}\NormalTok{(h,h,}\DataTypeTok{z=}\KeywordTok{array}\NormalTok{(}\KeywordTok{ifelse}\NormalTok{(pred[,}\DecValTok{1}\NormalTok{]}\OperatorTok{==}\StringTok{"Red"}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{),}
          \DataTypeTok{dim=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\OperatorTok{*}\NormalTok{n}\OperatorTok{+}\DecValTok{1}\NormalTok{,}\DecValTok{2}\OperatorTok{*}\NormalTok{n}\OperatorTok{+}\DecValTok{1}\NormalTok{)),}\DataTypeTok{col=}\StringTok{"blue"}\NormalTok{,}\DataTypeTok{lwd=}\DecValTok{2}\NormalTok{,}\DataTypeTok{add=}\NormalTok{T)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

We build a few different models:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#dev.new(noRStudioGD=FALSE) #direct plotting output to a new window}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{)) }\CommentTok{#set up the canvas for 2x2 plots}
\KeywordTok{plotC}\NormalTok{( }\StringTok{"DL"}\NormalTok{, }\KeywordTok{h2o.deeplearning}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,spiral,}\DataTypeTok{epochs=}\FloatTok{1e3}\NormalTok{))}
\KeywordTok{plotC}\NormalTok{(}\StringTok{"GBM"}\NormalTok{, }\KeywordTok{h2o.gbm}\NormalTok{         (}\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,spiral))}
\KeywordTok{plotC}\NormalTok{(}\StringTok{"DRF"}\NormalTok{, }\KeywordTok{h2o.randomForest}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,spiral))}
\KeywordTok{plotC}\NormalTok{(}\StringTok{"GLM"}\NormalTok{, }\KeywordTok{h2o.glm}\NormalTok{         (}\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,spiral,}\DataTypeTok{family=}\StringTok{"binomial"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{nn-comparison-classification_regression_with_h2o_deep_learning_files/figure-latex/deeplearning_gbm_drf_glm-1} \end{center}

Let's investigate some more Deep Learning models. First, we explore the evolution over training time (number of passes over the data), and we use checkpointing to continue training the same model:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#dev.new(noRStudioGD=FALSE) #direct plotting output to a new window}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{)) }\CommentTok{#set up the canvas for 2x2 plots}
\NormalTok{ep <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{250}\NormalTok{,}\DecValTok{500}\NormalTok{,}\DecValTok{750}\NormalTok{)}
\KeywordTok{plotC}\NormalTok{(}\KeywordTok{paste0}\NormalTok{(}\StringTok{"DL "}\NormalTok{,ep[}\DecValTok{1}\NormalTok{],}\StringTok{" epochs"}\NormalTok{),}
      \KeywordTok{h2o.deeplearning}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,spiral,}\DataTypeTok{epochs=}\NormalTok{ep[}\DecValTok{1}\NormalTok{],}
                              \DataTypeTok{model_id=}\StringTok{"dl_1"}\NormalTok{))}
\KeywordTok{plotC}\NormalTok{(}\KeywordTok{paste0}\NormalTok{(}\StringTok{"DL "}\NormalTok{,ep[}\DecValTok{2}\NormalTok{],}\StringTok{" epochs"}\NormalTok{),}
      \KeywordTok{h2o.deeplearning}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,spiral,}\DataTypeTok{epochs=}\NormalTok{ep[}\DecValTok{2}\NormalTok{],}
            \DataTypeTok{checkpoint=}\StringTok{"dl_1"}\NormalTok{,}\DataTypeTok{model_id=}\StringTok{"dl_2"}\NormalTok{))}
\KeywordTok{plotC}\NormalTok{(}\KeywordTok{paste0}\NormalTok{(}\StringTok{"DL "}\NormalTok{,ep[}\DecValTok{3}\NormalTok{],}\StringTok{" epochs"}\NormalTok{),}
      \KeywordTok{h2o.deeplearning}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,spiral,}\DataTypeTok{epochs=}\NormalTok{ep[}\DecValTok{3}\NormalTok{],}
            \DataTypeTok{checkpoint=}\StringTok{"dl_2"}\NormalTok{,}\DataTypeTok{model_id=}\StringTok{"dl_3"}\NormalTok{))}
\KeywordTok{plotC}\NormalTok{(}\KeywordTok{paste0}\NormalTok{(}\StringTok{"DL "}\NormalTok{,ep[}\DecValTok{4}\NormalTok{],}\StringTok{" epochs"}\NormalTok{),}
      \KeywordTok{h2o.deeplearning}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,spiral,}\DataTypeTok{epochs=}\NormalTok{ep[}\DecValTok{4}\NormalTok{],}
            \DataTypeTok{checkpoint=}\StringTok{"dl_3"}\NormalTok{,}\DataTypeTok{model_id=}\StringTok{"dl_4"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{nn-comparison-classification_regression_with_h2o_deep_learning_files/figure-latex/deeplearning_evolution_training_time-1} \end{center}

You can see how the network learns the structure of the spirals with enough training time. We explore different network architectures next:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#dev.new(noRStudioGD=FALSE) #direct plotting output to a new window}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{)) }\CommentTok{#set up the canvas for 2x2 plots}
\ControlFlowTok{for}\NormalTok{ (hidden }\ControlFlowTok{in} \KeywordTok{list}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{11}\NormalTok{,}\DecValTok{13}\NormalTok{,}\DecValTok{17}\NormalTok{,}\DecValTok{19}\NormalTok{),}\KeywordTok{c}\NormalTok{(}\DecValTok{42}\NormalTok{,}\DecValTok{42}\NormalTok{,}\DecValTok{42}\NormalTok{),}\KeywordTok{c}\NormalTok{(}\DecValTok{200}\NormalTok{,}\DecValTok{200}\NormalTok{),}\KeywordTok{c}\NormalTok{(}\DecValTok{1000}\NormalTok{))) \{}
  \KeywordTok{plotC}\NormalTok{(}\KeywordTok{paste0}\NormalTok{(}\StringTok{"DL hidden="}\NormalTok{,}\KeywordTok{paste0}\NormalTok{(hidden, }\DataTypeTok{collapse=}\StringTok{"x"}\NormalTok{)),}
        \KeywordTok{h2o.deeplearning}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{ ,spiral, }\DataTypeTok{hidden=}\NormalTok{hidden, }\DataTypeTok{epochs=}\DecValTok{500}\NormalTok{))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{nn-comparison-classification_regression_with_h2o_deep_learning_files/figure-latex/deeplearning_for_loop_hidden-1} \end{center}

It is clear that different configurations can achieve similar performance, and that tuning will be required for optimal performance. Next, we compare between different activation functions, including one with 50\% dropout regularization in the hidden layers:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#dev.new(noRStudioGD=FALSE) #direct plotting output to a new window}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{)) }\CommentTok{#set up the canvas for 2x2 plots}

\ControlFlowTok{for}\NormalTok{ (act }\ControlFlowTok{in} \KeywordTok{c}\NormalTok{(}\StringTok{"Tanh"}\NormalTok{, }\StringTok{"Maxout"}\NormalTok{, }\StringTok{"Rectifier"}\NormalTok{, }\StringTok{"RectifierWithDropout"}\NormalTok{)) \{}
  \KeywordTok{plotC}\NormalTok{(}\KeywordTok{paste0}\NormalTok{(}\StringTok{"DL "}\NormalTok{,act,}\StringTok{" activation"}\NormalTok{), }
        \KeywordTok{h2o.deeplearning}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{, spiral,}
              \DataTypeTok{activation =}\NormalTok{ act, }
              \DataTypeTok{hidden =} \KeywordTok{c}\NormalTok{(}\DecValTok{100}\NormalTok{,}\DecValTok{100}\NormalTok{), }
              \DataTypeTok{epochs =} \DecValTok{1000}\NormalTok{))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{nn-comparison-classification_regression_with_h2o_deep_learning_files/figure-latex/compare_activation_functions-1} \end{center}

Clearly, the dropout rate was too high or the number of epochs was too low for the last configuration, which often ends up performing the best on larger datasets where generalization is important.

More information about the parameters can be found in the H2O Deep Learning booklet.

\hypertarget{cover-type-dataset}{%
\section{Cover Type Dataset}\label{cover-type-dataset}}

We important the full cover type dataset (581k rows, 13 columns, 10 numerical, 3 categorical). We also split the data 3 ways: 60\% for training, 20\% for validation (hyper parameter tuning) and 20\% for final testing.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df <-}\StringTok{ }\KeywordTok{h2o.importFile}\NormalTok{(}\DataTypeTok{path =} \KeywordTok{normalizePath}\NormalTok{(}\KeywordTok{file.path}\NormalTok{(data_raw_dir, }\StringTok{"covtype.full.csv"}\NormalTok{)))}
\CommentTok{#> }
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|}\StringTok{                                                                 }\ErrorTok{|}\StringTok{   }\DecValTok{0}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==================}\StringTok{                                               }\ErrorTok{|}\StringTok{  }\DecValTok{28}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=================================================================|}\StringTok{ }\DecValTok{100}\NormalTok{%}
\KeywordTok{dim}\NormalTok{(df)}
\CommentTok{#> [1] 581012     13}
\NormalTok{df}
\CommentTok{#>   Elevation Aspect Slope Horizontal_Distance_To_Hydrology}
\CommentTok{#> 1      3066    124     5                                0}
\CommentTok{#> 2      3136     32    20                              450}
\CommentTok{#> 3      2655     28    14                               42}
\CommentTok{#> 4      3191     45    19                              323}
\CommentTok{#> 5      3217     80    13                               30}
\CommentTok{#> 6      3119    293    13                               30}
\CommentTok{#>   Vertical_Distance_To_Hydrology Horizontal_Distance_To_Roadways}
\CommentTok{#> 1                              0                            1533}
\CommentTok{#> 2                            -38                            1290}
\CommentTok{#> 3                              8                            1890}
\CommentTok{#> 4                             88                            3932}
\CommentTok{#> 5                              1                            3901}
\CommentTok{#> 6                             10                            4810}
\CommentTok{#>   Hillshade_9am Hillshade_Noon Hillshade_3pm}
\CommentTok{#> 1           229            236           141}
\CommentTok{#> 2           211            193           111}
\CommentTok{#> 3           214            209           128}
\CommentTok{#> 4           221            195           100}
\CommentTok{#> 5           237            217           109}
\CommentTok{#> 6           182            237           194}
\CommentTok{#>   Horizontal_Distance_To_Fire_Points Wilderness_Area Soil_Type Cover_Type}
\CommentTok{#> 1                                459          area_0   type_22    class_1}
\CommentTok{#> 2                               1112          area_0   type_28    class_1}
\CommentTok{#> 3                               1001          area_2    type_9    class_2}
\CommentTok{#> 4                               2919          area_0   type_39    class_2}
\CommentTok{#> 5                               2859          area_0   type_22    class_7}
\CommentTok{#> 6                               1200          area_0   type_21    class_1}
\CommentTok{#> }
\CommentTok{#> [581012 rows x 13 columns]}
\NormalTok{splits <-}\StringTok{ }\KeywordTok{h2o.splitFrame}\NormalTok{(df, }\KeywordTok{c}\NormalTok{(}\FloatTok{0.6}\NormalTok{, }\FloatTok{0.2}\NormalTok{), }\DataTypeTok{seed=}\DecValTok{1234}\NormalTok{)}
\NormalTok{train  <-}\StringTok{ }\KeywordTok{h2o.assign}\NormalTok{(splits[[}\DecValTok{1}\NormalTok{]], }\StringTok{"train.hex"}\NormalTok{) }\CommentTok{# 60%}
\NormalTok{valid  <-}\StringTok{ }\KeywordTok{h2o.assign}\NormalTok{(splits[[}\DecValTok{2}\NormalTok{]], }\StringTok{"valid.hex"}\NormalTok{) }\CommentTok{# 20%}
\NormalTok{test   <-}\StringTok{ }\KeywordTok{h2o.assign}\NormalTok{(splits[[}\DecValTok{3}\NormalTok{]], }\StringTok{"test.hex"}\NormalTok{)  }\CommentTok{# 20%}
\end{Highlighting}
\end{Shaded}

Here's a scalable way to do scatter plots via binning (works for categorical and numeric columns) to get more familiar with the dataset.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#dev.new(noRStudioGD=FALSE) #direct plotting output to a new window}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)) }\CommentTok{# reset canvas}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{h2o.tabulate}\NormalTok{(df, }\StringTok{"Elevation"}\NormalTok{,                       }\StringTok{"Cover_Type"}\NormalTok{))}
\CommentTok{#> Registered S3 methods overwritten by 'ggplot2':}
\CommentTok{#>   method         from }
\CommentTok{#>   [.quosures     rlang}
\CommentTok{#>   c.quosures     rlang}
\CommentTok{#>   print.quosures rlang}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{h2o.tabulate}\NormalTok{(df, }\StringTok{"Horizontal_Distance_To_Roadways"}\NormalTok{, }\StringTok{"Cover_Type"}\NormalTok{))}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{h2o.tabulate}\NormalTok{(df, }\StringTok{"Soil_Type"}\NormalTok{,                       }\StringTok{"Cover_Type"}\NormalTok{))}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{h2o.tabulate}\NormalTok{(df, }\StringTok{"Horizontal_Distance_To_Roadways"}\NormalTok{, }\StringTok{"Elevation"}\NormalTok{ ))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{nn-comparison-classification_regression_with_h2o_deep_learning_files/figure-latex/plot_cover_type-1} \includegraphics[width=0.7\linewidth]{nn-comparison-classification_regression_with_h2o_deep_learning_files/figure-latex/plot_cover_type-2} \includegraphics[width=0.7\linewidth]{nn-comparison-classification_regression_with_h2o_deep_learning_files/figure-latex/plot_cover_type-3} \includegraphics[width=0.7\linewidth]{nn-comparison-classification_regression_with_h2o_deep_learning_files/figure-latex/plot_cover_type-4} \end{center}

\hypertarget{first-run-of-h2o-deep-learning}{%
\subsection{First Run of H2O Deep Learning}\label{first-run-of-h2o-deep-learning}}

Let's run our first Deep Learning model on the covtype dataset. We want to predict the \texttt{Cover\_Type} column, a categorical feature with 7 levels, and the Deep Learning model will be tasked to perform (multi-class) classification. It uses the other 12 predictors of the dataset, of which 10 are numerical, and 2 are categorical with a total of 44 levels. We can expect the Deep Learning model to have 56 input neurons (after automatic one-hot encoding).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{response <-}\StringTok{ "Cover_Type"}
\NormalTok{predictors <-}\StringTok{ }\KeywordTok{setdiff}\NormalTok{(}\KeywordTok{names}\NormalTok{(df), response)}
\NormalTok{predictors}
\CommentTok{#>  [1] "Elevation"                         }
\CommentTok{#>  [2] "Aspect"                            }
\CommentTok{#>  [3] "Slope"                             }
\CommentTok{#>  [4] "Horizontal_Distance_To_Hydrology"  }
\CommentTok{#>  [5] "Vertical_Distance_To_Hydrology"    }
\CommentTok{#>  [6] "Horizontal_Distance_To_Roadways"   }
\CommentTok{#>  [7] "Hillshade_9am"                     }
\CommentTok{#>  [8] "Hillshade_Noon"                    }
\CommentTok{#>  [9] "Hillshade_3pm"                     }
\CommentTok{#> [10] "Horizontal_Distance_To_Fire_Points"}
\CommentTok{#> [11] "Wilderness_Area"                   }
\CommentTok{#> [12] "Soil_Type"}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{train_df <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(train)}
\KeywordTok{str}\NormalTok{(train_df)}
\CommentTok{#> 'data.frame':    349015 obs. of  13 variables:}
\CommentTok{#>  $ Elevation                         : int  3136 3217 3119 2679 3261 2885 3227 2843 2853 2883 ...}
\CommentTok{#>  $ Aspect                            : int  32 80 293 48 322 26 32 12 124 177 ...}
\CommentTok{#>  $ Slope                             : int  20 13 13 7 13 9 6 18 12 9 ...}
\CommentTok{#>  $ Horizontal_Distance_To_Hydrology  : int  450 30 30 150 30 192 108 335 30 426 ...}
\CommentTok{#>  $ Vertical_Distance_To_Hydrology    : int  -38 1 10 24 5 38 13 50 -5 126 ...}
\CommentTok{#>  $ Horizontal_Distance_To_Roadways   : int  1290 3901 4810 1588 5701 3271 5542 2642 1485 2139 ...}
\CommentTok{#>  $ Hillshade_9am                     : int  211 237 182 223 186 216 219 199 240 225 ...}
\CommentTok{#>  $ Hillshade_Noon                    : int  193 217 237 224 226 220 227 201 231 246 ...}
\CommentTok{#>  $ Hillshade_3pm                     : int  111 109 194 136 180 140 145 135 119 153 ...}
\CommentTok{#>  $ Horizontal_Distance_To_Fire_Points: int  1112 2859 1200 6265 769 2643 765 1719 2497 713 ...}
\CommentTok{#>  $ Wilderness_Area                   : Factor w/ 4 levels "area_0","area_1",..: 1 1 1 1 1 1 1 3 3 3 ...}
\CommentTok{#>  $ Soil_Type                         : Factor w/ 40 levels "type_0","type_1",..: 22 16 15 4 15 22 15 27 12 25 ...}
\CommentTok{#>  $ Cover_Type                        : Factor w/ 7 levels "class_1","class_2",..: 1 7 1 2 1 2 1 2 1 2 ...}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{valid_df <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(valid)}
\KeywordTok{str}\NormalTok{(valid_df)}
\CommentTok{#> 'data.frame':    116018 obs. of  13 variables:}
\CommentTok{#>  $ Elevation                         : int  3066 2655 2902 2994 2697 2990 3237 2884 2972 2696 ...}
\CommentTok{#>  $ Aspect                            : int  124 28 304 61 93 59 135 71 100 169 ...}
\CommentTok{#>  $ Slope                             : int  5 14 22 9 9 12 14 9 4 10 ...}
\CommentTok{#>  $ Horizontal_Distance_To_Hydrology  : int  0 42 511 391 306 108 240 459 175 323 ...}
\CommentTok{#>  $ Vertical_Distance_To_Hydrology    : int  0 8 18 57 -2 10 -11 141 13 149 ...}
\CommentTok{#>  $ Horizontal_Distance_To_Roadways   : int  1533 1890 1273 4286 553 2190 1189 1214 5031 2452 ...}
\CommentTok{#>  $ Hillshade_9am                     : int  229 214 155 227 234 229 241 231 227 228 ...}
\CommentTok{#>  $ Hillshade_Noon                    : int  236 209 223 222 227 215 233 222 234 244 ...}
\CommentTok{#>  $ Hillshade_3pm                     : int  141 128 206 128 125 117 118 124 142 148 ...}
\CommentTok{#>  $ Horizontal_Distance_To_Fire_Points: int  459 1001 1347 1928 1716 1048 2748 1355 6198 1044 ...}
\CommentTok{#>  $ Wilderness_Area                   : Factor w/ 4 levels "area_0","area_1",..: 1 3 3 1 1 3 1 3 1 3 ...}
\CommentTok{#>  $ Soil_Type                         : Factor w/ 39 levels "type_0","type_1",..: 15 39 25 4 4 25 14 25 11 23 ...}
\CommentTok{#>  $ Cover_Type                        : Factor w/ 7 levels "class_1","class_2",..: 1 2 2 2 2 2 1 2 1 3 ...}
\end{Highlighting}
\end{Shaded}

To keep it fast, we only run for one epoch (one pass over the training data).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m1 <-}\StringTok{ }\KeywordTok{h2o.deeplearning}\NormalTok{(}
  \DataTypeTok{model_id=}\StringTok{"dl_model_first"}\NormalTok{, }
  \DataTypeTok{training_frame =}\NormalTok{ train, }
  \DataTypeTok{validation_frame =}\NormalTok{ valid,   }\CommentTok{## validation dataset: used for scoring and early stopping}
  \DataTypeTok{x =}\NormalTok{ predictors,}
  \DataTypeTok{y =}\NormalTok{ response,}
  \CommentTok{#activation="Rectifier",  ## default}
  \CommentTok{#hidden=c(200,200),       ## default: 2 hidden layers with 200 neurons each}
  \DataTypeTok{epochs =} \DecValTok{1}\NormalTok{,}
  \DataTypeTok{variable_importances=}\NormalTok{T    }\CommentTok{## not enabled by default}
\NormalTok{)}
\CommentTok{#> }
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|}\StringTok{                                                                 }\ErrorTok{|}\StringTok{   }\DecValTok{0}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|======}\StringTok{                                                           }\ErrorTok{|}\StringTok{  }\DecValTok{10}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=============}\StringTok{                                                    }\ErrorTok{|}\StringTok{  }\DecValTok{20}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|===================}\StringTok{                                              }\ErrorTok{|}\StringTok{  }\DecValTok{30}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==========================}\StringTok{                                       }\ErrorTok{|}\StringTok{  }\DecValTok{40}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|================================}\StringTok{                                 }\ErrorTok{|}\StringTok{  }\DecValTok{50}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=======================================}\StringTok{                          }\ErrorTok{|}\StringTok{  }\DecValTok{60}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=============================================}\StringTok{                    }\ErrorTok{|}\StringTok{  }\DecValTok{70}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|====================================================}\StringTok{             }\ErrorTok{|}\StringTok{  }\DecValTok{80}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==========================================================}\StringTok{       }\ErrorTok{|}\StringTok{  }\DecValTok{90}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=================================================================|}\StringTok{ }\DecValTok{100}\NormalTok{%}
\KeywordTok{summary}\NormalTok{(m1)}
\CommentTok{#> Model Details:}
\CommentTok{#> ==============}
\CommentTok{#> }
\CommentTok{#> H2OMultinomialModel: deeplearning}
\CommentTok{#> Model Key:  dl_model_first }
\CommentTok{#> Status of Neuron Layers: predicting Cover_Type, 7-class classification, multinomial distribution, CrossEntropy loss, 53,007 weights/biases, 634.3 KB, 382,862 training samples, mini-batch size 1}
\CommentTok{#>   layer units      type dropout       l1       l2 mean_rate rate_rms}
\CommentTok{#> 1     1    56     Input  0.00 %       NA       NA        NA       NA}
\CommentTok{#> 2     2   200 Rectifier  0.00 % 0.000000 0.000000  0.052280 0.214135}
\CommentTok{#> 3     3   200 Rectifier  0.00 % 0.000000 0.000000  0.009215 0.008708}
\CommentTok{#> 4     4     7   Softmax      NA 0.000000 0.000000  0.117055 0.287708}
\CommentTok{#>   momentum mean_weight weight_rms mean_bias bias_rms}
\CommentTok{#> 1       NA          NA         NA        NA       NA}
\CommentTok{#> 2 0.000000   -0.011302   0.119766  0.022118 0.121956}
\CommentTok{#> 3 0.000000   -0.026712   0.116801  0.681338 0.397288}
\CommentTok{#> 4 0.000000   -0.373111   0.512719 -0.598332 0.174839}
\CommentTok{#> }
\CommentTok{#> H2OMultinomialMetrics: deeplearning}
\CommentTok{#> ** Reported on training data. **}
\CommentTok{#> ** Metrics reported on temporary training frame with 9899 samples **}
\CommentTok{#> }
\CommentTok{#> Training Set Metrics: }
\CommentTok{#> =====================}
\CommentTok{#> }
\CommentTok{#> MSE: (Extract with `h2o.mse`) 0.13}
\CommentTok{#> RMSE: (Extract with `h2o.rmse`) 0.361}
\CommentTok{#> Logloss: (Extract with `h2o.logloss`) 0.417}
\CommentTok{#> Mean Per-Class Error: 0.328}
\CommentTok{#> Confusion Matrix: Extract with `h2o.confusionMatrix(<model>,train = TRUE)`)}
\CommentTok{#> =========================================================================}
\CommentTok{#> Confusion Matrix: Row labels: Actual class; Column labels: Predicted class}
\CommentTok{#>         class_1 class_2 class_3 class_4 class_5 class_6 class_7  Error}
\CommentTok{#> class_1    2761     827       2       0       8       0      74 0.2481}
\CommentTok{#> class_2     338    4425      44       0      20       9       4 0.0857}
\CommentTok{#> class_3       0      40     509       7       1       8       0 0.0991}
\CommentTok{#> class_4       0       0      14      26       0       0       0 0.3500}
\CommentTok{#> class_5       4      90       6       0      53       1       0 0.6558}
\CommentTok{#> class_6       0      69     152       3       1      73       0 0.7550}
\CommentTok{#> class_7      27       7       0       0       0       0     296 0.1030}
\CommentTok{#> Totals     3130    5458     727      36      83      91     374 0.1774}
\CommentTok{#>                    Rate}
\CommentTok{#> class_1 =   911 / 3,672}
\CommentTok{#> class_2 =   415 / 4,840}
\CommentTok{#> class_3 =      56 / 565}
\CommentTok{#> class_4 =       14 / 40}
\CommentTok{#> class_5 =     101 / 154}
\CommentTok{#> class_6 =     225 / 298}
\CommentTok{#> class_7 =      34 / 330}
\CommentTok{#> Totals  = 1,756 / 9,899}
\CommentTok{#> }
\CommentTok{#> Hit Ratio Table: Extract with `h2o.hit_ratio_table(<model>,train = TRUE)`}
\CommentTok{#> =======================================================================}
\CommentTok{#> Top-7 Hit Ratios: }
\CommentTok{#>   k hit_ratio}
\CommentTok{#> 1 1  0.822608}
\CommentTok{#> 2 2  0.984645}
\CommentTok{#> 3 3  0.998485}
\CommentTok{#> 4 4  0.999596}
\CommentTok{#> 5 5  1.000000}
\CommentTok{#> 6 6  1.000000}
\CommentTok{#> 7 7  1.000000}
\CommentTok{#> }
\CommentTok{#> }
\CommentTok{#> H2OMultinomialMetrics: deeplearning}
\CommentTok{#> ** Reported on validation data. **}
\CommentTok{#> ** Metrics reported on full validation frame **}
\CommentTok{#> }
\CommentTok{#> Validation Set Metrics: }
\CommentTok{#> =====================}
\CommentTok{#> }
\CommentTok{#> Extract validation frame with `h2o.getFrame("valid.hex")`}
\CommentTok{#> MSE: (Extract with `h2o.mse`) 0.133}
\CommentTok{#> RMSE: (Extract with `h2o.rmse`) 0.365}
\CommentTok{#> Logloss: (Extract with `h2o.logloss`) 0.428}
\CommentTok{#> Mean Per-Class Error: 0.326}
\CommentTok{#> Confusion Matrix: Extract with `h2o.confusionMatrix(<model>,valid = TRUE)`)}
\CommentTok{#> =========================================================================}
\CommentTok{#> Confusion Matrix: Row labels: Actual class; Column labels: Predicted class}
\CommentTok{#>         class_1 class_2 class_3 class_4 class_5 class_6 class_7  Error}
\CommentTok{#> class_1   31963    9459      13       0      63      16     986 0.2479}
\CommentTok{#> class_2    4390   51058     499       8     244     112      69 0.0944}
\CommentTok{#> class_3       0     484    6459      55       8     137       0 0.0958}
\CommentTok{#> class_4       0       0     216     343       0       3       0 0.3897}
\CommentTok{#> class_5      53     998      59       0     754       6       0 0.5968}
\CommentTok{#> class_6       1     856    1678      44       4     881       0 0.7457}
\CommentTok{#> class_7     389      77       3       0       0       0    3630 0.1144}
\CommentTok{#> Totals    36796   62932    8927     450    1073    1155    4685 0.1804}
\CommentTok{#>                       Rate}
\CommentTok{#> class_1 =  10,537 / 42,500}
\CommentTok{#> class_2 =   5,322 / 56,380}
\CommentTok{#> class_3 =      684 / 7,143}
\CommentTok{#> class_4 =        219 / 562}
\CommentTok{#> class_5 =    1,116 / 1,870}
\CommentTok{#> class_6 =    2,583 / 3,464}
\CommentTok{#> class_7 =      469 / 4,099}
\CommentTok{#> Totals  = 20,930 / 116,018}
\CommentTok{#> }
\CommentTok{#> Hit Ratio Table: Extract with `h2o.hit_ratio_table(<model>,valid = TRUE)`}
\CommentTok{#> =======================================================================}
\CommentTok{#> Top-7 Hit Ratios: }
\CommentTok{#>   k hit_ratio}
\CommentTok{#> 1 1  0.819597}
\CommentTok{#> 2 2  0.983882}
\CommentTok{#> 3 3  0.998052}
\CommentTok{#> 4 4  0.999578}
\CommentTok{#> 5 5  1.000000}
\CommentTok{#> 6 6  1.000000}
\CommentTok{#> 7 7  1.000000}
\CommentTok{#> }
\CommentTok{#> }
\CommentTok{#> }
\CommentTok{#> }
\CommentTok{#> Scoring History: }
\CommentTok{#>             timestamp   duration training_speed  epochs iterations}
\CommentTok{#> 1 2019-09-20 15:12:01  0.000 sec             NA 0.00000          0}
\CommentTok{#> 2 2019-09-20 15:12:05  6.067 sec   7903 obs/sec 0.09944          1}
\CommentTok{#> 3 2019-09-20 15:12:21 22.168 sec  10943 obs/sec 0.59907          6}
\CommentTok{#> 4 2019-09-20 15:12:34 35.323 sec  12379 obs/sec 1.09698         11}
\CommentTok{#>         samples training_rmse training_logloss training_r2}
\CommentTok{#> 1      0.000000            NA               NA          NA}
\CommentTok{#> 2  34705.000000       0.45370          0.65859     0.89158}
\CommentTok{#> 3 209084.000000       0.38547          0.46547     0.92173}
\CommentTok{#> 4 382862.000000       0.36121          0.41746     0.93128}
\CommentTok{#>   training_classification_error validation_rmse validation_logloss}
\CommentTok{#> 1                            NA              NA                 NA}
\CommentTok{#> 2                       0.27821         0.46368            0.68261}
\CommentTok{#> 3                       0.20295         0.39299            0.48243}
\CommentTok{#> 4                       0.17739         0.36527            0.42791}
\CommentTok{#>   validation_r2 validation_classification_error}
\CommentTok{#> 1            NA                              NA}
\CommentTok{#> 2       0.88979                         0.29197}
\CommentTok{#> 3       0.92084                         0.21145}
\CommentTok{#> 4       0.93161                         0.18040}
\CommentTok{#> }
\CommentTok{#> Variable Importances: (Extract with `h2o.varimp`) }
\CommentTok{#> =================================================}
\CommentTok{#> }
\CommentTok{#> Variable Importances: }
\CommentTok{#>                             variable relative_importance scaled_importance}
\CommentTok{#> 1             Wilderness_Area.area_0            1.000000          1.000000}
\CommentTok{#> 2    Horizontal_Distance_To_Roadways            0.883062          0.883062}
\CommentTok{#> 3                          Elevation            0.873007          0.873007}
\CommentTok{#> 4 Horizontal_Distance_To_Fire_Points            0.852707          0.852707}
\CommentTok{#> 5             Wilderness_Area.area_2            0.766709          0.766709}
\CommentTok{#>   percentage}
\CommentTok{#> 1   0.033708}
\CommentTok{#> 2   0.029766}
\CommentTok{#> 3   0.029427}
\CommentTok{#> 4   0.028743}
\CommentTok{#> 5   0.025844}
\CommentTok{#> }
\CommentTok{#> ---}
\CommentTok{#>                       variable relative_importance scaled_importance}
\CommentTok{#> 51            Soil_Type.type_7            0.409154          0.409154}
\CommentTok{#> 52               Hillshade_3pm            0.401568          0.401568}
\CommentTok{#> 53                       Slope            0.395765          0.395765}
\CommentTok{#> 54                      Aspect            0.279821          0.279821}
\CommentTok{#> 55       Soil_Type.missing(NA)            0.000000          0.000000}
\CommentTok{#> 56 Wilderness_Area.missing(NA)            0.000000          0.000000}
\CommentTok{#>    percentage}
\CommentTok{#> 51   0.013792}
\CommentTok{#> 52   0.013536}
\CommentTok{#> 53   0.013340}
\CommentTok{#> 54   0.009432}
\CommentTok{#> 55   0.000000}
\CommentTok{#> 56   0.000000}
\end{Highlighting}
\end{Shaded}

Inspect the model in Flow for more information about model building etc. by issuing a cell with the content getModel ``dl\_model\_first'', and pressing Ctrl-Enter.

\hypertarget{variable-importances}{%
\subsection{Variable Importances}\label{variable-importances}}

Variable importances for Neural Network models are notoriously difficult to compute, and there are many pitfalls. H2O Deep Learning has implemented the method of Gedeon, and returns relative variable importances in descending order of importance.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(}\KeywordTok{as.data.frame}\NormalTok{(}\KeywordTok{h2o.varimp}\NormalTok{(m1)))}
\CommentTok{#>                             variable relative_importance scaled_importance}
\CommentTok{#> 1             Wilderness_Area.area_0               1.000             1.000}
\CommentTok{#> 2    Horizontal_Distance_To_Roadways               0.883             0.883}
\CommentTok{#> 3                          Elevation               0.873             0.873}
\CommentTok{#> 4 Horizontal_Distance_To_Fire_Points               0.853             0.853}
\CommentTok{#> 5             Wilderness_Area.area_2               0.767             0.767}
\CommentTok{#> 6             Wilderness_Area.area_3               0.706             0.706}
\CommentTok{#>   percentage}
\CommentTok{#> 1     0.0337}
\CommentTok{#> 2     0.0298}
\CommentTok{#> 3     0.0294}
\CommentTok{#> 4     0.0287}
\CommentTok{#> 5     0.0258}
\CommentTok{#> 6     0.0238}
\end{Highlighting}
\end{Shaded}

\hypertarget{early-stopping}{%
\subsection{Early Stopping}\label{early-stopping}}

Now we run another, smaller network, and we let it stop automatically once the misclassification rate converges (specifically, if the moving average of length 2 does not improve by at least 1\% for 2 consecutive scoring events). We also sample the validation set to 10,000 rows for faster scoring.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m2 <-}\StringTok{ }\KeywordTok{h2o.deeplearning}\NormalTok{(}
  \DataTypeTok{model_id=}\StringTok{"dl_model_faster"}\NormalTok{, }
  \DataTypeTok{training_frame=}\NormalTok{train, }
  \DataTypeTok{validation_frame=}\NormalTok{valid,}
  \DataTypeTok{x=}\NormalTok{predictors,}
  \DataTypeTok{y=}\NormalTok{response,}
  \DataTypeTok{hidden=}\KeywordTok{c}\NormalTok{(}\DecValTok{32}\NormalTok{,}\DecValTok{32}\NormalTok{,}\DecValTok{32}\NormalTok{),                  }\CommentTok{## small network, runs faster}
  \DataTypeTok{epochs=}\DecValTok{1000000}\NormalTok{,                      }\CommentTok{## hopefully converges earlier...}
  \DataTypeTok{score_validation_samples=}\DecValTok{10000}\NormalTok{,      }\CommentTok{## sample the validation dataset (faster)}
  \DataTypeTok{stopping_rounds=}\DecValTok{2}\NormalTok{,}
  \DataTypeTok{stopping_metric=}\StringTok{"misclassification"}\NormalTok{, }\CommentTok{## could be "MSE","logloss","r2"}
  \DataTypeTok{stopping_tolerance=}\FloatTok{0.01}
\NormalTok{)}
\CommentTok{#> }
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|}\StringTok{                                                                 }\ErrorTok{|}\StringTok{   }\DecValTok{0}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=================================================================|}\StringTok{ }\DecValTok{100}\NormalTok{%}
\KeywordTok{summary}\NormalTok{(m2)}
\CommentTok{#> Model Details:}
\CommentTok{#> ==============}
\CommentTok{#> }
\CommentTok{#> H2OMultinomialModel: deeplearning}
\CommentTok{#> Model Key:  dl_model_faster }
\CommentTok{#> Status of Neuron Layers: predicting Cover_Type, 7-class classification, multinomial distribution, CrossEntropy loss, 4,167 weights/biases, 59.0 KB, 7,598,110 training samples, mini-batch size 1}
\CommentTok{#>   layer units      type dropout       l1       l2 mean_rate rate_rms}
\CommentTok{#> 1     1    56     Input  0.00 %       NA       NA        NA       NA}
\CommentTok{#> 2     2    32 Rectifier  0.00 % 0.000000 0.000000  0.044995 0.204363}
\CommentTok{#> 3     3    32 Rectifier  0.00 % 0.000000 0.000000  0.000295 0.000192}
\CommentTok{#> 4     4    32 Rectifier  0.00 % 0.000000 0.000000  0.000841 0.001651}
\CommentTok{#> 5     5     7   Softmax      NA 0.000000 0.000000  0.089028 0.263416}
\CommentTok{#>   momentum mean_weight weight_rms mean_bias bias_rms}
\CommentTok{#> 1       NA          NA         NA        NA       NA}
\CommentTok{#> 2 0.000000   -0.018824   0.317631  0.338853 0.373385}
\CommentTok{#> 3 0.000000   -0.064138   0.382940  0.669714 0.570015}
\CommentTok{#> 4 0.000000    0.017968   0.640646  0.647194 1.184354}
\CommentTok{#> 5 0.000000   -4.756408   3.588252 -3.596137 1.350429}
\CommentTok{#> }
\CommentTok{#> H2OMultinomialMetrics: deeplearning}
\CommentTok{#> ** Reported on training data. **}
\CommentTok{#> ** Metrics reported on temporary training frame with 9932 samples **}
\CommentTok{#> }
\CommentTok{#> Training Set Metrics: }
\CommentTok{#> =====================}
\CommentTok{#> }
\CommentTok{#> MSE: (Extract with `h2o.mse`) 0.108}
\CommentTok{#> RMSE: (Extract with `h2o.rmse`) 0.329}
\CommentTok{#> Logloss: (Extract with `h2o.logloss`) 0.369}
\CommentTok{#> Mean Per-Class Error: 0.263}
\CommentTok{#> Confusion Matrix: Extract with `h2o.confusionMatrix(<model>,train = TRUE)`)}
\CommentTok{#> =========================================================================}
\CommentTok{#> Confusion Matrix: Row labels: Actual class; Column labels: Predicted class}
\CommentTok{#>         class_1 class_2 class_3 class_4 class_5 class_6 class_7  Error}
\CommentTok{#> class_1    3007     490       1       0       5       3      28 0.1491}
\CommentTok{#> class_2     412    4397      33       0      33      31       2 0.1041}
\CommentTok{#> class_3       0      23     518       4       1      50       0 0.1309}
\CommentTok{#> class_4       0       0      11      40       0       8       0 0.3220}
\CommentTok{#> class_5       4      89       5       0      85       2       0 0.5405}
\CommentTok{#> class_6       1      31      56       4       1     185       0 0.3345}
\CommentTok{#> class_7      86      11       0       0       0       0     275 0.2608}
\CommentTok{#> Totals     3510    5041     624      48     125     279     305 0.1435}
\CommentTok{#>                    Rate}
\CommentTok{#> class_1 =   527 / 3,534}
\CommentTok{#> class_2 =   511 / 4,908}
\CommentTok{#> class_3 =      78 / 596}
\CommentTok{#> class_4 =       19 / 59}
\CommentTok{#> class_5 =     100 / 185}
\CommentTok{#> class_6 =      93 / 278}
\CommentTok{#> class_7 =      97 / 372}
\CommentTok{#> Totals  = 1,425 / 9,932}
\CommentTok{#> }
\CommentTok{#> Hit Ratio Table: Extract with `h2o.hit_ratio_table(<model>,train = TRUE)`}
\CommentTok{#> =======================================================================}
\CommentTok{#> Top-7 Hit Ratios: }
\CommentTok{#>   k hit_ratio}
\CommentTok{#> 1 1  0.856524}
\CommentTok{#> 2 2  0.984595}
\CommentTok{#> 3 3  0.998188}
\CommentTok{#> 4 4  0.999899}
\CommentTok{#> 5 5  1.000000}
\CommentTok{#> 6 6  1.000000}
\CommentTok{#> 7 7  1.000000}
\CommentTok{#> }
\CommentTok{#> }
\CommentTok{#> H2OMultinomialMetrics: deeplearning}
\CommentTok{#> ** Reported on validation data. **}
\CommentTok{#> ** Metrics reported on temporary validation frame with 9913 samples **}
\CommentTok{#> }
\CommentTok{#> Validation Set Metrics: }
\CommentTok{#> =====================}
\CommentTok{#> }
\CommentTok{#> MSE: (Extract with `h2o.mse`) 0.11}
\CommentTok{#> RMSE: (Extract with `h2o.rmse`) 0.332}
\CommentTok{#> Logloss: (Extract with `h2o.logloss`) 0.376}
\CommentTok{#> Mean Per-Class Error: 0.23}
\CommentTok{#> Confusion Matrix: Extract with `h2o.confusionMatrix(<model>,valid = TRUE)`)}
\CommentTok{#> =========================================================================}
\CommentTok{#> Confusion Matrix: Row labels: Actual class; Column labels: Predicted class}
\CommentTok{#>         class_1 class_2 class_3 class_4 class_5 class_6 class_7  Error}
\CommentTok{#> class_1    3163     547       1       0       3       0      21 0.1531}
\CommentTok{#> class_2     390    4207      23       1      27      51       9 0.1064}
\CommentTok{#> class_3       0      27     521      16       0      55       0 0.1583}
\CommentTok{#> class_4       0       0       3      41       0       1       0 0.0889}
\CommentTok{#> class_5      10      62       2       0      65       4       0 0.5455}
\CommentTok{#> class_6       0      35      53       6       2     202       0 0.3221}
\CommentTok{#> class_7      80       5       0       0       0       0     280 0.2329}
\CommentTok{#> Totals     3643    4883     603      64      97     313     310 0.1447}
\CommentTok{#>                    Rate}
\CommentTok{#> class_1 =   572 / 3,735}
\CommentTok{#> class_2 =   501 / 4,708}
\CommentTok{#> class_3 =      98 / 619}
\CommentTok{#> class_4 =        4 / 45}
\CommentTok{#> class_5 =      78 / 143}
\CommentTok{#> class_6 =      96 / 298}
\CommentTok{#> class_7 =      85 / 365}
\CommentTok{#> Totals  = 1,434 / 9,913}
\CommentTok{#> }
\CommentTok{#> Hit Ratio Table: Extract with `h2o.hit_ratio_table(<model>,valid = TRUE)`}
\CommentTok{#> =======================================================================}
\CommentTok{#> Top-7 Hit Ratios: }
\CommentTok{#>   k hit_ratio}
\CommentTok{#> 1 1  0.855341}
\CommentTok{#> 2 2  0.985373}
\CommentTok{#> 3 3  0.998083}
\CommentTok{#> 4 4  0.999899}
\CommentTok{#> 5 5  1.000000}
\CommentTok{#> 6 6  1.000000}
\CommentTok{#> 7 7  1.000000}
\CommentTok{#> }
\CommentTok{#> }
\CommentTok{#> }
\CommentTok{#> }
\CommentTok{#> Scoring History: }
\CommentTok{#>              timestamp          duration training_speed   epochs}
\CommentTok{#> 1  2019-09-20 15:12:37         0.000 sec             NA  0.00000}
\CommentTok{#> 2  2019-09-20 15:12:38         1.133 sec  96270 obs/sec  0.28604}
\CommentTok{#> 3  2019-09-20 15:12:43         6.693 sec 106412 obs/sec  2.00010}
\CommentTok{#> 4  2019-09-20 15:12:48        11.965 sec 110052 obs/sec  3.71670}
\CommentTok{#> 5  2019-09-20 15:12:54        17.649 sec 114508 obs/sec  5.72221}
\CommentTok{#> 6  2019-09-20 15:13:00        23.318 sec 116879 obs/sec  7.72611}
\CommentTok{#> 7  2019-09-20 15:13:05        28.700 sec 119550 obs/sec  9.73526}
\CommentTok{#> 8  2019-09-20 15:13:11        34.181 sec 121006 obs/sec 11.74058}
\CommentTok{#> 9  2019-09-20 15:13:16        39.757 sec 121766 obs/sec 13.74575}
\CommentTok{#> 10 2019-09-20 15:13:22        45.124 sec 122907 obs/sec 15.75115}
\CommentTok{#> 11 2019-09-20 15:13:27        50.497 sec 123792 obs/sec 17.75799}
\CommentTok{#> 12 2019-09-20 15:13:32        55.882 sec 124486 obs/sec 19.76427}
\CommentTok{#> 13 2019-09-20 15:13:38  1 min  1.196 sec 125201 obs/sec 21.77015}
\CommentTok{#> 14 2019-09-20 15:13:38  1 min  1.235 sec 125197 obs/sec 21.77015}
\CommentTok{#>    iterations        samples training_rmse training_logloss training_r2}
\CommentTok{#> 1           0       0.000000            NA               NA          NA}
\CommentTok{#> 2           1   99833.000000       0.45275          0.62097     0.89680}
\CommentTok{#> 3           7  698066.000000       0.38388          0.47987     0.92581}
\CommentTok{#> 4          13 1297185.000000       0.36290          0.42781     0.93370}
\CommentTok{#> 5          20 1997138.000000       0.35440          0.41127     0.93677}
\CommentTok{#> 6          27 2696527.000000       0.35010          0.40644     0.93830}
\CommentTok{#> 7          34 3397751.000000       0.35273          0.40268     0.93736}
\CommentTok{#> 8          41 4097640.000000       0.34165          0.39531     0.94124}
\CommentTok{#> 9          48 4797472.000000       0.33552          0.38066     0.94333}
\CommentTok{#> 10         55 5497388.000000       0.34058          0.40336     0.94160}
\CommentTok{#> 11         62 6197805.000000       0.32917          0.36920     0.94545}
\CommentTok{#> 12         69 6898026.000000       0.33564          0.38261     0.94329}
\CommentTok{#> 13         76 7598110.000000       0.33031          0.37585     0.94507}
\CommentTok{#> 14         76 7598110.000000       0.32917          0.36920     0.94545}
\CommentTok{#>    training_classification_error validation_rmse validation_logloss}
\CommentTok{#> 1                             NA              NA                 NA}
\CommentTok{#> 2                        0.27789         0.45999            0.63511}
\CommentTok{#> 3                        0.19764         0.38998            0.48931}
\CommentTok{#> 4                        0.17600         0.36883            0.43908}
\CommentTok{#> 5                        0.16885         0.35687            0.41543}
\CommentTok{#> 6                        0.16452         0.35616            0.41901}
\CommentTok{#> 7                        0.16573         0.35551            0.40939}
\CommentTok{#> 8                        0.15586         0.34039            0.38953}
\CommentTok{#> 9                        0.15012         0.33897            0.38881}
\CommentTok{#> 10                       0.15415         0.34215            0.40425}
\CommentTok{#> 11                       0.14348         0.33233            0.37600}
\CommentTok{#> 12                       0.15143         0.33941            0.39483}
\CommentTok{#> 13                       0.14398         0.33497            0.38471}
\CommentTok{#> 14                       0.14348         0.33233            0.37600}
\CommentTok{#>    validation_r2 validation_classification_error}
\CommentTok{#> 1             NA                              NA}
\CommentTok{#> 2        0.89351                         0.28740}
\CommentTok{#> 3        0.92346                         0.20438}
\CommentTok{#> 4        0.93153                         0.18219}
\CommentTok{#> 5        0.93590                         0.16998}
\CommentTok{#> 6        0.93616                         0.17169}
\CommentTok{#> 7        0.93639                         0.16968}
\CommentTok{#> 8        0.94169                         0.15565}
\CommentTok{#> 9        0.94217                         0.15535}
\CommentTok{#> 10       0.94108                         0.15586}
\CommentTok{#> 11       0.94441                         0.14466}
\CommentTok{#> 12       0.94202                         0.15454}
\CommentTok{#> 13       0.94353                         0.14698}
\CommentTok{#> 14       0.94441                         0.14466}
\CommentTok{#> }
\CommentTok{#> Variable Importances: (Extract with `h2o.varimp`) }
\CommentTok{#> =================================================}
\CommentTok{#> }
\CommentTok{#> Variable Importances: }
\CommentTok{#>                          variable relative_importance scaled_importance}
\CommentTok{#> 1          Wilderness_Area.area_3            1.000000          1.000000}
\CommentTok{#> 2                       Elevation            0.966004          0.966004}
\CommentTok{#> 3 Horizontal_Distance_To_Roadways            0.965409          0.965409}
\CommentTok{#> 4          Wilderness_Area.area_1            0.915726          0.915726}
\CommentTok{#> 5          Wilderness_Area.area_2            0.845369          0.845369}
\CommentTok{#>   percentage}
\CommentTok{#> 1   0.032479}
\CommentTok{#> 2   0.031374}
\CommentTok{#> 3   0.031355}
\CommentTok{#> 4   0.029741}
\CommentTok{#> 5   0.027456}
\CommentTok{#> }
\CommentTok{#> ---}
\CommentTok{#>                          variable relative_importance scaled_importance}
\CommentTok{#> 51              Soil_Type.type_14            0.338649          0.338649}
\CommentTok{#> 52 Vertical_Distance_To_Hydrology            0.251830          0.251830}
\CommentTok{#> 53                          Slope            0.226702          0.226702}
\CommentTok{#> 54                         Aspect            0.045678          0.045678}
\CommentTok{#> 55          Soil_Type.missing(NA)            0.000000          0.000000}
\CommentTok{#> 56    Wilderness_Area.missing(NA)            0.000000          0.000000}
\CommentTok{#>    percentage}
\CommentTok{#> 51   0.010999}
\CommentTok{#> 52   0.008179}
\CommentTok{#> 53   0.007363}
\CommentTok{#> 54   0.001484}
\CommentTok{#> 55   0.000000}
\CommentTok{#> 56   0.000000}
\KeywordTok{plot}\NormalTok{(m2)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{nn-comparison-classification_regression_with_h2o_deep_learning_files/figure-latex/dl_ctype_early_stopping-1} \end{center}

\hypertarget{adaptive-learning-rate}{%
\subsection{Adaptive Learning Rate}\label{adaptive-learning-rate}}

By default, H2O Deep Learning uses an adaptive learning rate (ADADELTA) for its stochastic gradient descent optimization. There are only two tuning parameters for this method: rho and epsilon, which balance the global and local search efficiencies. rho is the similarity to prior weight updates (similar to momentum), and epsilon is a parameter that prevents the optimization to get stuck in local optima. Defaults are rho=0.99 and epsilon=1e-8. For cases where convergence speed is very important, it might make sense to perform a few runs to optimize these two parameters (e.g., with rho in c(0.9,0.95,0.99,0.999) and epsilon in c(1e-10,1e-8,1e-6,1e-4)). Of course, as always with grid searches, caution has to be applied when extrapolating grid search results to a different parameter regime (e.g., for more epochs or different layer topologies or activation functions, etc.).

If adaptive\_rate is disabled, several manual learning rate parameters become important: rate, rate\_annealing, rate\_decay, momentum\_start, momentum\_ramp, momentum\_stable and \texttt{nesterov\_accelerated\_gradient}, the discussion of which we leave to H2O Deep Learning booklet.

\hypertarget{tuning}{%
\subsection{Tuning}\label{tuning}}

With some tuning, it is possible to obtain less than 10\% test set error rate in about one minute. Error rates of below 5\% are possible with larger models. Note that deep tree methods can be more effective for this dataset than Deep Learning, as they directly partition the space into sectors, which seems to be needed here.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m3 <-}\StringTok{ }\KeywordTok{h2o.deeplearning}\NormalTok{(}
  \DataTypeTok{model_id=}\StringTok{"dl_model_tuned"}\NormalTok{, }
  \DataTypeTok{training_frame=}\NormalTok{train, }
  \DataTypeTok{validation_frame=}\NormalTok{valid, }
  \DataTypeTok{x=}\NormalTok{predictors, }
  \DataTypeTok{y=}\NormalTok{response, }
  \DataTypeTok{overwrite_with_best_model=}\NormalTok{F,    }\CommentTok{## Return final model after 10 epochs, even if not the best}
  \DataTypeTok{hidden=}\KeywordTok{c}\NormalTok{(}\DecValTok{128}\NormalTok{,}\DecValTok{128}\NormalTok{,}\DecValTok{128}\NormalTok{),          }\CommentTok{## more hidden layers -> more complex interactions}
  \DataTypeTok{epochs=}\DecValTok{10}\NormalTok{,                      }\CommentTok{## to keep it short enough}
  \DataTypeTok{score_validation_samples=}\DecValTok{10000}\NormalTok{, }\CommentTok{## downsample validation set for faster scoring}
  \DataTypeTok{score_duty_cycle=}\FloatTok{0.025}\NormalTok{,         }\CommentTok{## don't score more than 2.5% of the wall time}
  \DataTypeTok{adaptive_rate=}\NormalTok{F,                }\CommentTok{## manually tuned learning rate}
  \DataTypeTok{rate=}\FloatTok{0.01}\NormalTok{, }
  \DataTypeTok{rate_annealing=}\FloatTok{2e-6}\NormalTok{,            }
  \DataTypeTok{momentum_start=}\FloatTok{0.2}\NormalTok{,             }\CommentTok{## manually tuned momentum}
  \DataTypeTok{momentum_stable=}\FloatTok{0.4}\NormalTok{, }
  \DataTypeTok{momentum_ramp=}\FloatTok{1e7}\NormalTok{, }
  \DataTypeTok{l1=}\FloatTok{1e-5}\NormalTok{,                        }\CommentTok{## add some L1/L2 regularization}
  \DataTypeTok{l2=}\FloatTok{1e-5}\NormalTok{,}
  \DataTypeTok{max_w2=}\DecValTok{10}                       \CommentTok{## helps stability for Rectifier}
\NormalTok{) }
\CommentTok{#> }
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|}\StringTok{                                                                 }\ErrorTok{|}\StringTok{   }\DecValTok{0}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==}\StringTok{                                                               }\ErrorTok{|}\StringTok{   }\DecValTok{3}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|====}\StringTok{                                                             }\ErrorTok{|}\StringTok{   }\DecValTok{6}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|======}\StringTok{                                                           }\ErrorTok{|}\StringTok{   }\DecValTok{9}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=======}\StringTok{                                                          }\ErrorTok{|}\StringTok{  }\DecValTok{11}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=========}\StringTok{                                                        }\ErrorTok{|}\StringTok{  }\DecValTok{14}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|===========}\StringTok{                                                      }\ErrorTok{|}\StringTok{  }\DecValTok{17}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=============}\StringTok{                                                    }\ErrorTok{|}\StringTok{  }\DecValTok{20}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|===============}\StringTok{                                                  }\ErrorTok{|}\StringTok{  }\DecValTok{23}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=================}\StringTok{                                                }\ErrorTok{|}\StringTok{  }\DecValTok{26}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|===================}\StringTok{                                              }\ErrorTok{|}\StringTok{  }\DecValTok{29}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|====================}\StringTok{                                             }\ErrorTok{|}\StringTok{  }\DecValTok{32}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|======================}\StringTok{                                           }\ErrorTok{|}\StringTok{  }\DecValTok{34}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|========================}\StringTok{                                         }\ErrorTok{|}\StringTok{  }\DecValTok{37}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==========================}\StringTok{                                       }\ErrorTok{|}\StringTok{  }\DecValTok{40}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|============================}\StringTok{                                     }\ErrorTok{|}\StringTok{  }\DecValTok{43}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==============================}\StringTok{                                   }\ErrorTok{|}\StringTok{  }\DecValTok{46}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|================================}\StringTok{                                 }\ErrorTok{|}\StringTok{  }\DecValTok{49}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==================================}\StringTok{                               }\ErrorTok{|}\StringTok{  }\DecValTok{52}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|===================================}\StringTok{                              }\ErrorTok{|}\StringTok{  }\DecValTok{54}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=====================================}\StringTok{                            }\ErrorTok{|}\StringTok{  }\DecValTok{57}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=======================================}\StringTok{                          }\ErrorTok{|}\StringTok{  }\DecValTok{60}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=========================================}\StringTok{                        }\ErrorTok{|}\StringTok{  }\DecValTok{63}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|===========================================}\StringTok{                      }\ErrorTok{|}\StringTok{  }\DecValTok{66}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=============================================}\StringTok{                    }\ErrorTok{|}\StringTok{  }\DecValTok{69}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|===============================================}\StringTok{                  }\ErrorTok{|}\StringTok{  }\DecValTok{72}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|================================================}\StringTok{                 }\ErrorTok{|}\StringTok{  }\DecValTok{74}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==================================================}\StringTok{               }\ErrorTok{|}\StringTok{  }\DecValTok{77}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|====================================================}\StringTok{             }\ErrorTok{|}\StringTok{  }\DecValTok{80}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|======================================================}\StringTok{           }\ErrorTok{|}\StringTok{  }\DecValTok{83}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|========================================================}\StringTok{         }\ErrorTok{|}\StringTok{  }\DecValTok{86}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==========================================================}\StringTok{       }\ErrorTok{|}\StringTok{  }\DecValTok{89}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|============================================================}\StringTok{     }\ErrorTok{|}\StringTok{  }\DecValTok{92}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=============================================================}\StringTok{    }\ErrorTok{|}\StringTok{  }\DecValTok{95}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|===============================================================}\StringTok{  }\ErrorTok{|}\StringTok{  }\DecValTok{97}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=================================================================|}\StringTok{ }\DecValTok{100}\NormalTok{%}
\KeywordTok{summary}\NormalTok{(m3)}
\CommentTok{#> Model Details:}
\CommentTok{#> ==============}
\CommentTok{#> }
\CommentTok{#> H2OMultinomialModel: deeplearning}
\CommentTok{#> Model Key:  dl_model_tuned }
\CommentTok{#> Status of Neuron Layers: predicting Cover_Type, 7-class classification, multinomial distribution, CrossEntropy loss, 41,223 weights/biases, 334.1 KB, 3,501,792 training samples, mini-batch size 1}
\CommentTok{#>   layer units      type dropout       l1       l2 mean_rate rate_rms}
\CommentTok{#> 1     1    56     Input  0.00 %       NA       NA        NA       NA}
\CommentTok{#> 2     2   128 Rectifier  0.00 % 0.000010 0.000010  0.001249 0.000000}
\CommentTok{#> 3     3   128 Rectifier  0.00 % 0.000010 0.000010  0.001249 0.000000}
\CommentTok{#> 4     4   128 Rectifier  0.00 % 0.000010 0.000010  0.001249 0.000000}
\CommentTok{#> 5     5     7   Softmax      NA 0.000010 0.000010  0.001249 0.000000}
\CommentTok{#>   momentum mean_weight weight_rms mean_bias bias_rms}
\CommentTok{#> 1       NA          NA         NA        NA       NA}
\CommentTok{#> 2 0.270036   -0.011629   0.315012 -0.006181 0.328483}
\CommentTok{#> 3 0.270036   -0.058545   0.223404  0.857652 0.355832}
\CommentTok{#> 4 0.270036   -0.060407   0.215493  0.776759 0.199413}
\CommentTok{#> 5 0.270036   -0.026817   0.270108  0.041117 0.939719}
\CommentTok{#> }
\CommentTok{#> H2OMultinomialMetrics: deeplearning}
\CommentTok{#> ** Reported on training data. **}
\CommentTok{#> ** Metrics reported on temporary training frame with 9942 samples **}
\CommentTok{#> }
\CommentTok{#> Training Set Metrics: }
\CommentTok{#> =====================}
\CommentTok{#> }
\CommentTok{#> MSE: (Extract with `h2o.mse`) 0.0542}
\CommentTok{#> RMSE: (Extract with `h2o.rmse`) 0.233}
\CommentTok{#> Logloss: (Extract with `h2o.logloss`) 0.178}
\CommentTok{#> Mean Per-Class Error: 0.109}
\CommentTok{#> Confusion Matrix: Extract with `h2o.confusionMatrix(<model>,train = TRUE)`)}
\CommentTok{#> =========================================================================}
\CommentTok{#> Confusion Matrix: Row labels: Actual class; Column labels: Predicted class}
\CommentTok{#>         class_1 class_2 class_3 class_4 class_5 class_6 class_7  Error}
\CommentTok{#> class_1    3354     329       0       0       4       0      20 0.0952}
\CommentTok{#> class_2     152    4547      12       1      25       6       0 0.0413}
\CommentTok{#> class_3       0      14     564      11       0      31       0 0.0903}
\CommentTok{#> class_4       0       0       5      45       0       0       0 0.1000}
\CommentTok{#> class_5       3      23       2       0     140       0       0 0.1667}
\CommentTok{#> class_6       2      19      37       3       0     218       0 0.2186}
\CommentTok{#> class_7      18       2       0       0       0       0     355 0.0533}
\CommentTok{#> Totals     3529    4934     620      60     169     255     375 0.0723}
\CommentTok{#>                  Rate}
\CommentTok{#> class_1 = 353 / 3,707}
\CommentTok{#> class_2 = 196 / 4,743}
\CommentTok{#> class_3 =    56 / 620}
\CommentTok{#> class_4 =      5 / 50}
\CommentTok{#> class_5 =    28 / 168}
\CommentTok{#> class_6 =    61 / 279}
\CommentTok{#> class_7 =    20 / 375}
\CommentTok{#> Totals  = 719 / 9,942}
\CommentTok{#> }
\CommentTok{#> Hit Ratio Table: Extract with `h2o.hit_ratio_table(<model>,train = TRUE)`}
\CommentTok{#> =======================================================================}
\CommentTok{#> Top-7 Hit Ratios: }
\CommentTok{#>   k hit_ratio}
\CommentTok{#> 1 1  0.927681}
\CommentTok{#> 2 2  0.996982}
\CommentTok{#> 3 3  0.999698}
\CommentTok{#> 4 4  1.000000}
\CommentTok{#> 5 5  1.000000}
\CommentTok{#> 6 6  1.000000}
\CommentTok{#> 7 7  1.000000}
\CommentTok{#> }
\CommentTok{#> }
\CommentTok{#> H2OMultinomialMetrics: deeplearning}
\CommentTok{#> ** Reported on validation data. **}
\CommentTok{#> ** Metrics reported on temporary validation frame with 9973 samples **}
\CommentTok{#> }
\CommentTok{#> Validation Set Metrics: }
\CommentTok{#> =====================}
\CommentTok{#> }
\CommentTok{#> MSE: (Extract with `h2o.mse`) 0.0638}
\CommentTok{#> RMSE: (Extract with `h2o.rmse`) 0.253}
\CommentTok{#> Logloss: (Extract with `h2o.logloss`) 0.213}
\CommentTok{#> Mean Per-Class Error: 0.138}
\CommentTok{#> Confusion Matrix: Extract with `h2o.confusionMatrix(<model>,valid = TRUE)`)}
\CommentTok{#> =========================================================================}
\CommentTok{#> Confusion Matrix: Row labels: Actual class; Column labels: Predicted class}
\CommentTok{#>         class_1 class_2 class_3 class_4 class_5 class_6 class_7  Error}
\CommentTok{#> class_1    3275     387       0       0       1       1      15 0.1098}
\CommentTok{#> class_2     193    4571      16       0      35      11       2 0.0532}
\CommentTok{#> class_3       0      18     544       6       2      35       0 0.1008}
\CommentTok{#> class_4       0       0       9      33       0       3       0 0.2667}
\CommentTok{#> class_5       3      27       4       0     133       0       0 0.2036}
\CommentTok{#> class_6       2      20      27       4       0     253       0 0.1732}
\CommentTok{#> class_7      18       3       0       0       0       0     322 0.0612}
\CommentTok{#> Totals     3491    5026     600      43     171     303     339 0.0844}
\CommentTok{#>                  Rate}
\CommentTok{#> class_1 = 404 / 3,679}
\CommentTok{#> class_2 = 257 / 4,828}
\CommentTok{#> class_3 =    61 / 605}
\CommentTok{#> class_4 =     12 / 45}
\CommentTok{#> class_5 =    34 / 167}
\CommentTok{#> class_6 =    53 / 306}
\CommentTok{#> class_7 =    21 / 343}
\CommentTok{#> Totals  = 842 / 9,973}
\CommentTok{#> }
\CommentTok{#> Hit Ratio Table: Extract with `h2o.hit_ratio_table(<model>,valid = TRUE)`}
\CommentTok{#> =======================================================================}
\CommentTok{#> Top-7 Hit Ratios: }
\CommentTok{#>   k hit_ratio}
\CommentTok{#> 1 1  0.915572}
\CommentTok{#> 2 2  0.995588}
\CommentTok{#> 3 3  0.999799}
\CommentTok{#> 4 4  1.000000}
\CommentTok{#> 5 5  1.000000}
\CommentTok{#> 6 6  1.000000}
\CommentTok{#> 7 7  1.000000}
\CommentTok{#> }
\CommentTok{#> }
\CommentTok{#> }
\CommentTok{#> }
\CommentTok{#> Scoring History: }
\CommentTok{#>              timestamp          duration training_speed   epochs}
\CommentTok{#> 1  2019-09-20 15:13:38         0.000 sec             NA  0.00000}
\CommentTok{#> 2  2019-09-20 15:13:44         5.558 sec  18903 obs/sec  0.28560}
\CommentTok{#> 3  2019-09-20 15:13:55        16.864 sec  24391 obs/sec  1.14643}
\CommentTok{#> 4  2019-09-20 15:14:05        26.830 sec  26703 obs/sec  2.00354}
\CommentTok{#> 5  2019-09-20 15:14:15        36.575 sec  27964 obs/sec  2.86512}
\CommentTok{#> 6  2019-09-20 15:14:24        46.047 sec  28855 obs/sec  3.72548}
\CommentTok{#> 7  2019-09-20 15:14:32        54.163 sec  28317 obs/sec  4.30076}
\CommentTok{#> 8  2019-09-20 15:14:42  1 min  3.539 sec  28966 obs/sec  5.16261}
\CommentTok{#> 9  2019-09-20 15:14:51  1 min 13.283 sec  29289 obs/sec  6.02348}
\CommentTok{#> 10 2019-09-20 15:15:01  1 min 23.203 sec  29472 obs/sec  6.88366}
\CommentTok{#> 11 2019-09-20 15:15:11  1 min 32.510 sec  29801 obs/sec  7.74054}
\CommentTok{#> 12 2019-09-20 15:15:20  1 min 41.827 sec  30082 obs/sec  8.60118}
\CommentTok{#> 13 2019-09-20 15:15:29  1 min 51.334 sec  30256 obs/sec  9.46029}
\CommentTok{#> 14 2019-09-20 15:15:36  1 min 57.593 sec  30395 obs/sec 10.03336}
\CommentTok{#>    iterations        samples training_rmse training_logloss training_r2}
\CommentTok{#> 1           0       0.000000            NA               NA          NA}
\CommentTok{#> 2           1   99677.000000       0.42954          0.57028     0.90760}
\CommentTok{#> 3           4  400123.000000       0.35428          0.39831     0.93714}
\CommentTok{#> 4           7  699264.000000       0.32503          0.33535     0.94709}
\CommentTok{#> 5          10  999969.000000       0.30876          0.30346     0.95226}
\CommentTok{#> 6          13 1300249.000000       0.28809          0.26678     0.95844}
\CommentTok{#> 7          15 1501031.000000       0.28144          0.25617     0.96033}
\CommentTok{#> 8          18 1801828.000000       0.26740          0.23221     0.96419}
\CommentTok{#> 9          21 2102286.000000       0.26200          0.22176     0.96562}
\CommentTok{#> 10         24 2402502.000000       0.25772          0.21608     0.96674}
\CommentTok{#> 11         27 2701563.000000       0.25290          0.20629     0.96797}
\CommentTok{#> 12         30 3001941.000000       0.24526          0.19643     0.96988}
\CommentTok{#> 13         33 3301782.000000       0.23535          0.18278     0.97226}
\CommentTok{#> 14         35 3501792.000000       0.23277          0.17826     0.97286}
\CommentTok{#>    training_classification_error validation_rmse validation_logloss}
\CommentTok{#> 1                             NA              NA                 NA}
\CommentTok{#> 2                        0.24814         0.43012            0.57466}
\CommentTok{#> 3                        0.17029         0.36301            0.41871}
\CommentTok{#> 4                        0.14243         0.33326            0.35389}
\CommentTok{#> 5                        0.12945         0.31775            0.32413}
\CommentTok{#> 6                        0.11275         0.29831            0.28753}
\CommentTok{#> 7                        0.10682         0.29290            0.27746}
\CommentTok{#> 8                        0.09747         0.28134            0.25833}
\CommentTok{#> 9                        0.09153         0.27138            0.24210}
\CommentTok{#> 10                       0.09053         0.26804            0.23534}
\CommentTok{#> 11                       0.08892         0.26551            0.23446}
\CommentTok{#> 12                       0.08389         0.25881            0.22341}
\CommentTok{#> 13                       0.07594         0.25030            0.20947}
\CommentTok{#> 14                       0.07232         0.25255            0.21251}
\CommentTok{#>    validation_r2 validation_classification_error}
\CommentTok{#> 1             NA                              NA}
\CommentTok{#> 2        0.90498                         0.24647}
\CommentTok{#> 3        0.93232                         0.17758}
\CommentTok{#> 4        0.94296                         0.15061}
\CommentTok{#> 5        0.94814                         0.13787}
\CommentTok{#> 6        0.95429                         0.11822}
\CommentTok{#> 7        0.95594                         0.11822}
\CommentTok{#> 8        0.95935                         0.10809}
\CommentTok{#> 9        0.96217                         0.09877}
\CommentTok{#> 10       0.96310                         0.09646}
\CommentTok{#> 11       0.96379                         0.09365}
\CommentTok{#> 12       0.96560                         0.09044}
\CommentTok{#> 13       0.96782                         0.08202}
\CommentTok{#> 14       0.96724                         0.08443}
\CommentTok{#> }
\CommentTok{#> Variable Importances: (Extract with `h2o.varimp`) }
\CommentTok{#> =================================================}
\CommentTok{#> }
\CommentTok{#> Variable Importances: }
\CommentTok{#>                             variable relative_importance scaled_importance}
\CommentTok{#> 1                          Elevation            1.000000          1.000000}
\CommentTok{#> 2    Horizontal_Distance_To_Roadways            0.937629          0.937629}
\CommentTok{#> 3 Horizontal_Distance_To_Fire_Points            0.926569          0.926569}
\CommentTok{#> 4             Wilderness_Area.area_0            0.665309          0.665309}
\CommentTok{#> 5             Wilderness_Area.area_2            0.605383          0.605383}
\CommentTok{#>   percentage}
\CommentTok{#> 1   0.047802}
\CommentTok{#> 2   0.044821}
\CommentTok{#> 3   0.044292}
\CommentTok{#> 4   0.031803}
\CommentTok{#> 5   0.028939}
\CommentTok{#> }
\CommentTok{#> ---}
\CommentTok{#>                       variable relative_importance scaled_importance}
\CommentTok{#> 51           Soil_Type.type_13            0.165890          0.165890}
\CommentTok{#> 52           Soil_Type.type_14            0.155661          0.155661}
\CommentTok{#> 53            Soil_Type.type_6            0.155113          0.155113}
\CommentTok{#> 54           Soil_Type.type_35            0.152966          0.152966}
\CommentTok{#> 55       Soil_Type.missing(NA)            0.000000          0.000000}
\CommentTok{#> 56 Wilderness_Area.missing(NA)            0.000000          0.000000}
\CommentTok{#>    percentage}
\CommentTok{#> 51   0.007930}
\CommentTok{#> 52   0.007441}
\CommentTok{#> 53   0.007415}
\CommentTok{#> 54   0.007312}
\CommentTok{#> 55   0.000000}
\CommentTok{#> 56   0.000000}
\end{Highlighting}
\end{Shaded}

Let's compare the training error with the validation and test set errors

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{h2o.performance}\NormalTok{(m3, }\DataTypeTok{train=}\NormalTok{T)          }\CommentTok{## sampled training data (from model building)}
\CommentTok{#> H2OMultinomialMetrics: deeplearning}
\CommentTok{#> ** Reported on training data. **}
\CommentTok{#> ** Metrics reported on temporary training frame with 9942 samples **}
\CommentTok{#> }
\CommentTok{#> Training Set Metrics: }
\CommentTok{#> =====================}
\CommentTok{#> }
\CommentTok{#> MSE: (Extract with `h2o.mse`) 0.0542}
\CommentTok{#> RMSE: (Extract with `h2o.rmse`) 0.233}
\CommentTok{#> Logloss: (Extract with `h2o.logloss`) 0.178}
\CommentTok{#> Mean Per-Class Error: 0.109}
\CommentTok{#> Confusion Matrix: Extract with `h2o.confusionMatrix(<model>,train = TRUE)`)}
\CommentTok{#> =========================================================================}
\CommentTok{#> Confusion Matrix: Row labels: Actual class; Column labels: Predicted class}
\CommentTok{#>         class_1 class_2 class_3 class_4 class_5 class_6 class_7  Error}
\CommentTok{#> class_1    3354     329       0       0       4       0      20 0.0952}
\CommentTok{#> class_2     152    4547      12       1      25       6       0 0.0413}
\CommentTok{#> class_3       0      14     564      11       0      31       0 0.0903}
\CommentTok{#> class_4       0       0       5      45       0       0       0 0.1000}
\CommentTok{#> class_5       3      23       2       0     140       0       0 0.1667}
\CommentTok{#> class_6       2      19      37       3       0     218       0 0.2186}
\CommentTok{#> class_7      18       2       0       0       0       0     355 0.0533}
\CommentTok{#> Totals     3529    4934     620      60     169     255     375 0.0723}
\CommentTok{#>                  Rate}
\CommentTok{#> class_1 = 353 / 3,707}
\CommentTok{#> class_2 = 196 / 4,743}
\CommentTok{#> class_3 =    56 / 620}
\CommentTok{#> class_4 =      5 / 50}
\CommentTok{#> class_5 =    28 / 168}
\CommentTok{#> class_6 =    61 / 279}
\CommentTok{#> class_7 =    20 / 375}
\CommentTok{#> Totals  = 719 / 9,942}
\CommentTok{#> }
\CommentTok{#> Hit Ratio Table: Extract with `h2o.hit_ratio_table(<model>,train = TRUE)`}
\CommentTok{#> =======================================================================}
\CommentTok{#> Top-7 Hit Ratios: }
\CommentTok{#>   k hit_ratio}
\CommentTok{#> 1 1  0.927681}
\CommentTok{#> 2 2  0.996982}
\CommentTok{#> 3 3  0.999698}
\CommentTok{#> 4 4  1.000000}
\CommentTok{#> 5 5  1.000000}
\CommentTok{#> 6 6  1.000000}
\CommentTok{#> 7 7  1.000000}
\KeywordTok{h2o.performance}\NormalTok{(m3, }\DataTypeTok{valid=}\NormalTok{T)          }\CommentTok{## sampled validation data (from model building)}
\CommentTok{#> H2OMultinomialMetrics: deeplearning}
\CommentTok{#> ** Reported on validation data. **}
\CommentTok{#> ** Metrics reported on temporary validation frame with 9973 samples **}
\CommentTok{#> }
\CommentTok{#> Validation Set Metrics: }
\CommentTok{#> =====================}
\CommentTok{#> }
\CommentTok{#> MSE: (Extract with `h2o.mse`) 0.0638}
\CommentTok{#> RMSE: (Extract with `h2o.rmse`) 0.253}
\CommentTok{#> Logloss: (Extract with `h2o.logloss`) 0.213}
\CommentTok{#> Mean Per-Class Error: 0.138}
\CommentTok{#> Confusion Matrix: Extract with `h2o.confusionMatrix(<model>,valid = TRUE)`)}
\CommentTok{#> =========================================================================}
\CommentTok{#> Confusion Matrix: Row labels: Actual class; Column labels: Predicted class}
\CommentTok{#>         class_1 class_2 class_3 class_4 class_5 class_6 class_7  Error}
\CommentTok{#> class_1    3275     387       0       0       1       1      15 0.1098}
\CommentTok{#> class_2     193    4571      16       0      35      11       2 0.0532}
\CommentTok{#> class_3       0      18     544       6       2      35       0 0.1008}
\CommentTok{#> class_4       0       0       9      33       0       3       0 0.2667}
\CommentTok{#> class_5       3      27       4       0     133       0       0 0.2036}
\CommentTok{#> class_6       2      20      27       4       0     253       0 0.1732}
\CommentTok{#> class_7      18       3       0       0       0       0     322 0.0612}
\CommentTok{#> Totals     3491    5026     600      43     171     303     339 0.0844}
\CommentTok{#>                  Rate}
\CommentTok{#> class_1 = 404 / 3,679}
\CommentTok{#> class_2 = 257 / 4,828}
\CommentTok{#> class_3 =    61 / 605}
\CommentTok{#> class_4 =     12 / 45}
\CommentTok{#> class_5 =    34 / 167}
\CommentTok{#> class_6 =    53 / 306}
\CommentTok{#> class_7 =    21 / 343}
\CommentTok{#> Totals  = 842 / 9,973}
\CommentTok{#> }
\CommentTok{#> Hit Ratio Table: Extract with `h2o.hit_ratio_table(<model>,valid = TRUE)`}
\CommentTok{#> =======================================================================}
\CommentTok{#> Top-7 Hit Ratios: }
\CommentTok{#>   k hit_ratio}
\CommentTok{#> 1 1  0.915572}
\CommentTok{#> 2 2  0.995588}
\CommentTok{#> 3 3  0.999799}
\CommentTok{#> 4 4  1.000000}
\CommentTok{#> 5 5  1.000000}
\CommentTok{#> 6 6  1.000000}
\CommentTok{#> 7 7  1.000000}
\KeywordTok{h2o.performance}\NormalTok{(m3, }\DataTypeTok{newdata=}\NormalTok{train)    }\CommentTok{## full training data}
\CommentTok{#> H2OMultinomialMetrics: deeplearning}
\CommentTok{#> }
\CommentTok{#> Test Set Metrics: }
\CommentTok{#> =====================}
\CommentTok{#> }
\CommentTok{#> MSE: (Extract with `h2o.mse`) 0.0554}
\CommentTok{#> RMSE: (Extract with `h2o.rmse`) 0.235}
\CommentTok{#> Logloss: (Extract with `h2o.logloss`) 0.183}
\CommentTok{#> Mean Per-Class Error: 0.12}
\CommentTok{#> Confusion Matrix: Extract with `h2o.confusionMatrix(<model>, <data>)`)}
\CommentTok{#> =========================================================================}
\CommentTok{#> Confusion Matrix: Row labels: Actual class; Column labels: Predicted class}
\CommentTok{#>         class_1 class_2 class_3 class_4 class_5 class_6 class_7  Error}
\CommentTok{#> class_1  115214   11181       0       0      84      14     627 0.0937}
\CommentTok{#> class_2    6086  162617     512      10     886     193      38 0.0453}
\CommentTok{#> class_3      10     428   19568     224      34    1178       0 0.0874}
\CommentTok{#> class_4       0       0     183    1390       0      85       0 0.1616}
\CommentTok{#> class_5     105     871      84       0    4651       9       0 0.1869}
\CommentTok{#> class_6      37     721    1222      68       9    8376       0 0.1972}
\CommentTok{#> class_7     675     150       0       0       3       0   11472 0.0673}
\CommentTok{#> Totals   122127  175968   21569    1692    5667    9855   12137 0.0737}
\CommentTok{#>                       Rate}
\CommentTok{#> class_1 = 11,906 / 127,120}
\CommentTok{#> class_2 =  7,725 / 170,342}
\CommentTok{#> class_3 =   1,874 / 21,442}
\CommentTok{#> class_4 =      268 / 1,658}
\CommentTok{#> class_5 =    1,069 / 5,720}
\CommentTok{#> class_6 =   2,057 / 10,433}
\CommentTok{#> class_7 =     828 / 12,300}
\CommentTok{#> Totals  = 25,727 / 349,015}
\CommentTok{#> }
\CommentTok{#> Hit Ratio Table: Extract with `h2o.hit_ratio_table(<model>, <data>)`}
\CommentTok{#> =======================================================================}
\CommentTok{#> Top-7 Hit Ratios: }
\CommentTok{#>   k hit_ratio}
\CommentTok{#> 1 1  0.926287}
\CommentTok{#> 2 2  0.996582}
\CommentTok{#> 3 3  0.999736}
\CommentTok{#> 4 4  0.999983}
\CommentTok{#> 5 5  1.000000}
\CommentTok{#> 6 6  1.000000}
\CommentTok{#> 7 7  1.000000}
\KeywordTok{h2o.performance}\NormalTok{(m3, }\DataTypeTok{newdata=}\NormalTok{valid)    }\CommentTok{## full validation data}
\CommentTok{#> H2OMultinomialMetrics: deeplearning}
\CommentTok{#> }
\CommentTok{#> Test Set Metrics: }
\CommentTok{#> =====================}
\CommentTok{#> }
\CommentTok{#> MSE: (Extract with `h2o.mse`) 0.0618}
\CommentTok{#> RMSE: (Extract with `h2o.rmse`) 0.249}
\CommentTok{#> Logloss: (Extract with `h2o.logloss`) 0.205}
\CommentTok{#> Mean Per-Class Error: 0.134}
\CommentTok{#> Confusion Matrix: Extract with `h2o.confusionMatrix(<model>, <data>)`)}
\CommentTok{#> =========================================================================}
\CommentTok{#> Confusion Matrix: Row labels: Actual class; Column labels: Predicted class}
\CommentTok{#>         class_1 class_2 class_3 class_4 class_5 class_6 class_7  Error}
\CommentTok{#> class_1   38194    4033       1       0      33       5     234 0.1013}
\CommentTok{#> class_2    2277   53466     164       4     368      85      16 0.0517}
\CommentTok{#> class_3       3     170    6463      84      13     410       0 0.0952}
\CommentTok{#> class_4       0       0      74     451       0      37       0 0.1975}
\CommentTok{#> class_5      35     300      43       0    1487       5       0 0.2048}
\CommentTok{#> class_6      12     302     412      22       6    2710       0 0.2177}
\CommentTok{#> class_7     249      50       0       0       1       0    3799 0.0732}
\CommentTok{#> Totals    40770   58321    7157     561    1908    3252    4049 0.0814}
\CommentTok{#>                      Rate}
\CommentTok{#> class_1 =  4,306 / 42,500}
\CommentTok{#> class_2 =  2,914 / 56,380}
\CommentTok{#> class_3 =     680 / 7,143}
\CommentTok{#> class_4 =       111 / 562}
\CommentTok{#> class_5 =     383 / 1,870}
\CommentTok{#> class_6 =     754 / 3,464}
\CommentTok{#> class_7 =     300 / 4,099}
\CommentTok{#> Totals  = 9,448 / 116,018}
\CommentTok{#> }
\CommentTok{#> Hit Ratio Table: Extract with `h2o.hit_ratio_table(<model>, <data>)`}
\CommentTok{#> =======================================================================}
\CommentTok{#> Top-7 Hit Ratios: }
\CommentTok{#>   k hit_ratio}
\CommentTok{#> 1 1  0.918564}
\CommentTok{#> 2 2  0.995656}
\CommentTok{#> 3 3  0.999664}
\CommentTok{#> 4 4  0.999957}
\CommentTok{#> 5 5  1.000000}
\CommentTok{#> 6 6  1.000000}
\CommentTok{#> 7 7  1.000000}
\KeywordTok{h2o.performance}\NormalTok{(m3, }\DataTypeTok{newdata=}\NormalTok{test)     }\CommentTok{## full test data}
\CommentTok{#> H2OMultinomialMetrics: deeplearning}
\CommentTok{#> }
\CommentTok{#> Test Set Metrics: }
\CommentTok{#> =====================}
\CommentTok{#> }
\CommentTok{#> MSE: (Extract with `h2o.mse`) 0.0614}
\CommentTok{#> RMSE: (Extract with `h2o.rmse`) 0.248}
\CommentTok{#> Logloss: (Extract with `h2o.logloss`) 0.204}
\CommentTok{#> Mean Per-Class Error: 0.134}
\CommentTok{#> Confusion Matrix: Extract with `h2o.confusionMatrix(<model>, <data>)`)}
\CommentTok{#> =========================================================================}
\CommentTok{#> Confusion Matrix: Row labels: Actual class; Column labels: Predicted class}
\CommentTok{#>         class_1 class_2 class_3 class_4 class_5 class_6 class_7  Error}
\CommentTok{#> class_1   37965    3974       0       0      35       1     245 0.1008}
\CommentTok{#> class_2    2257   53674     191       3     332      91      31 0.0513}
\CommentTok{#> class_3       2     170    6498      92       9     398       0 0.0936}
\CommentTok{#> class_4       0       0      70     429       0      28       0 0.1860}
\CommentTok{#> class_5      31     317      32       0    1513      10       0 0.2049}
\CommentTok{#> class_6      13     271     425      29       8    2724       0 0.2150}
\CommentTok{#> class_7     299      49       0       0       1       0    3762 0.0849}
\CommentTok{#> Totals    40567   58455    7216     553    1898    3252    4038 0.0812}
\CommentTok{#>                      Rate}
\CommentTok{#> class_1 =  4,255 / 42,220}
\CommentTok{#> class_2 =  2,905 / 56,579}
\CommentTok{#> class_3 =     671 / 7,169}
\CommentTok{#> class_4 =        98 / 527}
\CommentTok{#> class_5 =     390 / 1,903}
\CommentTok{#> class_6 =     746 / 3,470}
\CommentTok{#> class_7 =     349 / 4,111}
\CommentTok{#> Totals  = 9,414 / 115,979}
\CommentTok{#> }
\CommentTok{#> Hit Ratio Table: Extract with `h2o.hit_ratio_table(<model>, <data>)`}
\CommentTok{#> =======================================================================}
\CommentTok{#> Top-7 Hit Ratios: }
\CommentTok{#>   k hit_ratio}
\CommentTok{#> 1 1  0.918830}
\CommentTok{#> 2 2  0.995525}
\CommentTok{#> 3 3  0.999595}
\CommentTok{#> 4 4  0.999948}
\CommentTok{#> 5 5  1.000000}
\CommentTok{#> 6 6  1.000000}
\CommentTok{#> 7 7  1.000000}
\end{Highlighting}
\end{Shaded}

To confirm that the reported confusion matrix on the validation set (here, the test set) was correct, we make a prediction on the test set and compare the confusion matrices explicitly:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pred <-}\StringTok{ }\KeywordTok{h2o.predict}\NormalTok{(m3, test)}
\CommentTok{#> }
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|}\StringTok{                                                                 }\ErrorTok{|}\StringTok{   }\DecValTok{0}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=================================================================|}\StringTok{ }\DecValTok{100}\NormalTok{%}
\NormalTok{pred}
\CommentTok{#>   predict  class_1  class_2  class_3  class_4  class_5  class_6  class_7}
\CommentTok{#> 1 class_2 1.32e-01 0.868377 3.06e-05 1.13e-06 5.96e-06 2.03e-05 1.49e-05}
\CommentTok{#> 2 class_1 1.00e+00 0.000337 4.50e-07 7.93e-07 1.06e-07 7.64e-07 2.45e-06}
\CommentTok{#> 3 class_1 9.98e-01 0.001618 6.70e-09 8.48e-10 1.18e-10 4.16e-09 2.14e-05}
\CommentTok{#> 4 class_1 9.98e-01 0.002007 1.27e-06 6.85e-07 3.29e-07 2.08e-07 4.33e-06}
\CommentTok{#> 5 class_2 7.74e-02 0.920466 6.87e-06 8.27e-06 2.10e-03 1.03e-05 1.60e-07}
\CommentTok{#> 6 class_5 2.79e-05 0.239081 1.37e-07 5.33e-09 7.61e-01 2.93e-06 3.28e-10}
\CommentTok{#> }
\CommentTok{#> [115979 rows x 8 columns]}
\NormalTok{test}\OperatorTok{$}\NormalTok{Accuracy <-}\StringTok{ }\NormalTok{pred}\OperatorTok{$}\NormalTok{predict }\OperatorTok{==}\StringTok{ }\NormalTok{test}\OperatorTok{$}\NormalTok{Cover_Type}
\DecValTok{1}\OperatorTok{-}\KeywordTok{mean}\NormalTok{(test}\OperatorTok{$}\NormalTok{Accuracy)}
\CommentTok{#> [1] 0.0812}
\end{Highlighting}
\end{Shaded}

\hypertarget{hyper-parameter-tuning-with-grid-search}{%
\subsection{Hyper-parameter Tuning with Grid Search}\label{hyper-parameter-tuning-with-grid-search}}

Since there are a lot of parameters that can impact model accuracy, hyper-parameter tuning is especially important for Deep Learning:

For speed, we will only train on the first 10,000 rows of the training dataset:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sampled_train=train[}\DecValTok{1}\OperatorTok{:}\DecValTok{10000}\NormalTok{,]}
\end{Highlighting}
\end{Shaded}

The simplest hyperparameter search method is a brute-force scan of the full Cartesian product of all combinations specified by a grid search:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{hyper_params <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}
  \DataTypeTok{hidden=}\KeywordTok{list}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{32}\NormalTok{,}\DecValTok{32}\NormalTok{,}\DecValTok{32}\NormalTok{),}\KeywordTok{c}\NormalTok{(}\DecValTok{64}\NormalTok{,}\DecValTok{64}\NormalTok{)),}
  \DataTypeTok{input_dropout_ratio=}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\FloatTok{0.05}\NormalTok{),}
  \DataTypeTok{rate=}\KeywordTok{c}\NormalTok{(}\FloatTok{0.01}\NormalTok{,}\FloatTok{0.02}\NormalTok{),}
  \DataTypeTok{rate_annealing=}\KeywordTok{c}\NormalTok{(}\FloatTok{1e-8}\NormalTok{,}\FloatTok{1e-7}\NormalTok{,}\FloatTok{1e-6}\NormalTok{)}
\NormalTok{)}
\NormalTok{hyper_params}
\CommentTok{#> $hidden}
\CommentTok{#> $hidden[[1]]}
\CommentTok{#> [1] 32 32 32}
\CommentTok{#> }
\CommentTok{#> $hidden[[2]]}
\CommentTok{#> [1] 64 64}
\CommentTok{#> }
\CommentTok{#> }
\CommentTok{#> $input_dropout_ratio}
\CommentTok{#> [1] 0.00 0.05}
\CommentTok{#> }
\CommentTok{#> $rate}
\CommentTok{#> [1] 0.01 0.02}
\CommentTok{#> }
\CommentTok{#> $rate_annealing}
\CommentTok{#> [1] 1e-08 1e-07 1e-06}
\NormalTok{grid <-}\StringTok{ }\KeywordTok{h2o.grid}\NormalTok{(}
  \DataTypeTok{algorithm=}\StringTok{"deeplearning"}\NormalTok{,}
  \DataTypeTok{grid_id=}\StringTok{"dl_grid"}\NormalTok{, }
  \DataTypeTok{training_frame=}\NormalTok{sampled_train,}
  \DataTypeTok{validation_frame=}\NormalTok{valid, }
  \DataTypeTok{x=}\NormalTok{predictors, }
  \DataTypeTok{y=}\NormalTok{response,}
  \DataTypeTok{epochs=}\DecValTok{10}\NormalTok{,}
  \DataTypeTok{stopping_metric=}\StringTok{"misclassification"}\NormalTok{,}
  \DataTypeTok{stopping_tolerance=}\FloatTok{1e-2}\NormalTok{,        }\CommentTok{## stop when misclassification does not improve by >=1% for 2 scoring events}
  \DataTypeTok{stopping_rounds=}\DecValTok{2}\NormalTok{,}
  \DataTypeTok{score_validation_samples=}\DecValTok{10000}\NormalTok{, }\CommentTok{## downsample validation set for faster scoring}
  \DataTypeTok{score_duty_cycle=}\FloatTok{0.025}\NormalTok{,         }\CommentTok{## don't score more than 2.5% of the wall time}
  \DataTypeTok{adaptive_rate=}\NormalTok{F,                }\CommentTok{## manually tuned learning rate}
  \DataTypeTok{momentum_start=}\FloatTok{0.5}\NormalTok{,             }\CommentTok{## manually tuned momentum}
  \DataTypeTok{momentum_stable=}\FloatTok{0.9}\NormalTok{, }
  \DataTypeTok{momentum_ramp=}\FloatTok{1e7}\NormalTok{, }
  \DataTypeTok{l1=}\FloatTok{1e-5}\NormalTok{,}
  \DataTypeTok{l2=}\FloatTok{1e-5}\NormalTok{,}
  \DataTypeTok{activation=}\KeywordTok{c}\NormalTok{(}\StringTok{"Rectifier"}\NormalTok{),}
  \DataTypeTok{max_w2=}\DecValTok{10}\NormalTok{,                      }\CommentTok{## can help improve stability for Rectifier}
  \DataTypeTok{hyper_params=}\NormalTok{hyper_params}
\NormalTok{)}
\CommentTok{#> }
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|}\StringTok{                                                                 }\ErrorTok{|}\StringTok{   }\DecValTok{0}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|===}\StringTok{                                                              }\ErrorTok{|}\StringTok{   }\DecValTok{5}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=====}\StringTok{                                                            }\ErrorTok{|}\StringTok{   }\DecValTok{8}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|========}\StringTok{                                                         }\ErrorTok{|}\StringTok{  }\DecValTok{13}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|===========}\StringTok{                                                      }\ErrorTok{|}\StringTok{  }\DecValTok{17}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==============}\StringTok{                                                   }\ErrorTok{|}\StringTok{  }\DecValTok{22}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==================}\StringTok{                                               }\ErrorTok{|}\StringTok{  }\DecValTok{27}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=====================}\StringTok{                                            }\ErrorTok{|}\StringTok{  }\DecValTok{32}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|========================}\StringTok{                                         }\ErrorTok{|}\StringTok{  }\DecValTok{38}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|===========================}\StringTok{                                      }\ErrorTok{|}\StringTok{  }\DecValTok{41}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==============================}\StringTok{                                   }\ErrorTok{|}\StringTok{  }\DecValTok{46}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|================================}\StringTok{                                 }\ErrorTok{|}\StringTok{  }\DecValTok{49}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|===================================}\StringTok{                              }\ErrorTok{|}\StringTok{  }\DecValTok{55}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|======================================}\StringTok{                           }\ErrorTok{|}\StringTok{  }\DecValTok{59}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=========================================}\StringTok{                        }\ErrorTok{|}\StringTok{  }\DecValTok{64}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=============================================}\StringTok{                    }\ErrorTok{|}\StringTok{  }\DecValTok{69}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|===============================================}\StringTok{                  }\ErrorTok{|}\StringTok{  }\DecValTok{73}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==================================================}\StringTok{               }\ErrorTok{|}\StringTok{  }\DecValTok{76}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=====================================================}\StringTok{            }\ErrorTok{|}\StringTok{  }\DecValTok{81}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=======================================================}\StringTok{          }\ErrorTok{|}\StringTok{  }\DecValTok{85}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|===========================================================}\StringTok{      }\ErrorTok{|}\StringTok{  }\DecValTok{90}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==============================================================}\StringTok{   }\ErrorTok{|}\StringTok{  }\DecValTok{96}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=================================================================|}\StringTok{ }\DecValTok{100}\NormalTok{%}
\NormalTok{grid}
\CommentTok{#> H2O Grid Details}
\CommentTok{#> ================}
\CommentTok{#> }
\CommentTok{#> Grid ID: dl_grid }
\CommentTok{#> Used hyper parameters: }
\CommentTok{#>   -  hidden }
\CommentTok{#>   -  input_dropout_ratio }
\CommentTok{#>   -  rate }
\CommentTok{#>   -  rate_annealing }
\CommentTok{#> Number of models: 24 }
\CommentTok{#> Number of failed models: 0 }
\CommentTok{#> }
\CommentTok{#> Hyper-Parameter Search Summary: ordered by increasing logloss}
\CommentTok{#>     hidden input_dropout_ratio rate rate_annealing        model_ids}
\CommentTok{#> 1 [64, 64]                 0.0 0.01         1.0E-6 dl_grid_model_18}
\CommentTok{#> 2 [64, 64]                 0.0 0.01         1.0E-7 dl_grid_model_10}
\CommentTok{#> 3 [64, 64]                 0.0 0.01         1.0E-8  dl_grid_model_2}
\CommentTok{#> 4 [64, 64]                0.05 0.01         1.0E-8  dl_grid_model_4}
\CommentTok{#> 5 [64, 64]                0.05 0.01         1.0E-7 dl_grid_model_12}
\CommentTok{#>              logloss}
\CommentTok{#> 1 0.5655348085079595}
\CommentTok{#> 2 0.5710151424815758}
\CommentTok{#> 3 0.5859372209341824}
\CommentTok{#> 4 0.5884748164131279}
\CommentTok{#> 5  0.590855031873655}
\CommentTok{#> }
\CommentTok{#> ---}
\CommentTok{#>          hidden input_dropout_ratio rate rate_annealing        model_ids}
\CommentTok{#> 19 [32, 32, 32]                 0.0 0.02         1.0E-8  dl_grid_model_5}
\CommentTok{#> 20     [64, 64]                0.05 0.02         1.0E-6 dl_grid_model_24}
\CommentTok{#> 21 [32, 32, 32]                 0.0 0.01         1.0E-8  dl_grid_model_1}
\CommentTok{#> 22 [32, 32, 32]                 0.0 0.02         1.0E-7 dl_grid_model_13}
\CommentTok{#> 23 [32, 32, 32]                0.05 0.01         1.0E-8  dl_grid_model_3}
\CommentTok{#> 24 [32, 32, 32]                 0.0 0.02         1.0E-6 dl_grid_model_21}
\CommentTok{#>               logloss}
\CommentTok{#> 19 0.6483325937974176}
\CommentTok{#> 20 0.6494497620049914}
\CommentTok{#> 21 0.6643165406615816}
\CommentTok{#> 22 0.6853226995665436}
\CommentTok{#> 23 0.6923421232689934}
\CommentTok{#> 24 0.7942616802375754}
\end{Highlighting}
\end{Shaded}

Let's see which model had the lowest validation error:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{grid <-}\StringTok{ }\KeywordTok{h2o.getGrid}\NormalTok{(}\StringTok{"dl_grid"}\NormalTok{,}\DataTypeTok{sort_by=}\StringTok{"err"}\NormalTok{,}\DataTypeTok{decreasing=}\OtherTok{FALSE}\NormalTok{)}
\NormalTok{grid}
\CommentTok{#> H2O Grid Details}
\CommentTok{#> ================}
\CommentTok{#> }
\CommentTok{#> Grid ID: dl_grid }
\CommentTok{#> Used hyper parameters: }
\CommentTok{#>   -  hidden }
\CommentTok{#>   -  input_dropout_ratio }
\CommentTok{#>   -  rate }
\CommentTok{#>   -  rate_annealing }
\CommentTok{#> Number of models: 24 }
\CommentTok{#> Number of failed models: 0 }
\CommentTok{#> }
\CommentTok{#> Hyper-Parameter Search Summary: ordered by increasing err}
\CommentTok{#>         hidden input_dropout_ratio rate rate_annealing        model_ids}
\CommentTok{#> 1 [32, 32, 32]                0.05 0.01         1.0E-7 dl_grid_model_11}
\CommentTok{#> 2     [64, 64]                0.05 0.01         1.0E-7 dl_grid_model_12}
\CommentTok{#> 3     [64, 64]                0.05 0.01         1.0E-6 dl_grid_model_20}
\CommentTok{#> 4     [64, 64]                 0.0 0.01         1.0E-6 dl_grid_model_18}
\CommentTok{#> 5     [64, 64]                 0.0 0.01         1.0E-7 dl_grid_model_10}
\CommentTok{#>                   err}
\CommentTok{#> 1 0.25161546873446666}
\CommentTok{#> 2  0.2533002481389578}
\CommentTok{#> 3 0.25361300682456844}
\CommentTok{#> 4 0.25504092633260134}
\CommentTok{#> 5 0.25569264720633966}
\CommentTok{#> }
\CommentTok{#> ---}
\CommentTok{#>          hidden input_dropout_ratio rate rate_annealing        model_ids}
\CommentTok{#> 19 [32, 32, 32]                0.05 0.01         1.0E-8  dl_grid_model_3}
\CommentTok{#> 20 [32, 32, 32]                 0.0 0.02         1.0E-8  dl_grid_model_5}
\CommentTok{#> 21 [32, 32, 32]                0.05 0.02         1.0E-7 dl_grid_model_15}
\CommentTok{#> 22     [64, 64]                0.05 0.02         1.0E-6 dl_grid_model_24}
\CommentTok{#> 23 [32, 32, 32]                 0.0 0.02         1.0E-7 dl_grid_model_13}
\CommentTok{#> 24 [32, 32, 32]                 0.0 0.02         1.0E-6 dl_grid_model_21}
\CommentTok{#>                    err}
\CommentTok{#> 19 0.27659787617711884}
\CommentTok{#> 20 0.28005204684215795}
\CommentTok{#> 21 0.28219620508679855}
\CommentTok{#> 22 0.28436684728075456}
\CommentTok{#> 23  0.2872138358492452}
\CommentTok{#> 24 0.30268698616402645}

\CommentTok{## To see what other "sort_by" criteria are allowed}
\CommentTok{#grid <- h2o.getGrid("dl_grid",sort_by="wrong_thing",decreasing=FALSE)}

\CommentTok{## Sort by logloss}
\KeywordTok{h2o.getGrid}\NormalTok{(}\StringTok{"dl_grid"}\NormalTok{,}\DataTypeTok{sort_by=}\StringTok{"logloss"}\NormalTok{,}\DataTypeTok{decreasing=}\OtherTok{FALSE}\NormalTok{)}
\CommentTok{#> H2O Grid Details}
\CommentTok{#> ================}
\CommentTok{#> }
\CommentTok{#> Grid ID: dl_grid }
\CommentTok{#> Used hyper parameters: }
\CommentTok{#>   -  hidden }
\CommentTok{#>   -  input_dropout_ratio }
\CommentTok{#>   -  rate }
\CommentTok{#>   -  rate_annealing }
\CommentTok{#> Number of models: 24 }
\CommentTok{#> Number of failed models: 0 }
\CommentTok{#> }
\CommentTok{#> Hyper-Parameter Search Summary: ordered by increasing logloss}
\CommentTok{#>     hidden input_dropout_ratio rate rate_annealing        model_ids}
\CommentTok{#> 1 [64, 64]                 0.0 0.01         1.0E-6 dl_grid_model_18}
\CommentTok{#> 2 [64, 64]                 0.0 0.01         1.0E-7 dl_grid_model_10}
\CommentTok{#> 3 [64, 64]                 0.0 0.01         1.0E-8  dl_grid_model_2}
\CommentTok{#> 4 [64, 64]                0.05 0.01         1.0E-8  dl_grid_model_4}
\CommentTok{#> 5 [64, 64]                0.05 0.01         1.0E-7 dl_grid_model_12}
\CommentTok{#>              logloss}
\CommentTok{#> 1 0.5655348085079595}
\CommentTok{#> 2 0.5710151424815758}
\CommentTok{#> 3 0.5859372209341824}
\CommentTok{#> 4 0.5884748164131279}
\CommentTok{#> 5  0.590855031873655}
\CommentTok{#> }
\CommentTok{#> ---}
\CommentTok{#>          hidden input_dropout_ratio rate rate_annealing        model_ids}
\CommentTok{#> 19 [32, 32, 32]                 0.0 0.02         1.0E-8  dl_grid_model_5}
\CommentTok{#> 20     [64, 64]                0.05 0.02         1.0E-6 dl_grid_model_24}
\CommentTok{#> 21 [32, 32, 32]                 0.0 0.01         1.0E-8  dl_grid_model_1}
\CommentTok{#> 22 [32, 32, 32]                 0.0 0.02         1.0E-7 dl_grid_model_13}
\CommentTok{#> 23 [32, 32, 32]                0.05 0.01         1.0E-8  dl_grid_model_3}
\CommentTok{#> 24 [32, 32, 32]                 0.0 0.02         1.0E-6 dl_grid_model_21}
\CommentTok{#>               logloss}
\CommentTok{#> 19 0.6483325937974176}
\CommentTok{#> 20 0.6494497620049914}
\CommentTok{#> 21 0.6643165406615816}
\CommentTok{#> 22 0.6853226995665436}
\CommentTok{#> 23 0.6923421232689934}
\CommentTok{#> 24 0.7942616802375754}

\CommentTok{## Find the best model and its full set of parameters}
\NormalTok{grid}\OperatorTok{@}\NormalTok{summary_table[}\DecValTok{1}\NormalTok{,]}
\CommentTok{#> Hyper-Parameter Search Summary: ordered by increasing err}
\CommentTok{#>         hidden input_dropout_ratio rate rate_annealing        model_ids}
\CommentTok{#> 1 [32, 32, 32]                0.05 0.01         1.0E-7 dl_grid_model_11}
\CommentTok{#>                   err}
\CommentTok{#> 1 0.25161546873446666}
\NormalTok{best_model <-}\StringTok{ }\KeywordTok{h2o.getModel}\NormalTok{(grid}\OperatorTok{@}\NormalTok{model_ids[[}\DecValTok{1}\NormalTok{]])}
\NormalTok{best_model}
\CommentTok{#> Model Details:}
\CommentTok{#> ==============}
\CommentTok{#> }
\CommentTok{#> H2OMultinomialModel: deeplearning}
\CommentTok{#> Model ID:  dl_grid_model_11 }
\CommentTok{#> Status of Neuron Layers: predicting Cover_Type, 7-class classification, multinomial distribution, CrossEntropy loss, 4,167 weights/biases, 40.8 KB, 100,000 training samples, mini-batch size 1}
\CommentTok{#>   layer units      type dropout       l1       l2 mean_rate rate_rms}
\CommentTok{#> 1     1    56     Input  5.00 %       NA       NA        NA       NA}
\CommentTok{#> 2     2    32 Rectifier  0.00 % 0.000010 0.000010  0.009901 0.000000}
\CommentTok{#> 3     3    32 Rectifier  0.00 % 0.000010 0.000010  0.009901 0.000000}
\CommentTok{#> 4     4    32 Rectifier  0.00 % 0.000010 0.000010  0.009901 0.000000}
\CommentTok{#> 5     5     7   Softmax      NA 0.000010 0.000010  0.009901 0.000000}
\CommentTok{#>   momentum mean_weight weight_rms mean_bias bias_rms}
\CommentTok{#> 1       NA          NA         NA        NA       NA}
\CommentTok{#> 2 0.504000   -0.005652   0.248677  0.162806 0.181253}
\CommentTok{#> 3 0.504000   -0.075886   0.255297  0.867495 0.168337}
\CommentTok{#> 4 0.504000   -0.095631   0.228149  0.834338 0.103173}
\CommentTok{#> 5 0.504000    0.017759   0.531430 -0.009904 0.667407}
\CommentTok{#> }
\CommentTok{#> }
\CommentTok{#> H2OMultinomialMetrics: deeplearning}
\CommentTok{#> ** Reported on training data. **}
\CommentTok{#> ** Metrics reported on full training frame **}
\CommentTok{#> }
\CommentTok{#> Training Set Metrics: }
\CommentTok{#> =====================}
\CommentTok{#> }
\CommentTok{#> Extract training frame with `h2o.getFrame("RTMP_sid_ab5c_9")`}
\CommentTok{#> MSE: (Extract with `h2o.mse`) 0.183}
\CommentTok{#> RMSE: (Extract with `h2o.rmse`) 0.428}
\CommentTok{#> Logloss: (Extract with `h2o.logloss`) 0.568}
\CommentTok{#> Mean Per-Class Error: 0.456}
\CommentTok{#> Confusion Matrix: Extract with `h2o.confusionMatrix(<model>,train = TRUE)`)}
\CommentTok{#> =========================================================================}
\CommentTok{#> Confusion Matrix: Row labels: Actual class; Column labels: Predicted class}
\CommentTok{#>         class_1 class_2 class_3 class_4 class_5 class_6 class_7  Error}
\CommentTok{#> class_1    2698     934       1       0       9       3      43 0.2684}
\CommentTok{#> class_2     714    3965      95       0      38      11      12 0.1799}
\CommentTok{#> class_3       0      37     586       1       0       6       0 0.0698}
\CommentTok{#> class_4       0       0      33      11       0       0       0 0.7500}
\CommentTok{#> class_5       1      99      14       0      42       0       0 0.7308}
\CommentTok{#> class_6       0      68     202       0       0      39       0 0.8738}
\CommentTok{#> class_7     105       2       1       0       0       0     230 0.3195}
\CommentTok{#> Totals     3518    5105     932      12      89      59     285 0.2429}
\CommentTok{#>                     Rate}
\CommentTok{#> class_1 =    990 / 3,688}
\CommentTok{#> class_2 =    870 / 4,835}
\CommentTok{#> class_3 =       44 / 630}
\CommentTok{#> class_4 =        33 / 44}
\CommentTok{#> class_5 =      114 / 156}
\CommentTok{#> class_6 =      270 / 309}
\CommentTok{#> class_7 =      108 / 338}
\CommentTok{#> Totals  = 2,429 / 10,000}
\CommentTok{#> }
\CommentTok{#> Hit Ratio Table: Extract with `h2o.hit_ratio_table(<model>,train = TRUE)`}
\CommentTok{#> =======================================================================}
\CommentTok{#> Top-7 Hit Ratios: }
\CommentTok{#>   k hit_ratio}
\CommentTok{#> 1 1  0.757100}
\CommentTok{#> 2 2  0.971100}
\CommentTok{#> 3 3  0.995300}
\CommentTok{#> 4 4  0.999400}
\CommentTok{#> 5 5  0.999900}
\CommentTok{#> 6 6  1.000000}
\CommentTok{#> 7 7  1.000000}
\CommentTok{#> }
\CommentTok{#> }
\CommentTok{#> H2OMultinomialMetrics: deeplearning}
\CommentTok{#> ** Reported on validation data. **}
\CommentTok{#> ** Metrics reported on temporary validation frame with 10059 samples **}
\CommentTok{#> }
\CommentTok{#> Validation Set Metrics: }
\CommentTok{#> =====================}
\CommentTok{#> }
\CommentTok{#> MSE: (Extract with `h2o.mse`) 0.19}
\CommentTok{#> RMSE: (Extract with `h2o.rmse`) 0.435}
\CommentTok{#> Logloss: (Extract with `h2o.logloss`) 0.593}
\CommentTok{#> Mean Per-Class Error: 0.472}
\CommentTok{#> Confusion Matrix: Extract with `h2o.confusionMatrix(<model>,valid = TRUE)`)}
\CommentTok{#> =========================================================================}
\CommentTok{#> Confusion Matrix: Row labels: Actual class; Column labels: Predicted class}
\CommentTok{#>         class_1 class_2 class_3 class_4 class_5 class_6 class_7  Error}
\CommentTok{#> class_1    2631     993       0       0       3       0      46 0.2837}
\CommentTok{#> class_2     701    3995     100       0      54      13       7 0.1797}
\CommentTok{#> class_3       0      51     589       2       1       7       0 0.0938}
\CommentTok{#> class_4       0       0      36      13       0       0       0 0.7347}
\CommentTok{#> class_5       2     113      13       0      39       0       0 0.7665}
\CommentTok{#> class_6       0      67     201       1       1      27       0 0.9091}
\CommentTok{#> class_7     118       0       1       0       0       0     234 0.3371}
\CommentTok{#> Totals     3452    5219     940      16      98      47     287 0.2516}
\CommentTok{#>                     Rate}
\CommentTok{#> class_1 =  1,042 / 3,673}
\CommentTok{#> class_2 =    875 / 4,870}
\CommentTok{#> class_3 =       61 / 650}
\CommentTok{#> class_4 =        36 / 49}
\CommentTok{#> class_5 =      128 / 167}
\CommentTok{#> class_6 =      270 / 297}
\CommentTok{#> class_7 =      119 / 353}
\CommentTok{#> Totals  = 2,531 / 10,059}
\CommentTok{#> }
\CommentTok{#> Hit Ratio Table: Extract with `h2o.hit_ratio_table(<model>,valid = TRUE)`}
\CommentTok{#> =======================================================================}
\CommentTok{#> Top-7 Hit Ratios: }
\CommentTok{#>   k hit_ratio}
\CommentTok{#> 1 1  0.748385}
\CommentTok{#> 2 2  0.968784}
\CommentTok{#> 3 3  0.995825}
\CommentTok{#> 4 4  0.998907}
\CommentTok{#> 5 5  0.999801}
\CommentTok{#> 6 6  1.000000}
\CommentTok{#> 7 7  1.000000}

\KeywordTok{print}\NormalTok{(best_model}\OperatorTok{@}\NormalTok{allparameters)}
\CommentTok{#> $model_id}
\CommentTok{#> [1] "dl_grid_model_11"}
\CommentTok{#> }
\CommentTok{#> $training_frame}
\CommentTok{#> [1] "RTMP_sid_ab5c_9"}
\CommentTok{#> }
\CommentTok{#> $validation_frame}
\CommentTok{#> [1] "valid.hex"}
\CommentTok{#> }
\CommentTok{#> $nfolds}
\CommentTok{#> [1] 0}
\CommentTok{#> }
\CommentTok{#> $keep_cross_validation_models}
\CommentTok{#> [1] TRUE}
\CommentTok{#> }
\CommentTok{#> $keep_cross_validation_predictions}
\CommentTok{#> [1] FALSE}
\CommentTok{#> }
\CommentTok{#> $keep_cross_validation_fold_assignment}
\CommentTok{#> [1] FALSE}
\CommentTok{#> }
\CommentTok{#> $fold_assignment}
\CommentTok{#> [1] "AUTO"}
\CommentTok{#> }
\CommentTok{#> $ignore_const_cols}
\CommentTok{#> [1] TRUE}
\CommentTok{#> }
\CommentTok{#> $score_each_iteration}
\CommentTok{#> [1] FALSE}
\CommentTok{#> }
\CommentTok{#> $balance_classes}
\CommentTok{#> [1] FALSE}
\CommentTok{#> }
\CommentTok{#> $max_after_balance_size}
\CommentTok{#> [1] 5}
\CommentTok{#> }
\CommentTok{#> $max_confusion_matrix_size}
\CommentTok{#> [1] 20}
\CommentTok{#> }
\CommentTok{#> $max_hit_ratio_k}
\CommentTok{#> [1] 0}
\CommentTok{#> }
\CommentTok{#> $overwrite_with_best_model}
\CommentTok{#> [1] TRUE}
\CommentTok{#> }
\CommentTok{#> $use_all_factor_levels}
\CommentTok{#> [1] TRUE}
\CommentTok{#> }
\CommentTok{#> $standardize}
\CommentTok{#> [1] TRUE}
\CommentTok{#> }
\CommentTok{#> $activation}
\CommentTok{#> [1] "Rectifier"}
\CommentTok{#> }
\CommentTok{#> $hidden}
\CommentTok{#> [1] 32 32 32}
\CommentTok{#> }
\CommentTok{#> $epochs}
\CommentTok{#> [1] 10}
\CommentTok{#> }
\CommentTok{#> $train_samples_per_iteration}
\CommentTok{#> [1] -2}
\CommentTok{#> }
\CommentTok{#> $target_ratio_comm_to_comp}
\CommentTok{#> [1] 0.05}
\CommentTok{#> }
\CommentTok{#> $seed}
\CommentTok{#> [1] 8.23e+17}
\CommentTok{#> }
\CommentTok{#> $adaptive_rate}
\CommentTok{#> [1] FALSE}
\CommentTok{#> }
\CommentTok{#> $rho}
\CommentTok{#> [1] 0.99}
\CommentTok{#> }
\CommentTok{#> $epsilon}
\CommentTok{#> [1] 1e-08}
\CommentTok{#> }
\CommentTok{#> $rate}
\CommentTok{#> [1] 0.01}
\CommentTok{#> }
\CommentTok{#> $rate_annealing}
\CommentTok{#> [1] 1e-07}
\CommentTok{#> }
\CommentTok{#> $rate_decay}
\CommentTok{#> [1] 1}
\CommentTok{#> }
\CommentTok{#> $momentum_start}
\CommentTok{#> [1] 0.5}
\CommentTok{#> }
\CommentTok{#> $momentum_ramp}
\CommentTok{#> [1] 1e+07}
\CommentTok{#> }
\CommentTok{#> $momentum_stable}
\CommentTok{#> [1] 0.9}
\CommentTok{#> }
\CommentTok{#> $nesterov_accelerated_gradient}
\CommentTok{#> [1] TRUE}
\CommentTok{#> }
\CommentTok{#> $input_dropout_ratio}
\CommentTok{#> [1] 0.05}
\CommentTok{#> }
\CommentTok{#> $l1}
\CommentTok{#> [1] 1e-05}
\CommentTok{#> }
\CommentTok{#> $l2}
\CommentTok{#> [1] 1e-05}
\CommentTok{#> }
\CommentTok{#> $max_w2}
\CommentTok{#> [1] 10}
\CommentTok{#> }
\CommentTok{#> $initial_weight_distribution}
\CommentTok{#> [1] "UniformAdaptive"}
\CommentTok{#> }
\CommentTok{#> $initial_weight_scale}
\CommentTok{#> [1] 1}
\CommentTok{#> }
\CommentTok{#> $loss}
\CommentTok{#> [1] "Automatic"}
\CommentTok{#> }
\CommentTok{#> $distribution}
\CommentTok{#> [1] "AUTO"}
\CommentTok{#> }
\CommentTok{#> $quantile_alpha}
\CommentTok{#> [1] 0.5}
\CommentTok{#> }
\CommentTok{#> $tweedie_power}
\CommentTok{#> [1] 1.5}
\CommentTok{#> }
\CommentTok{#> $huber_alpha}
\CommentTok{#> [1] 0.9}
\CommentTok{#> }
\CommentTok{#> $score_interval}
\CommentTok{#> [1] 5}
\CommentTok{#> }
\CommentTok{#> $score_training_samples}
\CommentTok{#> [1] 10000}
\CommentTok{#> }
\CommentTok{#> $score_validation_samples}
\CommentTok{#> [1] 10000}
\CommentTok{#> }
\CommentTok{#> $score_duty_cycle}
\CommentTok{#> [1] 0.025}
\CommentTok{#> }
\CommentTok{#> $classification_stop}
\CommentTok{#> [1] 0}
\CommentTok{#> }
\CommentTok{#> $regression_stop}
\CommentTok{#> [1] 1e-06}
\CommentTok{#> }
\CommentTok{#> $stopping_rounds}
\CommentTok{#> [1] 2}
\CommentTok{#> }
\CommentTok{#> $stopping_metric}
\CommentTok{#> [1] "misclassification"}
\CommentTok{#> }
\CommentTok{#> $stopping_tolerance}
\CommentTok{#> [1] 0.01}
\CommentTok{#> }
\CommentTok{#> $max_runtime_secs}
\CommentTok{#> [1] 1.8e+308}
\CommentTok{#> }
\CommentTok{#> $score_validation_sampling}
\CommentTok{#> [1] "Uniform"}
\CommentTok{#> }
\CommentTok{#> $diagnostics}
\CommentTok{#> [1] TRUE}
\CommentTok{#> }
\CommentTok{#> $fast_mode}
\CommentTok{#> [1] TRUE}
\CommentTok{#> }
\CommentTok{#> $force_load_balance}
\CommentTok{#> [1] TRUE}
\CommentTok{#> }
\CommentTok{#> $variable_importances}
\CommentTok{#> [1] TRUE}
\CommentTok{#> }
\CommentTok{#> $replicate_training_data}
\CommentTok{#> [1] TRUE}
\CommentTok{#> }
\CommentTok{#> $single_node_mode}
\CommentTok{#> [1] FALSE}
\CommentTok{#> }
\CommentTok{#> $shuffle_training_data}
\CommentTok{#> [1] FALSE}
\CommentTok{#> }
\CommentTok{#> $missing_values_handling}
\CommentTok{#> [1] "MeanImputation"}
\CommentTok{#> }
\CommentTok{#> $quiet_mode}
\CommentTok{#> [1] FALSE}
\CommentTok{#> }
\CommentTok{#> $autoencoder}
\CommentTok{#> [1] FALSE}
\CommentTok{#> }
\CommentTok{#> $sparse}
\CommentTok{#> [1] FALSE}
\CommentTok{#> }
\CommentTok{#> $col_major}
\CommentTok{#> [1] FALSE}
\CommentTok{#> }
\CommentTok{#> $average_activation}
\CommentTok{#> [1] 0}
\CommentTok{#> }
\CommentTok{#> $sparsity_beta}
\CommentTok{#> [1] 0}
\CommentTok{#> }
\CommentTok{#> $max_categorical_features}
\CommentTok{#> [1] 2147483647}
\CommentTok{#> }
\CommentTok{#> $reproducible}
\CommentTok{#> [1] FALSE}
\CommentTok{#> }
\CommentTok{#> $export_weights_and_biases}
\CommentTok{#> [1] FALSE}
\CommentTok{#> }
\CommentTok{#> $mini_batch_size}
\CommentTok{#> [1] 1}
\CommentTok{#> }
\CommentTok{#> $categorical_encoding}
\CommentTok{#> [1] "AUTO"}
\CommentTok{#> }
\CommentTok{#> $elastic_averaging}
\CommentTok{#> [1] FALSE}
\CommentTok{#> }
\CommentTok{#> $elastic_averaging_moving_rate}
\CommentTok{#> [1] 0.9}
\CommentTok{#> }
\CommentTok{#> $elastic_averaging_regularization}
\CommentTok{#> [1] 0.001}
\CommentTok{#> }
\CommentTok{#> $x}
\CommentTok{#>  [1] "Soil_Type"                         }
\CommentTok{#>  [2] "Wilderness_Area"                   }
\CommentTok{#>  [3] "Elevation"                         }
\CommentTok{#>  [4] "Aspect"                            }
\CommentTok{#>  [5] "Slope"                             }
\CommentTok{#>  [6] "Horizontal_Distance_To_Hydrology"  }
\CommentTok{#>  [7] "Vertical_Distance_To_Hydrology"    }
\CommentTok{#>  [8] "Horizontal_Distance_To_Roadways"   }
\CommentTok{#>  [9] "Hillshade_9am"                     }
\CommentTok{#> [10] "Hillshade_Noon"                    }
\CommentTok{#> [11] "Hillshade_3pm"                     }
\CommentTok{#> [12] "Horizontal_Distance_To_Fire_Points"}
\CommentTok{#> }
\CommentTok{#> $y}
\CommentTok{#> [1] "Cover_Type"}
\KeywordTok{print}\NormalTok{(}\KeywordTok{h2o.performance}\NormalTok{(best_model, }\DataTypeTok{valid=}\NormalTok{T))}
\CommentTok{#> H2OMultinomialMetrics: deeplearning}
\CommentTok{#> ** Reported on validation data. **}
\CommentTok{#> ** Metrics reported on temporary validation frame with 10059 samples **}
\CommentTok{#> }
\CommentTok{#> Validation Set Metrics: }
\CommentTok{#> =====================}
\CommentTok{#> }
\CommentTok{#> MSE: (Extract with `h2o.mse`) 0.19}
\CommentTok{#> RMSE: (Extract with `h2o.rmse`) 0.435}
\CommentTok{#> Logloss: (Extract with `h2o.logloss`) 0.593}
\CommentTok{#> Mean Per-Class Error: 0.472}
\CommentTok{#> Confusion Matrix: Extract with `h2o.confusionMatrix(<model>,valid = TRUE)`)}
\CommentTok{#> =========================================================================}
\CommentTok{#> Confusion Matrix: Row labels: Actual class; Column labels: Predicted class}
\CommentTok{#>         class_1 class_2 class_3 class_4 class_5 class_6 class_7  Error}
\CommentTok{#> class_1    2631     993       0       0       3       0      46 0.2837}
\CommentTok{#> class_2     701    3995     100       0      54      13       7 0.1797}
\CommentTok{#> class_3       0      51     589       2       1       7       0 0.0938}
\CommentTok{#> class_4       0       0      36      13       0       0       0 0.7347}
\CommentTok{#> class_5       2     113      13       0      39       0       0 0.7665}
\CommentTok{#> class_6       0      67     201       1       1      27       0 0.9091}
\CommentTok{#> class_7     118       0       1       0       0       0     234 0.3371}
\CommentTok{#> Totals     3452    5219     940      16      98      47     287 0.2516}
\CommentTok{#>                     Rate}
\CommentTok{#> class_1 =  1,042 / 3,673}
\CommentTok{#> class_2 =    875 / 4,870}
\CommentTok{#> class_3 =       61 / 650}
\CommentTok{#> class_4 =        36 / 49}
\CommentTok{#> class_5 =      128 / 167}
\CommentTok{#> class_6 =      270 / 297}
\CommentTok{#> class_7 =      119 / 353}
\CommentTok{#> Totals  = 2,531 / 10,059}
\CommentTok{#> }
\CommentTok{#> Hit Ratio Table: Extract with `h2o.hit_ratio_table(<model>,valid = TRUE)`}
\CommentTok{#> =======================================================================}
\CommentTok{#> Top-7 Hit Ratios: }
\CommentTok{#>   k hit_ratio}
\CommentTok{#> 1 1  0.748385}
\CommentTok{#> 2 2  0.968784}
\CommentTok{#> 3 3  0.995825}
\CommentTok{#> 4 4  0.998907}
\CommentTok{#> 5 5  0.999801}
\CommentTok{#> 6 6  1.000000}
\CommentTok{#> 7 7  1.000000}
\KeywordTok{print}\NormalTok{(}\KeywordTok{h2o.logloss}\NormalTok{(best_model, }\DataTypeTok{valid=}\NormalTok{T))}
\CommentTok{#> [1] 0.593}
\end{Highlighting}
\end{Shaded}

\hypertarget{random-hyper-parameter-search}{%
\subsection{Random Hyper-Parameter Search}\label{random-hyper-parameter-search}}

Often, hyper-parameter search for more than 4 parameters can be done more efficiently with random parameter search than with grid search. Basically, chances are good to find one of many good models in less time than performing an exhaustive grid search. We simply build up to max\_models models with parameters drawn randomly from user-specified distributions (here, uniform). For this example, we use the adaptive learning rate and focus on tuning the network architecture and the regularization parameters. We also let the grid search stop automatically once the performance at the top of the leaderboard doesn't change much anymore, i.e., once the search has converged.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{hyper_params <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}
  \DataTypeTok{activation=}\KeywordTok{c}\NormalTok{(}\StringTok{"Rectifier"}\NormalTok{,}\StringTok{"Tanh"}\NormalTok{,}\StringTok{"Maxout"}\NormalTok{,}\StringTok{"RectifierWithDropout"}\NormalTok{,}\StringTok{"TanhWithDropout"}\NormalTok{,}\StringTok{"MaxoutWithDropout"}\NormalTok{),}
  \DataTypeTok{hidden=}\KeywordTok{list}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{20}\NormalTok{,}\DecValTok{20}\NormalTok{),}\KeywordTok{c}\NormalTok{(}\DecValTok{50}\NormalTok{,}\DecValTok{50}\NormalTok{),}\KeywordTok{c}\NormalTok{(}\DecValTok{30}\NormalTok{,}\DecValTok{30}\NormalTok{,}\DecValTok{30}\NormalTok{),}\KeywordTok{c}\NormalTok{(}\DecValTok{25}\NormalTok{,}\DecValTok{25}\NormalTok{,}\DecValTok{25}\NormalTok{,}\DecValTok{25}\NormalTok{)),}
  \DataTypeTok{input_dropout_ratio=}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\FloatTok{0.05}\NormalTok{),}
  \DataTypeTok{l1=}\KeywordTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{,}\FloatTok{1e-4}\NormalTok{,}\FloatTok{1e-6}\NormalTok{),}
  \DataTypeTok{l2=}\KeywordTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{,}\FloatTok{1e-4}\NormalTok{,}\FloatTok{1e-6}\NormalTok{)}
\NormalTok{)}
\NormalTok{hyper_params}

\CommentTok{## Stop once the top 5 models are within 1% of each other (i.e., the windowed average varies less than 1%)}
\NormalTok{search_criteria =}\StringTok{ }\KeywordTok{list}\NormalTok{(}\DataTypeTok{strategy =} \StringTok{"RandomDiscrete"}\NormalTok{, }\DataTypeTok{max_runtime_secs =} \DecValTok{360}\NormalTok{, }\DataTypeTok{max_models =} \DecValTok{100}\NormalTok{, }\DataTypeTok{seed=}\DecValTok{1234567}\NormalTok{, }\DataTypeTok{stopping_rounds=}\DecValTok{5}\NormalTok{, }\DataTypeTok{stopping_tolerance=}\FloatTok{1e-2}\NormalTok{)}
\NormalTok{dl_random_grid <-}\StringTok{ }\KeywordTok{h2o.grid}\NormalTok{(}
  \DataTypeTok{algorithm=}\StringTok{"deeplearning"}\NormalTok{,}
  \DataTypeTok{grid_id =} \StringTok{"dl_grid_random"}\NormalTok{,}
  \DataTypeTok{training_frame=}\NormalTok{sampled_train,}
  \DataTypeTok{validation_frame=}\NormalTok{valid, }
  \DataTypeTok{x=}\NormalTok{predictors, }
  \DataTypeTok{y=}\NormalTok{response,}
  \DataTypeTok{epochs=}\DecValTok{1}\NormalTok{,}
  \DataTypeTok{stopping_metric=}\StringTok{"logloss"}\NormalTok{,}
  \DataTypeTok{stopping_tolerance=}\FloatTok{1e-2}\NormalTok{,        }\CommentTok{## stop when logloss does not improve by >=1% for 2 scoring events}
  \DataTypeTok{stopping_rounds=}\DecValTok{2}\NormalTok{,}
  \DataTypeTok{score_validation_samples=}\DecValTok{10000}\NormalTok{, }\CommentTok{## downsample validation set for faster scoring}
  \DataTypeTok{score_duty_cycle=}\FloatTok{0.025}\NormalTok{,         }\CommentTok{## don't score more than 2.5% of the wall time}
  \DataTypeTok{max_w2=}\DecValTok{10}\NormalTok{,                      }\CommentTok{## can help improve stability for Rectifier}
  \DataTypeTok{hyper_params =}\NormalTok{ hyper_params,}
  \DataTypeTok{search_criteria =}\NormalTok{ search_criteria}
\NormalTok{)                                }
\NormalTok{grid <-}\StringTok{ }\KeywordTok{h2o.getGrid}\NormalTok{(}\StringTok{"dl_grid_random"}\NormalTok{,}\DataTypeTok{sort_by=}\StringTok{"logloss"}\NormalTok{,}\DataTypeTok{decreasing=}\OtherTok{FALSE}\NormalTok{)}
\NormalTok{grid}

\NormalTok{grid}\OperatorTok{@}\NormalTok{summary_table[}\DecValTok{1}\NormalTok{,]}
\NormalTok{best_model <-}\StringTok{ }\KeywordTok{h2o.getModel}\NormalTok{(grid}\OperatorTok{@}\NormalTok{model_ids[[}\DecValTok{1}\NormalTok{]]) }\CommentTok{## model with lowest logloss}
\NormalTok{best_model}
\end{Highlighting}
\end{Shaded}

Let's look at the model with the lowest validation misclassification rate:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{grid <-}\StringTok{ }\KeywordTok{h2o.getGrid}\NormalTok{(}\StringTok{"dl_grid"}\NormalTok{,}\DataTypeTok{sort_by=}\StringTok{"err"}\NormalTok{,}\DataTypeTok{decreasing=}\OtherTok{FALSE}\NormalTok{)}
\NormalTok{best_model <-}\StringTok{ }\KeywordTok{h2o.getModel}\NormalTok{(grid}\OperatorTok{@}\NormalTok{model_ids[[}\DecValTok{1}\NormalTok{]]) }\CommentTok{## model with lowest classification error (on validation, since it was available during training)}
\KeywordTok{h2o.confusionMatrix}\NormalTok{(best_model,}\DataTypeTok{valid=}\NormalTok{T)}
\CommentTok{#> Confusion Matrix: Row labels: Actual class; Column labels: Predicted class}
\CommentTok{#>         class_1 class_2 class_3 class_4 class_5 class_6 class_7  Error}
\CommentTok{#> class_1    2631     993       0       0       3       0      46 0.2837}
\CommentTok{#> class_2     701    3995     100       0      54      13       7 0.1797}
\CommentTok{#> class_3       0      51     589       2       1       7       0 0.0938}
\CommentTok{#> class_4       0       0      36      13       0       0       0 0.7347}
\CommentTok{#> class_5       2     113      13       0      39       0       0 0.7665}
\CommentTok{#> class_6       0      67     201       1       1      27       0 0.9091}
\CommentTok{#> class_7     118       0       1       0       0       0     234 0.3371}
\CommentTok{#> Totals     3452    5219     940      16      98      47     287 0.2516}
\CommentTok{#>                     Rate}
\CommentTok{#> class_1 =  1,042 / 3,673}
\CommentTok{#> class_2 =    875 / 4,870}
\CommentTok{#> class_3 =       61 / 650}
\CommentTok{#> class_4 =        36 / 49}
\CommentTok{#> class_5 =      128 / 167}
\CommentTok{#> class_6 =      270 / 297}
\CommentTok{#> class_7 =      119 / 353}
\CommentTok{#> Totals  = 2,531 / 10,059}
\NormalTok{best_params <-}\StringTok{ }\NormalTok{best_model}\OperatorTok{@}\NormalTok{allparameters}
\NormalTok{best_params}\OperatorTok{$}\NormalTok{activation}
\CommentTok{#> [1] "Rectifier"}
\NormalTok{best_params}\OperatorTok{$}\NormalTok{hidden}
\CommentTok{#> [1] 32 32 32}
\NormalTok{best_params}\OperatorTok{$}\NormalTok{input_dropout_ratio}
\CommentTok{#> [1] 0.05}
\NormalTok{best_params}\OperatorTok{$}\NormalTok{l1}
\CommentTok{#> [1] 1e-05}
\NormalTok{best_params}\OperatorTok{$}\NormalTok{l2}
\CommentTok{#> [1] 1e-05}
\end{Highlighting}
\end{Shaded}

\hypertarget{checkpointing}{%
\subsection{Checkpointing}\label{checkpointing}}

Let's continue training the manually tuned model from before, for 2 more epochs. Note that since many important parameters such as epochs, l1, l2, max\_w2, score\_interval, train\_samples\_per\_iteration, input\_dropout\_ratio, hidden\_dropout\_ratios, score\_duty\_cycle, classification\_stop, regression\_stop, variable\_importances, force\_load\_balance can be modified between checkpoint restarts, it is best to specify as many parameters as possible explicitly.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{max_epochs <-}\StringTok{ }\DecValTok{12} \CommentTok{## Add two more epochs}
\NormalTok{m_cont <-}\StringTok{ }\KeywordTok{h2o.deeplearning}\NormalTok{(}
  \DataTypeTok{model_id=}\StringTok{"dl_model_tuned_continued"}\NormalTok{, }
  \DataTypeTok{checkpoint=}\StringTok{"dl_model_tuned"}\NormalTok{, }
  \DataTypeTok{training_frame=}\NormalTok{train, }
  \DataTypeTok{validation_frame=}\NormalTok{valid, }
  \DataTypeTok{x=}\NormalTok{predictors, }
  \DataTypeTok{y=}\NormalTok{response, }
  \DataTypeTok{hidden=}\KeywordTok{c}\NormalTok{(}\DecValTok{128}\NormalTok{,}\DecValTok{128}\NormalTok{,}\DecValTok{128}\NormalTok{),          }\CommentTok{## more hidden layers -> more complex interactions}
  \DataTypeTok{epochs=}\NormalTok{max_epochs,              }\CommentTok{## hopefully long enough to converge (otherwise restart again)}
  \DataTypeTok{stopping_metric=}\StringTok{"logloss"}\NormalTok{,      }\CommentTok{## logloss is directly optimized by Deep Learning}
  \DataTypeTok{stopping_tolerance=}\FloatTok{1e-2}\NormalTok{,        }\CommentTok{## stop when validation logloss does not improve by >=1% for 2 scoring events}
  \DataTypeTok{stopping_rounds=}\DecValTok{2}\NormalTok{,}
  \DataTypeTok{score_validation_samples=}\DecValTok{10000}\NormalTok{, }\CommentTok{## downsample validation set for faster scoring}
  \DataTypeTok{score_duty_cycle=}\FloatTok{0.025}\NormalTok{,         }\CommentTok{## don't score more than 2.5% of the wall time}
  \DataTypeTok{adaptive_rate=}\NormalTok{F,                }\CommentTok{## manually tuned learning rate}
  \DataTypeTok{rate=}\FloatTok{0.01}\NormalTok{, }
  \DataTypeTok{rate_annealing=}\FloatTok{2e-6}\NormalTok{,            }
  \DataTypeTok{momentum_start=}\FloatTok{0.2}\NormalTok{,             }\CommentTok{## manually tuned momentum}
  \DataTypeTok{momentum_stable=}\FloatTok{0.4}\NormalTok{, }
  \DataTypeTok{momentum_ramp=}\FloatTok{1e7}\NormalTok{, }
  \DataTypeTok{l1=}\FloatTok{1e-5}\NormalTok{,                        }\CommentTok{## add some L1/L2 regularization}
  \DataTypeTok{l2=}\FloatTok{1e-5}\NormalTok{,}
  \DataTypeTok{max_w2=}\DecValTok{10}                       \CommentTok{## helps stability for Rectifier}
\NormalTok{) }
\KeywordTok{summary}\NormalTok{(m_cont)}
\KeywordTok{plot}\NormalTok{(m_cont)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{nn-comparison-classification_regression_with_h2o_deep_learning_files/figure-latex/checpointing-1} \end{center}

Once we are satisfied with the results, we can save the model to disk (on the cluster). In this example, we store the model in a directory called \texttt{mybest\_deeplearning\_covtype\_model}, which will be created for us since force=TRUE.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{path <-}\StringTok{ }\KeywordTok{h2o.saveModel}\NormalTok{(m_cont, }
          \DataTypeTok{path =} \KeywordTok{file.path}\NormalTok{(data_out_dir, }\StringTok{"mybest_deeplearning_covtype_model"}\NormalTok{), }\DataTypeTok{force=}\OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

It can be loaded later with the following command:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{print}\NormalTok{(path)}
\CommentTok{#> [1] "/home/datascience/repos/machine-learning-rsuite/export/mybest_deeplearning_covtype_model/dl_model_tuned_continued"}
\NormalTok{m_loaded <-}\StringTok{ }\KeywordTok{h2o.loadModel}\NormalTok{(path)}
\KeywordTok{summary}\NormalTok{(m_loaded)}
\CommentTok{#> Model Details:}
\CommentTok{#> ==============}
\CommentTok{#> }
\CommentTok{#> H2OMultinomialModel: deeplearning}
\CommentTok{#> Model Key:  dl_model_tuned_continued }
\CommentTok{#> Status of Neuron Layers: predicting Cover_Type, 7-class classification, multinomial distribution, CrossEntropy loss, 41,223 weights/biases, 334.1 KB, 3,901,469 training samples, mini-batch size 1}
\CommentTok{#>   layer units      type dropout       l1       l2 mean_rate rate_rms}
\CommentTok{#> 1     1    56     Input  0.00 %       NA       NA        NA       NA}
\CommentTok{#> 2     2   128 Rectifier  0.00 % 0.000010 0.000010  0.001136 0.000000}
\CommentTok{#> 3     3   128 Rectifier  0.00 % 0.000010 0.000010  0.001136 0.000000}
\CommentTok{#> 4     4   128 Rectifier  0.00 % 0.000010 0.000010  0.001136 0.000000}
\CommentTok{#> 5     5     7   Softmax      NA 0.000010 0.000010  0.001136 0.000000}
\CommentTok{#>   momentum mean_weight weight_rms mean_bias bias_rms}
\CommentTok{#> 1       NA          NA         NA        NA       NA}
\CommentTok{#> 2 0.278029   -0.011299   0.320373 -0.009410 0.334944}
\CommentTok{#> 3 0.278029   -0.058611   0.226093  0.858487 0.365056}
\CommentTok{#> 4 0.278029   -0.060740   0.218582  0.776620 0.201415}
\CommentTok{#> 5 0.278029   -0.027095   0.268963  0.042468 0.944277}
\CommentTok{#> }
\CommentTok{#> H2OMultinomialMetrics: deeplearning}
\CommentTok{#> ** Reported on training data. **}
\CommentTok{#> ** Metrics reported on temporary training frame with 10112 samples **}
\CommentTok{#> }
\CommentTok{#> Training Set Metrics: }
\CommentTok{#> =====================}
\CommentTok{#> }
\CommentTok{#> MSE: (Extract with `h2o.mse`) 0.0545}
\CommentTok{#> RMSE: (Extract with `h2o.rmse`) 0.233}
\CommentTok{#> Logloss: (Extract with `h2o.logloss`) 0.182}
\CommentTok{#> Mean Per-Class Error: 0.117}
\CommentTok{#> Confusion Matrix: Extract with `h2o.confusionMatrix(<model>,train = TRUE)`)}
\CommentTok{#> =========================================================================}
\CommentTok{#> Confusion Matrix: Row labels: Actual class; Column labels: Predicted class}
\CommentTok{#>         class_1 class_2 class_3 class_4 class_5 class_6 class_7  Error}
\CommentTok{#> class_1    3447     309       0       0       1       0      12 0.0854}
\CommentTok{#> class_2     213    4586      14       0      16       8       1 0.0521}
\CommentTok{#> class_3       0       8     519       7       1      31       0 0.0830}
\CommentTok{#> class_4       0       0       7      42       0       2       0 0.1765}
\CommentTok{#> class_5       1      33       4       0     135       2       0 0.2286}
\CommentTok{#> class_6       0      17      18       1       0     283       0 0.1129}
\CommentTok{#> class_7      28       4       0       0       0       0     362 0.0812}
\CommentTok{#> Totals     3689    4957     562      50     153     326     375 0.0730}
\CommentTok{#>                   Rate}
\CommentTok{#> class_1 =  322 / 3,769}
\CommentTok{#> class_2 =  252 / 4,838}
\CommentTok{#> class_3 =     47 / 566}
\CommentTok{#> class_4 =       9 / 51}
\CommentTok{#> class_5 =     40 / 175}
\CommentTok{#> class_6 =     36 / 319}
\CommentTok{#> class_7 =     32 / 394}
\CommentTok{#> Totals  = 738 / 10,112}
\CommentTok{#> }
\CommentTok{#> Hit Ratio Table: Extract with `h2o.hit_ratio_table(<model>,train = TRUE)`}
\CommentTok{#> =======================================================================}
\CommentTok{#> Top-7 Hit Ratios: }
\CommentTok{#>   k hit_ratio}
\CommentTok{#> 1 1  0.927017}
\CommentTok{#> 2 2  0.996934}
\CommentTok{#> 3 3  0.999901}
\CommentTok{#> 4 4  1.000000}
\CommentTok{#> 5 5  1.000000}
\CommentTok{#> 6 6  1.000000}
\CommentTok{#> 7 7  1.000000}
\CommentTok{#> }
\CommentTok{#> }
\CommentTok{#> H2OMultinomialMetrics: deeplearning}
\CommentTok{#> ** Reported on validation data. **}
\CommentTok{#> ** Metrics reported on temporary validation frame with 10022 samples **}
\CommentTok{#> }
\CommentTok{#> Validation Set Metrics: }
\CommentTok{#> =====================}
\CommentTok{#> }
\CommentTok{#> MSE: (Extract with `h2o.mse`) 0.0636}
\CommentTok{#> RMSE: (Extract with `h2o.rmse`) 0.252}
\CommentTok{#> Logloss: (Extract with `h2o.logloss`) 0.211}
\CommentTok{#> Mean Per-Class Error: 0.145}
\CommentTok{#> Confusion Matrix: Extract with `h2o.confusionMatrix(<model>,valid = TRUE)`)}
\CommentTok{#> =========================================================================}
\CommentTok{#> Confusion Matrix: Row labels: Actual class; Column labels: Predicted class}
\CommentTok{#>         class_1 class_2 class_3 class_4 class_5 class_6 class_7  Error}
\CommentTok{#> class_1    3322     331       0       0       3       0      19 0.0961}
\CommentTok{#> class_2     228    4579       9       0      17      17       2 0.0563}
\CommentTok{#> class_3       0      19     531       7       0      45       0 0.1179}
\CommentTok{#> class_4       0       0       5      44       0       6       0 0.2000}
\CommentTok{#> class_5       4      40       6       0     114       2       0 0.3133}
\CommentTok{#> class_6       1      15      23       0       0     261       0 0.1300}
\CommentTok{#> class_7      32       6       0       0       0       0     334 0.1022}
\CommentTok{#> Totals     3587    4990     574      51     134     331     355 0.0835}
\CommentTok{#>                   Rate}
\CommentTok{#> class_1 =  353 / 3,675}
\CommentTok{#> class_2 =  273 / 4,852}
\CommentTok{#> class_3 =     71 / 602}
\CommentTok{#> class_4 =      11 / 55}
\CommentTok{#> class_5 =     52 / 166}
\CommentTok{#> class_6 =     39 / 300}
\CommentTok{#> class_7 =     38 / 372}
\CommentTok{#> Totals  = 837 / 10,022}
\CommentTok{#> }
\CommentTok{#> Hit Ratio Table: Extract with `h2o.hit_ratio_table(<model>,valid = TRUE)`}
\CommentTok{#> =======================================================================}
\CommentTok{#> Top-7 Hit Ratios: }
\CommentTok{#>   k hit_ratio}
\CommentTok{#> 1 1  0.916484}
\CommentTok{#> 2 2  0.995111}
\CommentTok{#> 3 3  0.999701}
\CommentTok{#> 4 4  0.999900}
\CommentTok{#> 5 5  1.000000}
\CommentTok{#> 6 6  1.000000}
\CommentTok{#> 7 7  1.000000}
\CommentTok{#> }
\CommentTok{#> }
\CommentTok{#> }
\CommentTok{#> }
\CommentTok{#> Scoring History: }
\CommentTok{#>              timestamp          duration training_speed   epochs}
\CommentTok{#> 1  2019-09-20 15:13:38         0.000 sec             NA  0.00000}
\CommentTok{#> 2  2019-09-20 15:13:44         5.558 sec  18903 obs/sec  0.28560}
\CommentTok{#> 3  2019-09-20 15:13:55        16.864 sec  24391 obs/sec  1.14643}
\CommentTok{#> 4  2019-09-20 15:14:05        26.830 sec  26703 obs/sec  2.00354}
\CommentTok{#> 5  2019-09-20 15:14:15        36.575 sec  27964 obs/sec  2.86512}
\CommentTok{#> 6  2019-09-20 15:14:24        46.047 sec  28855 obs/sec  3.72548}
\CommentTok{#> 7  2019-09-20 15:14:32        54.163 sec  28317 obs/sec  4.30076}
\CommentTok{#> 8  2019-09-20 15:14:42  1 min  3.539 sec  28966 obs/sec  5.16261}
\CommentTok{#> 9  2019-09-20 15:14:51  1 min 13.283 sec  29289 obs/sec  6.02348}
\CommentTok{#> 10 2019-09-20 15:15:01  1 min 23.203 sec  29472 obs/sec  6.88366}
\CommentTok{#> 11 2019-09-20 15:15:11  1 min 32.510 sec  29801 obs/sec  7.74054}
\CommentTok{#> 12 2019-09-20 15:15:20  1 min 41.827 sec  30082 obs/sec  8.60118}
\CommentTok{#> 13 2019-09-20 15:15:29  1 min 51.334 sec  30256 obs/sec  9.46029}
\CommentTok{#> 14 2019-09-20 15:15:36  1 min 57.593 sec  30395 obs/sec 10.03336}
\CommentTok{#> 15 2019-09-20 15:16:43  2 min  0.850 sec  30469 obs/sec 10.31960}
\CommentTok{#> 16 2019-09-20 15:16:53  2 min  9.993 sec  30677 obs/sec 11.17851}
\CommentTok{#>    iterations        samples training_rmse training_logloss training_r2}
\CommentTok{#> 1           0       0.000000            NA               NA          NA}
\CommentTok{#> 2           1   99677.000000       0.42954          0.57028     0.90760}
\CommentTok{#> 3           4  400123.000000       0.35428          0.39831     0.93714}
\CommentTok{#> 4           7  699264.000000       0.32503          0.33535     0.94709}
\CommentTok{#> 5          10  999969.000000       0.30876          0.30346     0.95226}
\CommentTok{#> 6          13 1300249.000000       0.28809          0.26678     0.95844}
\CommentTok{#> 7          15 1501031.000000       0.28144          0.25617     0.96033}
\CommentTok{#> 8          18 1801828.000000       0.26740          0.23221     0.96419}
\CommentTok{#> 9          21 2102286.000000       0.26200          0.22176     0.96562}
\CommentTok{#> 10         24 2402502.000000       0.25772          0.21608     0.96674}
\CommentTok{#> 11         27 2701563.000000       0.25290          0.20629     0.96797}
\CommentTok{#> 12         30 3001941.000000       0.24526          0.19643     0.96988}
\CommentTok{#> 13         33 3301782.000000       0.23535          0.18278     0.97226}
\CommentTok{#> 14         35 3501792.000000       0.23277          0.17826     0.97286}
\CommentTok{#> 15         36 3601696.000000       0.24007          0.19019     0.97228}
\CommentTok{#> 16         39 3901469.000000       0.23335          0.18201     0.97381}
\CommentTok{#>    training_classification_error validation_rmse validation_logloss}
\CommentTok{#> 1                             NA              NA                 NA}
\CommentTok{#> 2                        0.24814         0.43012            0.57466}
\CommentTok{#> 3                        0.17029         0.36301            0.41871}
\CommentTok{#> 4                        0.14243         0.33326            0.35389}
\CommentTok{#> 5                        0.12945         0.31775            0.32413}
\CommentTok{#> 6                        0.11275         0.29831            0.28753}
\CommentTok{#> 7                        0.10682         0.29290            0.27746}
\CommentTok{#> 8                        0.09747         0.28134            0.25833}
\CommentTok{#> 9                        0.09153         0.27138            0.24210}
\CommentTok{#> 10                       0.09053         0.26804            0.23534}
\CommentTok{#> 11                       0.08892         0.26551            0.23446}
\CommentTok{#> 12                       0.08389         0.25881            0.22341}
\CommentTok{#> 13                       0.07594         0.25030            0.20947}
\CommentTok{#> 14                       0.07232         0.25255            0.21251}
\CommentTok{#> 15                       0.07773         0.25827            0.22061}
\CommentTok{#> 16                       0.07298         0.25216            0.21114}
\CommentTok{#>    validation_r2 validation_classification_error}
\CommentTok{#> 1             NA                              NA}
\CommentTok{#> 2        0.90498                         0.24647}
\CommentTok{#> 3        0.93232                         0.17758}
\CommentTok{#> 4        0.94296                         0.15061}
\CommentTok{#> 5        0.94814                         0.13787}
\CommentTok{#> 6        0.95429                         0.11822}
\CommentTok{#> 7        0.95594                         0.11822}
\CommentTok{#> 8        0.95935                         0.10809}
\CommentTok{#> 9        0.96217                         0.09877}
\CommentTok{#> 10       0.96310                         0.09646}
\CommentTok{#> 11       0.96379                         0.09365}
\CommentTok{#> 12       0.96560                         0.09044}
\CommentTok{#> 13       0.96782                         0.08202}
\CommentTok{#> 14       0.96724                         0.08443}
\CommentTok{#> 15       0.96667                         0.08831}
\CommentTok{#> 16       0.96823                         0.08352}
\CommentTok{#> }
\CommentTok{#> Variable Importances: (Extract with `h2o.varimp`) }
\CommentTok{#> =================================================}
\CommentTok{#> }
\CommentTok{#> Variable Importances: }
\CommentTok{#>                             variable relative_importance scaled_importance}
\CommentTok{#> 1                          Elevation            1.000000          1.000000}
\CommentTok{#> 2    Horizontal_Distance_To_Roadways            0.936633          0.936633}
\CommentTok{#> 3 Horizontal_Distance_To_Fire_Points            0.923248          0.923248}
\CommentTok{#> 4             Wilderness_Area.area_0            0.661243          0.661243}
\CommentTok{#> 5             Wilderness_Area.area_2            0.596564          0.596564}
\CommentTok{#>   percentage}
\CommentTok{#> 1   0.048200}
\CommentTok{#> 2   0.045145}
\CommentTok{#> 3   0.044500}
\CommentTok{#> 4   0.031872}
\CommentTok{#> 5   0.028754}
\CommentTok{#> }
\CommentTok{#> ---}
\CommentTok{#>                       variable relative_importance scaled_importance}
\CommentTok{#> 51           Soil_Type.type_13            0.163302          0.163302}
\CommentTok{#> 52           Soil_Type.type_14            0.152124          0.152124}
\CommentTok{#> 53            Soil_Type.type_6            0.151641          0.151641}
\CommentTok{#> 54           Soil_Type.type_35            0.149299          0.149299}
\CommentTok{#> 55       Soil_Type.missing(NA)            0.000000          0.000000}
\CommentTok{#> 56 Wilderness_Area.missing(NA)            0.000000          0.000000}
\CommentTok{#>    percentage}
\CommentTok{#> 51   0.007871}
\CommentTok{#> 52   0.007332}
\CommentTok{#> 53   0.007309}
\CommentTok{#> 54   0.007196}
\CommentTok{#> 55   0.000000}
\CommentTok{#> 56   0.000000}
\end{Highlighting}
\end{Shaded}

This model is fully functional and can be inspected, restarted, or used to score a dataset, etc. Note that binary compatibility between H2O versions is currently not guaranteed.

\hypertarget{cross-validation}{%
\subsection{Cross-Validation}\label{cross-validation}}

For N-fold cross-validation, specify \texttt{nfolds\textgreater{}1} instead of (or in addition to) a validation frame, and \texttt{N+1} models will be built: 1 model on the full training data, and N models with each 1/N-th of the data held out (there are different holdout strategies). Those N models then score on the held out data, and their combined predictions on the full training data are scored to get the cross-validation metrics.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dlmodel <-}\StringTok{ }\KeywordTok{h2o.deeplearning}\NormalTok{(}
  \DataTypeTok{x=}\NormalTok{predictors,}
  \DataTypeTok{y=}\NormalTok{response, }
  \DataTypeTok{training_frame=}\NormalTok{train,}
  \DataTypeTok{hidden=}\KeywordTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{,}\DecValTok{10}\NormalTok{),}
  \DataTypeTok{epochs=}\DecValTok{1}\NormalTok{,}
  \DataTypeTok{nfolds=}\DecValTok{5}\NormalTok{,}
  \DataTypeTok{fold_assignment=}\StringTok{"Modulo"} \CommentTok{# can be "AUTO", "Modulo", "Random" or "Stratified"}
\NormalTok{  )}
\NormalTok{dlmodel}
\end{Highlighting}
\end{Shaded}

N-fold cross-validation is especially useful with early stopping, as the main model will pick the ideal number of epochs from the convergence behavior of the cross-validation models.

\hypertarget{regression-and-binary-classification}{%
\section{Regression and Binary Classification}\label{regression-and-binary-classification}}

Assume we want to turn the multi-class problem above into a binary classification problem. We create a binary response as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{train}\OperatorTok{$}\NormalTok{bin_response <-}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(train[,response] }\OperatorTok{==}\StringTok{ "class_1"}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Let's build a quick model and inspect the model:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dlmodel <-}\StringTok{ }\KeywordTok{h2o.deeplearning}\NormalTok{(}
  \DataTypeTok{x=}\NormalTok{predictors,}
  \DataTypeTok{y=}\StringTok{"bin_response"}\NormalTok{, }
  \DataTypeTok{training_frame=}\NormalTok{train,}
  \DataTypeTok{hidden=}\KeywordTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{,}\DecValTok{10}\NormalTok{),}
  \DataTypeTok{epochs=}\FloatTok{0.1}
\NormalTok{)}
\KeywordTok{summary}\NormalTok{(dlmodel)}
\end{Highlighting}
\end{Shaded}

Instead of a binary classification model, we find a regression model (H2ORegressionModel) that contains only 1 output neuron (instead of 2). The reason is that the response was a numerical feature (ordinal numbers 0 and 1), and H2O Deep Learning was run with distribution=AUTO, which defaulted to a Gaussian regression problem for a real-valued response. H2O Deep Learning supports regression for distributions other than Gaussian such as Poisson, Gamma, Tweedie, Laplace. It also supports Huber loss and per-row offsets specified via an offset\_column. We refer to our H2O Deep Learning regression code examples for more information.

To perform classification, the response must first be turned into a categorical (factor) feature:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{train}\OperatorTok{$}\NormalTok{bin_response <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(train}\OperatorTok{$}\NormalTok{bin_response) }\CommentTok{##make categorical}
\NormalTok{dlmodel <-}\StringTok{ }\KeywordTok{h2o.deeplearning}\NormalTok{(}
  \DataTypeTok{x=}\NormalTok{predictors,}
  \DataTypeTok{y=}\StringTok{"bin_response"}\NormalTok{, }
  \DataTypeTok{training_frame=}\NormalTok{train,}
  \DataTypeTok{hidden=}\KeywordTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{,}\DecValTok{10}\NormalTok{),}
  \DataTypeTok{epochs=}\FloatTok{0.1}
  \CommentTok{#balance_classes=T    ## enable this for high class imbalance}
\NormalTok{)}
\KeywordTok{summary}\NormalTok{(dlmodel) }\CommentTok{## Now the model metrics contain AUC for binary classification}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{h2o.performance}\NormalTok{(dlmodel)) }\CommentTok{## display ROC curve}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{nn-comparison-classification_regression_with_h2o_deep_learning_files/figure-latex/bin_class_as_factor-1} \end{center}

Now the model performs (binary) classification, and has multiple (2) output neurons.

\hypertarget{unsupervised-anomaly-detection}{%
\section{Unsupervised Anomaly detection}\label{unsupervised-anomaly-detection}}

For instructions on how to build unsupervised models with H2O Deep Learning, we refer to our previous Tutorial on Anomaly Detection with H2O Deep Learning and our MNIST Anomaly detection code example, as well as our Stacked AutoEncoder R code example and another one for Unsupervised Pretraining with an AutoEncoder R code example.

\hypertarget{h2o-deep-learning-tips-tricks}{%
\section{H2O Deep Learning Tips \& Tricks}\label{h2o-deep-learning-tips-tricks}}

\hypertarget{performance-tuning}{%
\subsection{Performance Tuning}\label{performance-tuning}}

The Definitive H2O Deep Learning Performance Tuning blog post covers many of the following points that affect the computational efficiency, so it's highly recommended.

\hypertarget{activation-functions}{%
\subsection{Activation Functions}\label{activation-functions}}

While sigmoids have been used historically for neural networks, H2O Deep Learning implements Tanh, a scaled and shifted variant of the sigmoid which is symmetric around 0. Since its output values are bounded by -1..1, the stability of the neural network is rarely endangered. However, the derivative of the tanh function is always non-zero and back-propagation (training) of the weights is more computationally expensive than for rectified linear units, or Rectifier, which is max(0,x) and has vanishing gradient for x\textless{}=0, leading to much faster training speed for large networks and is often the fastest path to accuracy on larger problems. In case you encounter instabilities with the Rectifier (in which case model building is automatically aborted), try a limited value to re-scale the weights: max\_w2=10. The Maxout activation function is computationally more expensive, but can lead to higher accuracy. It is a generalized version of the Rectifier with two non-zero channels. In practice, the Rectifier (and RectifierWithDropout, see below) is the most versatile and performant option for most problems.

\hypertarget{generalization-techniques}{%
\subsection{Generalization Techniques}\label{generalization-techniques}}

L1 and L2 penalties can be applied by specifying the l1 and l2 parameters. Intuition: L1 lets only strong weights survive (constant pulling force towards zero), while L2 prevents any single weight from getting too big. Dropout has recently been introduced as a powerful generalization technique, and is available as a parameter per layer, including the input layer. \texttt{input\_dropout\_ratio} controls the amount of input layer neurons that are randomly dropped (set to zero), while \texttt{hidden\_dropout\_ratios} are specified for each hidden layer. The former controls overfitting with respect to the input data (useful for high-dimensional noisy data), while the latter controls overfitting of the learned features. Note that \texttt{hidden\_dropout\_ratios} require the activation function to end with \ldots{}WithDropout.

\hypertarget{early-stopping-and-optimizing-for-lowest-validation-error}{%
\subsection{Early stopping and optimizing for lowest validation error}\label{early-stopping-and-optimizing-for-lowest-validation-error}}

By default, Deep Learning training stops when the stopping\_metric does not improve by at least stopping\_tolerance (0.01 means 1\% improvement) for stopping\_rounds consecutive scoring events on the training (or validation) data. By default, \texttt{overwrite\_with\_best\_model} is enabled and the model returned after training for the specified number of epochs (or after stopping early due to convergence) is the model that has the best training set error (according to the metric specified by stopping\_metric), or, if a validation set is provided, the lowest validation set error. Note that the training or validation set errors can be based on a subset of the training or validation data, depending on the values for \texttt{score\_validation\_samples} or \texttt{score\_training\_samples}, see below. For early stopping on a predefined error rate on the training data (accuracy for classification or MSE for regression), specify \texttt{classification\_stop} or \texttt{regression\_stop.}

\hypertarget{training-samples-per-mapreduce-iteration}{%
\subsection{Training Samples per MapReduce Iteration}\label{training-samples-per-mapreduce-iteration}}

The parameter \texttt{train\_samples\_per\_iteration} matters especially in multi-node operation. It controls the number of rows trained on for each \texttt{MapReduce} iteration. Depending on the value selected, one MapReduce pass can sample observations, and multiple such passes are needed to train for one epoch. All H2O compute nodes then communicate to agree on the best model coefficients (weights/biases) so far, and the model may then be scored (controlled by other parameters below). The default value of -2 indicates auto-tuning, which attemps to keep the communication overhead at 5\% of the total runtime. The parameter \texttt{target\_ratio\_comm\_to\_comp} controls this ratio. This parameter is explained in more detail in the H2O Deep Learning booklet,

\hypertarget{categorical-data}{%
\subsection{Categorical Data}\label{categorical-data}}

For categorical data, a feature with K factor levels is automatically one-hot encoded (horizontalized) into \texttt{K-1} input neurons. Hence, the input neuron layer can grow substantially for datasets with high factor counts. In these cases, it might make sense to reduce the number of hidden neurons in the first hidden layer, such that large numbers of factor levels can be handled. In the limit of 1 neuron in the first hidden layer, the resulting model is similar to logistic regression with stochastic gradient descent, except that for classification problems, there's still a softmax output layer, and that the activation function is not necessarily a sigmoid (Tanh). If variable importances are computed, it is recommended to turn on \texttt{use\_all\_factor\_levels} (K input neurons for K levels). The experimental option max\_categorical\_features uses feature hashing to reduce the number of input neurons via the hash trick at the expense of hash collisions and reduced accuracy. Another way to reduce the dimensionality of the (categorical) features is to use \texttt{h2o.glrm()}, we refer to the GLRM tutorial for more details.

\hypertarget{sparse-data}{%
\subsection{Sparse Data}\label{sparse-data}}

If the input data is sparse (many zeros), then it might make sense to enable the sparse option. This will result in the input not being standardized (0 mean, 1 variance), but only de-scaled (1 variance) and 0 values remain 0, leading to more efficient back-propagation. Sparsity is also a reason why CPU implementations can be faster than GPU implementations, because they can take advantage of if/else statements more effectively.

\hypertarget{missing-values}{%
\subsection{Missing Values}\label{missing-values}}

H2O Deep Learning automatically does mean imputation for missing values during training (leaving the input layer activation at 0 after standardizing the values). For testing, missing test set values are also treated the same way by default. See the \texttt{h2o.impute} function to do your own mean imputation.

\hypertarget{loss-functions-distributions-offsets-observation-weights}{%
\subsection{Loss functions, Distributions, Offsets, Observation Weights}\label{loss-functions-distributions-offsets-observation-weights}}

H2O Deep Learning supports advanced statistical features such as multiple loss functions, non-Gaussian distributions, per-row offsets and observation weights. In addition to Gaussian distributions and Squared loss, H2O Deep Learning supports Poisson, Gamma, Tweedie and Laplace distributions. It also supports Absolute and Huber loss and per-row offsets specified via an \texttt{offset\_column.} Observation weights are supported via a user-specified weights\_column.

We refer to our H2O Deep Learning R test code examples for more information.

\hypertarget{exporting-weights-and-biases}{%
\subsection{Exporting Weights and Biases}\label{exporting-weights-and-biases}}

The model parameters (weights connecting two adjacent layers and per-neuron bias terms) can be stored as H2O Frames (like a dataset) by enabling \texttt{export\_weights\_and\_biases}, and they can be accessed as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iris_dl <-}\StringTok{ }\KeywordTok{h2o.deeplearning}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{4}\NormalTok{,}\DecValTok{5}\NormalTok{,}\KeywordTok{as.h2o}\NormalTok{(iris),}
             \DataTypeTok{export_weights_and_biases=}\NormalTok{T)}
\CommentTok{#> }
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|}\StringTok{                                                                 }\ErrorTok{|}\StringTok{   }\DecValTok{0}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=================================================================|}\StringTok{ }\DecValTok{100}\NormalTok{%}
\CommentTok{#> }
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|}\StringTok{                                                                 }\ErrorTok{|}\StringTok{   }\DecValTok{0}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=================================================================|}\StringTok{ }\DecValTok{100}\NormalTok{%}
\KeywordTok{h2o.weights}\NormalTok{(iris_dl, }\DataTypeTok{matrix_id=}\DecValTok{1}\NormalTok{)}
\CommentTok{#>   Sepal.Length Sepal.Width Petal.Length Petal.Width}
\CommentTok{#> 1      0.06976     0.09806      -0.0707      0.1154}
\CommentTok{#> 2      0.08570     0.16861       0.1448     -0.0958}
\CommentTok{#> 3      0.00658     0.05867       0.1243      0.1650}
\CommentTok{#> 4     -0.09349    -0.09891       0.0570     -0.1430}
\CommentTok{#> 5      0.14242     0.09565      -0.1405     -0.1648}
\CommentTok{#> 6      0.04994     0.00555       0.0950     -0.1218}
\CommentTok{#> }
\CommentTok{#> [200 rows x 4 columns]}
\KeywordTok{h2o.weights}\NormalTok{(iris_dl, }\DataTypeTok{matrix_id=}\DecValTok{2}\NormalTok{)}
\CommentTok{#>         C1       C2      C3       C4       C5        C6       C7      C8}
\CommentTok{#> 1 -0.00377 -0.04174 -0.0131 -0.11203 -0.10178 -0.117465 -0.11005  0.0344}
\CommentTok{#> 2 -0.08431 -0.08589 -0.0572  0.10628  0.00912 -0.038027 -0.05764  0.0590}
\CommentTok{#> 3  0.10611  0.00562 -0.0576 -0.01460  0.07344 -0.026180  0.00163  0.0257}
\CommentTok{#> 4 -0.06380 -0.09231 -0.0759 -0.02595 -0.00754  0.018112  0.11962  0.0780}
\CommentTok{#> 5 -0.11794 -0.03649 -0.0553  0.10858 -0.10098 -0.062671  0.05638  0.0626}
\CommentTok{#> 6 -0.04597  0.11525  0.0407  0.00922  0.08802 -0.000307 -0.02639 -0.0731}
\CommentTok{#>        C9     C10      C11      C12     C13     C14     C15     C16}
\CommentTok{#> 1 -0.0719 -0.0156  0.08596 -0.08704 -0.0714  0.0415 -0.0745  0.1037}
\CommentTok{#> 2 -0.0154  0.0699 -0.03310  0.09892 -0.0858 -0.0330  0.0861 -0.0398}
\CommentTok{#> 3  0.0275  0.0296 -0.05978 -0.07290 -0.0615  0.0363  0.0955  0.0890}
\CommentTok{#> 4 -0.1055  0.0597  0.03338 -0.01178  0.0603  0.0960  0.0301  0.1220}
\CommentTok{#> 5 -0.0567 -0.1251 -0.01381 -0.00642 -0.0288  0.1130 -0.1244 -0.1071}
\CommentTok{#> 6  0.1005  0.1172  0.00832  0.08649 -0.0327  0.0391 -0.0455 -0.0942}
\CommentTok{#>       C17     C18      C19     C20     C21      C22     C23     C24}
\CommentTok{#> 1 -0.1051 -0.0525  0.07253  0.0787 -0.0465  0.04187  0.0279  0.0800}
\CommentTok{#> 2  0.0855  0.0679 -0.00729  0.0417  0.0978  0.10999 -0.1012 -0.0305}
\CommentTok{#> 3 -0.0704 -0.0866  0.03215  0.0869 -0.1067  0.02024 -0.0973  0.0783}
\CommentTok{#> 4  0.0330 -0.0194 -0.05555  0.0428 -0.0954  0.05979  0.0573  0.0539}
\CommentTok{#> 5 -0.1090 -0.0831 -0.03706 -0.0530 -0.0235 -0.12264  0.0427  0.1037}
\CommentTok{#> 6 -0.0308  0.0310 -0.11167 -0.0129 -0.1004 -0.00906  0.0633  0.0602}
\CommentTok{#>       C25     C26       C27     C28     C29     C30     C31     C32}
\CommentTok{#> 1  0.0167 -0.0499 -0.034779 -0.0710  0.0180 -0.0595  0.0946 -0.0316}
\CommentTok{#> 2  0.0839  0.0187  0.000504 -0.0454 -0.0916  0.0967 -0.0901  0.0432}
\CommentTok{#> 3  0.0721  0.0379  0.102506  0.1121  0.1058 -0.1099  0.0628 -0.0910}
\CommentTok{#> 4  0.0641  0.0237  0.077027 -0.1125  0.0620 -0.0906 -0.0512  0.0147}
\CommentTok{#> 5  0.0941 -0.1066  0.107501  0.0827 -0.0724 -0.0843 -0.0773  0.0129}
\CommentTok{#> 6 -0.1061 -0.0969 -0.101528 -0.0073  0.0169 -0.0509  0.0177 -0.0679}
\CommentTok{#>       C33     C34      C35      C36      C37     C38     C39      C40}
\CommentTok{#> 1 -0.0570  0.0888 -0.01204  0.06730 -0.06546 -0.0266 -0.1184  0.01854}
\CommentTok{#> 2 -0.0677 -0.1186 -0.08013 -0.00764 -0.11780 -0.0494 -0.0620 -0.05213}
\CommentTok{#> 3  0.0960  0.0788 -0.04687  0.07302  0.12392  0.0815  0.0560 -0.05292}
\CommentTok{#> 4  0.1125  0.0517 -0.00872  0.04115 -0.00382  0.0824  0.0392 -0.05460}
\CommentTok{#> 5  0.0139  0.0277 -0.02410  0.00903 -0.08751 -0.0754  0.0790  0.02989}
\CommentTok{#> 6  0.0363  0.1400  0.02019  0.00907  0.09339  0.0097 -0.0141 -0.00437}
\CommentTok{#>        C41      C42     C43      C44      C45     C46     C47     C48}
\CommentTok{#> 1  0.09932 -0.02824  0.0970  0.00485  0.00835  0.0440  0.0810  0.0598}
\CommentTok{#> 2  0.08903  0.08798  0.0588 -0.03962  0.10481  0.0551  0.0389  0.0122}
\CommentTok{#> 3  0.02747 -0.00184 -0.0964  0.05819  0.02018  0.0520 -0.0526 -0.0421}
\CommentTok{#> 4  0.04287 -0.01656 -0.1153  0.10227 -0.11010  0.0783 -0.0314 -0.1167}
\CommentTok{#> 5 -0.11639  0.09970 -0.1193 -0.06725  0.04144 -0.0547 -0.1028 -0.0169}
\CommentTok{#> 6 -0.00681 -0.07943  0.0253  0.10209 -0.01227  0.0629 -0.0904 -0.0914}
\CommentTok{#>       C49      C50     C51      C52     C53      C54     C55     C56}
\CommentTok{#> 1  0.0587 -0.05867  0.0337  0.03401  0.0675 -0.05641 -0.0141  0.1214}
\CommentTok{#> 2  0.0477  0.06714 -0.0233  0.05875  0.0935 -0.00106 -0.0182 -0.0347}
\CommentTok{#> 3  0.0222 -0.07296 -0.0826 -0.10406 -0.0464 -0.06580 -0.0774  0.0391}
\CommentTok{#> 4 -0.1055 -0.06009  0.1103  0.08981 -0.0563 -0.04074 -0.0624  0.0853}
\CommentTok{#> 5 -0.0229 -0.00823  0.0932  0.10623 -0.0812  0.04683  0.0532 -0.0178}
\CommentTok{#> 6 -0.0802 -0.00374  0.0684  0.00065  0.0636  0.10308 -0.0117  0.0543}
\CommentTok{#>        C57     C58      C59     C60     C61      C62      C63     C64}
\CommentTok{#> 1  0.06464 -0.0875 -0.12534 -0.1017 -0.0806 -0.08198 -0.00313  0.0640}
\CommentTok{#> 2  0.01718 -0.0647 -0.07944 -0.0268  0.0500 -0.02901 -0.00279 -0.0398}
\CommentTok{#> 3 -0.03813 -0.0395  0.08025  0.0453 -0.0255  0.00533 -0.07721  0.0114}
\CommentTok{#> 4  0.06052  0.0573  0.02448 -0.0264 -0.0363 -0.04703  0.08987 -0.0535}
\CommentTok{#> 5  0.00484 -0.0460  0.00258 -0.0911  0.0399 -0.12054 -0.07178  0.0154}
\CommentTok{#> 6  0.11060 -0.0931 -0.06056  0.0570  0.0529  0.12127 -0.10266 -0.0140}
\CommentTok{#>       C65       C66     C67     C68     C69      C70     C71     C72}
\CommentTok{#> 1  0.0216  0.035300  0.0281  0.0426  0.0322 -0.00837 -0.1069  0.0608}
\CommentTok{#> 2  0.1090 -0.000995  0.0376  0.0732 -0.0203  0.10940  0.1067 -0.0273}
\CommentTok{#> 3 -0.0977 -0.097374 -0.0208  0.0587  0.0490 -0.08462 -0.0824 -0.0486}
\CommentTok{#> 4  0.0378  0.065128  0.1088 -0.1001 -0.0971  0.08386  0.0326 -0.0570}
\CommentTok{#> 5  0.0185  0.091271  0.1000  0.1158 -0.1202 -0.05090  0.0707  0.0180}
\CommentTok{#> 6  0.0663 -0.111192  0.0964  0.1116 -0.0160 -0.00503  0.0740  0.0854}
\CommentTok{#>       C73       C74      C75     C76      C77     C78     C79     C80}
\CommentTok{#> 1  0.0353  0.118306 -0.09539 -0.0605 -0.00387  0.0236 -0.0154  0.1069}
\CommentTok{#> 2 -0.0631  0.035527  0.09136 -0.0700 -0.02152 -0.1022 -0.0560  0.0686}
\CommentTok{#> 3 -0.1112 -0.030857 -0.11710 -0.0237  0.04397 -0.0840  0.0929  0.0863}
\CommentTok{#> 4  0.0863  0.000256  0.05556  0.0333 -0.08189  0.0688 -0.0824  0.0224}
\CommentTok{#> 5 -0.0287  0.068762 -0.06296 -0.0755 -0.03693  0.0176  0.0614 -0.1055}
\CommentTok{#> 6 -0.0627  0.110967 -0.00217 -0.0892  0.05248 -0.0705 -0.1093 -0.0446}
\CommentTok{#>       C81     C82     C83       C84      C85      C86     C87     C88}
\CommentTok{#> 1  0.0542  0.1078  0.1213 -0.115944  0.03729  0.02979 -0.0250  0.0880}
\CommentTok{#> 2  0.0299  0.0497  0.0816  0.019713  0.00425  0.03484  0.0315 -0.0258}
\CommentTok{#> 3  0.1043  0.0244 -0.0530  0.000481  0.00506  0.06324  0.0825  0.0691}
\CommentTok{#> 4 -0.0484  0.0901  0.0515  0.015654  0.09953 -0.00674 -0.0828  0.1057}
\CommentTok{#> 5 -0.0716  0.0471  0.0958 -0.019437 -0.08262 -0.02651  0.0679  0.0807}
\CommentTok{#> 6 -0.0816 -0.0300 -0.0924  0.070163 -0.01492  0.09377  0.0246  0.0571}
\CommentTok{#>       C89      C90     C91     C92     C93     C94     C95      C96}
\CommentTok{#> 1  0.0498 -0.01297 -0.0171 -0.0204 -0.0764 -0.0347 -0.0546 -0.10222}
\CommentTok{#> 2 -0.0144 -0.07934  0.0909  0.0488 -0.0961  0.0903  0.1013  0.07162}
\CommentTok{#> 3 -0.0360 -0.10760  0.1190  0.0394 -0.0290  0.0266 -0.1170 -0.00287}
\CommentTok{#> 4 -0.0562 -0.00604 -0.1202 -0.0297  0.0437  0.1212 -0.1156 -0.05647}
\CommentTok{#> 5 -0.0703  0.07844 -0.0807  0.0149 -0.0989  0.1154 -0.0481 -0.01692}
\CommentTok{#> 6 -0.1082  0.05962 -0.0859 -0.0700 -0.1046 -0.0455  0.1083 -0.09186}
\CommentTok{#>       C97     C98     C99    C100     C101     C102     C103     C104}
\CommentTok{#> 1  0.0733  0.1105 -0.1022 -0.0742 -0.06339  0.07266 -0.08954  0.00428}
\CommentTok{#> 2  0.0581  0.0659  0.1192  0.0168 -0.00323 -0.03385 -0.03187  0.07758}
\CommentTok{#> 3 -0.0562  0.0710 -0.0384  0.0429 -0.01505  0.06142  0.11854 -0.01580}
\CommentTok{#> 4  0.0773 -0.1137  0.0731  0.1095 -0.11849 -0.00227  0.07316  0.12024}
\CommentTok{#> 5 -0.1221 -0.0893  0.0254  0.0702 -0.07969 -0.11174  0.08665  0.00380}
\CommentTok{#> 6  0.0301 -0.0726  0.0146 -0.1029  0.12458  0.10706 -0.00359 -0.11074}
\CommentTok{#>      C105    C106    C107  C108    C109    C110    C111     C112    C113}
\CommentTok{#> 1  0.0739  0.0101  0.0206 0.122  0.0517  0.0607  0.1118 -0.00823  0.0900}
\CommentTok{#> 2 -0.0223  0.1158 -0.1012 0.113  0.0891 -0.0750 -0.1070 -0.11625 -0.0659}
\CommentTok{#> 3  0.1099 -0.0961 -0.0473 0.126 -0.0257 -0.0269 -0.0307 -0.11603  0.0415}
\CommentTok{#> 4  0.0359 -0.1226  0.0507 0.067 -0.0836 -0.0749  0.0667  0.05283  0.0727}
\CommentTok{#> 5  0.0359 -0.0923  0.0783 0.082  0.0589  0.0298  0.1002  0.07960 -0.0505}
\CommentTok{#> 6  0.0223  0.0962 -0.1091 0.100  0.0874 -0.0147  0.0208  0.03547  0.0917}
\CommentTok{#>      C114     C115     C116    C117     C118     C119    C120     C121}
\CommentTok{#> 1 -0.0337 -0.10484 -0.06381 -0.1035  0.04573  0.01921 -0.0926 -0.02637}
\CommentTok{#> 2  0.0839  0.00837 -0.01622 -0.0107  0.02460 -0.05904 -0.0931  0.00474}
\CommentTok{#> 3 -0.0407  0.01208  0.02241  0.0879 -0.07836 -0.00733  0.0962 -0.06328}
\CommentTok{#> 4  0.0674  0.09862 -0.00846 -0.1039  0.10639 -0.02641 -0.0770  0.07318}
\CommentTok{#> 5  0.0974 -0.08817  0.11588 -0.1085  0.00712  0.01767 -0.1234 -0.01964}
\CommentTok{#> 6 -0.1139 -0.00424 -0.08683 -0.0363 -0.07593  0.00190 -0.0578 -0.05446}
\CommentTok{#>       C122    C123     C124    C125    C126    C127    C128    C129}
\CommentTok{#> 1  0.03674  0.1237 -0.04520 -0.0306 -0.0006  0.0956  0.0549 -0.0365}
\CommentTok{#> 2 -0.09247 -0.1032  0.00108 -0.0504 -0.1196  0.0696 -0.0168  0.1106}
\CommentTok{#> 3 -0.00997  0.0917 -0.04865 -0.0474 -0.0735  0.0632 -0.0634 -0.0212}
\CommentTok{#> 4  0.07298 -0.0675 -0.01196 -0.0786  0.0490 -0.1089  0.0633  0.1110}
\CommentTok{#> 5 -0.09434 -0.0185  0.03656  0.0810 -0.0625  0.0937  0.0646  0.0848}
\CommentTok{#> 6 -0.08288  0.0630  0.01502 -0.0381 -0.0436 -0.1136  0.0679  0.0128}
\CommentTok{#>      C130     C131     C132    C133    C134    C135     C136     C137}
\CommentTok{#> 1  0.1094  0.03566 -0.00416  0.0793 -0.0716  0.0522 -0.06937 -0.05811}
\CommentTok{#> 2 -0.0294  0.01689 -0.07275  0.0711  0.0746  0.0742  0.06364  0.07468}
\CommentTok{#> 3  0.1089  0.00647  0.10485  0.0229  0.1020  0.1055  0.09774  0.12125}
\CommentTok{#> 4 -0.0522 -0.12090  0.10145  0.0843  0.0789  0.0595 -0.00789 -0.11945}
\CommentTok{#> 5 -0.0271 -0.11823  0.11296  0.0848 -0.0895 -0.0453  0.04522  0.00928}
\CommentTok{#> 6  0.0828  0.12114 -0.05873 -0.0632 -0.1148 -0.1225 -0.12078  0.11636}
\CommentTok{#>      C138    C139     C140     C141     C142    C143    C144    C145}
\CommentTok{#> 1 -0.0779 -0.0631  0.07777 -0.11201  0.00311 -0.1156  0.1226  0.0405}
\CommentTok{#> 2 -0.1163 -0.0163 -0.02705 -0.00984  0.04533  0.0614  0.0769 -0.0502}
\CommentTok{#> 3  0.0634 -0.0833 -0.05700  0.01440  0.04347  0.0483  0.0436 -0.0310}
\CommentTok{#> 4  0.0866  0.0292 -0.09940 -0.00437 -0.09841  0.0676  0.1093  0.1174}
\CommentTok{#> 5 -0.0982 -0.1104  0.00545 -0.09842  0.05746 -0.0560 -0.0477 -0.0236}
\CommentTok{#> 6  0.1167  0.0919 -0.04769  0.00281 -0.07250 -0.0707  0.0811  0.0252}
\CommentTok{#>      C146      C147    C148     C149     C150    C151    C152    C153}
\CommentTok{#> 1 -0.0446  0.051756  0.0204 -0.10556 -0.09319  0.1024 -0.1053  0.0131}
\CommentTok{#> 2  0.0175  0.002490  0.1199 -0.10575 -0.00637 -0.0945  0.1138  0.1064}
\CommentTok{#> 3  0.0315 -0.000557 -0.0663  0.02473  0.00801 -0.0731 -0.0099 -0.0533}
\CommentTok{#> 4  0.0363  0.110414  0.0865 -0.02161  0.02591  0.0187  0.0214  0.1200}
\CommentTok{#> 5  0.0353 -0.116950  0.0295 -0.00867  0.05730  0.1190 -0.0641 -0.0743}
\CommentTok{#> 6 -0.0363 -0.065581 -0.0586 -0.06431  0.01773  0.0332  0.0032 -0.1068}
\CommentTok{#>      C154    C155    C156    C157    C158     C159   C160    C161    C162}
\CommentTok{#> 1 -0.0742  0.0638 -0.1176 -0.0519 -0.0896 -0.11058 0.0315 -0.0246  0.0725}
\CommentTok{#> 2 -0.0546 -0.0351  0.0822  0.0386 -0.0407  0.10167 0.0842  0.0155  0.0232}
\CommentTok{#> 3  0.0185  0.0925 -0.1085  0.0350  0.0998  0.06522 0.0380  0.1071 -0.1132}
\CommentTok{#> 4 -0.1138 -0.1047 -0.1144  0.1047 -0.1101  0.00760 0.0680 -0.0235  0.0597}
\CommentTok{#> 5  0.0410  0.1011  0.0186 -0.1134  0.0829 -0.00998 0.1049 -0.0620  0.0325}
\CommentTok{#> 6 -0.1006 -0.1060 -0.0507  0.0699 -0.0810  0.03061 0.0830  0.0700  0.0897}
\CommentTok{#>      C163    C164    C165    C166     C167     C168     C169    C170}
\CommentTok{#> 1  0.0884 -0.1224 -0.0567 -0.0511  0.03312  0.06697  0.04004 -0.1166}
\CommentTok{#> 2  0.1050  0.0641 -0.0712 -0.0185 -0.01521 -0.04217  0.05469 -0.0865}
\CommentTok{#> 3 -0.1100 -0.0946  0.0529  0.0629  0.07493  0.00368  0.03301  0.0927}
\CommentTok{#> 4 -0.0668 -0.1182 -0.0306 -0.0694  0.00644  0.11020  0.04253  0.0938}
\CommentTok{#> 5 -0.1021  0.0284  0.0737 -0.0791 -0.06571 -0.06951  0.00477  0.0553}
\CommentTok{#> 6 -0.0951 -0.0398 -0.0636 -0.0611  0.09139  0.06308 -0.09140 -0.1021}
\CommentTok{#>        C171    C172    C173    C174    C175     C176     C177    C178}
\CommentTok{#> 1 -0.097466 -0.0892  0.0621 -0.0225  0.0276 -0.02481 -0.02909  0.1071}
\CommentTok{#> 2 -0.000841  0.0400 -0.1109  0.0279 -0.0747 -0.03990  0.07722 -0.0270}
\CommentTok{#> 3  0.025534  0.0963 -0.0939  0.0458 -0.0116  0.00887  0.05721 -0.0356}
\CommentTok{#> 4 -0.018872  0.0186 -0.0737  0.0114 -0.1107 -0.06354 -0.03085  0.0275}
\CommentTok{#> 5 -0.072176  0.0960 -0.0872  0.1130 -0.0462  0.04666 -0.05809 -0.0400}
\CommentTok{#> 6  0.045227 -0.0873 -0.1229  0.0859  0.1141  0.01828  0.00256  0.0851}
\CommentTok{#>      C179    C180    C181    C182    C183    C184     C185    C186    C187}
\CommentTok{#> 1  0.1158  0.0483 -0.1087  0.0204 -0.0801  0.1150 -0.00764 -0.0996 -0.0280}
\CommentTok{#> 2  0.0255 -0.0242 -0.0147 -0.0687  0.0958  0.0179 -0.11765 -0.0254  0.0715}
\CommentTok{#> 3  0.0127  0.0543  0.0099 -0.0356  0.0879  0.0908  0.11311 -0.0268 -0.0218}
\CommentTok{#> 4  0.0850 -0.0154 -0.0685 -0.0515 -0.0645 -0.0459  0.01678  0.0616 -0.0374}
\CommentTok{#> 5 -0.1246 -0.0758  0.0271 -0.0225 -0.0116 -0.0200  0.00247 -0.0165 -0.0770}
\CommentTok{#> 6  0.1152  0.0252  0.1172 -0.0365  0.0659  0.0993 -0.09316 -0.0675  0.0548}
\CommentTok{#>      C188    C189     C190    C191    C192      C193    C194     C195}
\CommentTok{#> 1  0.0850 -0.0344 -0.00327 -0.1018  0.0448  0.099353  0.0685  0.10706}
\CommentTok{#> 2  0.0154 -0.1148 -0.09576 -0.0554 -0.0829  0.090553  0.0120  0.12670}
\CommentTok{#> 3  0.0797 -0.1154 -0.06220  0.1137  0.0873 -0.000183 -0.1110 -0.08733}
\CommentTok{#> 4 -0.0701 -0.0110 -0.05203 -0.1060  0.0894  0.053941  0.0608  0.03834}
\CommentTok{#> 5  0.0018 -0.0970 -0.12031 -0.0553 -0.0341 -0.090961  0.0222  0.00542}
\CommentTok{#> 6 -0.0941  0.1250  0.10445 -0.0366 -0.0664 -0.073620  0.0849  0.11440}
\CommentTok{#>      C196      C197    C198    C199    C200}
\CommentTok{#> 1  0.0139 -0.000847  0.0977  0.0292 -0.0211}
\CommentTok{#> 2 -0.0577  0.054134 -0.0411  0.0177 -0.1136}
\CommentTok{#> 3 -0.1171  0.084142 -0.0184 -0.0742  0.0254}
\CommentTok{#> 4 -0.0309 -0.071778  0.0967  0.0785  0.0418}
\CommentTok{#> 5  0.0604 -0.061921 -0.0995  0.0962 -0.1252}
\CommentTok{#> 6  0.0905  0.112109 -0.0810  0.0379 -0.0375}
\CommentTok{#> }
\CommentTok{#> [200 rows x 200 columns]}
\KeywordTok{h2o.weights}\NormalTok{(iris_dl, }\DataTypeTok{matrix_id=}\DecValTok{3}\NormalTok{)}
\CommentTok{#>      C1     C2    C3     C4     C5     C6    C7    C8      C9   C10   C11}
\CommentTok{#> 1 0.342 -0.368 0.324 -0.364 -0.592 -0.203 0.585 0.516  0.0173 0.035 0.568}
\CommentTok{#> 2 0.482  0.293 0.359  0.228 -0.175  0.425 0.406 0.172  0.4495 0.650 0.113}
\CommentTok{#> 3 0.183 -0.666 0.292 -0.424  0.683  0.490 0.562 0.151 -0.0929 0.306 0.609}
\CommentTok{#>        C12    C13    C14    C15    C16    C17    C18     C19    C20    C21}
\CommentTok{#> 1 -0.00214 -0.589 -0.397 -0.598  0.660  0.230 -0.195  0.0347  0.141 0.2215}
\CommentTok{#> 2  0.26457 -0.236  0.672 -0.518  0.209 -0.540 -0.302 -0.5254 -0.612 0.0479}
\CommentTok{#> 3 -0.24274  0.248 -0.533  0.372 -0.144  0.421 -0.613 -0.1978  0.519 0.6486}
\CommentTok{#>      C22    C23    C24     C25    C26   C27    C28    C29    C30     C31}
\CommentTok{#> 1 -0.197 -0.580  0.357  0.0808 -0.188 0.533  0.365 -0.322 -0.153  0.0178}
\CommentTok{#> 2 -0.540  0.640  0.313 -0.2145  0.429 0.138 -0.625 -0.202  0.121 -0.1841}
\CommentTok{#> 3  0.649 -0.488 -0.197 -0.5021 -0.389 0.672 -0.364  0.287 -0.640  0.2210}
\CommentTok{#>      C32   C33      C34    C35    C36    C37     C38    C39    C40    C41}
\CommentTok{#> 1  0.332 0.460 -0.32759 -0.108 -0.445  0.657 -0.1124  0.558 -0.448 -0.513}
\CommentTok{#> 2 -0.294 0.391 -0.58829  0.647 -0.184 -0.274  0.2036  0.405 -0.446 -0.220}
\CommentTok{#> 3  0.313 0.673  0.00415 -0.196 -0.390  0.482 -0.0456 -0.159 -0.205 -0.430}
\CommentTok{#>      C42    C43    C44    C45    C46    C47     C48     C49     C50}
\CommentTok{#> 1 -0.532 -0.295 -0.496 -0.381  0.362 -0.511 -0.0958 -0.4112 -0.6348}
\CommentTok{#> 2 -0.115 -0.612 -0.192  0.484  0.217  0.154 -0.2259  0.0227 -0.0495}
\CommentTok{#> 3 -0.614  0.607 -0.378  0.505 -0.280  0.566  0.0986 -0.2556  0.5727}
\CommentTok{#>       C51    C52    C53    C54    C55    C56     C57     C58    C59    C60}
\CommentTok{#> 1  0.3852  0.274 -0.272 -0.441  0.559  0.667 -0.2140 -0.3457  0.137 -0.180}
\CommentTok{#> 2 -0.0425 -0.391 -0.156 -0.407  0.212 -0.189 -0.5148  0.5274 -0.507  0.418}
\CommentTok{#> 3  0.6496 -0.256  0.397  0.273 -0.523  0.115 -0.0471  0.0869  0.164 -0.484}
\CommentTok{#>        C61   C62    C63     C64    C65    C66     C67     C68     C69}
\CommentTok{#> 1 -0.43399 0.236 -0.627 -0.1996 -0.337  0.491 -0.1129  0.0639  0.0326}
\CommentTok{#> 2  0.00354 0.294 -0.630  0.1598 -0.201 -0.305  0.0646 -0.3334  0.3836}
\CommentTok{#> 3 -0.04962 0.484 -0.261  0.0953  0.284  0.171 -0.2263  0.2749 -0.6304}
\CommentTok{#>      C70    C71     C72     C73    C74    C75    C76     C77    C78   C79}
\CommentTok{#> 1 -0.350  0.563  0.0541  0.6424  0.246 -0.496 -0.114 -0.2509  0.138 0.406}
\CommentTok{#> 2  0.480  0.534 -0.4180 -0.0151 -0.185 -0.595  0.566 -0.0383  0.325 0.149}
\CommentTok{#> 3 -0.416 -0.120 -0.5839 -0.6403  0.532 -0.340  0.214  0.2159 -0.498 0.116}
\CommentTok{#>      C80    C81    C82    C83    C84     C85    C86    C87    C88     C89}
\CommentTok{#> 1 -0.674 -0.663 -0.206 -0.634 -0.413 -0.3357 -0.231 -0.435 -0.164  0.0199}
\CommentTok{#> 2  0.522 -0.618  0.681  0.433 -0.486  0.1022 -0.576  0.314 -0.435  0.1380}
\CommentTok{#> 3 -0.443 -0.498  0.464  0.125  0.413 -0.0961 -0.424  0.589 -0.349 -0.4965}
\CommentTok{#>      C90    C91     C92    C93    C94    C95    C96     C97    C98     C99}
\CommentTok{#> 1 -0.239 -0.407 -0.3047 -0.664 -0.108  0.504 -0.384  0.0026  0.602  0.0933}
\CommentTok{#> 2  0.186  0.485  0.0146 -0.573 -0.129 -0.324  0.227 -0.3302  0.296 -0.0159}
\CommentTok{#> 3  0.155 -0.443 -0.4446  0.127 -0.490  0.452 -0.555  0.1478 -0.272  0.2311}
\CommentTok{#>    C100   C101    C102   C103   C104   C105  C106     C107   C108   C109}
\CommentTok{#> 1 0.668  0.196 -0.5353  0.687 -0.155  0.233 0.145  0.43366 -0.260 -0.561}
\CommentTok{#> 2 0.114 -0.476 -0.0474 -0.202  0.489 -0.293 0.415 -0.07614  0.222 -0.161}
\CommentTok{#> 3 0.633  0.217  0.3521  0.181  0.406  0.245 0.401  0.00776 -0.276 -0.382}
\CommentTok{#>     C110   C111  C112   C113    C114   C115  C116  C117   C118   C119}
\CommentTok{#> 1 -0.547 -0.559 0.142  0.202 -0.6393 -0.235 0.325 0.384  0.661 -0.450}
\CommentTok{#> 2 -0.610  0.414 0.169 -0.313  0.0841  0.172 0.525 0.438 -0.668 -0.164}
\CommentTok{#> 3  0.620 -0.486 0.345  0.111  0.6790 -0.371 0.287 0.410  0.494 -0.357}
\CommentTok{#>     C120   C121   C122    C123   C124  C125   C126   C127     C128   C129}
\CommentTok{#> 1 -0.488  0.354  0.266 -0.6560 -0.684 0.495 -0.577 -0.172  0.33653 -0.632}
\CommentTok{#> 2  0.204  0.273 -0.454 -0.0724 -0.280 0.455  0.265  0.320 -0.00751  0.188}
\CommentTok{#> 3 -0.043 -0.108  0.675 -0.1612  0.648 0.376  0.178 -0.435 -0.60044  0.164}
\CommentTok{#>     C130   C131    C132   C133    C134   C135   C136   C137    C138  C139}
\CommentTok{#> 1  0.196 0.1892  0.1882  0.575 -0.1521 0.0399  0.603  0.371 -0.5318 0.660}
\CommentTok{#> 2 -0.175 0.6168  0.0231 -0.680  0.0942 0.2942 -0.512  0.223 -0.1942 0.504}
\CommentTok{#> 3  0.506 0.0157 -0.5676 -0.630 -0.6126 0.0769 -0.505 -0.221 -0.0263 0.100}
\CommentTok{#>      C140    C141   C142   C143   C144    C145   C146   C147    C148}
\CommentTok{#> 1  0.0922 -0.0641 -0.391  0.614  0.230 -0.0131 -0.558  0.632 -0.3995}
\CommentTok{#> 2 -0.5530  0.4508 -0.119 -0.570 -0.332  0.4490  0.452 -0.353 -0.0845}
\CommentTok{#> 3 -0.0205 -0.5195 -0.475  0.377  0.579  0.6530  0.682 -0.541 -0.6666}
\CommentTok{#>     C149   C150   C151     C152    C153    C154  C155   C156    C157}
\CommentTok{#> 1  0.652 -0.255 -0.662 -0.29231  0.5346 -0.6735 0.410  0.362  0.3094}
\CommentTok{#> 2 -0.620 -0.369 -0.114  0.00293  0.0844  0.3824 0.200 -0.159 -0.6263}
\CommentTok{#> 3  0.342 -0.397  0.513 -0.43492 -0.4018  0.0382 0.227  0.245 -0.0769}
\CommentTok{#>      C158    C159    C160   C161   C162  C163   C164     C165   C166}
\CommentTok{#> 1 -0.6355 -0.0757 0.00464  0.477  0.469 0.392 0.2368 -0.36293  0.261}
\CommentTok{#> 2 -0.0662 -0.1898 0.29747 -0.368 -0.570 0.149 0.0281  0.00839 -0.408}
\CommentTok{#> 3 -0.5992 -0.0279 0.47702  0.247  0.120 0.327 0.0274  0.49503  0.527}
\CommentTok{#>      C167  C168   C169   C170    C171    C172   C173   C174   C175   C176}
\CommentTok{#> 1 -0.0771 0.618 -0.587  0.299  0.4908 -0.2277  0.376 -0.648 -0.546 -0.314}
\CommentTok{#> 2  0.6645 0.602  0.546 -0.158 -0.2703 -0.3597  0.241 -0.202  0.609 -0.520}
\CommentTok{#> 3 -0.6659 0.404  0.161  0.453 -0.0947  0.0698 -0.179 -0.388 -0.160  0.631}
\CommentTok{#>      C177   C178   C179   C180   C181   C182    C183    C184    C185}
\CommentTok{#> 1  0.0109 -0.447 -0.428 -0.213 0.0703  0.624  0.0255  0.5505 -0.2055}
\CommentTok{#> 2 -0.1831 -0.352  0.331  0.301 0.4346 -0.659  0.6451 -0.6585  0.2983}
\CommentTok{#> 3  0.0763 -0.288 -0.631  0.383 0.3603  0.554 -0.0247  0.0292 -0.0477}
\CommentTok{#>      C186   C187   C188    C189  C190   C191   C192   C193   C194   C195}
\CommentTok{#> 1  0.0703 -0.440 -0.328  0.5271 0.231  0.671 -0.296  0.242  0.598 -0.479}
\CommentTok{#> 2 -0.3119  0.604  0.159 -0.2755 0.466 -0.222  0.590 -0.620  0.185 -0.301}
\CommentTok{#> 3 -0.5587 -0.168 -0.497  0.0699 0.283 -0.245  0.613 -0.644 -0.301  0.579}
\CommentTok{#>      C196     C197   C198   C199   C200}
\CommentTok{#> 1  0.0273 -0.00461  0.150 -0.628  0.379}
\CommentTok{#> 2 -0.5469 -0.09958 -0.492 -0.239 -0.595}
\CommentTok{#> 3  0.3862 -0.48212  0.444  0.135  0.178}
\CommentTok{#> }
\CommentTok{#> [3 rows x 200 columns]}
\KeywordTok{h2o.biases}\NormalTok{(iris_dl,  }\DataTypeTok{vector_id=}\DecValTok{1}\NormalTok{)}
\CommentTok{#>      C1}
\CommentTok{#> 1 0.490}
\CommentTok{#> 2 0.498}
\CommentTok{#> 3 0.492}
\CommentTok{#> 4 0.493}
\CommentTok{#> 5 0.498}
\CommentTok{#> 6 0.496}
\CommentTok{#> }
\CommentTok{#> [200 rows x 1 column]}
\KeywordTok{h2o.biases}\NormalTok{(iris_dl,  }\DataTypeTok{vector_id=}\DecValTok{2}\NormalTok{)}
\CommentTok{#>      C1}
\CommentTok{#> 1 1.003}
\CommentTok{#> 2 1.003}
\CommentTok{#> 3 1.002}
\CommentTok{#> 4 1.003}
\CommentTok{#> 5 0.994}
\CommentTok{#> 6 0.996}
\CommentTok{#> }
\CommentTok{#> [200 rows x 1 column]}
\KeywordTok{h2o.biases}\NormalTok{(iris_dl,  }\DataTypeTok{vector_id=}\DecValTok{3}\NormalTok{)}
\CommentTok{#>          C1}
\CommentTok{#> 1  1.33e-03}
\CommentTok{#> 2 -6.48e-05}
\CommentTok{#> 3 -2.62e-03}
\CommentTok{#> }
\CommentTok{#> [3 rows x 1 column]}
\CommentTok{#plot weights connecting `Sepal.Length` to first hidden neurons}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{as.data.frame}\NormalTok{(}\KeywordTok{h2o.weights}\NormalTok{(iris_dl,  }\DataTypeTok{matrix_id=}\DecValTok{1}\NormalTok{))[,}\DecValTok{1}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{nn-comparison-classification_regression_with_h2o_deep_learning_files/figure-latex/export_weight_biases-1} \end{center}

\hypertarget{reproducibility}{%
\subsection{Reproducibility}\label{reproducibility}}

Every run of DeepLearning results in different results since multithreading is done via Hogwild! that benefits from intentional lock-free race conditions between threads. To get reproducible results for small datasets and testing purposes, set \texttt{reproducible=T} and set \texttt{seed=1337} (pick any integer). This will not work for big data for technical reasons, and is probably also not desired because of the significant slowdown (runs on 1 core only).

\hypertarget{scoring-on-trainingvalidation-sets-during-training}{%
\subsection{Scoring on Training/Validation Sets During Training}\label{scoring-on-trainingvalidation-sets-during-training}}

The training and/or validation set errors can be based on a subset of the training or validation data, depending on the values for \texttt{score\_validation\_samples} (defaults to 0: all) or \texttt{score\_training\_samples} (defaults to 10,000 rows, since the training error is only used for early stopping and monitoring). For large datasets, Deep Learning can automatically sample the validation set to avoid spending too much time in scoring during training, especially since scoring results are not currently displayed in the model returned to R.

Note that the default value of \texttt{score\_duty\_cycle=0.1} limits the amount of time spent in scoring to 10\%, so a large number of scoring samples won't slow down overall training progress too much, but it will always score once after the first MapReduce iteration, and once at the end of training.

Stratified sampling of the validation dataset can help with scoring on datasets with class imbalance. Note that this option also requires \texttt{balance\_classes} to be enabled (used to over/under-sample the training dataset, based on the max. relative size of the resulting training dataset, max\_after\_balance\_size):

More information can be found in the H2O Deep Learning booklet, in our H2O SlideShare Presentations, our H2O YouTube channel, as well as on our H2O Github Repository, especially in our H2O Deep Learning R tests, and H2O Deep Learning Python tests.

\hypertarget{all-done-shutdown-h2o}{%
\section{All done, shutdown H2O}\label{all-done-shutdown-h2o}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{h2o.shutdown}\NormalTok{(}\DataTypeTok{prompt=}\OtherTok{FALSE}\NormalTok{)}
\CommentTok{#> [1] TRUE}
\end{Highlighting}
\end{Shaded}

\hypertarget{regression-with-ann---yacht-hydrodynamics}{%
\chapter{Regression with ANN - Yacht Hydrodynamics}\label{regression-with-ann---yacht-hydrodynamics}}

\hypertarget{introduction-21}{%
\section{Introduction}\label{introduction-21}}

Regression ANNs predict an output variable as a function of the inputs. The input features (independent variables) can be categorical or numeric types, however, for regression ANNs, we require a numeric dependent variable. If the output variable is a categorical variable (or binary) the ANN will function as a classifier (see next tutorial).

Source: \url{http://uc-r.github.io/ann_regression}

In this tutorial we introduce a neural network used for numeric predictions and cover:

\begin{itemize}
\tightlist
\item
  Replication requirements: What you'll need to reproduce the analysis in this tutorial.
\item
  Data Preparation: Preparing our data.
\item
  1st Regression ANN: Constructing a 1-hidden layer ANN with 1 neuron.
\item
  Regression Hyperparameters: Tuning the model.
\item
  Wrapping Up: Final comments and some exercises to test your skills.
\end{itemize}

\hypertarget{replication-requirements}{%
\section{Replication Requirements}\label{replication-requirements}}

We require the following packages for the analysis.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}
\CommentTok{#> Registered S3 methods overwritten by 'ggplot2':}
\CommentTok{#>   method         from }
\CommentTok{#>   [.quosures     rlang}
\CommentTok{#>   c.quosures     rlang}
\CommentTok{#>   print.quosures rlang}
\CommentTok{#> Registered S3 method overwritten by 'rvest':}
\CommentTok{#>   method            from}
\CommentTok{#>   read_xml.response xml2}
\CommentTok{#> -- Attaching packages --------------------------------- tidyverse 1.2.1 --}
\CommentTok{#> v ggplot2 3.1.1       v purrr   0.3.2  }
\CommentTok{#> v tibble  2.1.1       v dplyr   0.8.0.1}
\CommentTok{#> v tidyr   0.8.3       v stringr 1.4.0  }
\CommentTok{#> v readr   1.3.1       v forcats 0.4.0}
\CommentTok{#> -- Conflicts ------------------------------------ tidyverse_conflicts() --}
\CommentTok{#> x dplyr::filter() masks stats::filter()}
\CommentTok{#> x dplyr::lag()    masks stats::lag()}
\KeywordTok{library}\NormalTok{(neuralnet)}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'neuralnet'}
\CommentTok{#> The following object is masked from 'package:dplyr':}
\CommentTok{#> }
\CommentTok{#>     compute}
\KeywordTok{library}\NormalTok{(GGally)}
\CommentTok{#> Registered S3 method overwritten by 'GGally':}
\CommentTok{#>   method from   }
\CommentTok{#>   +.gg   ggplot2}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'GGally'}
\CommentTok{#> The following object is masked from 'package:dplyr':}
\CommentTok{#> }
\CommentTok{#>     nasa}
\end{Highlighting}
\end{Shaded}

\hypertarget{data-preparation}{%
\section{Data Preparation}\label{data-preparation}}

Our regression ANN will use the \textbf{Yacht Hydrodynamics} data set from UCI's Machine Learning Repository. The yacht data was provided by Dr.~Roberto Lopez email. This data set contains data contains results from 308 full-scale experiments performed at the Delft Ship Hydromechanics Laboratory where they test 22 different hull forms. Their experiment tested the effect of variations in the hull geometry and the ship's Froude number on the craft's residuary resistance per unit weight of displacement.

To begin we download the data from UCI.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{url <-}\StringTok{ 'http://archive.ics.uci.edu/ml/machine-learning-databases/00243/yacht_hydrodynamics.data'}

\NormalTok{Yacht_Data <-}\StringTok{ }\KeywordTok{read_table}\NormalTok{(}\DataTypeTok{file =}\NormalTok{ url,}
                         \DataTypeTok{col_names =} \KeywordTok{c}\NormalTok{(}\StringTok{'LongPos_COB'}\NormalTok{, }\StringTok{'Prismatic_Coeff'}\NormalTok{,}
                                       \StringTok{'Len_Disp_Ratio'}\NormalTok{, }\StringTok{'Beam_Draut_Ratio'}\NormalTok{, }
                                       \StringTok{'Length_Beam_Ratio'}\NormalTok{,}\StringTok{'Froude_Num'}\NormalTok{, }
                                       \StringTok{'Residuary_Resist'}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{na.omit}\NormalTok{()}
\CommentTok{#> Parsed with column specification:}
\CommentTok{#> cols(}
\CommentTok{#>   LongPos_COB = col_double(),}
\CommentTok{#>   Prismatic_Coeff = col_double(),}
\CommentTok{#>   Len_Disp_Ratio = col_double(),}
\CommentTok{#>   Beam_Draut_Ratio = col_double(),}
\CommentTok{#>   Length_Beam_Ratio = col_double(),}
\CommentTok{#>   Froude_Num = col_double(),}
\CommentTok{#>   Residuary_Resist = col_double()}
\CommentTok{#> )}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{glimpse}\NormalTok{(Yacht_Data)}
\CommentTok{#> Observations: 308}
\CommentTok{#> Variables: 7}
\CommentTok{#> $ LongPos_COB       <dbl> -2.3, -2.3, -2.3, -2.3, -2.3, -2.3, -2.3, -2...}
\CommentTok{#> $ Prismatic_Coeff   <dbl> 0.568, 0.568, 0.568, 0.568, 0.568, 0.568, 0....}
\CommentTok{#> $ Len_Disp_Ratio    <dbl> 4.78, 4.78, 4.78, 4.78, 4.78, 4.78, 4.78, 4....}
\CommentTok{#> $ Beam_Draut_Ratio  <dbl> 3.99, 3.99, 3.99, 3.99, 3.99, 3.99, 3.99, 3....}
\CommentTok{#> $ Length_Beam_Ratio <dbl> 3.17, 3.17, 3.17, 3.17, 3.17, 3.17, 3.17, 3....}
\CommentTok{#> $ Froude_Num        <dbl> 0.125, 0.150, 0.175, 0.200, 0.225, 0.250, 0....}
\CommentTok{#> $ Residuary_Resist  <dbl> 0.11, 0.27, 0.47, 0.78, 1.18, 1.82, 2.61, 3....}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# save the dataset locally}
\KeywordTok{write.csv}\NormalTok{(Yacht_Data, }\DataTypeTok{file =} \KeywordTok{file.path}\NormalTok{(data_raw_dir, }\StringTok{"yach_data.csv"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Prior to any data analysis lets take a look at the data set.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggpairs}\NormalTok{(Yacht_Data, }\DataTypeTok{title =} \StringTok{"Scatterplot Matrix of the Features of the Yacht Data Set"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{nn-regression_142-neural_network_yacht_files/figure-latex/unnamed-chunk-6-1} \end{center}

Here we see an excellent summary of the variation of each feature in our data set. Draw your attention to the bottom-most strip of scatter-plots. This shows the residuary resistance as a function of the other data set features (independent experimental values). The greatest variation appears with the Froude Number feature. It will be interesting to see how this pattern appears in the subsequent regression ANNs.

Prior to regression ANN construction we first must split the Yacht data set into test and training data sets. Before we split, first scale each feature to fall in the
\texttt{{[}0,1{]}} interval.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Scale the Data}
\NormalTok{scale01 <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x)\{}
\NormalTok{  (x }\OperatorTok{-}\StringTok{ }\KeywordTok{min}\NormalTok{(x)) }\OperatorTok{/}\StringTok{ }\NormalTok{(}\KeywordTok{max}\NormalTok{(x) }\OperatorTok{-}\StringTok{ }\KeywordTok{min}\NormalTok{(x))}
\NormalTok{\}}

\NormalTok{Yacht_Data <-}\StringTok{ }\NormalTok{Yacht_Data }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate_all}\NormalTok{(scale01)}

\CommentTok{# Split into test and train sets}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{12345}\NormalTok{)}
\NormalTok{Yacht_Data_Train <-}\StringTok{ }\KeywordTok{sample_frac}\NormalTok{(}\DataTypeTok{tbl =}\NormalTok{ Yacht_Data, }\DataTypeTok{replace =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{size =} \FloatTok{0.80}\NormalTok{)}
\NormalTok{Yacht_Data_Test <-}\StringTok{ }\KeywordTok{anti_join}\NormalTok{(Yacht_Data, Yacht_Data_Train)}
\CommentTok{#> Joining, by = c("LongPos_COB", "Prismatic_Coeff", "Len_Disp_Ratio", "Beam_Draut_Ratio", "Length_Beam_Ratio", "Froude_Num", "Residuary_Resist")}
\end{Highlighting}
\end{Shaded}

The \texttt{scale01()} function maps each data observation onto the \texttt{{[}0,1{]}} interval as called in the dplyr \texttt{mutate\_all()} function. We then provided a seed for reproducible results and randomly extracted (without replacement) 80\% of the observations to build the \texttt{Yacht\_Data\_Train} data set. Using dplyr's \texttt{anti\_join()} function we extracted all the observations not within the \texttt{Yacht\_Data\_Train} data set as our test data set in \texttt{Yacht\_Data\_Test}.

\hypertarget{st-regression-ann}{%
\section{1st Regression ANN}\label{st-regression-ann}}

To begin we construct a 1-hidden layer ANN with 1 neuron, the simplest of all neural networks.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{12321}\NormalTok{)}
\NormalTok{Yacht_NN1 <-}\StringTok{ }\KeywordTok{neuralnet}\NormalTok{(Residuary_Resist }\OperatorTok{~}\StringTok{ }\NormalTok{LongPos_COB }\OperatorTok{+}\StringTok{ }\NormalTok{Prismatic_Coeff }\OperatorTok{+}\StringTok{ }
\StringTok{                         }\NormalTok{Len_Disp_Ratio }\OperatorTok{+}\StringTok{ }\NormalTok{Beam_Draut_Ratio }\OperatorTok{+}\StringTok{ }\NormalTok{Length_Beam_Ratio }\OperatorTok{+}
\StringTok{                         }\NormalTok{Froude_Num, }\DataTypeTok{data =}\NormalTok{ Yacht_Data_Train)}
\end{Highlighting}
\end{Shaded}

The \texttt{Yacht\_NN1} is a list containing all parameters of the regression ANN as well as the results of the neural network on the test data set. To view a diagram of the \texttt{Yacht\_NN1} use the \texttt{plot()} function.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(Yacht_NN1, }\DataTypeTok{rep =} \StringTok{'best'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{nn-regression_142-neural_network_yacht_files/figure-latex/unnamed-chunk-9-1} \end{center}

This plot shows the weights learned by the \texttt{Yacht\_NN1} neural network, and displays the number of iterations before convergence, as well as the SSE of the training data set. To manually compute the SSE you can use the following:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{NN1_Train_SSE <-}\StringTok{ }\KeywordTok{sum}\NormalTok{((Yacht_NN1}\OperatorTok{$}\NormalTok{net.result }\OperatorTok{-}\StringTok{ }\NormalTok{Yacht_Data_Train[, }\DecValTok{7}\NormalTok{])}\OperatorTok{^}\DecValTok{2}\NormalTok{)}\OperatorTok{/}\DecValTok{2}
\KeywordTok{paste}\NormalTok{(}\StringTok{"SSE: "}\NormalTok{, }\KeywordTok{round}\NormalTok{(NN1_Train_SSE, }\DecValTok{4}\NormalTok{))}
\CommentTok{#> [1] "SSE:  0.0365"}
\CommentTok{## [1] "SSE:  0.0361"}
\end{Highlighting}
\end{Shaded}

This SSE is the error associated with the training data set. A superior metric for estimating the generalization capability of the ANN would be the SSE of the test data set. Recall, the test data set contains observations not used to train the \texttt{Yacht\_NN1\ ANN}. To calculate the test error, we first must run our test observations through the \texttt{Yacht\_NN1} ANN. This is accomplished with the neuralnet package \texttt{compute()} function, which takes as its first input the desired neural network object created by the \texttt{neuralnet()} function, and the second argument the test data set feature (independent variable(s)) values.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Test_NN1_Output <-}\StringTok{ }\KeywordTok{compute}\NormalTok{(Yacht_NN1, Yacht_Data_Test[, }\DecValTok{1}\OperatorTok{:}\DecValTok{6}\NormalTok{])}\OperatorTok{$}\NormalTok{net.result}
\NormalTok{NN1_Test_SSE <-}\StringTok{ }\KeywordTok{sum}\NormalTok{((Test_NN1_Output }\OperatorTok{-}\StringTok{ }\NormalTok{Yacht_Data_Test[, }\DecValTok{7}\NormalTok{])}\OperatorTok{^}\DecValTok{2}\NormalTok{)}\OperatorTok{/}\DecValTok{2}
\NormalTok{NN1_Test_SSE}
\CommentTok{#> [1] 0.0139}
\CommentTok{## [1] 0.008417631461}
\end{Highlighting}
\end{Shaded}

The \texttt{compute()} function outputs the response variable, in our case the \texttt{Residuary\_Resist}, as estimated by the neural network. Once we have the ANN estimated response we can compute the test SSE. Comparing the test error of 0.0084 to the training error of 0.0361 we see that in our case our test error is smaller than our training error.

\hypertarget{regression-hyperparameters}{%
\section{Regression Hyperparameters}\label{regression-hyperparameters}}

We have constructed the most basic of regression ANNs without modifying any of the default hyperparameters associated with the \texttt{neuralnet()} function. We should try and improve the network by modifying its basic structure and hyperparameter modification. To begin we will add depth to the hidden layer of the network, then we will change the activation function from the logistic to the tangent hyperbolicus (tanh) to determine if these modifications can improve the test data set SSE. When using the tanh activation function, we first must rescale the data from \([0,1]\) to \([-1,1]\) using the \texttt{rescale} package. For the purposes of this exercise we will use the same random seed for reproducible results, generally this is not a best practice.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# 2-Hidden Layers, Layer-1 4-neurons, Layer-2, 1-neuron, logistic activation}
\CommentTok{# function}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{12321}\NormalTok{)}
\NormalTok{Yacht_NN2 <-}\StringTok{ }\KeywordTok{neuralnet}\NormalTok{(Residuary_Resist }\OperatorTok{~}\StringTok{ }\NormalTok{LongPos_COB }\OperatorTok{+}\StringTok{ }\NormalTok{Prismatic_Coeff }\OperatorTok{+}\StringTok{ }\NormalTok{Len_Disp_Ratio }\OperatorTok{+}\StringTok{ }\NormalTok{Beam_Draut_Ratio }\OperatorTok{+}\StringTok{ }\NormalTok{Length_Beam_Ratio }\OperatorTok{+}\StringTok{ }\NormalTok{Froude_Num, }
                       \DataTypeTok{data =}\NormalTok{ Yacht_Data_Train, }
                       \DataTypeTok{hidden =} \KeywordTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{, }\DecValTok{1}\NormalTok{), }
                       \DataTypeTok{act.fct =} \StringTok{"logistic"}\NormalTok{)}

\CommentTok{## Training Error}
\NormalTok{NN2_Train_SSE <-}\StringTok{ }\KeywordTok{sum}\NormalTok{((Yacht_NN2}\OperatorTok{$}\NormalTok{net.result }\OperatorTok{-}\StringTok{ }\NormalTok{Yacht_Data_Train[, }\DecValTok{7}\NormalTok{])}\OperatorTok{^}\DecValTok{2}\NormalTok{)}\OperatorTok{/}\DecValTok{2}

\CommentTok{## Test Error}
\NormalTok{Test_NN2_Output <-}\StringTok{ }\KeywordTok{compute}\NormalTok{(Yacht_NN2, Yacht_Data_Test[, }\DecValTok{1}\OperatorTok{:}\DecValTok{6}\NormalTok{])}\OperatorTok{$}\NormalTok{net.result}
\NormalTok{NN2_Test_SSE <-}\StringTok{ }\KeywordTok{sum}\NormalTok{((Test_NN2_Output }\OperatorTok{-}\StringTok{ }\NormalTok{Yacht_Data_Test[, }\DecValTok{7}\NormalTok{])}\OperatorTok{^}\DecValTok{2}\NormalTok{)}\OperatorTok{/}\DecValTok{2}

\CommentTok{# Rescale for tanh activation function}
\NormalTok{scale11 <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x) \{}
\NormalTok{    (}\DecValTok{2} \OperatorTok{*}\StringTok{ }\NormalTok{((x }\OperatorTok{-}\StringTok{ }\KeywordTok{min}\NormalTok{(x))}\OperatorTok{/}\NormalTok{(}\KeywordTok{max}\NormalTok{(x) }\OperatorTok{-}\StringTok{ }\KeywordTok{min}\NormalTok{(x)))) }\OperatorTok{-}\StringTok{ }\DecValTok{1}
\NormalTok{\}}
\NormalTok{Yacht_Data_Train <-}\StringTok{ }\NormalTok{Yacht_Data_Train }\OperatorTok{%>%}\StringTok{ }\KeywordTok{mutate_all}\NormalTok{(scale11)}
\NormalTok{Yacht_Data_Test <-}\StringTok{ }\NormalTok{Yacht_Data_Test }\OperatorTok{%>%}\StringTok{ }\KeywordTok{mutate_all}\NormalTok{(scale11)}

\CommentTok{# 2-Hidden Layers, Layer-1 4-neurons, Layer-2, 1-neuron, tanh activation}
\CommentTok{# function}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{12321}\NormalTok{)}
\NormalTok{Yacht_NN3 <-}\StringTok{ }\KeywordTok{neuralnet}\NormalTok{(Residuary_Resist }\OperatorTok{~}\StringTok{ }\NormalTok{LongPos_COB }\OperatorTok{+}\StringTok{ }\NormalTok{Prismatic_Coeff }\OperatorTok{+}\StringTok{ }\NormalTok{Len_Disp_Ratio }\OperatorTok{+}\StringTok{ }\NormalTok{Beam_Draut_Ratio }\OperatorTok{+}\StringTok{ }\NormalTok{Length_Beam_Ratio }\OperatorTok{+}\StringTok{ }\NormalTok{Froude_Num, }
                       \DataTypeTok{data =}\NormalTok{ Yacht_Data_Train, }
                       \DataTypeTok{hidden =} \KeywordTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{, }\DecValTok{1}\NormalTok{), }
                       \DataTypeTok{act.fct =} \StringTok{"tanh"}\NormalTok{)}

\CommentTok{## Training Error}
\NormalTok{NN3_Train_SSE <-}\StringTok{ }\KeywordTok{sum}\NormalTok{((Yacht_NN3}\OperatorTok{$}\NormalTok{net.result }\OperatorTok{-}\StringTok{ }\NormalTok{Yacht_Data_Train[, }\DecValTok{7}\NormalTok{])}\OperatorTok{^}\DecValTok{2}\NormalTok{)}\OperatorTok{/}\DecValTok{2}

\CommentTok{## Test Error}
\NormalTok{Test_NN3_Output <-}\StringTok{ }\KeywordTok{compute}\NormalTok{(Yacht_NN3, Yacht_Data_Test[, }\DecValTok{1}\OperatorTok{:}\DecValTok{6}\NormalTok{])}\OperatorTok{$}\NormalTok{net.result}
\NormalTok{NN3_Test_SSE <-}\StringTok{ }\KeywordTok{sum}\NormalTok{((Test_NN3_Output }\OperatorTok{-}\StringTok{ }\NormalTok{Yacht_Data_Test[, }\DecValTok{7}\NormalTok{])}\OperatorTok{^}\DecValTok{2}\NormalTok{)}\OperatorTok{/}\DecValTok{2}

\CommentTok{# 1-Hidden Layer, 1-neuron, tanh activation function}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{12321}\NormalTok{)}
\NormalTok{Yacht_NN4 <-}\StringTok{ }\KeywordTok{neuralnet}\NormalTok{(Residuary_Resist }\OperatorTok{~}\StringTok{ }\NormalTok{LongPos_COB }\OperatorTok{+}\StringTok{ }\NormalTok{Prismatic_Coeff }\OperatorTok{+}\StringTok{ }\NormalTok{Len_Disp_Ratio }\OperatorTok{+}\StringTok{ }\NormalTok{Beam_Draut_Ratio }\OperatorTok{+}\StringTok{ }\NormalTok{Length_Beam_Ratio }\OperatorTok{+}\StringTok{ }\NormalTok{Froude_Num, }
                       \DataTypeTok{data =}\NormalTok{ Yacht_Data_Train, }
                       \DataTypeTok{act.fct =} \StringTok{"tanh"}\NormalTok{)}

\CommentTok{## Training Error}
\NormalTok{NN4_Train_SSE <-}\StringTok{ }\KeywordTok{sum}\NormalTok{((Yacht_NN4}\OperatorTok{$}\NormalTok{net.result }\OperatorTok{-}\StringTok{ }\NormalTok{Yacht_Data_Train[, }\DecValTok{7}\NormalTok{])}\OperatorTok{^}\DecValTok{2}\NormalTok{)}\OperatorTok{/}\DecValTok{2}

\CommentTok{## Test Error}
\NormalTok{Test_NN4_Output <-}\StringTok{ }\KeywordTok{compute}\NormalTok{(Yacht_NN4, Yacht_Data_Test[, }\DecValTok{1}\OperatorTok{:}\DecValTok{6}\NormalTok{])}\OperatorTok{$}\NormalTok{net.result}
\NormalTok{NN4_Test_SSE <-}\StringTok{ }\KeywordTok{sum}\NormalTok{((Test_NN4_Output }\OperatorTok{-}\StringTok{ }\NormalTok{Yacht_Data_Test[, }\DecValTok{7}\NormalTok{])}\OperatorTok{^}\DecValTok{2}\NormalTok{)}\OperatorTok{/}\DecValTok{2}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Bar plot of results}
\NormalTok{Regression_NN_Errors <-}\StringTok{ }\KeywordTok{tibble}\NormalTok{(}\DataTypeTok{Network =} \KeywordTok{rep}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"NN1"}\NormalTok{, }\StringTok{"NN2"}\NormalTok{, }\StringTok{"NN3"}\NormalTok{, }\StringTok{"NN4"}\NormalTok{), }\DataTypeTok{each =} \DecValTok{2}\NormalTok{), }
                               \DataTypeTok{DataSet =} \KeywordTok{rep}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"Train"}\NormalTok{, }\StringTok{"Test"}\NormalTok{), }\DataTypeTok{time =} \DecValTok{4}\NormalTok{), }
                               \DataTypeTok{SSE =} \KeywordTok{c}\NormalTok{(NN1_Train_SSE, NN1_Test_SSE, }
\NormalTok{                                       NN2_Train_SSE, NN2_Test_SSE, }
\NormalTok{                                       NN3_Train_SSE, NN3_Test_SSE, }
\NormalTok{                                       NN4_Train_SSE, NN4_Test_SSE))}

\NormalTok{Regression_NN_Errors }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(Network, SSE, }\DataTypeTok{fill =}\NormalTok{ DataSet)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_col}\NormalTok{(}\DataTypeTok{position =} \StringTok{"dodge"}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"Regression ANN's SSE"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{nn-regression_142-neural_network_yacht_files/figure-latex/unnamed-chunk-13-1} \end{center}

As evident from the plot, we see that the best regression ANN we found was \texttt{Yacht\_NN2} with a training and test SSE of 0.0188 and 0.0057. We make this determination by the value of the training and test SSEs only. \texttt{Yacht\_NN2}'s structure is presented here:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(Yacht_NN2, }\DataTypeTok{rep =} \StringTok{"best"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{nn-regression_142-neural_network_yacht_files/figure-latex/unnamed-chunk-14-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{12321}\NormalTok{)}
\NormalTok{Yacht_NN2 <-}\StringTok{ }\KeywordTok{neuralnet}\NormalTok{(Residuary_Resist }\OperatorTok{~}\StringTok{ }\NormalTok{LongPos_COB }\OperatorTok{+}\StringTok{ }\NormalTok{Prismatic_Coeff }\OperatorTok{+}\StringTok{ }\NormalTok{Len_Disp_Ratio }\OperatorTok{+}\StringTok{ }\NormalTok{Beam_Draut_Ratio }\OperatorTok{+}\StringTok{ }\NormalTok{Length_Beam_Ratio }\OperatorTok{+}\StringTok{ }\NormalTok{Froude_Num, }
                       \DataTypeTok{data =}\NormalTok{ Yacht_Data_Train, }
                       \DataTypeTok{hidden =} \KeywordTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{, }\DecValTok{1}\NormalTok{), }
                       \DataTypeTok{act.fct =} \StringTok{"logistic"}\NormalTok{, }
                       \DataTypeTok{rep =} \DecValTok{10}\NormalTok{)}

\KeywordTok{plot}\NormalTok{(Yacht_NN2, }\DataTypeTok{rep =} \StringTok{"best"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{nn-regression_142-neural_network_yacht_files/figure-latex/unnamed-chunk-15-1} \end{center}

By setting the same seed, prior to running the 10 repetitions of ANNs, we force the software to reproduce the exact same \texttt{Yacht\_NN2} ANN for the first replication. The subsequent 9 generated ANNs, use a different random set of starting weights. Comparing the `best' of the 10 repetitions, to the Yacht\_NN2, we observe a decrease in training set error indicating we have a superior set of weights.

\hypertarget{wrapping-up-2}{%
\section{Wrapping Up}\label{wrapping-up-2}}

We have briefly covered regression ANNs in this tutorial. In the next tutorial we will cover classification ANNs. The neuralnet package used in this tutorial is one of many tools available for ANN implementation in R. Others include:

\begin{itemize}
\tightlist
\item
  nnet
\item
  autoencoder
\item
  caret
\item
  RSNNS
\item
  h2o
\end{itemize}

Before you move on to the next tutorial, test your new knowledge on the exercises that follow.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Why do we split the yacht data into a training and test data sets?
\item
  Re-load the Yacht Data from the UCI Machine learning repository yacht data without scaling. Run any regression ANN. What happens? Why do you think this happens?
\item
  After completing exercise question 1, re-scale the yacht data. Perform a simple linear regression fitting \texttt{Residuary\_Resist} as a function of all other features. Now run a regression neural network (see 1st Regression ANN section). Plot the regression ANN and compare the weights on the features in the ANN to the p-values for the regressors.
\item
  Build your own regression ANN using the scaled yacht data modifying one hyperparameter. Use \texttt{?neuralnet} to see the function options. Plot your ANN.
\end{enumerate}

\hypertarget{regression---cereals-dataset}{%
\chapter{Regression - cereals dataset}\label{regression---cereals-dataset}}

\hypertarget{introduction-22}{%
\section{Introduction}\label{introduction-22}}

Source: \url{https://www.analyticsvidhya.com/blog/2017/09/creating-visualizing-neural-network-in-r/}

Neural network is an information-processing machine and can be viewed as analogous to human nervous system. Just like human nervous system, which is made up of interconnected neurons, a neural network is made up of interconnected information processing units. The information processing units do not work in a linear manner. In fact, neural network draws its strength from parallel processing of information, which allows it to deal with non-linearity. Neural network becomes handy to infer meaning and detect patterns from complex data sets.

Neural network is considered as one of the most useful technique in the world of data analytics. However, it is complex and is often regarded as a black box, i.e.~users view the input and output of a neural network but remain clueless about the knowledge generating process. We hope that the article will help readers learn about the internal mechanism of a neural network and get hands-on experience to implement it in R.

\hypertarget{the-basics-of-neural-networks}{%
\section{The Basics of Neural Networks}\label{the-basics-of-neural-networks}}

A neural network is a model characterized by an activation function, which is used by interconnected information processing units to transform input into output. A neural network has always been compared to human nervous system. Information in passed through interconnected units analogous to information passage through neurons in humans. The first layer of the neural network receives the raw input, processes it and passes the processed information to the hidden layers. The hidden layer passes the information to the last layer, which produces the output. The advantage of neural network is that it is adaptive in nature. It learns from the information provided, i.e.~trains itself from the data, which has a known outcome and optimizes its weights for a better prediction in situations with unknown outcome.

A perceptron, viz. single layer neural network, is the most basic form of a neural network. A perceptron receives multidimensional input and processes it using a weighted summation and an activation function. It is trained using a labeled data and learning algorithm that optimize the weights in the summation processor. A major limitation of perceptron model is its inability to deal with non-linearity. A multilayered neural network overcomes this limitation and helps solve non-linear problems. The input layer connects with hidden layer, which in turn connects to the output layer. The connections are weighted and weights are optimized using a learning rule.

There are many learning rules that are used with neural network:

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\tightlist
\item
  least mean square;
\item
  gradient descent;
\item
  newton's rule;
\item
  conjugate gradient etc.
\end{enumerate}

The learning rules can be used in conjunction with backpropgation error method. The learning rule is used to calculate the error at the output unit. This error is backpropagated to all the units such that the error at each unit is proportional to the contribution of that unit towards total error at the output unit. The errors at each unit are then used to optimize the weight at each connection. Figure 1 displays the structure of a simple neural network model for better understanding.

\hypertarget{fitting-a-neural-network-in-r}{%
\section{Fitting a Neural Network in R}\label{fitting-a-neural-network-in-r}}

Now we will fit a neural network model in R. In this article, we use a subset of cereal dataset shared by Carnegie Mellon University (CMU). The details of the dataset are on the following link: \url{http://lib.stat.cmu.edu/DASL/Datafiles/Cereals.html}. The objective is to predict rating of the cereals variables such as calories, proteins, fat etc. The R script is provided side by side and is commented for better understanding of the user. . The data is in .csv format and can be downloaded by clicking: cereals.

Please set working directory in R using setwd( ) function, and keep cereal.csv in the working directory. We use rating as the dependent variable and calories, proteins, fat, sodium and fiber as the independent variables. We divide the data into training and test set. Training set is used to find the relationship between dependent and independent variables while the test set assesses the performance of the model. We use 60\% of the dataset as training set. The assignment of the data to training and test set is done using random sampling. We perform random sampling on R using sample ( ) function. We have used set.seed( ) to generate same random sample everytime and maintain consistency. We will use the index variable while fitting neural network to create training and test data sets. The R script is as follows:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## Creating index variable }

\CommentTok{# Read the Data}
\NormalTok{data =}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\KeywordTok{file.path}\NormalTok{(data_raw_dir, }\StringTok{"cereals.csv"}\NormalTok{), }\DataTypeTok{header=}\NormalTok{T)}

\CommentTok{# Random sampling}
\NormalTok{samplesize =}\StringTok{ }\FloatTok{0.60} \OperatorTok{*}\StringTok{ }\KeywordTok{nrow}\NormalTok{(data)}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{80}\NormalTok{)}
\NormalTok{index =}\StringTok{ }\KeywordTok{sample}\NormalTok{( }\KeywordTok{seq_len}\NormalTok{ ( }\KeywordTok{nrow}\NormalTok{ ( data ) ), }\DataTypeTok{size =}\NormalTok{ samplesize )}

\CommentTok{# Create training and test set  }
\NormalTok{datatrain =}\StringTok{ }\NormalTok{data[ index, ]}
\NormalTok{datatest =}\StringTok{ }\NormalTok{data[ }\OperatorTok{-}\NormalTok{index, ]}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{glimpse}\NormalTok{(data)}
\CommentTok{#> Observations: 75}
\CommentTok{#> Variables: 6}
\CommentTok{#> $ calories <int> 70, 120, 70, 50, 110, 110, 130, 90, 90, 120, 110, 120...}
\CommentTok{#> $ protein  <int> 4, 3, 4, 4, 2, 2, 3, 2, 3, 1, 6, 1, 3, 1, 2, 2, 1, 1,...}
\CommentTok{#> $ fat      <int> 1, 5, 1, 0, 2, 0, 2, 1, 0, 2, 2, 3, 2, 1, 0, 0, 0, 1,...}
\CommentTok{#> $ sodium   <int> 130, 15, 260, 140, 180, 125, 210, 200, 210, 220, 290,...}
\CommentTok{#> $ fiber    <dbl> 10.0, 2.0, 9.0, 14.0, 1.5, 1.0, 2.0, 4.0, 5.0, 0.0, 2...}
\CommentTok{#> $ rating   <dbl> 68.4, 34.0, 59.4, 93.7, 29.5, 33.2, 37.0, 49.1, 53.3,...}
\end{Highlighting}
\end{Shaded}

Now we fit a neural network on our data. We use neuralnet library for the analysis. The first step is to scale the cereal dataset. The scaling of data is essential because otherwise a variable may have large impact on the prediction variable only because of its scale. Using unscaled may lead to meaningless results. The common techniques to scale data are: min-max normalization, Z-score normalization, median and MAD, and tan-h estimators. The min-max normalization transforms the data into a common range, thus removing the scaling effect from all the variables. Unlike Z-score normalization and median and MAD method, the min-max method retains the original distribution of the variables. We use min-max normalization to scale the data. The R script for scaling the data is as follows.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## Scale data for neural network}

\NormalTok{max =}\StringTok{ }\KeywordTok{apply}\NormalTok{(data , }\DecValTok{2}\NormalTok{ , max)}
\NormalTok{min =}\StringTok{ }\KeywordTok{apply}\NormalTok{(data, }\DecValTok{2}\NormalTok{ , min)}
\NormalTok{scaled =}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(}\KeywordTok{scale}\NormalTok{(data, }\DataTypeTok{center =}\NormalTok{ min, }\DataTypeTok{scale =}\NormalTok{ max }\OperatorTok{-}\StringTok{ }\NormalTok{min))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## Fit neural network }

\CommentTok{# install library}
\CommentTok{# install.packages("neuralnet ")}

\CommentTok{# load library}
\KeywordTok{library}\NormalTok{(neuralnet)}

\CommentTok{# creating training and test set}
\NormalTok{trainNN =}\StringTok{ }\NormalTok{scaled[index , ]}
\NormalTok{testNN =}\StringTok{ }\NormalTok{scaled[}\OperatorTok{-}\NormalTok{index , ]}

\CommentTok{# fit neural network}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{2}\NormalTok{)}
\NormalTok{NN =}\StringTok{ }\KeywordTok{neuralnet}\NormalTok{(rating }\OperatorTok{~}\StringTok{ }\NormalTok{calories }\OperatorTok{+}\StringTok{ }\NormalTok{protein }\OperatorTok{+}\StringTok{ }\NormalTok{fat }\OperatorTok{+}\StringTok{ }\NormalTok{sodium }\OperatorTok{+}\StringTok{ }\NormalTok{fiber, }
\NormalTok{               trainNN, }\DataTypeTok{hidden =} \DecValTok{3}\NormalTok{ , }\DataTypeTok{linear.output =}\NormalTok{ T )}

\CommentTok{# plot neural network}
\KeywordTok{plot}\NormalTok{(NN)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## Prediction using neural network}

\NormalTok{predict_testNN =}\StringTok{ }\KeywordTok{compute}\NormalTok{(NN, testNN[,}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{5}\NormalTok{)])}
\NormalTok{predict_testNN =}\StringTok{ }\NormalTok{(predict_testNN}\OperatorTok{$}\NormalTok{net.result }\OperatorTok{*}\StringTok{ }\NormalTok{(}\KeywordTok{max}\NormalTok{(data}\OperatorTok{$}\NormalTok{rating) }\OperatorTok{-}\StringTok{ }\KeywordTok{min}\NormalTok{(data}\OperatorTok{$}\NormalTok{rating))) }\OperatorTok{+}\StringTok{ }\KeywordTok{min}\NormalTok{(data}\OperatorTok{$}\NormalTok{rating)}

\KeywordTok{plot}\NormalTok{(datatest}\OperatorTok{$}\NormalTok{rating, predict_testNN, }\DataTypeTok{col=}\StringTok{'blue'}\NormalTok{, }\DataTypeTok{pch=}\DecValTok{16}\NormalTok{, }\DataTypeTok{ylab =} \StringTok{"predicted rating NN"}\NormalTok{, }\DataTypeTok{xlab =} \StringTok{"real rating"}\NormalTok{)}

\KeywordTok{abline}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{)}

\CommentTok{# Calculate Root Mean Square Error (RMSE)}
\NormalTok{RMSE.NN =}\StringTok{ }\NormalTok{(}\KeywordTok{sum}\NormalTok{((datatest}\OperatorTok{$}\NormalTok{rating }\OperatorTok{-}\StringTok{ }\NormalTok{predict_testNN)}\OperatorTok{^}\DecValTok{2}\NormalTok{) }\OperatorTok{/}\StringTok{ }\KeywordTok{nrow}\NormalTok{(datatest)) }\OperatorTok{^}\StringTok{ }\FloatTok{0.5}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{nn-regression_144-nn_cereals_neuralnet_files/figure-latex/predict_nn-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## Cross validation of neural network model}

\CommentTok{# install relevant libraries}
\CommentTok{# install.packages("boot")}
\CommentTok{# install.packages("plyr")}

\CommentTok{# Load libraries}
\KeywordTok{library}\NormalTok{(boot)}
\KeywordTok{library}\NormalTok{(plyr)}

\CommentTok{# Initialize variables}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{50}\NormalTok{)}
\NormalTok{k =}\StringTok{ }\DecValTok{100}
\NormalTok{RMSE.NN =}\StringTok{ }\OtherTok{NULL}

\NormalTok{List =}\StringTok{ }\KeywordTok{list}\NormalTok{( )}

\CommentTok{# Fit neural network model within nested for loop}
\ControlFlowTok{for}\NormalTok{(j }\ControlFlowTok{in} \DecValTok{10}\OperatorTok{:}\DecValTok{65}\NormalTok{)\{}
    \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{k) \{}
\NormalTok{        index =}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\KeywordTok{nrow}\NormalTok{(data),j )}

\NormalTok{        trainNN =}\StringTok{ }\NormalTok{scaled[index,]}
\NormalTok{        testNN =}\StringTok{ }\NormalTok{scaled[}\OperatorTok{-}\NormalTok{index,]}
\NormalTok{        datatest =}\StringTok{ }\NormalTok{data[}\OperatorTok{-}\NormalTok{index,]}

\NormalTok{        NN =}\StringTok{ }\KeywordTok{neuralnet}\NormalTok{(rating }\OperatorTok{~}\StringTok{ }\NormalTok{calories }\OperatorTok{+}\StringTok{ }\NormalTok{protein }\OperatorTok{+}\StringTok{ }\NormalTok{fat }\OperatorTok{+}\StringTok{ }\NormalTok{sodium }\OperatorTok{+}\StringTok{ }\NormalTok{fiber, trainNN, }\DataTypeTok{hidden =} \DecValTok{3}\NormalTok{, }\DataTypeTok{linear.output=}\NormalTok{ T)}
\NormalTok{        predict_testNN =}\StringTok{ }\KeywordTok{compute}\NormalTok{(NN,testNN[,}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{5}\NormalTok{)])}
\NormalTok{        predict_testNN =}\StringTok{ }\NormalTok{(predict_testNN}\OperatorTok{$}\NormalTok{net.result}\OperatorTok{*}\NormalTok{(}\KeywordTok{max}\NormalTok{(data}\OperatorTok{$}\NormalTok{rating)}\OperatorTok{-}\KeywordTok{min}\NormalTok{(data}\OperatorTok{$}\NormalTok{rating)))}\OperatorTok{+}\KeywordTok{min}\NormalTok{(data}\OperatorTok{$}\NormalTok{rating)}

\NormalTok{        RMSE.NN [i]<-}\StringTok{ }\NormalTok{(}\KeywordTok{sum}\NormalTok{((datatest}\OperatorTok{$}\NormalTok{rating }\OperatorTok{-}\StringTok{ }\NormalTok{predict_testNN)}\OperatorTok{^}\DecValTok{2}\NormalTok{)}\OperatorTok{/}\KeywordTok{nrow}\NormalTok{(datatest))}\OperatorTok{^}\FloatTok{0.5}
\NormalTok{    \}}
\NormalTok{    List[[j]] =}\StringTok{ }\NormalTok{RMSE.NN}
\NormalTok{\}}

\NormalTok{Matrix.RMSE =}\StringTok{ }\KeywordTok{do.call}\NormalTok{(cbind, List)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## Prepare boxplot}
\KeywordTok{boxplot}\NormalTok{(Matrix.RMSE[,}\DecValTok{56}\NormalTok{], }\DataTypeTok{ylab =} \StringTok{"RMSE"}\NormalTok{, }\DataTypeTok{main =} \StringTok{"RMSE BoxPlot (length of traning set = 65)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{nn-regression_144-nn_cereals_neuralnet_files/figure-latex/boxplot-1} \end{center}



\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## Variation of median RMSE }
\CommentTok{# install.packages("matrixStats")}
\KeywordTok{library}\NormalTok{(matrixStats)}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'matrixStats'}
\CommentTok{#> The following object is masked from 'package:plyr':}
\CommentTok{#> }
\CommentTok{#>     count}

\NormalTok{med =}\StringTok{ }\KeywordTok{colMedians}\NormalTok{(Matrix.RMSE)}

\NormalTok{X =}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\DecValTok{10}\NormalTok{,}\DecValTok{65}\NormalTok{)}

\KeywordTok{plot}\NormalTok{ (med}\OperatorTok{~}\NormalTok{X, }\DataTypeTok{type =} \StringTok{"l"}\NormalTok{, }\DataTypeTok{xlab =} \StringTok{"length of training set"}\NormalTok{, }\DataTypeTok{ylab =} \StringTok{"median RMSE"}\NormalTok{, }\DataTypeTok{main =} \StringTok{"Variation of RMSE with length of training set"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{nn-regression_144-nn_cereals_neuralnet_files/figure-latex/var-rmse-1} 

}

\caption{Variation of RMSE}\label{fig:var-rmse}
\end{figure}

Figure \ref{fig:var-rmse}) shows that the median RMSE of our model decreases as the length of the training the set. This is an important result. The reader must remember that the model accuracy is dependent on the length of training set. The performance of neural network model is sensitive to training-test split.

\hypertarget{end-notes-1}{%
\section{End Notes}\label{end-notes-1}}

The article discusses the theoretical aspects of a neural network, its implementation in R and post training evaluation. Neural network is inspired from biological nervous system. Similar to nervous system the information is passed through layers of processors. The significance of variables is represented by weights of each connection. The article provides basic understanding of back propagation algorithm, which is used to assign these weights. In this article we also implement neural network on R. We use a publically available dataset shared by CMU. The aim is to predict the rating of cereals using information such as calories, fat, protein etc. After constructing the neural network we evaluate the model for accuracy and robustness. We compute RMSE and perform cross-validation analysis. In cross validation, we check the variation in model accuracy as the length of training set is changed. We consider training sets with length 10 to 65. For each length a 100 samples are random picked and median RMSE is calculated. We show that model accuracy increases when training set is large. Before using the model for prediction, it is important to check the robustness of performance through cross validation.

The article provides a quick review neural network and is a useful reference for data enthusiasts. We have provided commented R code throughout the article to help readers with hands on experience of using neural networks.

\hypertarget{fitting-a-neural-network}{%
\chapter{Fitting a neural network}\label{fitting-a-neural-network}}

\hypertarget{introduction-23}{%
\section{Introduction}\label{introduction-23}}

\url{https://www.r-bloggers.com/fitting-a-neural-network-in-r-neuralnet-package/}

\url{https://datascienceplus.com/fitting-neural-network-in-r/}

Neural networks have always been one of the fascinating machine learning models in my opinion, not only because of the fancy backpropagation algorithm but also because of their complexity (think of deep learning with many hidden layers) and structure inspired by the brain.

Neural networks have not always been popular, partly because they were, and still are in some cases, computationally expensive and partly because they did not seem to yield better results when compared with simpler methods such as support vector machines (SVMs). Nevertheless, Neural Networks have, once again, raised attention and become popular.

Update: We published another post about Network analysis at DataScience+ Network analysis of Game of Thrones

In this post, we are going to fit a simple neural network using the neuralnet package and fit a linear model as a comparison.

\hypertarget{the-dataset-1}{%
\section{The dataset}\label{the-dataset-1}}

We are going to use the Boston dataset in the MASS package.
The Boston dataset is a collection of data about housing values in the suburbs of Boston. Our goal is to predict the median value of owner-occupied homes (medv) using all the other continuous variables available.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{500}\NormalTok{)}
\KeywordTok{library}\NormalTok{(MASS)}
\NormalTok{data <-}\StringTok{ }\NormalTok{Boston}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{glimpse}\NormalTok{(data)}
\CommentTok{#> Observations: 506}
\CommentTok{#> Variables: 14}
\CommentTok{#> $ crim    <dbl> 0.00632, 0.02731, 0.02729, 0.03237, 0.06905, 0.02985, ...}
\CommentTok{#> $ zn      <dbl> 18.0, 0.0, 0.0, 0.0, 0.0, 0.0, 12.5, 12.5, 12.5, 12.5,...}
\CommentTok{#> $ indus   <dbl> 2.31, 7.07, 7.07, 2.18, 2.18, 2.18, 7.87, 7.87, 7.87, ...}
\CommentTok{#> $ chas    <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...}
\CommentTok{#> $ nox     <dbl> 0.538, 0.469, 0.469, 0.458, 0.458, 0.458, 0.524, 0.524...}
\CommentTok{#> $ rm      <dbl> 6.58, 6.42, 7.18, 7.00, 7.15, 6.43, 6.01, 6.17, 5.63, ...}
\CommentTok{#> $ age     <dbl> 65.2, 78.9, 61.1, 45.8, 54.2, 58.7, 66.6, 96.1, 100.0,...}
\CommentTok{#> $ dis     <dbl> 4.09, 4.97, 4.97, 6.06, 6.06, 6.06, 5.56, 5.95, 6.08, ...}
\CommentTok{#> $ rad     <int> 1, 2, 2, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 4, 4, 4, 4, 4, ...}
\CommentTok{#> $ tax     <dbl> 296, 242, 242, 222, 222, 222, 311, 311, 311, 311, 311,...}
\CommentTok{#> $ ptratio <dbl> 15.3, 17.8, 17.8, 18.7, 18.7, 18.7, 15.2, 15.2, 15.2, ...}
\CommentTok{#> $ black   <dbl> 397, 397, 393, 395, 397, 394, 396, 397, 387, 387, 393,...}
\CommentTok{#> $ lstat   <dbl> 4.98, 9.14, 4.03, 2.94, 5.33, 5.21, 12.43, 19.15, 29.9...}
\CommentTok{#> $ medv    <dbl> 24.0, 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, ...}
\end{Highlighting}
\end{Shaded}

First we need to check that no datapoint is missing, otherwise we need to fix the dataset.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{apply}\NormalTok{(data,}\DecValTok{2}\NormalTok{,}\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{sum}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(x)))}
\CommentTok{#>    crim      zn   indus    chas     nox      rm     age     dis     rad }
\CommentTok{#>       0       0       0       0       0       0       0       0       0 }
\CommentTok{#>     tax ptratio   black   lstat    medv }
\CommentTok{#>       0       0       0       0       0}
\end{Highlighting}
\end{Shaded}

There is no missing data, good. We proceed by randomly splitting the data into a train and a test set, then we fit a linear regression model and test it on the test set. Note that I am using the gml() function instead of the lm() this will become useful later when cross validating the linear model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{index <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\KeywordTok{nrow}\NormalTok{(data),}\KeywordTok{round}\NormalTok{(}\FloatTok{0.75}\OperatorTok{*}\KeywordTok{nrow}\NormalTok{(data)))}
\NormalTok{train <-}\StringTok{ }\NormalTok{data[index,]}
\NormalTok{test <-}\StringTok{ }\NormalTok{data[}\OperatorTok{-}\NormalTok{index,]}
\NormalTok{lm.fit <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(medv}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{train)}
\KeywordTok{summary}\NormalTok{(lm.fit)}
\CommentTok{#> }
\CommentTok{#> Call:}
\CommentTok{#> glm(formula = medv ~ ., data = train)}
\CommentTok{#> }
\CommentTok{#> Deviance Residuals: }
\CommentTok{#>     Min       1Q   Median       3Q      Max  }
\CommentTok{#> -15.211   -2.559   -0.655    1.828   29.711  }
\CommentTok{#> }
\CommentTok{#> Coefficients:}
\CommentTok{#>              Estimate Std. Error t value Pr(>|t|)    }
\CommentTok{#> (Intercept)  31.11170    5.45981    5.70  2.5e-08 ***}
\CommentTok{#> crim         -0.11137    0.03326   -3.35  0.00090 ***}
\CommentTok{#> zn            0.04263    0.01431    2.98  0.00308 ** }
\CommentTok{#> indus         0.00148    0.06745    0.02  0.98247    }
\CommentTok{#> chas          1.75684    0.98109    1.79  0.07417 .  }
\CommentTok{#> nox         -18.18485    4.47157   -4.07  5.8e-05 ***}
\CommentTok{#> rm            4.76034    0.48047    9.91  < 2e-16 ***}
\CommentTok{#> age          -0.01344    0.01410   -0.95  0.34119    }
\CommentTok{#> dis          -1.55375    0.21893   -7.10  6.7e-12 ***}
\CommentTok{#> rad           0.28818    0.07202    4.00  7.6e-05 ***}
\CommentTok{#> tax          -0.01374    0.00406   -3.38  0.00079 ***}
\CommentTok{#> ptratio      -0.94755    0.14012   -6.76  5.4e-11 ***}
\CommentTok{#> black         0.00950    0.00290    3.28  0.00115 ** }
\CommentTok{#> lstat        -0.38890    0.05973   -6.51  2.5e-10 ***}
\CommentTok{#> ---}
\CommentTok{#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1}
\CommentTok{#> }
\CommentTok{#> (Dispersion parameter for gaussian family taken to be 20.2)}
\CommentTok{#> }
\CommentTok{#>     Null deviance: 32463.5  on 379  degrees of freedom}
\CommentTok{#> Residual deviance:  7407.1  on 366  degrees of freedom}
\CommentTok{#> AIC: 2237}
\CommentTok{#> }
\CommentTok{#> Number of Fisher Scoring iterations: 2}
\NormalTok{pr.lm <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(lm.fit,test)}
\NormalTok{MSE.lm <-}\StringTok{ }\KeywordTok{sum}\NormalTok{((pr.lm }\OperatorTok{-}\StringTok{ }\NormalTok{test}\OperatorTok{$}\NormalTok{medv)}\OperatorTok{^}\DecValTok{2}\NormalTok{)}\OperatorTok{/}\KeywordTok{nrow}\NormalTok{(test)}
\end{Highlighting}
\end{Shaded}

The sample(x,size) function simply outputs a vector of the specified size of randomly selected samples from the vector x. By default the sampling is without replacement: index is essentially a random vector of indeces.
Since we are dealing with a regression problem, we are going to use the mean squared error (MSE) as a measure of how much our predictions are far away from the real data.

\hypertarget{preparing-to-fit-the-neural-network}{%
\section{Preparing to fit the neural network}\label{preparing-to-fit-the-neural-network}}

Before fitting a neural network, some preparation need to be done. Neural networks are not that easy to train and tune.

As a first step, we are going to address data preprocessing.
It is good practice to normalize your data before training a neural network. I cannot emphasize enough how important this step is: depending on your dataset, avoiding normalization may lead to useless results or to a very difficult training process (most of the times the algorithm will not converge before the number of maximum iterations allowed). You can choose different methods to scale the data (z-normalization, min-max scale, etc\ldots{}). I chose to use the min-max method and scale the data in the interval {[}0,1{]}. Usually scaling in the intervals {[}0,1{]} or {[}-1,1{]} tends to give better results.
We therefore scale and split the data before moving on:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{maxs <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(data, }\DecValTok{2}\NormalTok{, max) }
\NormalTok{mins <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(data, }\DecValTok{2}\NormalTok{, min)}

\NormalTok{scaled <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(}\KeywordTok{scale}\NormalTok{(data, }\DataTypeTok{center =}\NormalTok{ mins, }\DataTypeTok{scale =}\NormalTok{ maxs }\OperatorTok{-}\StringTok{ }\NormalTok{mins))}

\NormalTok{train_ <-}\StringTok{ }\NormalTok{scaled[index,]}
\NormalTok{test_ <-}\StringTok{ }\NormalTok{scaled[}\OperatorTok{-}\NormalTok{index,]}
\end{Highlighting}
\end{Shaded}

Note that scale returns a matrix that needs to be coerced into a data.frame.

\hypertarget{parameters}{%
\section{Parameters}\label{parameters}}

As far as I know there is no fixed rule as to how many layers and neurons to use although there are several more or less accepted rules of thumb. Usually, if at all necessary, one hidden layer is enough for a vast numbers of applications. As far as the number of neurons is concerned, it should be between the input layer size and the output layer size, usually 2/3 of the input size. At least in my brief experience testing again and again is the best solution since there is no guarantee that any of these rules will fit your model best.
Since this is a toy example, we are going to use 2 hidden layers with this configuration: 13:5:3:1. The input layer has 13 inputs, the two hidden layers have 5 and 3 neurons and the output layer has, of course, a single output since we are doing regression.
Let's fit the net:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(neuralnet)}
\NormalTok{n <-}\StringTok{ }\KeywordTok{names}\NormalTok{(train_)}
\NormalTok{f <-}\StringTok{ }\KeywordTok{as.formula}\NormalTok{(}\KeywordTok{paste}\NormalTok{(}\StringTok{"medv ~"}\NormalTok{, }\KeywordTok{paste}\NormalTok{(n[}\OperatorTok{!}\NormalTok{n }\OperatorTok{%in%}\StringTok{ "medv"}\NormalTok{], }\DataTypeTok{collapse =} \StringTok{" + "}\NormalTok{)))}
\NormalTok{nn <-}\StringTok{ }\KeywordTok{neuralnet}\NormalTok{(f,}\DataTypeTok{data=}\NormalTok{train_,}\DataTypeTok{hidden=}\KeywordTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{,}\DecValTok{3}\NormalTok{),}\DataTypeTok{linear.output=}\NormalTok{T)}
\end{Highlighting}
\end{Shaded}

\textbf{A couple of notes:}

\begin{itemize}
\item
  For some reason the formula y\textasciitilde{}. is not accepted in the neuralnet() function. You need to first write the formula and then pass it as an argument in the fitting function.
\item
  The hidden argument accepts a vector with the number of neurons for each hidden layer, while the argument linear.output is used to specify whether we want to do regression linear.output=TRUE or classification linear.output=FALSE
\end{itemize}

The \texttt{neuralnet} package provides a nice tool to plot the model:

This is the graphical representation of the model with the weights on each connection:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(nn)}
\end{Highlighting}
\end{Shaded}

The black lines show the connections between each layer and the weights on each connection while the blue lines show the bias term added in each step. The bias can be thought as the intercept of a linear model.
The net is essentially a black box so we cannot say that much about the fitting, the weights and the model. Suffice to say that the training algorithm has converged and therefore the model is ready to be used.

\hypertarget{predicting-medv-using-the-neural-network}{%
\section{Predicting medv using the neural network}\label{predicting-medv-using-the-neural-network}}

Now we can try to predict the values for the test set and calculate the MSE. Remember that the net will output a normalized prediction, so we need to scale it back in order to make a meaningful comparison (or just a simple prediction).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pr.nn <-}\StringTok{ }\KeywordTok{compute}\NormalTok{(nn,test_[,}\DecValTok{1}\OperatorTok{:}\DecValTok{13}\NormalTok{])}

\NormalTok{pr.nn_ <-}\StringTok{ }\NormalTok{pr.nn}\OperatorTok{$}\NormalTok{net.result}\OperatorTok{*}\NormalTok{(}\KeywordTok{max}\NormalTok{(data}\OperatorTok{$}\NormalTok{medv)}\OperatorTok{-}\KeywordTok{min}\NormalTok{(data}\OperatorTok{$}\NormalTok{medv))}\OperatorTok{+}\KeywordTok{min}\NormalTok{(data}\OperatorTok{$}\NormalTok{medv)}
\NormalTok{test.r <-}\StringTok{ }\NormalTok{(test_}\OperatorTok{$}\NormalTok{medv)}\OperatorTok{*}\NormalTok{(}\KeywordTok{max}\NormalTok{(data}\OperatorTok{$}\NormalTok{medv)}\OperatorTok{-}\KeywordTok{min}\NormalTok{(data}\OperatorTok{$}\NormalTok{medv))}\OperatorTok{+}\KeywordTok{min}\NormalTok{(data}\OperatorTok{$}\NormalTok{medv)}

\NormalTok{MSE.nn <-}\StringTok{ }\KeywordTok{sum}\NormalTok{((test.r }\OperatorTok{-}\StringTok{ }\NormalTok{pr.nn_)}\OperatorTok{^}\DecValTok{2}\NormalTok{)}\OperatorTok{/}\KeywordTok{nrow}\NormalTok{(test_)}
\end{Highlighting}
\end{Shaded}

we then compare the two MSEs

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{print}\NormalTok{(}\KeywordTok{paste}\NormalTok{(MSE.lm,MSE.nn))}
\CommentTok{#> [1] "31.2630222372615 16.4595537665717"}
\end{Highlighting}
\end{Shaded}

Apparently, the net is doing a better work than the linear model at predicting medv. Once again, be careful because this result depends on the train-test split performed above. Below, after the visual plot, we are going to perform a fast cross validation in order to be more confident about the results.

A first visual approach to the performance of the network and the linear model on the test set is plotted below

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{))}

\KeywordTok{plot}\NormalTok{(test}\OperatorTok{$}\NormalTok{medv,pr.nn_,}\DataTypeTok{col=}\StringTok{'red'}\NormalTok{,}\DataTypeTok{main=}\StringTok{'Real vs predicted NN'}\NormalTok{,}\DataTypeTok{pch=}\DecValTok{18}\NormalTok{,}\DataTypeTok{cex=}\FloatTok{0.7}\NormalTok{)}
\KeywordTok{abline}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DataTypeTok{lwd=}\DecValTok{2}\NormalTok{)}
\KeywordTok{legend}\NormalTok{(}\StringTok{'bottomright'}\NormalTok{,}\DataTypeTok{legend=}\StringTok{'NN'}\NormalTok{,}\DataTypeTok{pch=}\DecValTok{18}\NormalTok{,}\DataTypeTok{col=}\StringTok{'red'}\NormalTok{, }\DataTypeTok{bty=}\StringTok{'n'}\NormalTok{)}

\KeywordTok{plot}\NormalTok{(test}\OperatorTok{$}\NormalTok{medv,pr.lm,}\DataTypeTok{col=}\StringTok{'blue'}\NormalTok{,}\DataTypeTok{main=}\StringTok{'Real vs predicted lm'}\NormalTok{,}\DataTypeTok{pch=}\DecValTok{18}\NormalTok{, }\DataTypeTok{cex=}\FloatTok{0.7}\NormalTok{)}
\KeywordTok{abline}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DataTypeTok{lwd=}\DecValTok{2}\NormalTok{)}
\KeywordTok{legend}\NormalTok{(}\StringTok{'bottomright'}\NormalTok{,}\DataTypeTok{legend=}\StringTok{'LM'}\NormalTok{,}\DataTypeTok{pch=}\DecValTok{18}\NormalTok{,}\DataTypeTok{col=}\StringTok{'blue'}\NormalTok{, }\DataTypeTok{bty=}\StringTok{'n'}\NormalTok{, }\DataTypeTok{cex=}\NormalTok{.}\DecValTok{95}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{nn-regression_902-fitting_neural_network_files/figure-latex/unnamed-chunk-11-1} \end{center}

By visually inspecting the plot we can see that the predictions made by the neural network are (in general) more concetrated around the line (a perfect alignment with the line would indicate a MSE of 0 and thus an ideal perfect prediction) than those made by the linear model.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(test}\OperatorTok{$}\NormalTok{medv,pr.nn_,}\DataTypeTok{col=}\StringTok{'red'}\NormalTok{,}\DataTypeTok{main=}\StringTok{'Real vs predicted NN'}\NormalTok{,}\DataTypeTok{pch=}\DecValTok{18}\NormalTok{,}\DataTypeTok{cex=}\FloatTok{0.7}\NormalTok{)}
\KeywordTok{points}\NormalTok{(test}\OperatorTok{$}\NormalTok{medv,pr.lm,}\DataTypeTok{col=}\StringTok{'blue'}\NormalTok{,}\DataTypeTok{pch=}\DecValTok{18}\NormalTok{,}\DataTypeTok{cex=}\FloatTok{0.7}\NormalTok{)}
\KeywordTok{abline}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DataTypeTok{lwd=}\DecValTok{2}\NormalTok{)}
\KeywordTok{legend}\NormalTok{(}\StringTok{'bottomright'}\NormalTok{,}\DataTypeTok{legend=}\KeywordTok{c}\NormalTok{(}\StringTok{'NN'}\NormalTok{,}\StringTok{'LM'}\NormalTok{),}\DataTypeTok{pch=}\DecValTok{18}\NormalTok{,}\DataTypeTok{col=}\KeywordTok{c}\NormalTok{(}\StringTok{'red'}\NormalTok{,}\StringTok{'blue'}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{nn-regression_902-fitting_neural_network_files/figure-latex/unnamed-chunk-12-1} \end{center}

\hypertarget{a-fast-cross-validation}{%
\section{A (fast) cross validation}\label{a-fast-cross-validation}}

Cross validation is another very important step of building predictive models. While there are different kind of cross validation methods, the basic idea is repeating the following process a number of time:

\textbf{train-test split}

\begin{itemize}
\tightlist
\item
  Do the train-test split
\item
  Fit the model to the train set
\item
  Test the model on the test set
\item
  Calculate the prediction error
\item
  Repeat the process K times
\end{itemize}

Then by calculating the average error we can get a grasp of how the model is doing.

We are going to implement a fast cross validation using a for loop for the neural network and the cv.glm() function in the boot package for the linear model.
As far as I know, there is no built-in function in R to perform cross-validation on this kind of neural network, if you do know such a function, please let me know in the comments. Here is the 10 fold cross-validated MSE for the linear model:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(boot)}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{200}\NormalTok{)}
\NormalTok{lm.fit <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(medv}\OperatorTok{~}\NormalTok{.,}\DataTypeTok{data=}\NormalTok{data)}
\KeywordTok{cv.glm}\NormalTok{(data,lm.fit,}\DataTypeTok{K=}\DecValTok{10}\NormalTok{)}\OperatorTok{$}\NormalTok{delta[}\DecValTok{1}\NormalTok{]}
\CommentTok{#> [1] 23.2}
\end{Highlighting}
\end{Shaded}

Now the net. Note that I am splitting the data in this way: 90\% train set and 10\% test set in a random way for 10 times. I am also initializing a progress bar using the plyr library because I want to keep an eye on the status of the process since the fitting of the neural network may take a while.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{450}\NormalTok{)}
\NormalTok{cv.error <-}\StringTok{ }\OtherTok{NULL}
\NormalTok{k <-}\StringTok{ }\DecValTok{10}

\KeywordTok{library}\NormalTok{(plyr) }
\NormalTok{pbar <-}\StringTok{ }\KeywordTok{create_progress_bar}\NormalTok{(}\StringTok{'text'}\NormalTok{)}
\NormalTok{pbar}\OperatorTok{$}\KeywordTok{init}\NormalTok{(k)}
\CommentTok{#> }
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|}\StringTok{                                                                 }\ErrorTok{|}\StringTok{   }\DecValTok{0}\NormalTok{%}

\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{k)\{}
\NormalTok{    index <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\KeywordTok{nrow}\NormalTok{(data),}\KeywordTok{round}\NormalTok{(}\FloatTok{0.9}\OperatorTok{*}\KeywordTok{nrow}\NormalTok{(data)))}
\NormalTok{    train.cv <-}\StringTok{ }\NormalTok{scaled[index,]}
\NormalTok{    test.cv <-}\StringTok{ }\NormalTok{scaled[}\OperatorTok{-}\NormalTok{index,]}
    
\NormalTok{    nn <-}\StringTok{ }\KeywordTok{neuralnet}\NormalTok{(f,}\DataTypeTok{data=}\NormalTok{train.cv,}\DataTypeTok{hidden=}\KeywordTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{,}\DecValTok{2}\NormalTok{),}\DataTypeTok{linear.output=}\NormalTok{T)}
    
\NormalTok{    pr.nn <-}\StringTok{ }\KeywordTok{compute}\NormalTok{(nn,test.cv[,}\DecValTok{1}\OperatorTok{:}\DecValTok{13}\NormalTok{])}
\NormalTok{    pr.nn <-}\StringTok{ }\NormalTok{pr.nn}\OperatorTok{$}\NormalTok{net.result}\OperatorTok{*}\NormalTok{(}\KeywordTok{max}\NormalTok{(data}\OperatorTok{$}\NormalTok{medv)}\OperatorTok{-}\KeywordTok{min}\NormalTok{(data}\OperatorTok{$}\NormalTok{medv))}\OperatorTok{+}\KeywordTok{min}\NormalTok{(data}\OperatorTok{$}\NormalTok{medv)}
    
\NormalTok{    test.cv.r <-}\StringTok{ }\NormalTok{(test.cv}\OperatorTok{$}\NormalTok{medv)}\OperatorTok{*}\NormalTok{(}\KeywordTok{max}\NormalTok{(data}\OperatorTok{$}\NormalTok{medv)}\OperatorTok{-}\KeywordTok{min}\NormalTok{(data}\OperatorTok{$}\NormalTok{medv))}\OperatorTok{+}\KeywordTok{min}\NormalTok{(data}\OperatorTok{$}\NormalTok{medv)}
    
\NormalTok{    cv.error[i] <-}\StringTok{ }\KeywordTok{sum}\NormalTok{((test.cv.r }\OperatorTok{-}\StringTok{ }\NormalTok{pr.nn)}\OperatorTok{^}\DecValTok{2}\NormalTok{)}\OperatorTok{/}\KeywordTok{nrow}\NormalTok{(test.cv)}
    
\NormalTok{    pbar}\OperatorTok{$}\KeywordTok{step}\NormalTok{()}
\NormalTok{\}}
\CommentTok{#> }
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|======}\StringTok{                                                           }\ErrorTok{|}\StringTok{  }\DecValTok{10}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=============}\StringTok{                                                    }\ErrorTok{|}\StringTok{  }\DecValTok{20}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|====================}\StringTok{                                             }\ErrorTok{|}\StringTok{  }\DecValTok{30}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==========================}\StringTok{                                       }\ErrorTok{|}\StringTok{  }\DecValTok{40}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|================================}\StringTok{                                 }\ErrorTok{|}\StringTok{  }\DecValTok{50}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=======================================}\StringTok{                          }\ErrorTok{|}\StringTok{  }\DecValTok{60}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==============================================}\StringTok{                   }\ErrorTok{|}\StringTok{  }\DecValTok{70}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|====================================================}\StringTok{             }\ErrorTok{|}\StringTok{  }\DecValTok{80}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|==========================================================}\StringTok{       }\ErrorTok{|}\StringTok{  }\DecValTok{90}\NormalTok{%}
  \OperatorTok{|}\StringTok{                                                                       }
\StringTok{  }\ErrorTok{|=================================================================|}\StringTok{ }\DecValTok{100}\NormalTok{%}
\end{Highlighting}
\end{Shaded}

After a while, the process is done, we calculate the average MSE and plot the results as a boxplot

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(cv.error)}
\CommentTok{#> [1] 7.64}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cv.error}
\CommentTok{#>  [1] 13.33  7.10  6.58  5.70  6.84  5.77 10.75  5.38  9.45  5.50}
\end{Highlighting}
\end{Shaded}

The code for the box plot:
The code above outputs the following boxplot:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{boxplot}\NormalTok{(cv.error,}\DataTypeTok{xlab=}\StringTok{'MSE CV'}\NormalTok{,}\DataTypeTok{col=}\StringTok{'cyan'}\NormalTok{,}
        \DataTypeTok{border=}\StringTok{'blue'}\NormalTok{,}\DataTypeTok{names=}\StringTok{'CV error (MSE)'}\NormalTok{,}
        \DataTypeTok{main=}\StringTok{'CV error (MSE) for NN'}\NormalTok{,}\DataTypeTok{horizontal=}\OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{nn-regression_902-fitting_neural_network_files/figure-latex/unnamed-chunk-17-1} \end{center}

As you can see, the average MSE for the neural network (10.33) is lower than the one of the linear model although there seems to be a certain degree of variation in the MSEs of the cross validation. This may depend on the splitting of the data or the random initialization of the weights in the net. By running the simulation different times with different seeds you can get a more precise point estimate for the average MSE.

\hypertarget{a-final-note-on-model-interpretability}{%
\section{A final note on model interpretability}\label{a-final-note-on-model-interpretability}}

Neural networks resemble black boxes a lot: explaining their outcome is much more difficult than explaining the outcome of simpler model such as a linear model. Therefore, depending on the kind of application you need, you might want to take into account this factor too. Furthermore, as you have seen above, extra care is needed to fit a neural network and small changes can lead to different results.

A gist with the full code for this post can be found here.

Thank you for reading this post, leave a comment below if you have any question.

\hypertarget{visualization-of-neural-networks}{%
\chapter{Visualization of neural networks}\label{visualization-of-neural-networks}}

\url{https://beckmw.wordpress.com/tag/neuralnet/}

In my last post I said I wasn't going to write anymore about neural networks (i.e., multilayer feedforward perceptron, supervised ANN, etc.). That was a lie. I've received several requests to update the neural network plotting function described in the original post. As previously explained, R does not provide a lot of options for visualizing neural networks. The only option I know of is a plotting method for objects from the neuralnet package. This may be my opinion, but I think this plot leaves much to be desired (see below). Also, no plotting methods exist for neural networks created in other packages, i.e., nnet and RSNNS. These packages are the only ones listed on the CRAN task view, so I've updated my original plotting function to work with all three. Additionally, I've added a new option for plotting a raw weight vector to allow use with neural networks created elsewhere. This blog describes these changes, as well as some new arguments added to the original function.

As usual, I'll simulate some data to use for creating the neural networks. The dataset contains eight input variables and two output variables. The final dataset is a data frame with all variables, as well as separate data frames for the input and output variables. I've retained separate datasets based on the syntax for each package.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(clusterGeneration)}
\CommentTok{#> Loading required package: MASS}
\KeywordTok{library}\NormalTok{(tictoc)}
 
\NormalTok{seed.val<-}\StringTok{ }\DecValTok{12345}
\KeywordTok{set.seed}\NormalTok{(seed.val)}
 
\NormalTok{num.vars<-}\DecValTok{8}
\NormalTok{num.obs<-}\DecValTok{1000}
 
\CommentTok{# input variables}
\NormalTok{cov.mat <-}\KeywordTok{genPositiveDefMat}\NormalTok{(num.vars,}\DataTypeTok{covMethod=}\KeywordTok{c}\NormalTok{(}\StringTok{"unifcorrmat"}\NormalTok{))}\OperatorTok{$}\NormalTok{Sigma}
\NormalTok{rand.vars <-}\KeywordTok{mvrnorm}\NormalTok{(num.obs,}\KeywordTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{,num.vars),}\DataTypeTok{Sigma=}\NormalTok{cov.mat)}
 
\CommentTok{# output variables}
\NormalTok{parms <-}\KeywordTok{runif}\NormalTok{(num.vars,}\OperatorTok{-}\DecValTok{10}\NormalTok{,}\DecValTok{10}\NormalTok{)}
\NormalTok{y1 <-}\StringTok{ }\NormalTok{rand.vars }\OperatorTok{%*%}\StringTok{ }\KeywordTok{matrix}\NormalTok{(parms) }\OperatorTok{+}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(num.obs,}\DataTypeTok{sd=}\DecValTok{20}\NormalTok{)}
\NormalTok{parms2 <-}\StringTok{ }\KeywordTok{runif}\NormalTok{(num.vars,}\OperatorTok{-}\DecValTok{10}\NormalTok{,}\DecValTok{10}\NormalTok{)}
\NormalTok{y2 <-}\StringTok{ }\NormalTok{rand.vars }\OperatorTok{%*%}\StringTok{ }\KeywordTok{matrix}\NormalTok{(parms2) }\OperatorTok{+}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(num.obs,}\DataTypeTok{sd=}\DecValTok{20}\NormalTok{)}
 
\CommentTok{# final datasets}
\NormalTok{rand.vars <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(rand.vars)}
\NormalTok{resp <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(y1,y2)}
\KeywordTok{names}\NormalTok{(resp) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{'Y1'}\NormalTok{,}\StringTok{'Y2'}\NormalTok{)}
\NormalTok{dat.in <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(resp, rand.vars)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{glimpse}\NormalTok{(dat.in)}
\CommentTok{#> Observations: 1,000}
\CommentTok{#> Variables: 10}
\CommentTok{#> $ Y1 <dbl> 25.442, -14.578, -36.214, 15.216, -6.393, -20.849, -28.665,...}
\CommentTok{#> $ Y2 <dbl> 16.9, 38.8, 31.2, -31.2, 93.3, 11.7, 59.7, -103.5, -49.8, 5...}
\CommentTok{#> $ X1 <dbl> 3.138, -0.705, -4.373, 0.837, 0.787, 1.923, -1.419, 1.121, ...}
\CommentTok{#> $ X2 <dbl> 0.195, -0.302, 0.773, 1.311, 3.506, 1.245, 3.800, -0.165, 0...}
\CommentTok{#> $ X3 <dbl> -1.795, -2.596, 2.308, 4.081, -3.921, 1.473, -0.926, 7.101,...}
\CommentTok{#> $ X4 <dbl> -2.7216, 3.0589, 1.2455, 3.4607, 2.3775, -2.9833, 2.6669, -...}
\CommentTok{#> $ X5 <dbl> 0.0407, 0.7602, -3.0217, -4.2799, 2.0859, 1.4765, 0.0561, 2...}
\CommentTok{#> $ X6 <dbl> -1.4820, -0.5014, 0.0603, -1.8551, 2.2817, 1.7386, 1.7450, ...}
\CommentTok{#> $ X7 <dbl> -0.7169, -0.3618, -1.5283, 4.2026, -6.1548, -0.3545, -6.028...}
\CommentTok{#> $ X8 <dbl> 1.152, 1.810, -1.357, 0.598, -1.425, -1.210, -1.004, 2.494,...}
\end{Highlighting}
\end{Shaded}

The various neural network packages are used to create separate models for plotting.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# first model with nnet}
\CommentTok{#nnet function from nnet package}
\KeywordTok{library}\NormalTok{(nnet)}
\KeywordTok{set.seed}\NormalTok{(seed.val)}
\KeywordTok{tic}\NormalTok{()}
\NormalTok{mod1 <-}\StringTok{ }\KeywordTok{nnet}\NormalTok{(rand.vars, resp, }\DataTypeTok{data =}\NormalTok{ dat.in, }\DataTypeTok{size =} \DecValTok{10}\NormalTok{, }\DataTypeTok{linout =}\NormalTok{ T)}
\CommentTok{#> # weights:  112}
\CommentTok{#> initial  value 4784162.893260 }
\CommentTok{#> iter  10 value 1794537.980652}
\CommentTok{#> iter  20 value 1577753.498759}
\CommentTok{#> iter  30 value 1485254.945755}
\CommentTok{#> iter  40 value 1449238.248788}
\CommentTok{#> iter  50 value 1427720.291804}
\CommentTok{#> iter  60 value 1416977.236373}
\CommentTok{#> iter  70 value 1405167.753521}
\CommentTok{#> iter  80 value 1395046.792257}
\CommentTok{#> iter  90 value 1370522.267277}
\CommentTok{#> iter 100 value 1363709.540981}
\CommentTok{#> final  value 1363709.540981 }
\CommentTok{#> stopped after 100 iterations}
\KeywordTok{toc}\NormalTok{()}
\CommentTok{#> 0.186 sec elapsed}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# nn <- neuralnet(form.in,}
\CommentTok{#                 data = dat.sc,}
\CommentTok{#                 # hidden = c(13, 10, 3),}
\CommentTok{#                 hidden = c(5),}
\CommentTok{#                 act.fct = "tanh",}
\CommentTok{#                 linear.output = FALSE,}
\CommentTok{#                 lifesign = "minimal")}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# 2nd model with neuralnet}
\CommentTok{# neuralnet function from neuralnet package, notice use of only one response}
\KeywordTok{library}\NormalTok{(neuralnet)}

\NormalTok{softplus <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{log}\NormalTok{(}\DecValTok{1} \OperatorTok{+}\StringTok{ }\KeywordTok{exp}\NormalTok{(x))}
\NormalTok{sigmoid  <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{log}\NormalTok{(}\DecValTok{1} \OperatorTok{+}\StringTok{ }\KeywordTok{exp}\NormalTok{(}\OperatorTok{-}\NormalTok{x))}

\NormalTok{dat.sc <-}\StringTok{ }\KeywordTok{scale}\NormalTok{(dat.in)}
\NormalTok{form.in <-}\StringTok{ }\KeywordTok{as.formula}\NormalTok{(}\StringTok{'Y1 ~ X1+X2+X3+X4+X5+X6+X7+X8'}\NormalTok{)}
\KeywordTok{set.seed}\NormalTok{(seed.val)}
\KeywordTok{tic}\NormalTok{()}
\NormalTok{mod2 <-}\StringTok{ }\KeywordTok{neuralnet}\NormalTok{(form.in, }\DataTypeTok{data =}\NormalTok{ dat.sc, }\DataTypeTok{hidden =} \DecValTok{10}\NormalTok{, }\DataTypeTok{lifesign =} \StringTok{"minimal"}\NormalTok{,}
                  \DataTypeTok{linear.output =} \OtherTok{FALSE}\NormalTok{,}
                  \DataTypeTok{act.fct =} \StringTok{"tanh"}\NormalTok{)}
\CommentTok{#> hidden: 10    thresh: 0.01    rep: 1/1    steps:   26361 error: 160.06372    time: 36.42 secs}
\KeywordTok{toc}\NormalTok{()}
\CommentTok{#> 36.424 sec elapsed}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# third model with RSNNS}
\CommentTok{# mlp function from RSNNS package}
\KeywordTok{library}\NormalTok{(RSNNS)}
\CommentTok{#> Loading required package: Rcpp}
\KeywordTok{set.seed}\NormalTok{(seed.val)}
\KeywordTok{tic}\NormalTok{()}
\NormalTok{mod3 <-}\StringTok{ }\KeywordTok{mlp}\NormalTok{(rand.vars, resp, }\DataTypeTok{size =} \DecValTok{10}\NormalTok{, }\DataTypeTok{linOut =}\NormalTok{ T)}
\KeywordTok{toc}\NormalTok{()}
\CommentTok{#> 0.348 sec elapsed}
\end{Highlighting}
\end{Shaded}

I've noticed some differences between the functions that could lead to some confusion. For simplicity, the above code represents my interpretation of the most direct way to create a neural network in each package. Be very aware that direct comparison of results is not advised given that the default arguments differ between the packages. A few key differences are as follows, although many others should be noted. First, the functions differ in the methods for passing the primary input variables.

The \texttt{nnet} function can take separate (or combined) x and y inputs as data frames or as a formula, the \texttt{neuralnet} function can only use a formula as input, and the mlp function can only take a data frame as combined or separate variables as input. As far as I know, the neuralnet function is not capable of modelling multiple response variables, unless the response is a categorical variable that uses one node for each outcome. Additionally, the default output for the neuralnet function is linear, whereas the opposite is true for the other two functions.

Specifics aside, here's how to use the updated plot function. Note that the same syntax is used to plot each model

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# import the function from Github}
\KeywordTok{library}\NormalTok{(devtools)}
\KeywordTok{source_url}\NormalTok{(}\StringTok{'https://gist.githubusercontent.com/fawda123/7471137/raw/466c1474d0a505ff044412703516c34f1a4684a5/nnet_plot_update.r'}\NormalTok{)}
\CommentTok{#> SHA-1 hash of file is 74c80bd5ddbc17ab3ae5ece9c0ed9beb612e87ef}
 
\CommentTok{# plot each model}
\KeywordTok{plot.nnet}\NormalTok{(mod1)}
\CommentTok{#> Loading required package: scales}
\CommentTok{#> Loading required package: reshape}
\KeywordTok{plot.nnet}\NormalTok{(mod2) }
\KeywordTok{plot.nnet}\NormalTok{(mod3)}
\CommentTok{#> Warning in plot.nnet(mod3): Bias layer not applicable for rsnns object}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{nn-regression_903-visualization_nn_files/figure-latex/unnamed-chunk-5-1} \includegraphics[width=0.7\linewidth]{nn-regression_903-visualization_nn_files/figure-latex/unnamed-chunk-5-2} \includegraphics[width=0.7\linewidth]{nn-regression_903-visualization_nn_files/figure-latex/unnamed-chunk-5-3} \end{center}

The plotting function can also now be used with an arbitrary weight vector, rather than a specific model object. The struct argument must also be included if this option is used. I thought the easiest way to use the plotting function with your own weights was to have the input weights as a numeric vector, including bias layers. I've shown how this can be done using the weights directly from mod1 for simplicity.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{wts.in <-}\StringTok{ }\NormalTok{mod1}\OperatorTok{$}\NormalTok{wts}
\NormalTok{struct <-}\StringTok{ }\NormalTok{mod1}\OperatorTok{$}\NormalTok{n}
\KeywordTok{plot.nnet}\NormalTok{(wts.in,}\DataTypeTok{struct=}\NormalTok{struct)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{nn-regression_903-visualization_nn_files/figure-latex/unnamed-chunk-6-1} \end{center}

Note that \texttt{wts.in} is a numeric vector with length equal to the expected given the architecture (i.e., for 8 10 2 network, 100 connection weights plus 12 bias weights). The plot should look the same as the plot for the neural network from nnet.

The weights in the input vector need to be in a specific order for correct plotting. I realize this is not clear by looking directly at wt.in but this was the simplest approach I could think of. The weight vector shows the weights for each hidden node in sequence, starting with the bias input for each node, then the weights for each output node in sequence, starting with the bias input for each output node. Note that the bias layer has to be included even if the network was not created with biases. If this is the case, simply input a random number where the bias values should go and use the argument bias=F. I'll show the correct order of the weights using an example with plot.nn from the neuralnet package since the weights are included directly on the plot.

If we pretend that the above figure wasn't created in R, we would input the mod.in argument for the updated plotting function as follows. Also note that struct must be included if using this approach.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mod.in<-}\KeywordTok{c}\NormalTok{(}\FloatTok{13.12}\NormalTok{,}\FloatTok{1.49}\NormalTok{,}\FloatTok{0.16}\NormalTok{,}\OperatorTok{-}\FloatTok{0.11}\NormalTok{,}\OperatorTok{-}\FloatTok{0.19}\NormalTok{,}\OperatorTok{-}\FloatTok{0.16}\NormalTok{,}\FloatTok{0.56}\NormalTok{,}\OperatorTok{-}\FloatTok{0.52}\NormalTok{,}\FloatTok{0.81}\NormalTok{)}
\NormalTok{struct<-}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{) }\CommentTok{#two inputs, two hidden, one output }
\KeywordTok{plot.nnet}\NormalTok{(mod.in, }\DataTypeTok{struct=}\NormalTok{struct)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{nn-regression_903-visualization_nn_files/figure-latex/unnamed-chunk-7-1} \end{center}

Note the comparability with the figure created using the neuralnet package. That is, larger weights have thicker lines and color indicates sign (+ black, -- grey).

One of these days I'll actually put these functions in a package. In the meantime, please let me know if any bugs are encountered.

\hypertarget{caret-and-plot-nn}{%
\section{caret and plot NN}\label{caret-and-plot-nn}}

I've changed the function to work with neural networks created using the train function from the caret package. The link above is updated but you can also grab it here.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(caret)}
\CommentTok{#> Loading required package: lattice}
\CommentTok{#> Loading required package: ggplot2}
\CommentTok{#> Registered S3 methods overwritten by 'ggplot2':}
\CommentTok{#>   method         from }
\CommentTok{#>   [.quosures     rlang}
\CommentTok{#>   c.quosures     rlang}
\CommentTok{#>   print.quosures rlang}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'caret'}
\CommentTok{#> The following objects are masked from 'package:RSNNS':}
\CommentTok{#> }
\CommentTok{#>     confusionMatrix, train}
\NormalTok{mod4 <-}\StringTok{ }\KeywordTok{train}\NormalTok{(Y1 }\OperatorTok{~}\NormalTok{., }\DataTypeTok{method=}\StringTok{'nnet'}\NormalTok{, }\DataTypeTok{data=}\NormalTok{dat.in, }\DataTypeTok{linout=}\NormalTok{T)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot.nnet}\NormalTok{(mod4,}\DataTypeTok{nid=}\NormalTok{T)}
\CommentTok{#> Warning in plot.nnet(mod4, nid = T): Using best nnet model from train}
\CommentTok{#> output}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{nn-regression_903-visualization_nn_files/figure-latex/unnamed-chunk-8-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fact<-}\KeywordTok{factor}\NormalTok{(}\KeywordTok{sample}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{'a'}\NormalTok{,}\StringTok{'b'}\NormalTok{,}\StringTok{'c'}\NormalTok{),}\DataTypeTok{size=}\NormalTok{num.obs,}\DataTypeTok{replace=}\NormalTok{T))}
\NormalTok{form.in<-}\KeywordTok{formula}\NormalTok{(}\StringTok{'cbind(Y2,Y1)~X1+X2+X3+fact'}\NormalTok{)}
\NormalTok{mod5<-}\KeywordTok{nnet}\NormalTok{(form.in,}\DataTypeTok{data=}\KeywordTok{cbind}\NormalTok{(dat.in,fact),}\DataTypeTok{size=}\DecValTok{10}\NormalTok{,}\DataTypeTok{linout=}\NormalTok{T)}
\CommentTok{#> # weights:  82}
\CommentTok{#> initial  value 4799569.423556 }
\CommentTok{#> iter  10 value 2864553.218126}
\CommentTok{#> iter  20 value 2595828.194160}
\CommentTok{#> iter  30 value 2517965.483941}
\CommentTok{#> iter  40 value 2464882.178217}
\CommentTok{#> iter  50 value 2444238.700834}
\CommentTok{#> iter  60 value 2424302.290643}
\CommentTok{#> iter  70 value 2395226.949866}
\CommentTok{#> iter  80 value 2375558.751266}
\CommentTok{#> iter  90 value 2343011.050867}
\CommentTok{#> iter 100 value 2298860.593948}
\CommentTok{#> final  value 2298860.593948 }
\CommentTok{#> stopped after 100 iterations}
\KeywordTok{plot.nnet}\NormalTok{(mod5,}\DataTypeTok{nid=}\NormalTok{T)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{nn-regression_903-visualization_nn_files/figure-latex/unnamed-chunk-9-1} \end{center}

\hypertarget{multiple-hidden-layers}{%
\section{Multiple hidden layers}\label{multiple-hidden-layers}}

More updates\ldots{} I've now modified the function to plot multiple hidden layers for networks created using the mlp function in the \texttt{RSNNS} package and neuralnet in the neuralnet package. As far as I know, these are the only neural network functions in R that can create multiple hidden layers. All others use a single hidden layer. I have not tested the plotting function using manual input for the weight vectors with multiple hidden layers. My guess is it won't work but I can't be bothered to change the function unless it's specifically requested. The updated function can be grabbed here (all above links to the function have also been changed).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(RSNNS)}
 
\CommentTok{# neural net with three hidden layers, 9, 11, and 8 nodes in each}
\KeywordTok{tic}\NormalTok{()}
\NormalTok{mod <-}\KeywordTok{mlp}\NormalTok{(rand.vars, resp, }
          \DataTypeTok{size =} \KeywordTok{c}\NormalTok{(}\DecValTok{9}\NormalTok{,}\DecValTok{11}\NormalTok{,}\DecValTok{8}\NormalTok{), }
          \DataTypeTok{linOut =}\NormalTok{ T)}
\KeywordTok{toc}\NormalTok{()}
\CommentTok{#> 0.586 sec elapsed}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mar=}\KeywordTok{numeric}\NormalTok{(}\DecValTok{4}\NormalTok{),}\DataTypeTok{family=}\StringTok{'serif'}\NormalTok{)}
\KeywordTok{plot.nnet}\NormalTok{(mod)}
\CommentTok{#> Warning in plot.nnet(mod): Bias layer not applicable for rsnns object}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{nn-regression_903-visualization_nn_files/figure-latex/run-rsnns-hidden_layers-1} \end{center}

\hypertarget{binary-predictors}{%
\section{Binary predictors}\label{binary-predictors}}

Here's an example using the \texttt{neuralnet} function with binary predictors and categorical outputs (credit to Tao Ma for the model code).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(neuralnet)}
 
\CommentTok{#response}
\NormalTok{AND<-}\KeywordTok{c}\NormalTok{(}\KeywordTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{7}\NormalTok{),}\DecValTok{1}\NormalTok{)}
\NormalTok{OR<-}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\KeywordTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{7}\NormalTok{))}
 
\CommentTok{# response with predictors}
\NormalTok{binary.data <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\KeywordTok{expand.grid}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{), }\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{), }\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{)), AND, OR)}
 
\CommentTok{#model}
\KeywordTok{tic}\NormalTok{()}
\NormalTok{net <-}\StringTok{ }\KeywordTok{neuralnet}\NormalTok{(AND}\OperatorTok{+}\NormalTok{OR }\OperatorTok{~}\StringTok{ }\NormalTok{Var1}\OperatorTok{+}\NormalTok{Var2}\OperatorTok{+}\NormalTok{Var3,}
\NormalTok{                 binary.data, }\DataTypeTok{hidden =}\KeywordTok{c}\NormalTok{(}\DecValTok{6}\NormalTok{,}\DecValTok{12}\NormalTok{,}\DecValTok{8}\NormalTok{), }
                 \DataTypeTok{rep =} \DecValTok{10}\NormalTok{, }
                 \DataTypeTok{err.fct=}\StringTok{"ce"}\NormalTok{, }
                 \DataTypeTok{linear.output=}\OtherTok{FALSE}\NormalTok{)}
\KeywordTok{toc}\NormalTok{()}
\CommentTok{#> 0.183 sec elapsed}
\CommentTok{#plot ouput}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mar=}\KeywordTok{numeric}\NormalTok{(}\DecValTok{4}\NormalTok{),}\DataTypeTok{family=}\StringTok{'serif'}\NormalTok{)}
\KeywordTok{plot.nnet}\NormalTok{(net)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{nn-regression_903-visualization_nn_files/figure-latex/unnamed-chunk-10-1} \end{center}

\hypertarget{color-coding-the-input-layer}{%
\section{color coding the input layer}\label{color-coding-the-input-layer}}

The color vector argument (circle.col) for the nodes was changed to allow a separate color vector for the input layer.

The following example shows how this can be done using relative importance of the input variables to color-code the first layer.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# example showing use of separate colors for input layer}
\CommentTok{# color based on relative importance using 'gar.fun'}
 
\CommentTok{##}
\CommentTok{#create input data}
\NormalTok{seed.val<-}\DecValTok{3}
\KeywordTok{set.seed}\NormalTok{(seed.val)}
  
\NormalTok{num.vars<-}\DecValTok{8}
\NormalTok{num.obs<-}\DecValTok{1000}
  
\CommentTok{#input variables}
\KeywordTok{library}\NormalTok{(clusterGeneration)}
\NormalTok{cov.mat<-}\KeywordTok{genPositiveDefMat}\NormalTok{(num.vars,}\DataTypeTok{covMethod=}\KeywordTok{c}\NormalTok{(}\StringTok{"unifcorrmat"}\NormalTok{))}\OperatorTok{$}\NormalTok{Sigma}
\NormalTok{rand.vars<-}\KeywordTok{mvrnorm}\NormalTok{(num.obs,}\KeywordTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{,num.vars),}\DataTypeTok{Sigma=}\NormalTok{cov.mat)}
  
\CommentTok{# output variables}
\NormalTok{parms<-}\KeywordTok{runif}\NormalTok{(num.vars,}\OperatorTok{-}\DecValTok{10}\NormalTok{,}\DecValTok{10}\NormalTok{)}
\NormalTok{y1<-rand.vars }\OperatorTok{%*%}\StringTok{ }\KeywordTok{matrix}\NormalTok{(parms) }\OperatorTok{+}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(num.obs,}\DataTypeTok{sd=}\DecValTok{20}\NormalTok{)}
 
\CommentTok{# final datasets}
\NormalTok{rand.vars<-}\KeywordTok{data.frame}\NormalTok{(rand.vars)}
\NormalTok{resp<-}\KeywordTok{data.frame}\NormalTok{(y1)}
\KeywordTok{names}\NormalTok{(resp)<-}\StringTok{'Y1'}
\NormalTok{dat.in <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(resp,rand.vars)}
 
\CommentTok{##}
\CommentTok{# create model}
\KeywordTok{library}\NormalTok{(nnet)}
\NormalTok{mod1 <-}\StringTok{ }\KeywordTok{nnet}\NormalTok{(rand.vars,resp,}\DataTypeTok{data=}\NormalTok{dat.in,}\DataTypeTok{size=}\DecValTok{10}\NormalTok{,}\DataTypeTok{linout=}\NormalTok{T)}
\CommentTok{#> # weights:  101}
\CommentTok{#> initial  value 844959.580478 }
\CommentTok{#> iter  10 value 543616.101824}
\CommentTok{#> iter  20 value 479986.887846}
\CommentTok{#> iter  30 value 465607.784054}
\CommentTok{#> iter  40 value 454237.073298}
\CommentTok{#> iter  50 value 445032.412421}
\CommentTok{#> iter  60 value 433191.158624}
\CommentTok{#> iter  70 value 426321.161292}
\CommentTok{#> iter  80 value 424900.966883}
\CommentTok{#> iter  90 value 423816.437605}
\CommentTok{#> iter 100 value 422064.114812}
\CommentTok{#> final  value 422064.114812 }
\CommentTok{#> stopped after 100 iterations}
 
\CommentTok{##}
\CommentTok{# relative importance function}
\KeywordTok{library}\NormalTok{(devtools)}
\KeywordTok{source_url}\NormalTok{(}\StringTok{'https://gist.github.com/fawda123/6206737/raw/2e1bc9cbc48d1a56d2a79dd1d33f414213f5f1b1/gar_fun.r'}\NormalTok{)}
\CommentTok{#> SHA-1 hash of file is 9faa58824c46956c3ff78081696290d9b32d845f}
 
\CommentTok{# relative importance of input variables for Y1}
\NormalTok{rel.imp <-}\StringTok{ }\KeywordTok{gar.fun}\NormalTok{(}\StringTok{'Y1'}\NormalTok{,mod1,}\DataTypeTok{bar.plot=}\NormalTok{F)}\OperatorTok{$}\NormalTok{rel.imp}
 
\CommentTok{#color vector based on relative importance of input values}
\NormalTok{cols<-}\KeywordTok{colorRampPalette}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{'green'}\NormalTok{,}\StringTok{'red'}\NormalTok{))(num.vars)[}\KeywordTok{rank}\NormalTok{(rel.imp)]}
 
\CommentTok{##}
\CommentTok{#plotting function}
\KeywordTok{source_url}\NormalTok{(}\StringTok{'https://gist.githubusercontent.com/fawda123/7471137/raw/466c1474d0a505ff044412703516c34f1a4684a5/nnet_plot_update.r'}\NormalTok{)}
\CommentTok{#> SHA-1 hash of file is 74c80bd5ddbc17ab3ae5ece9c0ed9beb612e87ef}
  
\CommentTok{#plot model with new color vector}
\CommentTok{#separate colors for input vectors using a list for 'circle.col'}
\KeywordTok{plot}\NormalTok{(mod1,}\DataTypeTok{circle.col=}\KeywordTok{list}\NormalTok{(cols,}\StringTok{'lightblue'}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{nn-regression_903-visualization_nn_files/figure-latex/unnamed-chunk-11-1} \end{center}

\hypertarget{part-linear-regression}{%
\part{Linear Regression}\label{part-linear-regression}}

\hypertarget{temperature-modeling-using-nested-dataframes}{%
\chapter{Temperature modeling using nested dataframes}\label{temperature-modeling-using-nested-dataframes}}

\hypertarget{prepare-the-data}{%
\section{Prepare the data}\label{prepare-the-data}}

\url{http://ijlyttle.github.io/isugg_purrr/presentation.html\#(1)}

\hypertarget{packages-to-run-this-presentation}{%
\subsection{Packages to run this presentation}\label{packages-to-run-this-presentation}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(}\StringTok{"readr"}\NormalTok{)}
\KeywordTok{library}\NormalTok{(}\StringTok{"tibble"}\NormalTok{)}
\KeywordTok{library}\NormalTok{(}\StringTok{"dplyr"}\NormalTok{)}
\KeywordTok{library}\NormalTok{(}\StringTok{"tidyr"}\NormalTok{)}
\KeywordTok{library}\NormalTok{(}\StringTok{"stringr"}\NormalTok{)}
\KeywordTok{library}\NormalTok{(}\StringTok{"ggplot2"}\NormalTok{)}
\KeywordTok{library}\NormalTok{(}\StringTok{"purrr"}\NormalTok{)}
\KeywordTok{library}\NormalTok{(}\StringTok{"broom"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{motivation-1}{%
\subsection{Motivation}\label{motivation-1}}

As you know, purrr is a recent package from Hadley Wickham, focused on lists and functional programming, like dplyr is focused on data-frames.

I figure a good way to learn a new package is to try to solve a problem, so we have a dataset:

\begin{itemize}
\item
  you can \href{https://github.com/ijlyttle/isugg_purrr/blob/gh-pages/temperature.csv}{view} or \href{http://ijlyttle.github.io/isugg_purrr/temperature.csv}{download}
\item
  you can download the \href{http://ijlyttle.github.io/isugg_purrr/presentation.Rmd}{source} of this presentation
\item
  these are three temperatures recorded simultaneously in a piece of electronics
\item
  it will be very valuable to be able to characterize the transient temperature for each sensor
\item
  we want to apply the same set of models across all three sensors
\item
  it will be easier to show using pictures
\end{itemize}

\hypertarget{lets-get-the-data-into-shape}{%
\subsection{Let's get the data into shape}\label{lets-get-the-data-into-shape}}

Using the readr package

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{temperature_wide <-}\StringTok{ }
\StringTok{  }\KeywordTok{read_csv}\NormalTok{(}\KeywordTok{file.path}\NormalTok{(data_raw_dir, }\StringTok{"temperature.csv"}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{print}\NormalTok{()}
\CommentTok{#> Parsed with column specification:}
\CommentTok{#> cols(}
\CommentTok{#>   instant = col_datetime(format = ""),}
\CommentTok{#>   temperature_a = col_double(),}
\CommentTok{#>   temperature_b = col_double(),}
\CommentTok{#>   temperature_c = col_double()}
\CommentTok{#> )}
\CommentTok{#> # A tibble: 327 x 4}
\CommentTok{#>   instant             temperature_a temperature_b temperature_c}
\CommentTok{#>   <dttm>                      <dbl>         <dbl>         <dbl>}
\CommentTok{#> 1 2015-11-13 06:10:19          116.          91.7          84.2}
\CommentTok{#> 2 2015-11-13 06:10:23          116.          91.7          84.2}
\CommentTok{#> 3 2015-11-13 06:10:27          116.          91.6          84.2}
\CommentTok{#> 4 2015-11-13 06:10:31          116.          91.7          84.2}
\CommentTok{#> 5 2015-11-13 06:10:36          116.          91.7          84.2}
\CommentTok{#> 6 2015-11-13 06:10:41          116.          91.6          84.2}
\CommentTok{#> # ... with 321 more rows}
\end{Highlighting}
\end{Shaded}

\hypertarget{is-temperature_wide-tidy}{%
\subsection{\texorpdfstring{Is \texttt{temperature\_wide} ``tidy''?}{Is temperature\_wide ``tidy''?}}\label{is-temperature_wide-tidy}}

\begin{verbatim}
#> # A tibble: 327 x 4
#>   instant             temperature_a temperature_b temperature_c
#>   <dttm>                      <dbl>         <dbl>         <dbl>
#> 1 2015-11-13 06:10:19          116.          91.7          84.2
#> 2 2015-11-13 06:10:23          116.          91.7          84.2
#> 3 2015-11-13 06:10:27          116.          91.6          84.2
#> 4 2015-11-13 06:10:31          116.          91.7          84.2
#> 5 2015-11-13 06:10:36          116.          91.7          84.2
#> 6 2015-11-13 06:10:41          116.          91.6          84.2
#> # ... with 321 more rows
\end{verbatim}

Why or why not?

\hypertarget{tidy-data}{%
\subsection{Tidy data}\label{tidy-data}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Each column is a variable
\item
  Each row is an observation
\item
  Each cell is a value
\end{enumerate}

(\url{http://www.jstatsoft.org/v59/i10/paper})

My personal observation is that ``tidy'' can depend on the context, on what you want to do with the data.

\hypertarget{lets-get-this-into-a-tidy-form}{%
\subsection{Let's get this into a tidy form}\label{lets-get-this-into-a-tidy-form}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{temperature_tall <-}
\StringTok{  }\NormalTok{temperature_wide }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{gather}\NormalTok{(}\DataTypeTok{key =} \StringTok{"id_sensor"}\NormalTok{, }\DataTypeTok{value =} \StringTok{"temperature"}\NormalTok{, }\KeywordTok{starts_with}\NormalTok{(}\StringTok{"temp"}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{id_sensor =} \KeywordTok{str_replace}\NormalTok{(id_sensor, }\StringTok{"temperature_"}\NormalTok{, }\StringTok{""}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{print}\NormalTok{()}
\CommentTok{#> # A tibble: 981 x 3}
\CommentTok{#>   instant             id_sensor temperature}
\CommentTok{#>   <dttm>              <chr>           <dbl>}
\CommentTok{#> 1 2015-11-13 06:10:19 a                116.}
\CommentTok{#> 2 2015-11-13 06:10:23 a                116.}
\CommentTok{#> 3 2015-11-13 06:10:27 a                116.}
\CommentTok{#> 4 2015-11-13 06:10:31 a                116.}
\CommentTok{#> 5 2015-11-13 06:10:36 a                116.}
\CommentTok{#> 6 2015-11-13 06:10:41 a                116.}
\CommentTok{#> # ... with 975 more rows}
\end{Highlighting}
\end{Shaded}

\hypertarget{now-its-easier-to-visualize}{%
\subsection{Now, it's easier to visualize}\label{now-its-easier-to-visualize}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{temperature_tall }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ instant, }\DataTypeTok{y =}\NormalTok{ temperature, }\DataTypeTok{color =}\NormalTok{ id_sensor)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_line}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{regression_114-nested_temperature_files/figure-latex/unnamed-chunk-3-1} \end{center}

\hypertarget{calculate-delta-time-delta-t-and-delta-temperature-delta-t}{%
\subsection{\texorpdfstring{Calculate delta time (\(\Delta t\)) and delta temperature (\(\Delta T\))}{Calculate delta time (\textbackslash{}Delta t) and delta temperature (\textbackslash{}Delta T)}}\label{calculate-delta-time-delta-t-and-delta-temperature-delta-t}}

\textbf{\texttt{delta\_time}} \(\Delta t\)

change in time since event started, s

\textbf{\texttt{delta\_temperature}}: \(\Delta T\)

change in temperature since event started, C

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{delta <-}\StringTok{ }
\StringTok{  }\NormalTok{temperature_tall }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{arrange}\NormalTok{(id_sensor, instant) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(id_sensor) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}
    \DataTypeTok{delta_time =} \KeywordTok{as.numeric}\NormalTok{(instant) }\OperatorTok{-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(instant[[}\DecValTok{1}\NormalTok{]]),}
    \DataTypeTok{delta_temperature =}\NormalTok{ temperature }\OperatorTok{-}\StringTok{ }\NormalTok{temperature[[}\DecValTok{1}\NormalTok{]]}
\NormalTok{  ) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(id_sensor, delta_time, delta_temperature)}
\end{Highlighting}
\end{Shaded}

\hypertarget{lets-have-a-look}{%
\subsection{Let's have a look}\label{lets-have-a-look}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot delta time vs delta temperature, by sensor}
\NormalTok{delta }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ delta_time, }\DataTypeTok{y =}\NormalTok{ delta_temperature, }\DataTypeTok{color =}\NormalTok{ id_sensor)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_line}\NormalTok{()  }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{regression_114-nested_temperature_files/figure-latex/unnamed-chunk-5-1} \end{center}

\hypertarget{define-the-models}{%
\section{Define the models}\label{define-the-models}}

We want to see how three different curve-fits might perform on these three data-sets:

\hypertarget{newtonian-cooling}{%
\subsubsection{Newtonian cooling}\label{newtonian-cooling}}

\[\Delta T = \Delta {T_0} * (1 - e^{-\frac{\delta t}{\tau_0}})\]

\hypertarget{semi-infinite-solid}{%
\subsection{Semi-infinite solid}\label{semi-infinite-solid}}

\[\Delta T = \Delta T_0 * erfc(\sqrt{\frac{\tau_0}{\delta t}}))\]

\hypertarget{semi-infinite-solid-with-convection}{%
\subsection{Semi-infinite solid with convection}\label{semi-infinite-solid-with-convection}}

\[\Delta T = \Delta T_0 * \big [ \operatorname erfc(\sqrt{\frac{\tau_0}{\delta t}}) - e^ {Bi_0 + (\frac {Bi_0}{2})^2 \frac {\delta t}{\tau_0}} * \operatorname erfc (\sqrt \frac{\tau_0}{\delta t} + \frac {Bi_0}{2} * \sqrt \frac{\delta t }{\tau_0} \big]\]

\hypertarget{erf-and-erfc-functions}{%
\subsection{\texorpdfstring{\texttt{erf} and \texttt{erfc} functions}{erf and erfc functions}}\label{erf-and-erfc-functions}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# reference: http://stackoverflow.com/questions/29067916/r-error-function-erfz}
\CommentTok{# (see Abramowitz and Stegun 29.2.29)}
\NormalTok{erf <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x) }\DecValTok{2} \OperatorTok{*}\StringTok{ }\KeywordTok{pnorm}\NormalTok{(x }\OperatorTok{*}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(}\DecValTok{2}\NormalTok{)) }\OperatorTok{-}\StringTok{ }\DecValTok{1}
\NormalTok{erfc <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x) }\DecValTok{2} \OperatorTok{*}\StringTok{ }\KeywordTok{pnorm}\NormalTok{(x }\OperatorTok{*}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(}\DecValTok{2}\NormalTok{), }\DataTypeTok{lower =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{newton-cooling-equation}{%
\subsection{Newton cooling equation}\label{newton-cooling-equation}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{newton_cooling <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x) \{}
  \KeywordTok{nls}\NormalTok{(}
\NormalTok{    delta_temperature }\OperatorTok{~}\StringTok{ }\NormalTok{delta_temperature_}\DecValTok{0} \OperatorTok{*}\StringTok{ }\NormalTok{(}\DecValTok{1} \OperatorTok{-}\StringTok{ }\KeywordTok{exp}\NormalTok{(}\OperatorTok{-}\NormalTok{delta_time}\OperatorTok{/}\NormalTok{tau_}\DecValTok{0}\NormalTok{)),}
    \DataTypeTok{start =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{delta_temperature_0 =} \DecValTok{-10}\NormalTok{, }\DataTypeTok{tau_0 =} \DecValTok{50}\NormalTok{),}
    \DataTypeTok{data =}\NormalTok{ x}
\NormalTok{  )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\hypertarget{temperature-models-simple-and-convection}{%
\subsection{Temperature models: simple and convection}\label{temperature-models-simple-and-convection}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{semi_infinite_simple <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x) \{}
  \KeywordTok{nls}\NormalTok{(}
\NormalTok{    delta_temperature }\OperatorTok{~}\StringTok{ }\NormalTok{delta_temperature_}\DecValTok{0} \OperatorTok{*}\StringTok{ }\KeywordTok{erfc}\NormalTok{(}\KeywordTok{sqrt}\NormalTok{(tau_}\DecValTok{0} \OperatorTok{/}\StringTok{ }\NormalTok{delta_time)),}
    \DataTypeTok{start =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{delta_temperature_0 =} \DecValTok{-10}\NormalTok{, }\DataTypeTok{tau_0 =} \DecValTok{50}\NormalTok{),}
    \DataTypeTok{data =}\NormalTok{ x}
\NormalTok{  )    }
\NormalTok{\}}

\NormalTok{semi_infinite_convection <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x)\{}
  \KeywordTok{nls}\NormalTok{(}
\NormalTok{    delta_temperature }\OperatorTok{~}
\StringTok{      }\NormalTok{delta_temperature_}\DecValTok{0} \OperatorTok{*}\StringTok{ }\NormalTok{(}
        \KeywordTok{erfc}\NormalTok{(}\KeywordTok{sqrt}\NormalTok{(tau_}\DecValTok{0} \OperatorTok{/}\StringTok{ }\NormalTok{delta_time)) }\OperatorTok{-}
\StringTok{        }\KeywordTok{exp}\NormalTok{(Bi_}\DecValTok{0} \OperatorTok{+}\StringTok{ }\NormalTok{(Bi_}\DecValTok{0}\OperatorTok{/}\DecValTok{2}\NormalTok{)}\OperatorTok{^}\DecValTok{2} \OperatorTok{*}\StringTok{ }\NormalTok{delta_time }\OperatorTok{/}\StringTok{ }\NormalTok{tau_}\DecValTok{0}\NormalTok{) }\OperatorTok{*}
\StringTok{          }\KeywordTok{erfc}\NormalTok{(}\KeywordTok{sqrt}\NormalTok{(tau_}\DecValTok{0} \OperatorTok{/}\StringTok{ }\NormalTok{delta_time) }\OperatorTok{+}\StringTok{ }
\StringTok{        }\NormalTok{(Bi_}\DecValTok{0}\OperatorTok{/}\DecValTok{2}\NormalTok{) }\OperatorTok{*}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(delta_time }\OperatorTok{/}\StringTok{ }\NormalTok{tau_}\DecValTok{0}\NormalTok{))}
\NormalTok{      ),}
    \DataTypeTok{start =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{delta_temperature_0 =} \DecValTok{-5}\NormalTok{, }\DataTypeTok{tau_0 =} \DecValTok{50}\NormalTok{, }\DataTypeTok{Bi_0 =} \DecValTok{1}\NormalTok{.e6),}
    \DataTypeTok{data =}\NormalTok{ x}
\NormalTok{  )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\hypertarget{test-modeling-on-one-dataset}{%
\section{Test modeling on one dataset}\label{test-modeling-on-one-dataset}}

\hypertarget{before-going-into-purrr}{%
\subsection{\texorpdfstring{Before going into \texttt{purrr}}{Before going into purrr}}\label{before-going-into-purrr}}

Before doing anything, we want to show that we can do something with one dataset and one model-function:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# only one sensor; it is a test}
\NormalTok{tmp_data <-}\StringTok{ }\NormalTok{delta }\OperatorTok{%>%}\StringTok{ }\KeywordTok{filter}\NormalTok{(id_sensor }\OperatorTok{==}\StringTok{ "a"}\NormalTok{)}

\NormalTok{tmp_model <-}\StringTok{ }\KeywordTok{newton_cooling}\NormalTok{(tmp_data)}

\KeywordTok{summary}\NormalTok{(tmp_model)}
\CommentTok{#> }
\CommentTok{#> Formula: delta_temperature ~ delta_temperature_0 * (1 - exp(-delta_time/tau_0))}
\CommentTok{#> }
\CommentTok{#> Parameters:}
\CommentTok{#>                     Estimate Std. Error t value Pr(>|t|)    }
\CommentTok{#> delta_temperature_0 -15.0608     0.0526    -286   <2e-16 ***}
\CommentTok{#> tau_0               500.0138     4.8367     103   <2e-16 ***}
\CommentTok{#> ---}
\CommentTok{#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1}
\CommentTok{#> }
\CommentTok{#> Residual standard error: 0.327 on 325 degrees of freedom}
\CommentTok{#> }
\CommentTok{#> Number of iterations to convergence: 7 }
\CommentTok{#> Achieved convergence tolerance: 4.14e-06}
\end{Highlighting}
\end{Shaded}

\hypertarget{look-at-predictions}{%
\subsection{Look at predictions}\label{look-at-predictions}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# apply prediction and make it tidy}
\NormalTok{tmp_pred <-}\StringTok{ }
\StringTok{  }\NormalTok{tmp_data }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{modeled =} \KeywordTok{predict}\NormalTok{(tmp_model, }\DataTypeTok{data =}\NormalTok{ .)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(id_sensor, delta_time, }\DataTypeTok{measured =}\NormalTok{ delta_temperature, modeled) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{gather}\NormalTok{(}\StringTok{"type"}\NormalTok{, }\StringTok{"delta_temperature"}\NormalTok{, measured}\OperatorTok{:}\NormalTok{modeled) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{print}\NormalTok{()}
\CommentTok{#> # A tibble: 654 x 4}
\CommentTok{#> # Groups:   id_sensor [1]}
\CommentTok{#>   id_sensor delta_time type     delta_temperature}
\CommentTok{#>   <chr>          <dbl> <chr>                <dbl>}
\CommentTok{#> 1 a                  0 measured             0    }
\CommentTok{#> 2 a                  4 measured             0    }
\CommentTok{#> 3 a                  8 measured            -0.06 }
\CommentTok{#> 4 a                 12 measured            -0.06 }
\CommentTok{#> 5 a                 17 measured            -0.211}
\CommentTok{#> 6 a                 22 measured            -0.423}
\CommentTok{#> # ... with 648 more rows}
\end{Highlighting}
\end{Shaded}

\hypertarget{plot-newton-model}{%
\subsection{Plot Newton model}\label{plot-newton-model}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tmp_pred }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ delta_time, }\DataTypeTok{y =}\NormalTok{ delta_temperature, }\DataTypeTok{linetype =}\NormalTok{ type)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_line}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{title =} \StringTok{"Newton temperature model"}\NormalTok{, }\DataTypeTok{subtitle =} \StringTok{"One sensor: a"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{regression_114-nested_temperature_files/figure-latex/unnamed-chunk-11-1} \end{center}

\hypertarget{regular-data-frame-deltas}{%
\subsection{``Regular'' data-frame (deltas)}\label{regular-data-frame-deltas}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{print}\NormalTok{(delta)}
\CommentTok{#> # A tibble: 981 x 3}
\CommentTok{#> # Groups:   id_sensor [3]}
\CommentTok{#>   id_sensor delta_time delta_temperature}
\CommentTok{#>   <chr>          <dbl>             <dbl>}
\CommentTok{#> 1 a                  0             0    }
\CommentTok{#> 2 a                  4             0    }
\CommentTok{#> 3 a                  8            -0.06 }
\CommentTok{#> 4 a                 12            -0.06 }
\CommentTok{#> 5 a                 17            -0.211}
\CommentTok{#> 6 a                 22            -0.423}
\CommentTok{#> # ... with 975 more rows}
\end{Highlighting}
\end{Shaded}

Each column of the dataframe is a vector - in this case, a character vector and two doubles

\hypertarget{making-a-nested-dataframe}{%
\section{Making a nested dataframe}\label{making-a-nested-dataframe}}

\hypertarget{how-to-make-a-weird-data-frame}{%
\subsection{How to make a weird data-frame}\label{how-to-make-a-weird-data-frame}}

Here's where the fun starts - a column of a data-frame can be a list.

\begin{itemize}
\item
  use \texttt{tidyr::nest()} to makes a column \texttt{data}, which is a list of data-frames
\item
  this seems like a stronger expression of the \texttt{dplyr::group\_by()} idea
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# nest delta_time and delta_temperature variables}
\NormalTok{delta_nested <-}\StringTok{ }
\StringTok{  }\NormalTok{delta }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{nest}\NormalTok{(}\OperatorTok{-}\NormalTok{id_sensor) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{print}\NormalTok{()}
\CommentTok{#> # A tibble: 3 x 2}
\CommentTok{#>   id_sensor data              }
\CommentTok{#>   <chr>     <list>            }
\CommentTok{#> 1 a         <tibble [327 x 2]>}
\CommentTok{#> 2 b         <tibble [327 x 2]>}
\CommentTok{#> 3 c         <tibble [327 x 2]>}
\end{Highlighting}
\end{Shaded}

\hypertarget{map-dataframes-to-a-modeling-function-newton}{%
\subsection{Map dataframes to a modeling function (Newton)}\label{map-dataframes-to-a-modeling-function-newton}}

\begin{itemize}
\item
  \texttt{map()} is like \texttt{lapply()}
\item
  \texttt{map()} returns a list-column (it keeps the weirdness)
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model_nested <-}
\StringTok{  }\NormalTok{delta_nested }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{model =} \KeywordTok{map}\NormalTok{(data, newton_cooling)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{print}\NormalTok{()}
\CommentTok{#> # A tibble: 3 x 3}
\CommentTok{#>   id_sensor data               model }
\CommentTok{#>   <chr>     <list>             <list>}
\CommentTok{#> 1 a         <tibble [327 x 2]> <nls> }
\CommentTok{#> 2 b         <tibble [327 x 2]> <nls> }
\CommentTok{#> 3 c         <tibble [327 x 2]> <nls>}
\end{Highlighting}
\end{Shaded}

\begin{quote}
We get an additional list-column \texttt{model}.
\end{quote}

\hypertarget{we-can-use-map2-to-make-the-predictions}{%
\subsection{\texorpdfstring{We can use \texttt{map2()} to make the predictions}{We can use map2() to make the predictions}}\label{we-can-use-map2-to-make-the-predictions}}

\begin{itemize}
\item
  \texttt{map2()} is like \texttt{mapply()}
\item
  designed to map two colunms (\texttt{model}, \texttt{data}) to a function \texttt{predict()}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{predict_nested <-}
\StringTok{  }\NormalTok{model_nested }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{pred =} \KeywordTok{map2}\NormalTok{(model, data, predict)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{print}\NormalTok{()}
\CommentTok{#> # A tibble: 3 x 4}
\CommentTok{#>   id_sensor data               model  pred       }
\CommentTok{#>   <chr>     <list>             <list> <list>     }
\CommentTok{#> 1 a         <tibble [327 x 2]> <nls>  <dbl [327]>}
\CommentTok{#> 2 b         <tibble [327 x 2]> <nls>  <dbl [327]>}
\CommentTok{#> 3 c         <tibble [327 x 2]> <nls>  <dbl [327]>}
\end{Highlighting}
\end{Shaded}

\begin{quote}
Another list-column \texttt{pred} for the prediction results.
\end{quote}

\hypertarget{we-need-to-get-out-of-the-weirdness}{%
\subsection{We need to get out of the weirdness}\label{we-need-to-get-out-of-the-weirdness}}

\begin{itemize}
\tightlist
\item
  use \texttt{unnest()} to get back to a regular data-frame
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{predict_unnested <-}\StringTok{ }
\StringTok{  }\NormalTok{predict_nested }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{unnest}\NormalTok{(data, pred) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{print}\NormalTok{()}
\CommentTok{#> # A tibble: 981 x 4}
\CommentTok{#>   id_sensor   pred delta_time delta_temperature}
\CommentTok{#>   <chr>      <dbl>      <dbl>             <dbl>}
\CommentTok{#> 1 a          0              0             0    }
\CommentTok{#> 2 a         -0.120          4             0    }
\CommentTok{#> 3 a         -0.239          8            -0.06 }
\CommentTok{#> 4 a         -0.357         12            -0.06 }
\CommentTok{#> 5 a         -0.503         17            -0.211}
\CommentTok{#> 6 a         -0.648         22            -0.423}
\CommentTok{#> # ... with 975 more rows}
\end{Highlighting}
\end{Shaded}

\hypertarget{we-can-wrangle-the-predictions}{%
\subsection{We can wrangle the predictions}\label{we-can-wrangle-the-predictions}}

\begin{itemize}
\tightlist
\item
  get into a form that makes it easier to plot
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{predict_tall <-}\StringTok{ }
\StringTok{  }\NormalTok{predict_unnested }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{rename}\NormalTok{(}\DataTypeTok{modeled =}\NormalTok{ pred, }\DataTypeTok{measured =}\NormalTok{ delta_temperature) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{gather}\NormalTok{(}\StringTok{"type"}\NormalTok{, }\StringTok{"delta_temperature"}\NormalTok{, modeled, measured) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{print}\NormalTok{()}
\CommentTok{#> # A tibble: 1,962 x 4}
\CommentTok{#>   id_sensor delta_time type    delta_temperature}
\CommentTok{#>   <chr>          <dbl> <chr>               <dbl>}
\CommentTok{#> 1 a                  0 modeled             0    }
\CommentTok{#> 2 a                  4 modeled            -0.120}
\CommentTok{#> 3 a                  8 modeled            -0.239}
\CommentTok{#> 4 a                 12 modeled            -0.357}
\CommentTok{#> 5 a                 17 modeled            -0.503}
\CommentTok{#> 6 a                 22 modeled            -0.648}
\CommentTok{#> # ... with 1,956 more rows}
\end{Highlighting}
\end{Shaded}

\hypertarget{we-can-visualize-the-predictions}{%
\subsection{We can visualize the predictions}\label{we-can-visualize-the-predictions}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{predict_tall }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ delta_time, }\DataTypeTok{y =}\NormalTok{ delta_temperature)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_line}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{color =}\NormalTok{ id_sensor, }\DataTypeTok{linetype =}\NormalTok{ type)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{title =} \StringTok{"Newton temperature modeling"}\NormalTok{, }
       \DataTypeTok{subtitle =} \StringTok{"Three sensors: a, b, c"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{regression_114-nested_temperature_files/figure-latex/unnamed-chunk-18-1} \end{center}

\hypertarget{apply-multiple-models-on-a-nested-structure}{%
\section{Apply multiple models on a nested structure}\label{apply-multiple-models-on-a-nested-structure}}

\hypertarget{step-1-selection-of-models}{%
\subsection{Step 1: Selection of models}\label{step-1-selection-of-models}}

Make a list of functions to model:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{list_model <-}
\StringTok{  }\KeywordTok{list}\NormalTok{(}
    \DataTypeTok{newton_cooling =}\NormalTok{ newton_cooling,}
    \DataTypeTok{semi_infinite_simple =}\NormalTok{ semi_infinite_simple,}
    \DataTypeTok{semi_infinite_convection =}\NormalTok{ semi_infinite_convection}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\hypertarget{step-2-write-a-function-to-define-the-inner-loop}{%
\subsection{Step 2: write a function to define the ``inner'' loop}\label{step-2-write-a-function-to-define-the-inner-loop}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# add additional variable with the model name}

\NormalTok{fn_model <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(.model, df) \{}
  \CommentTok{# one parameter for the model in the list, the second for the data}
  \CommentTok{# safer to avoid non-standard evaluation}
  \CommentTok{# df %>% mutate(model = map(data, .model)) }
  
\NormalTok{  df}\OperatorTok{$}\NormalTok{model <-}\StringTok{ }\KeywordTok{map}\NormalTok{(df}\OperatorTok{$}\NormalTok{data, }\KeywordTok{possibly}\NormalTok{(.model, }\OtherTok{NULL}\NormalTok{))}
\NormalTok{  df}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\item
  for a given model-function and a given (weird) data-frame, return a modified version of that data-frame with a column \texttt{model}, which is the model-function applied to each element of the data-frame's \texttt{data} column (which is itself a list of data-frames)
\item
  the purrr functions \texttt{safely()} and \texttt{possibly()} are \textbf{very} interesting. I think they could be useful outside of purrr as a friendlier way to do error-handling.
\end{itemize}

\hypertarget{step-3-use-map_df-to-define-the-outer-loop}{%
\subsection{\texorpdfstring{Step 3: Use \texttt{map\_df()} to define the ``outer'' loop}{Step 3: Use map\_df() to define the ``outer'' loop}}\label{step-3-use-map_df-to-define-the-outer-loop}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# this dataframe will be the second input of fn_model}
\NormalTok{delta_nested }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{print}\NormalTok{()}
\CommentTok{#> # A tibble: 3 x 2}
\CommentTok{#>   id_sensor data              }
\CommentTok{#>   <chr>     <list>            }
\CommentTok{#> 1 a         <tibble [327 x 2]>}
\CommentTok{#> 2 b         <tibble [327 x 2]>}
\CommentTok{#> 3 c         <tibble [327 x 2]>}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# fn_model is receiving two inputs: one from list_model and from delta_nested}
\NormalTok{model_nested_new <-}
\StringTok{  }\NormalTok{list_model }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{map_df}\NormalTok{(fn_model, delta_nested, }\DataTypeTok{.id =} \StringTok{"id_model"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{print}\NormalTok{()}
\CommentTok{#> # A tibble: 9 x 4}
\CommentTok{#>   id_model             id_sensor data               model }
\CommentTok{#>   <chr>                <chr>     <list>             <list>}
\CommentTok{#> 1 newton_cooling       a         <tibble [327 x 2]> <nls> }
\CommentTok{#> 2 newton_cooling       b         <tibble [327 x 2]> <nls> }
\CommentTok{#> 3 newton_cooling       c         <tibble [327 x 2]> <nls> }
\CommentTok{#> 4 semi_infinite_simple a         <tibble [327 x 2]> <nls> }
\CommentTok{#> 5 semi_infinite_simple b         <tibble [327 x 2]> <nls> }
\CommentTok{#> 6 semi_infinite_simple c         <tibble [327 x 2]> <nls> }
\CommentTok{#> # ... with 3 more rows}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\item
  for each element of a list of model-functions, run the inner-loop function, and row-bind the results into a data-frame
\item
  we want to discard the rows where the model failed
\item
  we also want to investigate why they failed, but that's a different talk
\end{itemize}

\hypertarget{step-4-use-map-to-identify-the-null-models}{%
\subsection{\texorpdfstring{Step 4: Use \texttt{map()} to identify the null models}{Step 4: Use map() to identify the null models}}\label{step-4-use-map-to-identify-the-null-models}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model_nested_new <-}
\StringTok{  }\NormalTok{list_model }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{map_df}\NormalTok{(fn_model, delta_nested, }\DataTypeTok{.id =} \StringTok{"id_model"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{is_null =} \KeywordTok{map}\NormalTok{(model, is.null)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{print}\NormalTok{()}
\CommentTok{#> # A tibble: 9 x 5}
\CommentTok{#>   id_model             id_sensor data               model  is_null  }
\CommentTok{#>   <chr>                <chr>     <list>             <list> <list>   }
\CommentTok{#> 1 newton_cooling       a         <tibble [327 x 2]> <nls>  <lgl [1]>}
\CommentTok{#> 2 newton_cooling       b         <tibble [327 x 2]> <nls>  <lgl [1]>}
\CommentTok{#> 3 newton_cooling       c         <tibble [327 x 2]> <nls>  <lgl [1]>}
\CommentTok{#> 4 semi_infinite_simple a         <tibble [327 x 2]> <nls>  <lgl [1]>}
\CommentTok{#> 5 semi_infinite_simple b         <tibble [327 x 2]> <nls>  <lgl [1]>}
\CommentTok{#> 6 semi_infinite_simple c         <tibble [327 x 2]> <nls>  <lgl [1]>}
\CommentTok{#> # ... with 3 more rows}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  using \texttt{map(model,\ is.null)} returns a list column
\item
  to use \texttt{filter()}, we have to escape the weirdness
\end{itemize}

\hypertarget{step-5-map_lgl-to-identify-nulls-and-get-out-of-the-weirdness}{%
\subsection{\texorpdfstring{Step 5: \texttt{map\_lgl()} to identify nulls and get out of the weirdness}{Step 5: map\_lgl() to identify nulls and get out of the weirdness}}\label{step-5-map_lgl-to-identify-nulls-and-get-out-of-the-weirdness}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model_nested_new <-}
\StringTok{  }\NormalTok{list_model }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{map_df}\NormalTok{(fn_model, delta_nested, }\DataTypeTok{.id =} \StringTok{"id_model"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{is_null =} \KeywordTok{map_lgl}\NormalTok{(model, is.null)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{print}\NormalTok{()}
\CommentTok{#> # A tibble: 9 x 5}
\CommentTok{#>   id_model             id_sensor data               model  is_null}
\CommentTok{#>   <chr>                <chr>     <list>             <list> <lgl>  }
\CommentTok{#> 1 newton_cooling       a         <tibble [327 x 2]> <nls>  FALSE  }
\CommentTok{#> 2 newton_cooling       b         <tibble [327 x 2]> <nls>  FALSE  }
\CommentTok{#> 3 newton_cooling       c         <tibble [327 x 2]> <nls>  FALSE  }
\CommentTok{#> 4 semi_infinite_simple a         <tibble [327 x 2]> <nls>  FALSE  }
\CommentTok{#> 5 semi_infinite_simple b         <tibble [327 x 2]> <nls>  FALSE  }
\CommentTok{#> 6 semi_infinite_simple c         <tibble [327 x 2]> <nls>  FALSE  }
\CommentTok{#> # ... with 3 more rows}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  using \texttt{map\_lgl(model,\ is.null)} returns a vector column
\end{itemize}

\hypertarget{step-6-filter-nulls-and-select-variables-to-clean-up}{%
\subsection{\texorpdfstring{Step 6: \texttt{filter()} nulls and \texttt{select()} variables to clean up}{Step 6: filter() nulls and select() variables to clean up}}\label{step-6-filter-nulls-and-select-variables-to-clean-up}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model_nested_new <-}
\StringTok{  }\NormalTok{list_model }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{map_df}\NormalTok{(fn_model, delta_nested, }\DataTypeTok{.id =} \StringTok{"id_model"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{is_null =} \KeywordTok{map_lgl}\NormalTok{(model, is.null)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(}\OperatorTok{!}\NormalTok{is_null) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{is_null) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{print}\NormalTok{()}
\CommentTok{#> # A tibble: 6 x 4}
\CommentTok{#>   id_model             id_sensor data               model }
\CommentTok{#>   <chr>                <chr>     <list>             <list>}
\CommentTok{#> 1 newton_cooling       a         <tibble [327 x 2]> <nls> }
\CommentTok{#> 2 newton_cooling       b         <tibble [327 x 2]> <nls> }
\CommentTok{#> 3 newton_cooling       c         <tibble [327 x 2]> <nls> }
\CommentTok{#> 4 semi_infinite_simple a         <tibble [327 x 2]> <nls> }
\CommentTok{#> 5 semi_infinite_simple b         <tibble [327 x 2]> <nls> }
\CommentTok{#> 6 semi_infinite_simple c         <tibble [327 x 2]> <nls>}
\end{Highlighting}
\end{Shaded}

\hypertarget{step-7-calculate-predictions-on-nested-dataframe}{%
\subsection{Step 7: Calculate predictions on nested dataframe}\label{step-7-calculate-predictions-on-nested-dataframe}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{predict_nested <-}\StringTok{ }
\StringTok{  }\NormalTok{model_nested_new }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{pred =} \KeywordTok{map2}\NormalTok{(model, data, predict)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{print}\NormalTok{()}
\CommentTok{#> # A tibble: 6 x 5}
\CommentTok{#>   id_model             id_sensor data               model  pred       }
\CommentTok{#>   <chr>                <chr>     <list>             <list> <list>     }
\CommentTok{#> 1 newton_cooling       a         <tibble [327 x 2]> <nls>  <dbl [327]>}
\CommentTok{#> 2 newton_cooling       b         <tibble [327 x 2]> <nls>  <dbl [327]>}
\CommentTok{#> 3 newton_cooling       c         <tibble [327 x 2]> <nls>  <dbl [327]>}
\CommentTok{#> 4 semi_infinite_simple a         <tibble [327 x 2]> <nls>  <dbl [327]>}
\CommentTok{#> 5 semi_infinite_simple b         <tibble [327 x 2]> <nls>  <dbl [327]>}
\CommentTok{#> 6 semi_infinite_simple c         <tibble [327 x 2]> <nls>  <dbl [327]>}
\end{Highlighting}
\end{Shaded}

\hypertarget{unnest-make-it-tall-and-tidy}{%
\subsection{\texorpdfstring{\texttt{unnest()}, make it tall and tidy}{unnest(), make it tall and tidy}}\label{unnest-make-it-tall-and-tidy}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{predict_tall <-}
\StringTok{  }\NormalTok{predict_nested }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{unnest}\NormalTok{(data, pred) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{rename}\NormalTok{(}\DataTypeTok{modeled =}\NormalTok{ pred, }\DataTypeTok{measured =}\NormalTok{ delta_temperature) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{gather}\NormalTok{(}\StringTok{"type"}\NormalTok{, }\StringTok{"delta_temperature"}\NormalTok{, modeled, measured) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{print}\NormalTok{()}
\CommentTok{#> # A tibble: 3,924 x 5}
\CommentTok{#>   id_model       id_sensor delta_time type    delta_temperature}
\CommentTok{#>   <chr>          <chr>          <dbl> <chr>               <dbl>}
\CommentTok{#> 1 newton_cooling a                  0 modeled             0    }
\CommentTok{#> 2 newton_cooling a                  4 modeled            -0.120}
\CommentTok{#> 3 newton_cooling a                  8 modeled            -0.239}
\CommentTok{#> 4 newton_cooling a                 12 modeled            -0.357}
\CommentTok{#> 5 newton_cooling a                 17 modeled            -0.503}
\CommentTok{#> 6 newton_cooling a                 22 modeled            -0.648}
\CommentTok{#> # ... with 3,918 more rows}
\end{Highlighting}
\end{Shaded}

\hypertarget{visualize-the-predictions}{%
\subsection{Visualize the predictions}\label{visualize-the-predictions}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{predict_tall }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ delta_time, }\DataTypeTok{y =}\NormalTok{ delta_temperature)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_line}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{color =}\NormalTok{ id_sensor, }\DataTypeTok{linetype =}\NormalTok{ type)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{facet_grid}\NormalTok{(id_model }\OperatorTok{~}\StringTok{ }\NormalTok{.) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{title =} \StringTok{"Newton and Semi-infinite temperature modeling"}\NormalTok{, }
       \DataTypeTok{subtitle =} \StringTok{"Three sensors: a, b, c"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{regression_114-nested_temperature_files/figure-latex/unnamed-chunk-28-1} \end{center}

\hypertarget{lets-get-the-residuals}{%
\subsection{Let's get the residuals}\label{lets-get-the-residuals}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{resid <-}
\StringTok{  }\NormalTok{model_nested_new }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{resid =} \KeywordTok{map}\NormalTok{(model, resid)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{unnest}\NormalTok{(data, resid) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{print}\NormalTok{()}
\CommentTok{#> # A tibble: 1,962 x 5}
\CommentTok{#>   id_model       id_sensor resid delta_time delta_temperature}
\CommentTok{#>   <chr>          <chr>     <dbl>      <dbl>             <dbl>}
\CommentTok{#> 1 newton_cooling a         0              0             0    }
\CommentTok{#> 2 newton_cooling a         0.120          4             0    }
\CommentTok{#> 3 newton_cooling a         0.179          8            -0.06 }
\CommentTok{#> 4 newton_cooling a         0.297         12            -0.06 }
\CommentTok{#> 5 newton_cooling a         0.292         17            -0.211}
\CommentTok{#> 6 newton_cooling a         0.225         22            -0.423}
\CommentTok{#> # ... with 1,956 more rows}
\end{Highlighting}
\end{Shaded}

\hypertarget{and-visualize-them}{%
\subsection{And visualize them}\label{and-visualize-them}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{resid }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ delta_time, }\DataTypeTok{y =}\NormalTok{ resid)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_line}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{color =}\NormalTok{ id_sensor)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{facet_grid}\NormalTok{(id_model }\OperatorTok{~}\StringTok{ }\NormalTok{.) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{title =} \StringTok{"Residuals for Newton and Semi-infinite models"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{regression_114-nested_temperature_files/figure-latex/unnamed-chunk-30-1} \end{center}

\hypertarget{using-broom-package-to-look-at-model-statistics}{%
\section{Using broom package to look at model-statistics}\label{using-broom-package-to-look-at-model-statistics}}

We will use a previous defined dataframe with the model and data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model_nested_new }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{print}\NormalTok{()}
\CommentTok{#> # A tibble: 6 x 4}
\CommentTok{#>   id_model             id_sensor data               model }
\CommentTok{#>   <chr>                <chr>     <list>             <list>}
\CommentTok{#> 1 newton_cooling       a         <tibble [327 x 2]> <nls> }
\CommentTok{#> 2 newton_cooling       b         <tibble [327 x 2]> <nls> }
\CommentTok{#> 3 newton_cooling       c         <tibble [327 x 2]> <nls> }
\CommentTok{#> 4 semi_infinite_simple a         <tibble [327 x 2]> <nls> }
\CommentTok{#> 5 semi_infinite_simple b         <tibble [327 x 2]> <nls> }
\CommentTok{#> 6 semi_infinite_simple c         <tibble [327 x 2]> <nls>}
\end{Highlighting}
\end{Shaded}

The \texttt{tidy()} function extracts statistics from a model.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# apply over model_nested_new but only three variables}
\NormalTok{model_parameters <-}\StringTok{ }
\StringTok{  }\NormalTok{model_nested_new }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(id_model, id_sensor, model) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{tidy =} \KeywordTok{map}\NormalTok{(model, tidy)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{model) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{unnest}\NormalTok{() }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{print}\NormalTok{()}
\CommentTok{#> # A tibble: 12 x 7}
\CommentTok{#>   id_model    id_sensor term         estimate std.error statistic   p.value}
\CommentTok{#>   <chr>       <chr>     <chr>           <dbl>     <dbl>     <dbl>     <dbl>}
\CommentTok{#> 1 newton_coo~ a         delta_tempe~   -15.1     0.0526    -286.  0.       }
\CommentTok{#> 2 newton_coo~ a         tau_0          500.      4.84       103.  1.07e-250}
\CommentTok{#> 3 newton_coo~ b         delta_tempe~    -7.59    0.0676    -112.  6.38e-262}
\CommentTok{#> 4 newton_coo~ b         tau_0         1041.     16.2         64.2 9.05e-187}
\CommentTok{#> 5 newton_coo~ c         delta_tempe~    -9.87    0.704      -14.0 3.16e- 35}
\CommentTok{#> 6 newton_coo~ c         tau_0         3525.    299.          11.8 5.61e- 27}
\CommentTok{#> # ... with 6 more rows}
\end{Highlighting}
\end{Shaded}

\hypertarget{get-a-sense-of-the-coefficients}{%
\subsection{Get a sense of the coefficients}\label{get-a-sense-of-the-coefficients}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model_summary <-}
\StringTok{  }\NormalTok{model_parameters }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(id_model, id_sensor, term, estimate) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{spread}\NormalTok{(}\DataTypeTok{key =} \StringTok{"term"}\NormalTok{, }\DataTypeTok{value =} \StringTok{"estimate"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{print}\NormalTok{()}
\CommentTok{#> # A tibble: 6 x 4}
\CommentTok{#>   id_model             id_sensor delta_temperature_0 tau_0}
\CommentTok{#>   <chr>                <chr>                   <dbl> <dbl>}
\CommentTok{#> 1 newton_cooling       a                      -15.1   500.}
\CommentTok{#> 2 newton_cooling       b                       -7.59 1041.}
\CommentTok{#> 3 newton_cooling       c                       -9.87 3525.}
\CommentTok{#> 4 semi_infinite_simple a                      -21.5   139.}
\CommentTok{#> 5 semi_infinite_simple b                      -10.6   287.}
\CommentTok{#> 6 semi_infinite_simple c                       -8.04  500.}
\end{Highlighting}
\end{Shaded}

\hypertarget{summary-1}{%
\subsection{Summary}\label{summary-1}}

\begin{itemize}
\tightlist
\item
  this is just a smalll part of purrr
\item
  there seem to be parallels between \texttt{tidyr::nest()/purrr::map()} and \texttt{dplyr::group\_by()/dplyr::do()}

  \begin{itemize}
  \tightlist
  \item
    to my mind, the purrr framework is more understandable
  \item
    update tweet from \href{https://twitter.com/hadleywickham/status/719542847045636096}{Hadley}
  \end{itemize}
\end{itemize}

References from Hadley:

\begin{itemize}
\tightlist
\item
  \href{http://blog.rstudio.org/2015/09/29/purrr-0-1-0/}{purrr 0.1.0 announcement}
\item
  \href{http://blog.rstudio.org/2016/01/06/purrr-0-2-0/}{purrr 0.2.0 announcement}
\item
  \href{http://r4ds.had.co.nz/iteration.html}{chapter from Garrett Grolemund and Hadley's forthcoming book}
\end{itemize}

\hypertarget{linear-regression.-world-happiness}{%
\chapter{Linear Regression. World Happiness}\label{linear-regression.-world-happiness}}

\hypertarget{introduction-24}{%
\section{Introduction}\label{introduction-24}}

Source: \url{http://enhancedatascience.com/2017/04/25/r-basics-linear-regression-with-r/}
Data: \url{https://www.kaggle.com/unsdsn/world-happiness}

Linear regression is one of the basics of statistics and machine learning. Hence, it is a must-have to know how to perform a linear regression with R and how to interpret the results.

Linear regression algorithm will fit the best straight line that fits the data? To do so, it will minimise the squared distance between the points of the dataset and the fitted line.

For this tutorial, we will use the World Happiness report dataset from Kaggle. This report analyses the Happiness of each country according to several factors such as wealth, health, family life, \ldots{} Our goal will be to find the most important factors of happiness. What a noble goal!

\hypertarget{a-quick-exploration-of-the-data}{%
\section{A quick exploration of the data}\label{a-quick-exploration-of-the-data}}

Before fitting any model, we need to know our data better. First, let's import the data into R. Please download the dataset from Kaggle and put it in your working directory.

The code below imports the data as data.table and clean the column names (a lot of . were appearing in the original ones)

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{require}\NormalTok{(data.table)}
\CommentTok{#> Loading required package: data.table}
\NormalTok{data_happiness_dir <-}\StringTok{ }\KeywordTok{file.path}\NormalTok{(data_raw_dir, }\StringTok{"happiness"}\NormalTok{)}

\NormalTok{Happiness_Data =}\StringTok{ }\KeywordTok{data.table}\NormalTok{(}\KeywordTok{read.csv}\NormalTok{(}\KeywordTok{file.path}\NormalTok{(data_happiness_dir, }\StringTok{'2016.csv'}\NormalTok{)))}
\KeywordTok{colnames}\NormalTok{(Happiness_Data) <-}\StringTok{ }\KeywordTok{gsub}\NormalTok{(}\StringTok{'.'}\NormalTok{,}\StringTok{''}\NormalTok{,}\KeywordTok{colnames}\NormalTok{(Happiness_Data), }\DataTypeTok{fixed=}\NormalTok{T)}
\end{Highlighting}
\end{Shaded}

Now, let's plot a Scatter Plot Matrix to get a grasp of how our variables are related one to another. To do so, the GGally package is great.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{require}\NormalTok{(ggplot2)}
\CommentTok{#> Loading required package: ggplot2}
\CommentTok{#> Registered S3 methods overwritten by 'ggplot2':}
\CommentTok{#>   method         from }
\CommentTok{#>   [.quosures     rlang}
\CommentTok{#>   c.quosures     rlang}
\CommentTok{#>   print.quosures rlang}
\KeywordTok{require}\NormalTok{(GGally)}
\CommentTok{#> Loading required package: GGally}
\CommentTok{#> Registered S3 method overwritten by 'GGally':}
\CommentTok{#>   method from   }
\CommentTok{#>   +.gg   ggplot2}
\KeywordTok{ggpairs}\NormalTok{(Happiness_Data[,}\KeywordTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{,}\DecValTok{7}\OperatorTok{:}\DecValTok{13}\NormalTok{), }\DataTypeTok{with=}\NormalTok{F], }\DataTypeTok{lower =} \KeywordTok{list}\NormalTok{( }\DataTypeTok{continuous =} \StringTok{"smooth"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{regression_136-happiness_files/figure-latex/pairplot-1} \end{center}

All the variables are positively correlated with the Happiness score. We can expect that most of the coefficients in the linear regression will be positive. However, the correlation between the variable is often more than 0.5, so we can expect that multicollinearity will appear in the regression.

In the data, we also have access to the Country where the score was computed. Even if it's not useful for the regression, let's plot the data on a map!

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{require}\NormalTok{(}\StringTok{'rworldmap'}\NormalTok{)}
\CommentTok{#> Loading required package: rworldmap}
\CommentTok{#> Loading required package: sp}
\CommentTok{#> }\AlertTok{###}\CommentTok{ Welcome to rworldmap }\AlertTok{###}
\CommentTok{#> For a short introduction type :   vignette('rworldmap')}
\KeywordTok{library}\NormalTok{(reshape2)}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'reshape2'}
\CommentTok{#> The following objects are masked from 'package:data.table':}
\CommentTok{#> }
\CommentTok{#>     dcast, melt}

\NormalTok{map.world <-}\StringTok{ }\KeywordTok{map_data}\NormalTok{(}\DataTypeTok{map=}\StringTok{"world"}\NormalTok{)}

\NormalTok{dataPlot<-}\StringTok{ }\KeywordTok{melt}\NormalTok{(Happiness_Data, }\DataTypeTok{id.vars =}\StringTok{'Country'}\NormalTok{, }
                \DataTypeTok{measure.vars =} \KeywordTok{colnames}\NormalTok{(Happiness_Data)[}\KeywordTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{,}\DecValTok{7}\OperatorTok{:}\DecValTok{13}\NormalTok{)])}

\CommentTok{#Correcting names that are different}
\NormalTok{dataPlot[Country }\OperatorTok{==}\StringTok{ 'United States'}\NormalTok{, Country}\OperatorTok{:}\ErrorTok{=}\StringTok{'USA'}\NormalTok{]}
\NormalTok{dataPlot[Country }\OperatorTok{==}\StringTok{ 'United Kingdoms'}\NormalTok{, Country}\OperatorTok{:}\ErrorTok{=}\StringTok{'UK'}\NormalTok{]}

\CommentTok{##Rescaling each variable to have nice gradient}
\NormalTok{dataPlot[,value}\OperatorTok{:}\ErrorTok{=}\NormalTok{value}\OperatorTok{/}\KeywordTok{max}\NormalTok{(value), by=variable]}
\NormalTok{dataMap =}\StringTok{ }\KeywordTok{data.table}\NormalTok{(}\KeywordTok{merge}\NormalTok{(map.world, dataPlot, }
                           \DataTypeTok{by.x=}\StringTok{'region'}\NormalTok{, }
                           \DataTypeTok{by.y=}\StringTok{'Country'}\NormalTok{, }
                           \DataTypeTok{all.x=}\NormalTok{T))}
\NormalTok{dataMap =}\StringTok{ }\NormalTok{dataMap[}\KeywordTok{order}\NormalTok{(order)]}
\NormalTok{dataMap =}\StringTok{ }\NormalTok{dataMap[}\KeywordTok{order}\NormalTok{(order)][}\OperatorTok{!}\KeywordTok{is.na}\NormalTok{(variable)]}
\NormalTok{gg <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{()}
\NormalTok{gg <-}\StringTok{ }\NormalTok{gg }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{geom_map}\NormalTok{(}\DataTypeTok{data=}\NormalTok{dataMap, }\DataTypeTok{map=}\NormalTok{dataMap, }
             \KeywordTok{aes}\NormalTok{(}\DataTypeTok{map_id =}\NormalTok{ region, }\DataTypeTok{x=}\NormalTok{long, }\DataTypeTok{y=}\NormalTok{lat, }\DataTypeTok{fill=}\NormalTok{value)) }\OperatorTok{+}
\StringTok{    }\CommentTok{# facet_wrap(~variable, scale='free')}
\StringTok{    }\KeywordTok{facet_wrap}\NormalTok{(}\OperatorTok{~}\NormalTok{variable)}
\CommentTok{#> Warning: Ignoring unknown aesthetics: x, y}
\NormalTok{gg <-}\StringTok{ }\NormalTok{gg }\OperatorTok{+}\StringTok{ }\KeywordTok{scale_fill_gradient}\NormalTok{(}\DataTypeTok{low =} \StringTok{"navy"}\NormalTok{, }\DataTypeTok{high =} \StringTok{"lightblue"}\NormalTok{)}
\NormalTok{gg <-}\StringTok{ }\NormalTok{gg }\OperatorTok{+}\StringTok{ }\KeywordTok{coord_equal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

The code above is a classic code for a map. A few important points:

We reordered the point before plotting to avoid some artefacts.
The merge is a right outer join, all the points of the map need to be kept. Otherwise, points will be missing which will mess up the map.
Each variable is rescaled so that a facet\_wrap can be used. Here, the absolute level of a variable is not of primary interest. This is the relative level of a variable between countries that we want to visualise.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gg}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{regression_136-happiness_files/figure-latex/unnamed-chunk-2-1} \end{center}

The distinction between North and South is quite visible. In addition to this, countries that have suffered from the crisis are also really visible.

\hypertarget{linear-regression-with-r}{%
\section{Linear regression with R}\label{linear-regression-with-r}}

Now that we have taken a look at our data, a first model can be fitted. The explanatory variables are the DGP per capita, the life expectancy, the level of freedom and the trust in the government.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{##First model}
\NormalTok{model1 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(HappinessScore }\OperatorTok{~}\StringTok{ }\NormalTok{EconomyGDPperCapita }\OperatorTok{+}\StringTok{ }\NormalTok{Family }\OperatorTok{+}\StringTok{ }
\StringTok{                 }\NormalTok{HealthLifeExpectancy }\OperatorTok{+}\StringTok{ }\NormalTok{Freedom }\OperatorTok{+}\StringTok{ }\NormalTok{TrustGovernmentCorruption, }
             \DataTypeTok{data=}\NormalTok{Happiness_Data)}
\end{Highlighting}
\end{Shaded}

\hypertarget{regression-summary}{%
\section{Regression summary}\label{regression-summary}}

The summary function provides a very easy way to assess a linear regression in R.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{require}\NormalTok{(stargazer)}
\CommentTok{#> Loading required package: stargazer}
\CommentTok{#> }
\CommentTok{#> Please cite as:}
\CommentTok{#>  Hlavac, Marek (2018). stargazer: Well-Formatted Regression and Summary Statistics Tables.}
\CommentTok{#>  R package version 5.2.2. https://CRAN.R-project.org/package=stargazer}

\CommentTok{##Quick summary}
\NormalTok{sum1=}\KeywordTok{summary}\NormalTok{(model1)}
\NormalTok{sum1}
\CommentTok{#> }
\CommentTok{#> Call:}
\CommentTok{#> lm(formula = HappinessScore ~ EconomyGDPperCapita + Family + }
\CommentTok{#>     HealthLifeExpectancy + Freedom + TrustGovernmentCorruption, }
\CommentTok{#>     data = Happiness_Data)}
\CommentTok{#> }
\CommentTok{#> Residuals:}
\CommentTok{#>     Min      1Q  Median      3Q     Max }
\CommentTok{#> -1.4833 -0.2817 -0.0277  0.3280  1.4615 }
\CommentTok{#> }
\CommentTok{#> Coefficients:}
\CommentTok{#>                           Estimate Std. Error t value Pr(>|t|)    }
\CommentTok{#> (Intercept)                  2.212      0.150   14.73  < 2e-16 ***}
\CommentTok{#> EconomyGDPperCapita          0.697      0.209    3.33   0.0011 ** }
\CommentTok{#> Family                       1.234      0.229    5.39  2.6e-07 ***}
\CommentTok{#> HealthLifeExpectancy         1.462      0.343    4.26  3.5e-05 ***}
\CommentTok{#> Freedom                      1.559      0.373    4.18  5.0e-05 ***}
\CommentTok{#> TrustGovernmentCorruption    0.959      0.455    2.11   0.0365 *  }
\CommentTok{#> ---}
\CommentTok{#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1}
\CommentTok{#> }
\CommentTok{#> Residual standard error: 0.535 on 151 degrees of freedom}
\CommentTok{#> Multiple R-squared:  0.787,  Adjusted R-squared:  0.78 }
\CommentTok{#> F-statistic:  112 on 5 and 151 DF,  p-value: <2e-16}

\KeywordTok{stargazer}\NormalTok{(model1,}\DataTypeTok{type=}\StringTok{'text'}\NormalTok{)}
\CommentTok{#> }
\CommentTok{#> =====================================================}
\CommentTok{#>                               Dependent variable:    }
\CommentTok{#>                           ---------------------------}
\CommentTok{#>                                 HappinessScore       }
\CommentTok{#> -----------------------------------------------------}
\CommentTok{#> EconomyGDPperCapita                0.697***          }
\CommentTok{#>                                     (0.209)          }
\CommentTok{#>                                                      }
\CommentTok{#> Family                             1.230***          }
\CommentTok{#>                                     (0.229)          }
\CommentTok{#>                                                      }
\CommentTok{#> HealthLifeExpectancy               1.460***          }
\CommentTok{#>                                     (0.343)          }
\CommentTok{#>                                                      }
\CommentTok{#> Freedom                            1.560***          }
\CommentTok{#>                                     (0.373)          }
\CommentTok{#>                                                      }
\CommentTok{#> TrustGovernmentCorruption           0.959**          }
\CommentTok{#>                                     (0.455)          }
\CommentTok{#>                                                      }
\CommentTok{#> Constant                           2.210***          }
\CommentTok{#>                                     (0.150)          }
\CommentTok{#>                                                      }
\CommentTok{#> -----------------------------------------------------}
\CommentTok{#> Observations                          157            }
\CommentTok{#> R2                                   0.787           }
\CommentTok{#> Adjusted R2                          0.780           }
\CommentTok{#> Residual Std. Error            0.535 (df = 151)      }
\CommentTok{#> F Statistic                112.000*** (df = 5; 151)  }
\CommentTok{#> =====================================================}
\CommentTok{#> Note:                     *p<0.1; **p<0.05; ***p<0.01}
\end{Highlighting}
\end{Shaded}

A quick interpretation:

\begin{itemize}
\tightlist
\item
  All the coefficient are significative at a .05 threshold
\item
  The overall model is also significative
\item
  It explains 78.7\% of Happiness in the dataset
\item
  As expected all the relationship between the explanatory variables and the output variable are positives.
\end{itemize}

The model is doing well!

You can also easily get a given indicator of the model performance, such as R, the different coefficients or the p-value of the overall model.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{##R}
\NormalTok{sum1}\OperatorTok{$}\NormalTok{r.squared}\OperatorTok{*}\DecValTok{100}
\CommentTok{#> [1] 78.7}
\CommentTok{##Coefficients}
\NormalTok{sum1}\OperatorTok{$}\NormalTok{coefficients}
\CommentTok{#>                           Estimate Std. Error t value Pr(>|t|)}
\CommentTok{#> (Intercept)                  2.212      0.150   14.73 5.20e-31}
\CommentTok{#> EconomyGDPperCapita          0.697      0.209    3.33 1.10e-03}
\CommentTok{#> Family                       1.234      0.229    5.39 2.62e-07}
\CommentTok{#> HealthLifeExpectancy         1.462      0.343    4.26 3.53e-05}
\CommentTok{#> Freedom                      1.559      0.373    4.18 5.01e-05}
\CommentTok{#> TrustGovernmentCorruption    0.959      0.455    2.11 3.65e-02}
\CommentTok{##p-value}
\KeywordTok{df}\NormalTok{(sum1}\OperatorTok{$}\NormalTok{fstatistic[}\DecValTok{1}\NormalTok{],sum1}\OperatorTok{$}\NormalTok{fstatistic[}\DecValTok{2}\NormalTok{],sum1}\OperatorTok{$}\NormalTok{fstatistic[}\DecValTok{3}\NormalTok{])}
\CommentTok{#>    value }
\CommentTok{#> 3.39e-49}
 
\CommentTok{##Confidence interval of the coefficient}
\KeywordTok{confint}\NormalTok{(model1,}\DataTypeTok{level =} \FloatTok{0.95}\NormalTok{)}
\CommentTok{#>                            2.5 % 97.5 %}
\CommentTok{#> (Intercept)               1.9152   2.51}
\CommentTok{#> EconomyGDPperCapita       0.2833   1.11}
\CommentTok{#> Family                    0.7821   1.69}
\CommentTok{#> HealthLifeExpectancy      0.7846   2.14}
\CommentTok{#> Freedom                   0.8212   2.30}
\CommentTok{#> TrustGovernmentCorruption 0.0609   1.86}
\KeywordTok{confint}\NormalTok{(model1,}\DataTypeTok{level =} \FloatTok{0.99}\NormalTok{)}
\CommentTok{#>                            0.5 % 99.5 %}
\CommentTok{#> (Intercept)                1.820   2.60}
\CommentTok{#> EconomyGDPperCapita        0.151   1.24}
\CommentTok{#> Family                     0.637   1.83}
\CommentTok{#> HealthLifeExpectancy       0.568   2.36}
\CommentTok{#> Freedom                    0.585   2.53}
\CommentTok{#> TrustGovernmentCorruption -0.227   2.14}
\KeywordTok{confint}\NormalTok{(model1,}\DataTypeTok{level =} \FloatTok{0.90}\NormalTok{)}
\CommentTok{#>                             5 % 95 %}
\CommentTok{#> (Intercept)               1.963 2.46}
\CommentTok{#> EconomyGDPperCapita       0.350 1.04}
\CommentTok{#> Family                    0.856 1.61}
\CommentTok{#> HealthLifeExpectancy      0.895 2.03}
\CommentTok{#> Freedom                   0.941 2.18}
\CommentTok{#> TrustGovernmentCorruption 0.207 1.71}
\end{Highlighting}
\end{Shaded}

\hypertarget{regression-analysis}{%
\section{Regression analysis}\label{regression-analysis}}

\hypertarget{residual-analysis}{%
\subsection{Residual analysis}\label{residual-analysis}}

Now that the regression has been done, the analysis and validity of the result can be analysed. Let's begin with residuals and the assumption of normality and homoscedasticity.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Visualisation of residuals}
\KeywordTok{ggplot}\NormalTok{(model1, }\KeywordTok{aes}\NormalTok{(model1}\OperatorTok{$}\NormalTok{residuals)) }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{geom_histogram}\NormalTok{(}\DataTypeTok{bins=}\DecValTok{20}\NormalTok{, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{y =}\NormalTok{ ..density..)) }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{geom_density}\NormalTok{(}\DataTypeTok{color=}\StringTok{'blue'}\NormalTok{, }\DataTypeTok{fill =} \StringTok{'blue'}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{0.2}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{geom_vline}\NormalTok{(}\DataTypeTok{xintercept =} \KeywordTok{mean}\NormalTok{(model1}\OperatorTok{$}\NormalTok{residuals), }\DataTypeTok{color=}\StringTok{'red'}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{stat_function}\NormalTok{(}\DataTypeTok{fun=}\NormalTok{dnorm, }\DataTypeTok{color=}\StringTok{"red"}\NormalTok{, }\DataTypeTok{size=}\DecValTok{1}\NormalTok{, }
                  \DataTypeTok{args =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{mean =} \KeywordTok{mean}\NormalTok{(model1}\OperatorTok{$}\NormalTok{residuals), }
                            \DataTypeTok{sd =} \KeywordTok{sd}\NormalTok{(model1}\OperatorTok{$}\NormalTok{residuals))) }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{xlab}\NormalTok{(}\StringTok{'residuals values'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{regression_136-happiness_files/figure-latex/normal-curve-1} \end{center}

The residual versus fitted plot is used to see if the residuals behave the same for the different value of the output (i.e, they have the same variance and mean). The plot shows no strong evidence of heteroscedasticity.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(model1, }\KeywordTok{aes}\NormalTok{(model1}\OperatorTok{$}\NormalTok{fitted.values, model1}\OperatorTok{$}\NormalTok{residuals)) }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{geom_point}\NormalTok{() }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{geom_hline}\NormalTok{(}\DataTypeTok{yintercept =} \KeywordTok{c}\NormalTok{(}\FloatTok{1.96} \OperatorTok{*}\StringTok{ }\KeywordTok{sd}\NormalTok{(model1}\OperatorTok{$}\NormalTok{residuals), }
                              \OperatorTok{-}\StringTok{ }\FloatTok{1.96} \OperatorTok{*}\StringTok{ }\KeywordTok{sd}\NormalTok{(model1}\OperatorTok{$}\NormalTok{residuals)), }\DataTypeTok{color=}\StringTok{'red'}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{xlab}\NormalTok{(}\StringTok{'fitted value'}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{ylab}\NormalTok{(}\StringTok{'residuals values'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{regression_136-happiness_files/figure-latex/residuals-vs-fitted-1} \end{center}

\hypertarget{analysis-of-colinearity}{%
\section{Analysis of colinearity}\label{analysis-of-colinearity}}

The colinearity can be assessed using VIF, the car package provides a function to compute it directly.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{require}\NormalTok{(}\StringTok{'car'}\NormalTok{)}
\CommentTok{#> Loading required package: car}
\CommentTok{#> Loading required package: carData}
\KeywordTok{vif}\NormalTok{(model1)}
\CommentTok{#>       EconomyGDPperCapita                    Family }
\CommentTok{#>                      4.07                      2.03 }
\CommentTok{#>      HealthLifeExpectancy                   Freedom }
\CommentTok{#>                      3.37                      1.61 }
\CommentTok{#> TrustGovernmentCorruption }
\CommentTok{#>                      1.39}
\end{Highlighting}
\end{Shaded}

All the VIF are less than 5, and hence there is no sign of colinearity.

\hypertarget{what-drives-happiness}{%
\section{What drives happiness}\label{what-drives-happiness}}

Now let's compute standardised betas to see what really drives happiness.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{##Standardized betas}
\NormalTok{std_betas =}\StringTok{ }\NormalTok{sum1}\OperatorTok{$}\NormalTok{coefficients[}\OperatorTok{-}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OperatorTok{*}\StringTok{ }
\StringTok{    }\KeywordTok{data.table}\NormalTok{(model1}\OperatorTok{$}\NormalTok{model)[, }\KeywordTok{lapply}\NormalTok{(.SD, sd), .SDcols=}\DecValTok{2}\OperatorTok{:}\DecValTok{6}\NormalTok{] }\OperatorTok{/}\StringTok{ }
\StringTok{    }\KeywordTok{sd}\NormalTok{(model1}\OperatorTok{$}\NormalTok{model}\OperatorTok{$}\NormalTok{HappinessScore)}

\NormalTok{std_betas}
\CommentTok{#>    EconomyGDPperCapita Family HealthLifeExpectancy Freedom}
\CommentTok{#> 1:               0.252  0.288                0.294   0.199}
\CommentTok{#>    TrustGovernmentCorruption}
\CommentTok{#> 1:                    0.0933}
\end{Highlighting}
\end{Shaded}

Though the code above may seem complicated, it is just computing the standardised betas for all variables \texttt{std\_beta=beta*sd(x)/sd(y)}.

The top three coefficients are \textbf{Health and Life expectancy}, \textbf{Family} and \textbf{GDP per Capita}. Though money does not make happiness it is among the top three factors of Happiness!

Now you know how to perform a linear regression with R!

\hypertarget{linear-regression-on-advertising}{%
\chapter{Linear Regression on Advertising}\label{linear-regression-on-advertising}}

Videos, slides:

\begin{itemize}
\tightlist
\item
  \url{https://www.r-bloggers.com/in-depth-introduction-to-machine-learning-in-15-hours-of-expert-videos/}
\end{itemize}

Data:

\begin{itemize}
\tightlist
\item
  \url{http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv}
\end{itemize}

code:

\begin{itemize}
\tightlist
\item
  \url{http://subasish.github.io/pages/ISLwithR/}
\item
  \url{http://math480-s15-zarringhalam.wikispaces.umb.edu/R+Code}
\item
  \url{https://github.com/yahwes/ISLR}
\item
  \url{https://www.tau.ac.il/~saharon/IntroStatLearn.html}
\item
  \url{https://www.waxworksmath.com/Authors/G_M/James/WWW/chapter_3.html}
\item
  \url{https://github.com/asadoughi/stat-learning}
\end{itemize}

plots:

\begin{itemize}
\tightlist
\item
  \url{https://onlinecourses.science.psu.edu/stat857/node/28/}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(readr)}

\NormalTok{advertising <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(}\KeywordTok{file.path}\NormalTok{(data_raw_dir, }\StringTok{"Advertising.csv"}\NormalTok{))}
\CommentTok{#> Warning: Missing column names filled in: 'X1' [1]}
\CommentTok{#> Parsed with column specification:}
\CommentTok{#> cols(}
\CommentTok{#>   X1 = col_double(),}
\CommentTok{#>   TV = col_double(),}
\CommentTok{#>   radio = col_double(),}
\CommentTok{#>   newspaper = col_double(),}
\CommentTok{#>   sales = col_double()}
\CommentTok{#> )}
\NormalTok{advertising}
\CommentTok{#> # A tibble: 200 x 5}
\CommentTok{#>      X1    TV radio newspaper sales}
\CommentTok{#>   <dbl> <dbl> <dbl>     <dbl> <dbl>}
\CommentTok{#> 1     1 230.   37.8      69.2  22.1}
\CommentTok{#> 2     2  44.5  39.3      45.1  10.4}
\CommentTok{#> 3     3  17.2  45.9      69.3   9.3}
\CommentTok{#> 4     4 152.   41.3      58.5  18.5}
\CommentTok{#> 5     5 181.   10.8      58.4  12.9}
\CommentTok{#> 6     6   8.7  48.9      75     7.2}
\CommentTok{#> # ... with 194 more rows}
\end{Highlighting}
\end{Shaded}

The Advertising data set. The plot displays sales, in thousands
of units, as a function of TV, radio, and newspaper budgets, in thousands of
dollars, for 200 dierent markets.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{))}
\KeywordTok{plot}\NormalTok{(advertising}\OperatorTok{$}\NormalTok{TV, advertising}\OperatorTok{$}\NormalTok{sales, }\DataTypeTok{xlab =} \StringTok{"TV"}\NormalTok{, }\DataTypeTok{ylab =} \StringTok{"Sales"}\NormalTok{, }\DataTypeTok{col =} \StringTok{"red"}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(advertising}\OperatorTok{$}\NormalTok{radio, advertising}\OperatorTok{$}\NormalTok{sales, }\DataTypeTok{xlab=}\StringTok{"Radio"}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{"Sales"}\NormalTok{, }\DataTypeTok{col=}\StringTok{"red"}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(advertising}\OperatorTok{$}\NormalTok{radio, advertising}\OperatorTok{$}\NormalTok{newspaper, }\DataTypeTok{xlab=}\StringTok{"Newspaper"}\NormalTok{, }
     \DataTypeTok{ylab=}\StringTok{"Sales"}\NormalTok{, }\DataTypeTok{col=}\StringTok{"red"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{regression_138-advertising_files/figure-latex/unnamed-chunk-3-1} \end{center}

In each plot we show the simple least squares
t of sales to that variable, as described in Chapter 3. In other words, each blue
line represents a simple model that can be used to predict sales using TV, radio,
and newspaper, respectively.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{))}
\NormalTok{tv_model <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(sales }\OperatorTok{~}\StringTok{ }\NormalTok{TV, }\DataTypeTok{data =}\NormalTok{ advertising)}
\NormalTok{radio_model <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(sales }\OperatorTok{~}\StringTok{ }\NormalTok{radio, }\DataTypeTok{data =}\NormalTok{ advertising)}
\NormalTok{newspaper_model <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(sales }\OperatorTok{~}\StringTok{ }\NormalTok{newspaper, }\DataTypeTok{data =}\NormalTok{ advertising)}

\KeywordTok{plot}\NormalTok{(advertising}\OperatorTok{$}\NormalTok{TV, advertising}\OperatorTok{$}\NormalTok{sales, }\DataTypeTok{xlab =} \StringTok{"TV"}\NormalTok{, }\DataTypeTok{ylab =} \StringTok{"Sales"}\NormalTok{, }\DataTypeTok{col =} \StringTok{"red"}\NormalTok{)}
\KeywordTok{abline}\NormalTok{(tv_model, }\DataTypeTok{col =} \StringTok{"blue"}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(advertising}\OperatorTok{$}\NormalTok{radio, advertising}\OperatorTok{$}\NormalTok{sales, }\DataTypeTok{xlab=}\StringTok{"Radio"}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{"Sales"}\NormalTok{, }\DataTypeTok{col=}\StringTok{"red"}\NormalTok{)}
\KeywordTok{abline}\NormalTok{(radio_model)}
\KeywordTok{plot}\NormalTok{(advertising}\OperatorTok{$}\NormalTok{newspaper, advertising}\OperatorTok{$}\NormalTok{sales, }\DataTypeTok{xlab=}\StringTok{"Newspaper"}\NormalTok{, }
     \DataTypeTok{ylab=}\StringTok{"Sales"}\NormalTok{, }\DataTypeTok{col=}\StringTok{"red"}\NormalTok{)}
\KeywordTok{abline}\NormalTok{(newspaper_model)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{regression_138-advertising_files/figure-latex/unnamed-chunk-4-1} \end{center}

Recall the Advertising data from Chapter 2. Figure 2.1 displays sales
(in thousands of units) for a particular product as a function of advertis-
ing budgets (in thousands of dollars) for TV, radio, and newspaper media.
Suppose that in our role as statistical consultants we are asked to suggest,
on the basis of this data, a marketing plan for next year that will result in
high product sales. What information would be useful in order to provide
such a recommendation? Here are a few important questions that we might
seek to address:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Is there a relationship between advertising budget and sales?
\item
  How strong is the relationship between advertising budget and sales?
\item
  Which media contribute to sales?
\item
  How accurately can we estimate the eect of each medium on sales?
\end{enumerate}

For the Advertising data, the least squares fit for the regression
of sales onto TV is shown. The fit is found by minimizing the sum of squared
errors. Each grey line segment represents an error, and the fit makes a compro-
mise by averaging their squares. In this case a linear fit captures the essence of
the relationship, although it is somewhat deficient in the left of the plot.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tv_model <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(sales }\OperatorTok{~}\StringTok{ }\NormalTok{TV, }\DataTypeTok{data =}\NormalTok{ advertising)}
\KeywordTok{plot}\NormalTok{(advertising}\OperatorTok{$}\NormalTok{TV, advertising}\OperatorTok{$}\NormalTok{sales, }\DataTypeTok{xlab =} \StringTok{"TV"}\NormalTok{, }\DataTypeTok{ylab =} \StringTok{"Sales"}\NormalTok{, }
     \DataTypeTok{col =} \StringTok{"red"}\NormalTok{, }\DataTypeTok{pch=}\DecValTok{16}\NormalTok{)}
\KeywordTok{abline}\NormalTok{(tv_model, }\DataTypeTok{col =} \StringTok{"blue"}\NormalTok{, }\DataTypeTok{lwd=}\DecValTok{2}\NormalTok{)}
\KeywordTok{segments}\NormalTok{(advertising}\OperatorTok{$}\NormalTok{TV, advertising}\OperatorTok{$}\NormalTok{sales, advertising}\OperatorTok{$}\NormalTok{TV, }\KeywordTok{predict}\NormalTok{(tv_model), }
         \DataTypeTok{col =} \StringTok{"gray"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{regression_138-advertising_files/figure-latex/unnamed-chunk-5-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{smry <-}\StringTok{ }\KeywordTok{summary}\NormalTok{(tv_model)}
\NormalTok{smry}
\CommentTok{#> }
\CommentTok{#> Call:}
\CommentTok{#> lm(formula = sales ~ TV, data = advertising)}
\CommentTok{#> }
\CommentTok{#> Residuals:}
\CommentTok{#>    Min     1Q Median     3Q    Max }
\CommentTok{#> -8.386 -1.955 -0.191  2.067  7.212 }
\CommentTok{#> }
\CommentTok{#> Coefficients:}
\CommentTok{#>             Estimate Std. Error t value Pr(>|t|)    }
\CommentTok{#> (Intercept)  7.03259    0.45784    15.4   <2e-16 ***}
\CommentTok{#> TV           0.04754    0.00269    17.7   <2e-16 ***}
\CommentTok{#> ---}
\CommentTok{#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1}
\CommentTok{#> }
\CommentTok{#> Residual standard error: 3.26 on 198 degrees of freedom}
\CommentTok{#> Multiple R-squared:  0.612,  Adjusted R-squared:  0.61 }
\CommentTok{#> F-statistic:  312 on 1 and 198 DF,  p-value: <2e-16}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(lattice)}

\NormalTok{minRss <-}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{abs}\NormalTok{(}\KeywordTok{min}\NormalTok{(smry}\OperatorTok{$}\NormalTok{residuals))) }\OperatorTok{*}\StringTok{ }\KeywordTok{sign}\NormalTok{(}\KeywordTok{min}\NormalTok{(smry}\OperatorTok{$}\NormalTok{residuals))}
\NormalTok{maxRss <-}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{max}\NormalTok{(smry}\OperatorTok{$}\NormalTok{residuals))}

\NormalTok{twovar <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x, y) \{ }
\NormalTok{  x}\OperatorTok{^}\DecValTok{2} \OperatorTok{+}\StringTok{ }\NormalTok{y}\OperatorTok{^}\DecValTok{2}\NormalTok{ \}}

\NormalTok{mat <-}\StringTok{ }\KeywordTok{outer}\NormalTok{( }\KeywordTok{seq}\NormalTok{(minRss, maxRss, }\DataTypeTok{length =} \DecValTok{100}\NormalTok{),  }
                \KeywordTok{seq}\NormalTok{(minRss, maxRss, }\DataTypeTok{length =} \DecValTok{100}\NormalTok{), }
                \KeywordTok{Vectorize}\NormalTok{( }\ControlFlowTok{function}\NormalTok{(x,y) }\KeywordTok{twovar}\NormalTok{(x, y) ) )}



\KeywordTok{contourplot}\NormalTok{(mat, }\DataTypeTok{at =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{regression_138-advertising_files/figure-latex/unnamed-chunk-7-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tv_model}
\CommentTok{#> }
\CommentTok{#> Call:}
\CommentTok{#> lm(formula = sales ~ TV, data = advertising)}
\CommentTok{#> }
\CommentTok{#> Coefficients:}
\CommentTok{#> (Intercept)           TV  }
\CommentTok{#>      7.0326       0.0475}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tv.lm <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(sales }\OperatorTok{~}\StringTok{ }\KeywordTok{poly}\NormalTok{(sales, TV, }\DataTypeTok{degree=}\DecValTok{2}\NormalTok{), }\DataTypeTok{data =}\NormalTok{ advertising)}
\CommentTok{# contour(tv.lm, sales ~ TV)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(rsm)}
\NormalTok{mpg.lm <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(mpg }\OperatorTok{~}\StringTok{ }\KeywordTok{poly}\NormalTok{(hp, disp, }\DataTypeTok{degree =} \DecValTok{3}\NormalTok{), }\DataTypeTok{data =}\NormalTok{ mtcars)}
\KeywordTok{contour}\NormalTok{(mpg.lm, hp }\OperatorTok{~}\StringTok{ }\NormalTok{disp)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{regression_138-advertising_files/figure-latex/unnamed-chunk-10-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\DecValTok{-6}\OperatorTok{:}\DecValTok{16}
\NormalTok{op <-}\StringTok{ }\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow =} \KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\KeywordTok{contour}\NormalTok{(}\KeywordTok{outer}\NormalTok{(x, x), }\DataTypeTok{method =} \StringTok{"flattest"}\NormalTok{, }\DataTypeTok{vfont =} \KeywordTok{c}\NormalTok{(}\StringTok{"sans serif"}\NormalTok{, }\StringTok{"plain"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{regression_138-advertising_files/figure-latex/unnamed-chunk-11-1} \end{center}

\hypertarget{lab-3a-regression.-iris-dataset}{%
\chapter{\texorpdfstring{Lab 3A: Regression. \texttt{iris} dataset}{Lab 3A: Regression. iris dataset}}\label{lab-3a-regression.-iris-dataset}}

\hypertarget{introduction-25}{%
\section{Introduction}\label{introduction-25}}

\url{https://www.matthewrenze.com/workshops/practical-machine-learning-with-r/lab-3a-regression.html}

\hypertarget{explore-the-data}{%
\section{Explore the Data}\label{explore-the-data}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Load Iris data
\item
  Plot scatterplot
\item
  Plot correlogram
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{data}\NormalTok{(iris)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{write.csv}\NormalTok{(iris, }\KeywordTok{file.path}\NormalTok{(data_raw_dir, }\StringTok{"iris.csv"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Create scatterplot matrix

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(iris[}\DecValTok{1}\OperatorTok{:}\DecValTok{4}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{regression_141.1-3a-iris_dataset_files/figure-latex/unnamed-chunk-5-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(corrgram)}
\CommentTok{#> Registered S3 methods overwritten by 'ggplot2':}
\CommentTok{#>   method         from }
\CommentTok{#>   [.quosures     rlang}
\CommentTok{#>   c.quosures     rlang}
\CommentTok{#>   print.quosures rlang}
\CommentTok{#> Registered S3 method overwritten by 'seriation':}
\CommentTok{#>   method         from }
\CommentTok{#>   reorder.hclust gclus}
\KeywordTok{corrgram}\NormalTok{(iris[}\DecValTok{1}\OperatorTok{:}\DecValTok{4}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{regression_141.1-3a-iris_dataset_files/figure-latex/unnamed-chunk-6-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{cor}\NormalTok{(iris[}\DecValTok{1}\OperatorTok{:}\DecValTok{4}\NormalTok{])}
\CommentTok{#>              Sepal.Length Sepal.Width Petal.Length Petal.Width}
\CommentTok{#> Sepal.Length        1.000      -0.118        0.872       0.818}
\CommentTok{#> Sepal.Width        -0.118       1.000       -0.428      -0.366}
\CommentTok{#> Petal.Length        0.872      -0.428        1.000       0.963}
\CommentTok{#> Petal.Width         0.818      -0.366        0.963       1.000}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{cor}\NormalTok{(}
  \DataTypeTok{x =}\NormalTok{ iris}\OperatorTok{$}\NormalTok{Petal.Length, }
  \DataTypeTok{y =}\NormalTok{ iris}\OperatorTok{$}\NormalTok{Petal.Width)}
\CommentTok{#> [1] 0.963}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(}
  \DataTypeTok{x =}\NormalTok{ iris}\OperatorTok{$}\NormalTok{Petal.Length, }
  \DataTypeTok{y =}\NormalTok{ iris}\OperatorTok{$}\NormalTok{Petal.Width,}
  \DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\FloatTok{0.25}\NormalTok{, }\DecValTok{7}\NormalTok{),}
  \DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\FloatTok{0.25}\NormalTok{, }\FloatTok{2.5}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{regression_141.1-3a-iris_dataset_files/figure-latex/unnamed-chunk-9-1} \end{center}

\hypertarget{create-training-and-test-sets}{%
\section{Create Training and Test Sets}\label{create-training-and-test-sets}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{42}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{indexes <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(}
  \DataTypeTok{x =} \DecValTok{1}\OperatorTok{:}\DecValTok{150}\NormalTok{, }
  \DataTypeTok{size =} \DecValTok{100}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{train <-}\StringTok{ }\NormalTok{iris[indexes, ]}
\NormalTok{test <-}\StringTok{ }\NormalTok{iris[}\OperatorTok{-}\NormalTok{indexes, ]}
\end{Highlighting}
\end{Shaded}

\hypertarget{predict-with-simple-linear-regression}{%
\section{Predict with Simple Linear Regression}\label{predict-with-simple-linear-regression}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{simpleModel <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(}
  \DataTypeTok{formula =}\NormalTok{ Petal.Width }\OperatorTok{~}\StringTok{ }\NormalTok{Petal.Length,}
  \DataTypeTok{data =}\NormalTok{ train)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(}
  \DataTypeTok{x =}\NormalTok{ iris}\OperatorTok{$}\NormalTok{Petal.Length, }
  \DataTypeTok{y =}\NormalTok{ iris}\OperatorTok{$}\NormalTok{Petal.Width,}
  \DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\FloatTok{0.25}\NormalTok{, }\DecValTok{7}\NormalTok{),}
  \DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\FloatTok{0.25}\NormalTok{, }\FloatTok{2.5}\NormalTok{))}
  
\KeywordTok{lines}\NormalTok{(}
  \DataTypeTok{x =}\NormalTok{ train}\OperatorTok{$}\NormalTok{Petal.Length,}
  \DataTypeTok{y =}\NormalTok{ simpleModel}\OperatorTok{$}\NormalTok{fitted, }
  \DataTypeTok{col =} \StringTok{"red"}\NormalTok{,}
  \DataTypeTok{lwd =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{regression_141.1-3a-iris_dataset_files/figure-latex/unnamed-chunk-14-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(simpleModel)}
\CommentTok{#> }
\CommentTok{#> Call:}
\CommentTok{#> lm(formula = Petal.Width ~ Petal.Length, data = train)}
\CommentTok{#> }
\CommentTok{#> Residuals:}
\CommentTok{#>     Min      1Q  Median      3Q     Max }
\CommentTok{#> -0.5684 -0.1279 -0.0307  0.1280  0.6385 }
\CommentTok{#> }
\CommentTok{#> Coefficients:}
\CommentTok{#>              Estimate Std. Error t value Pr(>|t|)    }
\CommentTok{#> (Intercept)   -0.3486     0.0476   -7.33  6.7e-11 ***}
\CommentTok{#> Petal.Length   0.4137     0.0119   34.80  < 2e-16 ***}
\CommentTok{#> ---}
\CommentTok{#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1}
\CommentTok{#> }
\CommentTok{#> Residual standard error: 0.209 on 98 degrees of freedom}
\CommentTok{#> Multiple R-squared:  0.925,  Adjusted R-squared:  0.924 }
\CommentTok{#> F-statistic: 1.21e+03 on 1 and 98 DF,  p-value: <2e-16}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{simplePredictions <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(}
  \DataTypeTok{object =}\NormalTok{ simpleModel,}
  \DataTypeTok{newdata =}\NormalTok{ test)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(}
  \DataTypeTok{x =}\NormalTok{ iris}\OperatorTok{$}\NormalTok{Petal.Length, }
  \DataTypeTok{y =}\NormalTok{ iris}\OperatorTok{$}\NormalTok{Petal.Width,}
  \DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\FloatTok{0.25}\NormalTok{, }\DecValTok{7}\NormalTok{),}
  \DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\FloatTok{0.25}\NormalTok{, }\FloatTok{2.5}\NormalTok{))}
  
\KeywordTok{points}\NormalTok{(}
  \DataTypeTok{x =}\NormalTok{ test}\OperatorTok{$}\NormalTok{Petal.Length,}
  \DataTypeTok{y =}\NormalTok{ simplePredictions,}
  \DataTypeTok{col =} \StringTok{"blue"}\NormalTok{,}
  \DataTypeTok{pch =} \DecValTok{4}\NormalTok{,}
  \DataTypeTok{lwd =} \DecValTok{2}\NormalTok{)}

\KeywordTok{points}\NormalTok{(}
  \DataTypeTok{x =}\NormalTok{ test}\OperatorTok{$}\NormalTok{Petal.Length,}
  \DataTypeTok{y =}\NormalTok{ test}\OperatorTok{$}\NormalTok{Petal.Width,}
  \DataTypeTok{col =} \StringTok{"red"}\NormalTok{,}
  \DataTypeTok{pch =} \DecValTok{16}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{regression_141.1-3a-iris_dataset_files/figure-latex/unnamed-chunk-17-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{simpleRMSE <-}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{mean}\NormalTok{((test}\OperatorTok{$}\NormalTok{Petal.Width }\OperatorTok{-}\StringTok{ }\NormalTok{simplePredictions)}\OperatorTok{^}\DecValTok{2}\NormalTok{))}
\KeywordTok{print}\NormalTok{(simpleRMSE)}
\CommentTok{#> [1] 0.201}
\end{Highlighting}
\end{Shaded}

\hypertarget{predict-with-multiple-regression}{%
\section{Predict with Multiple Regression}\label{predict-with-multiple-regression}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{multipleModel <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(}
  \DataTypeTok{formula =}\NormalTok{ Petal.Width }\OperatorTok{~}\StringTok{ }\NormalTok{.,}
  \DataTypeTok{data =}\NormalTok{ train)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(multipleModel)}
\CommentTok{#> }
\CommentTok{#> Call:}
\CommentTok{#> lm(formula = Petal.Width ~ ., data = train)}
\CommentTok{#> }
\CommentTok{#> Residuals:}
\CommentTok{#>     Min      1Q  Median      3Q     Max }
\CommentTok{#> -0.5769 -0.0843 -0.0066  0.0978  0.4731 }
\CommentTok{#> }
\CommentTok{#> Coefficients:}
\CommentTok{#>                   Estimate Std. Error t value Pr(>|t|)    }
\CommentTok{#> (Intercept)        -0.5088     0.2277   -2.23  0.02779 *  }
\CommentTok{#> Sepal.Length       -0.0486     0.0593   -0.82  0.41435    }
\CommentTok{#> Sepal.Width         0.2032     0.0594    3.42  0.00092 ***}
\CommentTok{#> Petal.Length        0.2103     0.0641    3.28  0.00146 ** }
\CommentTok{#> Speciesversicolor   0.6769     0.1583    4.28  4.5e-05 ***}
\CommentTok{#> Speciesvirginica    1.0762     0.2126    5.06  2.1e-06 ***}
\CommentTok{#> ---}
\CommentTok{#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1}
\CommentTok{#> }
\CommentTok{#> Residual standard error: 0.176 on 94 degrees of freedom}
\CommentTok{#> Multiple R-squared:  0.949,  Adjusted R-squared:  0.947 }
\CommentTok{#> F-statistic:  352 on 5 and 94 DF,  p-value: <2e-16}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{multiplePredictions <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(}
  \DataTypeTok{object =}\NormalTok{ multipleModel,}
  \DataTypeTok{newdata =}\NormalTok{ test)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(}
  \DataTypeTok{x =}\NormalTok{ iris}\OperatorTok{$}\NormalTok{Petal.Length, }
  \DataTypeTok{y =}\NormalTok{ iris}\OperatorTok{$}\NormalTok{Petal.Width,}
  \DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\FloatTok{0.25}\NormalTok{, }\DecValTok{7}\NormalTok{),}
  \DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\FloatTok{0.25}\NormalTok{, }\FloatTok{2.5}\NormalTok{))}
  
\KeywordTok{points}\NormalTok{(}
  \DataTypeTok{x =}\NormalTok{ test}\OperatorTok{$}\NormalTok{Petal.Length,}
  \DataTypeTok{y =}\NormalTok{ multiplePredictions,}
  \DataTypeTok{col =} \StringTok{"blue"}\NormalTok{,}
  \DataTypeTok{pch =} \DecValTok{4}\NormalTok{,}
  \DataTypeTok{lwd =} \DecValTok{2}\NormalTok{)}

\KeywordTok{points}\NormalTok{(}
  \DataTypeTok{x =}\NormalTok{ test}\OperatorTok{$}\NormalTok{Petal.Length,}
  \DataTypeTok{y =}\NormalTok{ test}\OperatorTok{$}\NormalTok{Petal.Width,}
  \DataTypeTok{col =} \StringTok{"red"}\NormalTok{,}
  \DataTypeTok{pch =} \DecValTok{16}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{regression_141.1-3a-iris_dataset_files/figure-latex/unnamed-chunk-22-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{multipleRMSE <-}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{mean}\NormalTok{((test}\OperatorTok{$}\NormalTok{Petal.Width }\OperatorTok{-}\StringTok{ }\NormalTok{multiplePredictions)}\OperatorTok{^}\DecValTok{2}\NormalTok{))}
\KeywordTok{print}\NormalTok{(multipleRMSE)}
\CommentTok{#> [1] 0.15}
\end{Highlighting}
\end{Shaded}

\hypertarget{predict-with-neural-network-regression}{%
\section{5. Predict with Neural Network Regression}\label{predict-with-neural-network-regression}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{normalize <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x) \{}
\NormalTok{  (x }\OperatorTok{-}\StringTok{ }\KeywordTok{min}\NormalTok{(x)) }\OperatorTok{/}\StringTok{ }\NormalTok{(}\KeywordTok{max}\NormalTok{(x) }\OperatorTok{-}\StringTok{ }\KeywordTok{min}\NormalTok{(x)) }\OperatorTok{-}\StringTok{ }\FloatTok{0.5}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{denormalize <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x, y) \{}
\NormalTok{  ((x }\OperatorTok{+}\StringTok{ }\FloatTok{0.5}\NormalTok{) }\OperatorTok{*}\StringTok{ }\NormalTok{(}\KeywordTok{max}\NormalTok{(y) }\OperatorTok{-}\StringTok{ }\KeywordTok{min}\NormalTok{(y))) }\OperatorTok{+}\StringTok{ }\KeywordTok{min}\NormalTok{(y)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{scaledIris <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}
  \DataTypeTok{Sepal.Length =} \KeywordTok{normalize}\NormalTok{(iris}\OperatorTok{$}\NormalTok{Sepal.Length),}
  \DataTypeTok{Sepal.Width =} \KeywordTok{normalize}\NormalTok{(iris}\OperatorTok{$}\NormalTok{Sepal.Width),}
  \DataTypeTok{Petal.Length =} \KeywordTok{normalize}\NormalTok{(iris}\OperatorTok{$}\NormalTok{Petal.Length),}
  \DataTypeTok{Petal.Width =} \KeywordTok{normalize}\NormalTok{(iris}\OperatorTok{$}\NormalTok{Petal.Width),}
  \DataTypeTok{Species =}\NormalTok{ iris}\OperatorTok{$}\NormalTok{Species)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{scaledTrain <-}\StringTok{ }\NormalTok{scaledIris[indexes, ]}
\NormalTok{scaledTest <-}\StringTok{ }\NormalTok{scaledIris[}\OperatorTok{-}\NormalTok{indexes, ]}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(nnet)}

\NormalTok{neuralRegressor <-}\StringTok{ }\KeywordTok{nnet}\NormalTok{(}
  \DataTypeTok{formula =}\NormalTok{ Petal.Width }\OperatorTok{~}\StringTok{ }\NormalTok{.,}
  \DataTypeTok{data =}\NormalTok{ scaledTrain,}
  \DataTypeTok{linout =} \OtherTok{TRUE}\NormalTok{,}
  \DataTypeTok{skip =} \OtherTok{TRUE}\NormalTok{,}
  \DataTypeTok{size =} \DecValTok{4}\NormalTok{,}
  \DataTypeTok{decay =} \FloatTok{0.0001}\NormalTok{,}
  \DataTypeTok{maxit =} \DecValTok{500}\NormalTok{)}
\CommentTok{#> # weights:  34}
\CommentTok{#> initial  value 64.175158 }
\CommentTok{#> iter  10 value 0.498340}
\CommentTok{#> iter  20 value 0.439307}
\CommentTok{#> iter  30 value 0.419373}
\CommentTok{#> iter  40 value 0.415119}
\CommentTok{#> iter  50 value 0.412305}
\CommentTok{#> iter  60 value 0.410862}
\CommentTok{#> iter  70 value 0.404854}
\CommentTok{#> iter  80 value 0.402606}
\CommentTok{#> iter  90 value 0.397903}
\CommentTok{#> iter 100 value 0.396295}
\CommentTok{#> iter 110 value 0.394291}
\CommentTok{#> iter 120 value 0.392652}
\CommentTok{#> iter 130 value 0.390227}
\CommentTok{#> iter 140 value 0.389581}
\CommentTok{#> iter 150 value 0.388891}
\CommentTok{#> iter 160 value 0.387501}
\CommentTok{#> iter 170 value 0.382381}
\CommentTok{#> iter 180 value 0.377034}
\CommentTok{#> iter 190 value 0.371871}
\CommentTok{#> iter 200 value 0.364243}
\CommentTok{#> iter 210 value 0.357845}
\CommentTok{#> iter 220 value 0.353726}
\CommentTok{#> iter 230 value 0.348595}
\CommentTok{#> iter 240 value 0.345766}
\CommentTok{#> iter 250 value 0.341638}
\CommentTok{#> iter 260 value 0.340492}
\CommentTok{#> iter 270 value 0.339963}
\CommentTok{#> iter 280 value 0.338600}
\CommentTok{#> iter 290 value 0.338192}
\CommentTok{#> iter 300 value 0.336018}
\CommentTok{#> iter 310 value 0.332364}
\CommentTok{#> iter 320 value 0.331113}
\CommentTok{#> iter 330 value 0.330340}
\CommentTok{#> iter 340 value 0.329913}
\CommentTok{#> iter 350 value 0.329630}
\CommentTok{#> iter 360 value 0.329433}
\CommentTok{#> iter 370 value 0.328969}
\CommentTok{#> iter 380 value 0.328461}
\CommentTok{#> iter 390 value 0.327849}
\CommentTok{#> iter 400 value 0.326887}
\CommentTok{#> iter 410 value 0.326022}
\CommentTok{#> iter 420 value 0.325114}
\CommentTok{#> iter 430 value 0.323672}
\CommentTok{#> iter 440 value 0.321995}
\CommentTok{#> iter 450 value 0.320491}
\CommentTok{#> iter 460 value 0.318875}
\CommentTok{#> iter 470 value 0.317241}
\CommentTok{#> iter 480 value 0.316544}
\CommentTok{#> iter 490 value 0.316008}
\CommentTok{#> iter 500 value 0.315713}
\CommentTok{#> final  value 0.315713 }
\CommentTok{#> stopped after 500 iterations}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(NeuralNetTools)}

\KeywordTok{plotnet}\NormalTok{(neuralRegressor)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{regression_141.1-3a-iris_dataset_files/figure-latex/unnamed-chunk-28-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{scaledPredictions <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(}
  \DataTypeTok{object =}\NormalTok{ neuralRegressor, }
  \DataTypeTok{newdata =}\NormalTok{ scaledTest)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{neuralPredictions <-}\StringTok{ }\KeywordTok{denormalize}\NormalTok{(}
  \DataTypeTok{x =}\NormalTok{ scaledPredictions, }
  \DataTypeTok{y =}\NormalTok{ iris}\OperatorTok{$}\NormalTok{Petal.Width)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(}
  \DataTypeTok{x =}\NormalTok{ iris}\OperatorTok{$}\NormalTok{Petal.Length, }
  \DataTypeTok{y =}\NormalTok{ iris}\OperatorTok{$}\NormalTok{Petal.Width,}
  \DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\FloatTok{0.25}\NormalTok{, }\DecValTok{7}\NormalTok{),}
  \DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\FloatTok{0.25}\NormalTok{, }\FloatTok{2.5}\NormalTok{))}
  
\KeywordTok{points}\NormalTok{(}
  \DataTypeTok{x =}\NormalTok{ test}\OperatorTok{$}\NormalTok{Petal.Length,}
  \DataTypeTok{y =}\NormalTok{ neuralPredictions,}
  \DataTypeTok{col =} \StringTok{"blue"}\NormalTok{,}
  \DataTypeTok{pch =} \DecValTok{4}\NormalTok{,}
  \DataTypeTok{lwd =} \DecValTok{2}\NormalTok{)}

\KeywordTok{points}\NormalTok{(}
  \DataTypeTok{x =}\NormalTok{ test}\OperatorTok{$}\NormalTok{Petal.Length,}
  \DataTypeTok{y =}\NormalTok{ test}\OperatorTok{$}\NormalTok{Petal.Width,}
  \DataTypeTok{col =} \StringTok{"red"}\NormalTok{,}
  \DataTypeTok{pch =} \DecValTok{16}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{regression_141.1-3a-iris_dataset_files/figure-latex/unnamed-chunk-31-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{neuralRMSE <-}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{mean}\NormalTok{((test}\OperatorTok{$}\NormalTok{Petal.Width }\OperatorTok{-}\StringTok{ }\NormalTok{neuralPredictions)}\OperatorTok{^}\DecValTok{2}\NormalTok{))}
\KeywordTok{print}\NormalTok{(neuralRMSE)}
\CommentTok{#> [1] 0.183}
\end{Highlighting}
\end{Shaded}

\hypertarget{evaluate-all-the-regression-models}{%
\section{6. Evaluate all the regression Models}\label{evaluate-all-the-regression-models}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{print}\NormalTok{(simpleRMSE)}
\CommentTok{#> [1] 0.201}
\KeywordTok{print}\NormalTok{(multipleRMSE)}
\CommentTok{#> [1] 0.15}
\KeywordTok{print}\NormalTok{(neuralRMSE)}
\CommentTok{#> [1] 0.183}
\end{Highlighting}
\end{Shaded}

\hypertarget{regression-3b.-rates-dataset.-slr-mlr-nn}{%
\chapter{\texorpdfstring{Regression 3b. \texttt{Rates} dataset. (\emph{SLR, MLR, NN})}{Regression 3b. Rates dataset. (SLR, MLR, NN)}}\label{regression-3b.-rates-dataset.-slr-mlr-nn}}

\hypertarget{introduction-26}{%
\section{Introduction}\label{introduction-26}}

\begin{quote}
line 29 does not plot
\end{quote}

\textbf{Source:} \url{https://www.matthewrenze.com/workshops/practical-machine-learning-with-r/lab-3b-regression.html}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(readr)}

\NormalTok{policies <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(}\KeywordTok{file.path}\NormalTok{(data_raw_dir, }\StringTok{"Rates.csv"}\NormalTok{))}
\CommentTok{#> Parsed with column specification:}
\CommentTok{#> cols(}
\CommentTok{#>   Gender = col_character(),}
\CommentTok{#>   State = col_character(),}
\CommentTok{#>   State.Rate = col_double(),}
\CommentTok{#>   Height = col_double(),}
\CommentTok{#>   Weight = col_double(),}
\CommentTok{#>   BMI = col_double(),}
\CommentTok{#>   Age = col_double(),}
\CommentTok{#>   Rate = col_double()}
\CommentTok{#> )}
\NormalTok{policies}
\CommentTok{#> # A tibble: 1,942 x 8}
\CommentTok{#>   Gender State State.Rate Height Weight   BMI   Age   Rate}
\CommentTok{#>   <chr>  <chr>      <dbl>  <dbl>  <dbl> <dbl> <dbl>  <dbl>}
\CommentTok{#> 1 Male   MA        0.100     184   67.8  20.0    77 0.332 }
\CommentTok{#> 2 Male   VA        0.142     163   89.4  33.6    82 0.869 }
\CommentTok{#> 3 Male   NY        0.0908    170   81.2  28.1    31 0.01  }
\CommentTok{#> 4 Male   TN        0.120     175   99.7  32.6    39 0.0215}
\CommentTok{#> 5 Male   FL        0.110     184   72.1  21.3    68 0.150 }
\CommentTok{#> 6 Male   WA        0.163     166   98.4  35.7    64 0.211 }
\CommentTok{#> # ... with 1,936 more rows}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(policies)}
\CommentTok{#>     Gender             State             State.Rate        Height   }
\CommentTok{#>  Length:1942        Length:1942        Min.   :0.001   Min.   :150  }
\CommentTok{#>  Class :character   Class :character   1st Qu.:0.110   1st Qu.:162  }
\CommentTok{#>  Mode  :character   Mode  :character   Median :0.128   Median :170  }
\CommentTok{#>                                        Mean   :0.138   Mean   :170  }
\CommentTok{#>                                        3rd Qu.:0.144   3rd Qu.:176  }
\CommentTok{#>                                        Max.   :0.318   Max.   :190  }
\CommentTok{#>      Weight           BMI            Age            Rate      }
\CommentTok{#>  Min.   : 44.1   Min.   :16.0   Min.   :18.0   Min.   :0.001  }
\CommentTok{#>  1st Qu.: 68.6   1st Qu.:23.7   1st Qu.:34.0   1st Qu.:0.015  }
\CommentTok{#>  Median : 81.3   Median :28.1   Median :51.0   Median :0.046  }
\CommentTok{#>  Mean   : 81.2   Mean   :28.3   Mean   :50.8   Mean   :0.138  }
\CommentTok{#>  3rd Qu.: 93.8   3rd Qu.:32.5   3rd Qu.:68.0   3rd Qu.:0.173  }
\CommentTok{#>  Max.   :116.5   Max.   :46.8   Max.   :84.0   Max.   :0.999}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(RColorBrewer)}
\NormalTok{palette <-}\StringTok{ }\KeywordTok{brewer.pal}\NormalTok{(}\DecValTok{9}\NormalTok{, }\StringTok{"Reds"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot(}
\CommentTok{#   x = policies,}
\CommentTok{#   col = palette[cut(x = policies$Rate, breaks = 9)]}
\CommentTok{#   )}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(corrgram)}
\CommentTok{#> Registered S3 methods overwritten by 'ggplot2':}
\CommentTok{#>   method         from }
\CommentTok{#>   [.quosures     rlang}
\CommentTok{#>   c.quosures     rlang}
\CommentTok{#>   print.quosures rlang}
\CommentTok{#> Registered S3 method overwritten by 'seriation':}
\CommentTok{#>   method         from }
\CommentTok{#>   reorder.hclust gclus}

\KeywordTok{corrgram}\NormalTok{(policies)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{regression_141.2-3b-rates_dataset_files/figure-latex/unnamed-chunk-6-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{cor}\NormalTok{(policies[}\DecValTok{3}\OperatorTok{:}\DecValTok{8}\NormalTok{])}
\CommentTok{#>            State.Rate  Height  Weight     BMI     Age    Rate}
\CommentTok{#> State.Rate    1.00000 -0.0165 0.00923  0.0192  0.1123  0.2269}
\CommentTok{#> Height       -0.01652  1.0000 0.23809 -0.3170 -0.1648 -0.1286}
\CommentTok{#> Weight        0.00923  0.2381 1.00000  0.8396  0.0117  0.0609}
\CommentTok{#> BMI           0.01924 -0.3170 0.83963  1.0000  0.1023  0.1405}
\CommentTok{#> Age           0.11235 -0.1648 0.01168  0.1023  1.0000  0.7801}
\CommentTok{#> Rate          0.22685 -0.1286 0.06094  0.1405  0.7801  1.0000}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{cor}\NormalTok{(}
  \DataTypeTok{x =}\NormalTok{ policies}\OperatorTok{$}\NormalTok{Age, }
  \DataTypeTok{y =}\NormalTok{ policies}\OperatorTok{$}\NormalTok{Rate)}
\CommentTok{#> [1] 0.78}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(}
  \DataTypeTok{x =}\NormalTok{ policies}\OperatorTok{$}\NormalTok{Age, }
  \DataTypeTok{y =}\NormalTok{ policies}\OperatorTok{$}\NormalTok{Rate)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{regression_141.2-3b-rates_dataset_files/figure-latex/unnamed-chunk-9-1} \end{center}

\hypertarget{split-the-data-into-test-and-training-sets}{%
\section{Split the Data into Test and Training Sets}\label{split-the-data-into-test-and-training-sets}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{42}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(caret)}
\CommentTok{#> Loading required package: lattice}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'lattice'}
\CommentTok{#> The following object is masked from 'package:corrgram':}
\CommentTok{#> }
\CommentTok{#>     panel.fill}
\CommentTok{#> Loading required package: ggplot2}

\NormalTok{indexes <-}\StringTok{ }\KeywordTok{createDataPartition}\NormalTok{(}
  \DataTypeTok{y =}\NormalTok{ policies}\OperatorTok{$}\NormalTok{Rate,}
  \DataTypeTok{p =} \FloatTok{0.80}\NormalTok{,}
  \DataTypeTok{list =} \OtherTok{FALSE}\NormalTok{)}

\NormalTok{train <-}\StringTok{ }\NormalTok{policies[indexes, ]}
\NormalTok{test <-}\StringTok{ }\NormalTok{policies[}\OperatorTok{-}\NormalTok{indexes, ]}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{print}\NormalTok{(}\KeywordTok{nrow}\NormalTok{(train))}
\CommentTok{#> [1] 1555}
\KeywordTok{print}\NormalTok{(}\KeywordTok{nrow}\NormalTok{(test))}
\CommentTok{#> [1] 387}
\end{Highlighting}
\end{Shaded}

\hypertarget{predict-with-simple-linear-regression-1}{%
\section{Predict with Simple Linear Regression}\label{predict-with-simple-linear-regression-1}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{simpleModel <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(}
  \DataTypeTok{formula =}\NormalTok{ Rate }\OperatorTok{~}\StringTok{ }\NormalTok{Age,}
  \DataTypeTok{data =}\NormalTok{ train)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(}
  \DataTypeTok{x =}\NormalTok{ policies}\OperatorTok{$}\NormalTok{Age, }
  \DataTypeTok{y =}\NormalTok{ policies}\OperatorTok{$}\NormalTok{Rate)}
  
\KeywordTok{lines}\NormalTok{(}
  \DataTypeTok{x =}\NormalTok{ train}\OperatorTok{$}\NormalTok{Age,}
  \DataTypeTok{y =}\NormalTok{ simpleModel}\OperatorTok{$}\NormalTok{fitted, }
  \DataTypeTok{col =} \StringTok{"red"}\NormalTok{,}
  \DataTypeTok{lwd =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{regression_141.2-3b-rates_dataset_files/figure-latex/unnamed-chunk-14-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(simpleModel)}
\CommentTok{#> }
\CommentTok{#> Call:}
\CommentTok{#> lm(formula = Rate ~ Age, data = train)}
\CommentTok{#> }
\CommentTok{#> Residuals:}
\CommentTok{#>     Min      1Q  Median      3Q     Max }
\CommentTok{#> -0.1799 -0.0881 -0.0208  0.0617  0.6300 }
\CommentTok{#> }
\CommentTok{#> Coefficients:}
\CommentTok{#>              Estimate Std. Error t value Pr(>|t|)    }
\CommentTok{#> (Intercept) -0.265244   0.008780   -30.2   <2e-16 ***}
\CommentTok{#> Age          0.007928   0.000161    49.3   <2e-16 ***}
\CommentTok{#> ---}
\CommentTok{#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1}
\CommentTok{#> }
\CommentTok{#> Residual standard error: 0.123 on 1553 degrees of freedom}
\CommentTok{#> Multiple R-squared:  0.61,   Adjusted R-squared:  0.609 }
\CommentTok{#> F-statistic: 2.43e+03 on 1 and 1553 DF,  p-value: <2e-16}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{simplePredictions <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(}
  \DataTypeTok{object =}\NormalTok{ simpleModel,}
  \DataTypeTok{newdata =}\NormalTok{ test)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(}
  \DataTypeTok{x =}\NormalTok{ policies}\OperatorTok{$}\NormalTok{Age, }
  \DataTypeTok{y =}\NormalTok{ policies}\OperatorTok{$}\NormalTok{Rate)}


\KeywordTok{points}\NormalTok{(}
  \DataTypeTok{x =}\NormalTok{ test}\OperatorTok{$}\NormalTok{Age,}
  \DataTypeTok{y =}\NormalTok{ simplePredictions,}
  \DataTypeTok{col =} \StringTok{"blue"}\NormalTok{,}
  \DataTypeTok{pch =} \DecValTok{4}\NormalTok{,}
  \DataTypeTok{lwd =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{regression_141.2-3b-rates_dataset_files/figure-latex/unnamed-chunk-17-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{simpleRMSE <-}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{mean}\NormalTok{((test}\OperatorTok{$}\NormalTok{Rate }\OperatorTok{-}\StringTok{ }\NormalTok{simplePredictions)}\OperatorTok{^}\DecValTok{2}\NormalTok{))}
\KeywordTok{print}\NormalTok{(simpleRMSE)}
\CommentTok{#> [1] 0.119}
\end{Highlighting}
\end{Shaded}

\hypertarget{predict-with-multiple-linear-regression}{%
\section{Predict with Multiple Linear Regression}\label{predict-with-multiple-linear-regression}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{multipleModel <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(}
  \DataTypeTok{formula =}\NormalTok{ Rate }\OperatorTok{~}\StringTok{ }\NormalTok{Age }\OperatorTok{+}\StringTok{ }\NormalTok{Gender }\OperatorTok{+}\StringTok{ }\NormalTok{State.Rate }\OperatorTok{+}\StringTok{ }\NormalTok{BMI,}
  \DataTypeTok{data =}\NormalTok{ train)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(multipleModel)}
\CommentTok{#> }
\CommentTok{#> Call:}
\CommentTok{#> lm(formula = Rate ~ Age + Gender + State.Rate + BMI, data = train)}
\CommentTok{#> }
\CommentTok{#> Residuals:}
\CommentTok{#>     Min      1Q  Median      3Q     Max }
\CommentTok{#> -0.2255 -0.0865 -0.0292  0.0590  0.6053 }
\CommentTok{#> }
\CommentTok{#> Coefficients:}
\CommentTok{#>              Estimate Std. Error t value Pr(>|t|)    }
\CommentTok{#> (Intercept) -0.428141   0.018742  -22.84  < 2e-16 ***}
\CommentTok{#> Age          0.007703   0.000156   49.28  < 2e-16 ***}
\CommentTok{#> GenderMale   0.030350   0.006001    5.06  4.8e-07 ***}
\CommentTok{#> State.Rate   0.613139   0.068330    8.97  < 2e-16 ***}
\CommentTok{#> BMI          0.002634   0.000518    5.09  4.1e-07 ***}
\CommentTok{#> ---}
\CommentTok{#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1}
\CommentTok{#> }
\CommentTok{#> Residual standard error: 0.118 on 1550 degrees of freedom}
\CommentTok{#> Multiple R-squared:  0.64,   Adjusted R-squared:  0.639 }
\CommentTok{#> F-statistic:  688 on 4 and 1550 DF,  p-value: <2e-16}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{multiplePredictions <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(}
  \DataTypeTok{object =}\NormalTok{ multipleModel,}
  \DataTypeTok{newdata =}\NormalTok{ test)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(}
  \DataTypeTok{x =}\NormalTok{ policies}\OperatorTok{$}\NormalTok{Age, }
  \DataTypeTok{y =}\NormalTok{ policies}\OperatorTok{$}\NormalTok{Rate)}

\KeywordTok{points}\NormalTok{(}
  \DataTypeTok{x =}\NormalTok{ test}\OperatorTok{$}\NormalTok{Age,}
  \DataTypeTok{y =}\NormalTok{ multiplePredictions,}
  \DataTypeTok{col =} \StringTok{"blue"}\NormalTok{,}
  \DataTypeTok{pch =} \DecValTok{4}\NormalTok{,}
  \DataTypeTok{lwd =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{regression_141.2-3b-rates_dataset_files/figure-latex/unnamed-chunk-22-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{multipleRMSE <-}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{mean}\NormalTok{((test}\OperatorTok{$}\NormalTok{Rate }\OperatorTok{-}\StringTok{ }\NormalTok{multiplePredictions)}\OperatorTok{^}\DecValTok{2}\NormalTok{))}
\KeywordTok{print}\NormalTok{(multipleRMSE)}
\CommentTok{#> [1] 0.114}
\end{Highlighting}
\end{Shaded}

\hypertarget{predict-with-neural-network-regression-1}{%
\section{Predict with Neural Network Regression}\label{predict-with-neural-network-regression-1}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{normalize <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x) \{}
\NormalTok{  (x }\OperatorTok{-}\StringTok{ }\KeywordTok{min}\NormalTok{(x)) }\OperatorTok{/}\StringTok{ }\NormalTok{(}\KeywordTok{max}\NormalTok{(x) }\OperatorTok{-}\StringTok{ }\KeywordTok{min}\NormalTok{(x)) }\OperatorTok{-}\StringTok{ }\FloatTok{0.5}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{denormalize <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x, y) \{}
\NormalTok{  ((x }\OperatorTok{+}\StringTok{ }\FloatTok{0.5}\NormalTok{) }\OperatorTok{*}\StringTok{ }\NormalTok{(}\KeywordTok{max}\NormalTok{(y) }\OperatorTok{-}\StringTok{ }\KeywordTok{min}\NormalTok{(y))) }\OperatorTok{+}\StringTok{ }\KeywordTok{min}\NormalTok{(y)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{scaledPolicies <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}
  \DataTypeTok{Gender =}\NormalTok{ policies}\OperatorTok{$}\NormalTok{Gender,}
  \DataTypeTok{State.Rate =} \KeywordTok{normalize}\NormalTok{(policies}\OperatorTok{$}\NormalTok{State.Rate),}
  \DataTypeTok{BMI =} \KeywordTok{normalize}\NormalTok{(policies}\OperatorTok{$}\NormalTok{BMI),}
  \DataTypeTok{Age =} \KeywordTok{normalize}\NormalTok{(policies}\OperatorTok{$}\NormalTok{Age),}
  \DataTypeTok{Rate =} \KeywordTok{normalize}\NormalTok{(policies}\OperatorTok{$}\NormalTok{Rate))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{scaledTrain <-}\StringTok{ }\NormalTok{scaledPolicies[indexes, ]}
\NormalTok{scaledTest <-}\StringTok{ }\NormalTok{scaledPolicies[}\OperatorTok{-}\NormalTok{indexes, ]}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(nnet)}

\NormalTok{neuralRegressor <-}\StringTok{ }\KeywordTok{nnet}\NormalTok{(}
  \DataTypeTok{formula =}\NormalTok{ Rate }\OperatorTok{~}\StringTok{ }\NormalTok{.,}
  \DataTypeTok{data =}\NormalTok{ scaledTrain,}
  \DataTypeTok{linout =} \OtherTok{TRUE}\NormalTok{,}
  \DataTypeTok{size =} \DecValTok{5}\NormalTok{,}
  \DataTypeTok{decay =} \FloatTok{0.0001}\NormalTok{,}
  \DataTypeTok{maxit =} \DecValTok{1000}\NormalTok{)}
\CommentTok{#> # weights:  31}
\CommentTok{#> initial  value 548.090539 }
\CommentTok{#> iter  10 value 10.610284}
\CommentTok{#> iter  20 value 3.927378}
\CommentTok{#> iter  30 value 3.735266}
\CommentTok{#> iter  40 value 3.513899}
\CommentTok{#> iter  50 value 3.073390}
\CommentTok{#> iter  60 value 2.547202}
\CommentTok{#> iter  70 value 2.296126}
\CommentTok{#> iter  80 value 2.166120}
\CommentTok{#> iter  90 value 2.106996}
\CommentTok{#> iter 100 value 2.092654}
\CommentTok{#> iter 110 value 2.058596}
\CommentTok{#> iter 120 value 2.039404}
\CommentTok{#> iter 130 value 2.023721}
\CommentTok{#> iter 140 value 2.018781}
\CommentTok{#> iter 150 value 2.006931}
\CommentTok{#> iter 160 value 1.999122}
\CommentTok{#> iter 170 value 1.993920}
\CommentTok{#> iter 180 value 1.990678}
\CommentTok{#> iter 190 value 1.989269}
\CommentTok{#> iter 200 value 1.988846}
\CommentTok{#> iter 210 value 1.988042}
\CommentTok{#> iter 220 value 1.987739}
\CommentTok{#> iter 230 value 1.987678}
\CommentTok{#> iter 240 value 1.987598}
\CommentTok{#> iter 250 value 1.987574}
\CommentTok{#> iter 260 value 1.987549}
\CommentTok{#> iter 270 value 1.987536}
\CommentTok{#> iter 280 value 1.987529}
\CommentTok{#> final  value 1.987526 }
\CommentTok{#> converged}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{scaledPredictions <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(}
  \DataTypeTok{object =}\NormalTok{ neuralRegressor, }
  \DataTypeTok{newdata =}\NormalTok{ scaledTest)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{neuralPredictions <-}\StringTok{ }\KeywordTok{denormalize}\NormalTok{(}
  \DataTypeTok{x =}\NormalTok{ scaledPredictions, }
  \DataTypeTok{y =}\NormalTok{ policies}\OperatorTok{$}\NormalTok{Rate)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(}
  \DataTypeTok{x =}\NormalTok{ train}\OperatorTok{$}\NormalTok{Age, }
  \DataTypeTok{y =}\NormalTok{ train}\OperatorTok{$}\NormalTok{Rate)}

\KeywordTok{points}\NormalTok{(}
  \DataTypeTok{x =}\NormalTok{ test}\OperatorTok{$}\NormalTok{Age,}
  \DataTypeTok{y =}\NormalTok{ neuralPredictions,}
  \DataTypeTok{col =} \StringTok{"blue"}\NormalTok{,}
  \DataTypeTok{pch =} \DecValTok{4}\NormalTok{,}
  \DataTypeTok{lwd =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{regression_141.2-3b-rates_dataset_files/figure-latex/unnamed-chunk-31-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(NeuralNetTools)}

\KeywordTok{plotnet}\NormalTok{(neuralRegressor)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{regression_141.2-3b-rates_dataset_files/figure-latex/unnamed-chunk-32-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{neuralRMSE <-}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{mean}\NormalTok{((test}\OperatorTok{$}\NormalTok{Rate }\OperatorTok{-}\StringTok{ }\NormalTok{neuralPredictions)}\OperatorTok{^}\DecValTok{2}\NormalTok{))}
\KeywordTok{print}\NormalTok{(neuralRMSE)}
\CommentTok{#> [1] 0.0368}
\end{Highlighting}
\end{Shaded}

\hypertarget{evaluate-the-regression-models}{%
\section{Evaluate the Regression Models}\label{evaluate-the-regression-models}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{print}\NormalTok{(simpleRMSE)}
\CommentTok{#> [1] 0.119}
\KeywordTok{print}\NormalTok{(multipleRMSE)}
\CommentTok{#> [1] 0.114}
\KeywordTok{print}\NormalTok{(neuralRMSE)}
\CommentTok{#> [1] 0.0368}
\end{Highlighting}
\end{Shaded}

\hypertarget{regression-boston-nnet}{%
\chapter{Regression Boston nnet}\label{regression-boston-nnet}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{###}
\CommentTok{### prepare data}
\CommentTok{###}
\KeywordTok{library}\NormalTok{(mlbench)}
\KeywordTok{data}\NormalTok{(BostonHousing)}
 
\CommentTok{# inspect the range which is 1-50}
\KeywordTok{summary}\NormalTok{(BostonHousing}\OperatorTok{$}\NormalTok{medv)}
\CommentTok{#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. }
\CommentTok{#>     5.0    17.0    21.2    22.5    25.0    50.0}
 
 
\CommentTok{##}
\CommentTok{## model linear regression}
\CommentTok{##}
 
\NormalTok{lm.fit <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(medv }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data=}\NormalTok{BostonHousing)}
 
\NormalTok{lm.predict <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(lm.fit)}
 
\CommentTok{# mean squared error: 21.89483}
\KeywordTok{mean}\NormalTok{((lm.predict }\OperatorTok{-}\StringTok{ }\NormalTok{BostonHousing}\OperatorTok{$}\NormalTok{medv)}\OperatorTok{^}\DecValTok{2}\NormalTok{) }
\CommentTok{#> [1] 21.9}
 
\KeywordTok{plot}\NormalTok{(BostonHousing}\OperatorTok{$}\NormalTok{medv, lm.predict,}
    \DataTypeTok{main=}\StringTok{"Linear regression predictions vs actual"}\NormalTok{,}
    \DataTypeTok{xlab=}\StringTok{"Actual"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{regression_921-nn_lm_vs_nn_files/figure-latex/unnamed-chunk-2-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{##}
\CommentTok{## model neural network}
\CommentTok{##}
\KeywordTok{require}\NormalTok{(nnet)}
\CommentTok{#> Loading required package: nnet}
 
\CommentTok{# scale inputs: divide by 50 to get 0-1 range}
\NormalTok{nnet.fit <-}\StringTok{ }\KeywordTok{nnet}\NormalTok{(medv}\OperatorTok{/}\DecValTok{50} \OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data=}\NormalTok{BostonHousing, }\DataTypeTok{size=}\DecValTok{2}\NormalTok{) }
\CommentTok{#> # weights:  31}
\CommentTok{#> initial  value 17.039194 }
\CommentTok{#> iter  10 value 13.754559}
\CommentTok{#> iter  20 value 13.537235}
\CommentTok{#> iter  30 value 13.537183}
\CommentTok{#> iter  40 value 13.530522}
\CommentTok{#> final  value 13.529736 }
\CommentTok{#> converged}
 
\CommentTok{# multiply 50 to restore original scale}
\NormalTok{nnet.predict <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(nnet.fit)}\OperatorTok{*}\DecValTok{50} 
 
\CommentTok{# mean squared error: 16.40581}
\KeywordTok{mean}\NormalTok{((nnet.predict }\OperatorTok{-}\StringTok{ }\NormalTok{BostonHousing}\OperatorTok{$}\NormalTok{medv)}\OperatorTok{^}\DecValTok{2}\NormalTok{) }
\CommentTok{#> [1] 66.8}
 
\KeywordTok{plot}\NormalTok{(BostonHousing}\OperatorTok{$}\NormalTok{medv, nnet.predict,}
    \DataTypeTok{main=}\StringTok{"Neural network predictions vs actual"}\NormalTok{,}
    \DataTypeTok{xlab=}\StringTok{"Actual"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{regression_921-nn_lm_vs_nn_files/figure-latex/unnamed-chunk-3-1} \end{center}

\hypertarget{neural-network}{%
\section{Neural Network}\label{neural-network}}

Now, let's use the function train() from the package caret to optimize the neural network hyperparameters decay and size, Also, caret performs resampling to give a better estimate of the error. In this case we scale linear regression by the same value, so the error statistics are directly comparable.

\begin{Shaded}
\begin{Highlighting}[]
 \KeywordTok{library}\NormalTok{(mlbench)}
 \KeywordTok{data}\NormalTok{(BostonHousing)}
 
\KeywordTok{require}\NormalTok{(caret)}
\CommentTok{#> Loading required package: caret}
\CommentTok{#> Loading required package: lattice}
\CommentTok{#> Loading required package: ggplot2}
\CommentTok{#> Registered S3 methods overwritten by 'ggplot2':}
\CommentTok{#>   method         from }
\CommentTok{#>   [.quosures     rlang}
\CommentTok{#>   c.quosures     rlang}
\CommentTok{#>   print.quosures rlang}
 
\NormalTok{mygrid <-}\StringTok{ }\KeywordTok{expand.grid}\NormalTok{(}\DataTypeTok{.decay=}\KeywordTok{c}\NormalTok{(}\FloatTok{0.5}\NormalTok{, }\FloatTok{0.1}\NormalTok{), }\DataTypeTok{.size=}\KeywordTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{,}\DecValTok{5}\NormalTok{,}\DecValTok{6}\NormalTok{))}
\NormalTok{nnetfit <-}\StringTok{ }\KeywordTok{train}\NormalTok{(medv}\OperatorTok{/}\DecValTok{50} \OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data=}\NormalTok{BostonHousing, }\DataTypeTok{method=}\StringTok{"nnet"}\NormalTok{, }\DataTypeTok{maxit=}\DecValTok{1000}\NormalTok{, }\DataTypeTok{tuneGrid=}\NormalTok{mygrid, }\DataTypeTok{trace=}\NormalTok{F) }
\KeywordTok{print}\NormalTok{(nnetfit)}
\CommentTok{#> Neural Network }
\CommentTok{#> }
\CommentTok{#> 506 samples}
\CommentTok{#>  13 predictor}
\CommentTok{#> }
\CommentTok{#> No pre-processing}
\CommentTok{#> Resampling: Bootstrapped (25 reps) }
\CommentTok{#> Summary of sample sizes: 506, 506, 506, 506, 506, 506, ... }
\CommentTok{#> Resampling results across tuning parameters:}
\CommentTok{#> }
\CommentTok{#>   decay  size  RMSE    Rsquared  MAE   }
\CommentTok{#>   0.1    4     0.0835  0.787     0.0571}
\CommentTok{#>   0.1    5     0.0822  0.794     0.0565}
\CommentTok{#>   0.1    6     0.0799  0.806     0.0544}
\CommentTok{#>   0.5    4     0.0908  0.757     0.0626}
\CommentTok{#>   0.5    5     0.0900  0.761     0.0624}
\CommentTok{#>   0.5    6     0.0895  0.763     0.0622}
\CommentTok{#> }
\CommentTok{#> RMSE was used to select the optimal model using the smallest value.}
\CommentTok{#> The final values used for the model were size = 6 and decay = 0.1.}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
506 samples
 13 predictors
 
No pre-processing
Resampling: Bootstrap (25 reps) 
 
Summary of sample sizes: 506, 506, 506, 506, 506, 506, ... 
 
Resampling results across tuning parameters:
 
  size  decay  RMSE    Rsquared  RMSE SD  Rsquared SD
  4     0.1    0.0852  0.785     0.00863  0.0406     
  4     0.5    0.0923  0.753     0.00891  0.0436     
  5     0.1    0.0836  0.792     0.00829  0.0396     
  5     0.5    0.0899  0.765     0.00858  0.0399     
  6     0.1    0.0835  0.793     0.00804  0.0318     
  6     0.5    0.0895  0.768     0.00789  0.0344   
\end{verbatim}

\hypertarget{linear-regression}{%
\section{Linear Regression}\label{linear-regression}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ lmfit <-}\StringTok{ }\KeywordTok{train}\NormalTok{(medv}\OperatorTok{/}\DecValTok{50} \OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data=}\NormalTok{BostonHousing, }\DataTypeTok{method=}\StringTok{"lm"}\NormalTok{) }
 \KeywordTok{print}\NormalTok{(lmfit)}
\CommentTok{#> Linear Regression }
\CommentTok{#> }
\CommentTok{#> 506 samples}
\CommentTok{#>  13 predictor}
\CommentTok{#> }
\CommentTok{#> No pre-processing}
\CommentTok{#> Resampling: Bootstrapped (25 reps) }
\CommentTok{#> Summary of sample sizes: 506, 506, 506, 506, 506, 506, ... }
\CommentTok{#> Resampling results:}
\CommentTok{#> }
\CommentTok{#>   RMSE    Rsquared  MAE   }
\CommentTok{#>   0.0988  0.726     0.0692}
\CommentTok{#> }
\CommentTok{#> Tuning parameter 'intercept' was held constant at a value of TRUE}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
506 samples
 13 predictors
 
No pre-processing
Resampling: Bootstrap (25 reps) 
 
Summary of sample sizes: 506, 506, 506, 506, 506, 506, ... 
 
Resampling results
 
  RMSE    Rsquared  RMSE SD  Rsquared SD
  0.0994  0.703     0.00741  0.0389    
\end{verbatim}

A tuned neural network has a RMSE of 0.0835 compared to linear regression's RMSE of 0.0994.

\hypertarget{comparing-multiple-vs.neural-network-regression}{%
\chapter{Comparing Multiple vs.~Neural Network Regression}\label{comparing-multiple-vs.neural-network-regression}}

\hypertarget{introduction-27}{%
\section{Introduction}\label{introduction-27}}

Source: \url{http://beyondvalence.blogspot.com/2014/04/r-comparing-multiple-and-neural-network.html}

Here we will compare and evaluate the results from multiple regression and a neural network on the diamonds data set from the \texttt{ggplot2} package in R. Consisting of 53,940 observations with 10 variables, diamonds contains data on the carat, cut, color, clarity, price, and diamond dimensions. These variables have a particular effect on price, and we would like to see if they can predict the price of various diamonds.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(ggplot2)}
\CommentTok{#> Registered S3 methods overwritten by 'ggplot2':}
\CommentTok{#>   method         from }
\CommentTok{#>   [.quosures     rlang}
\CommentTok{#>   c.quosures     rlang}
\CommentTok{#>   print.quosures rlang}
\KeywordTok{library}\NormalTok{(RSNNS)}
\CommentTok{#> Loading required package: Rcpp}
\KeywordTok{library}\NormalTok{(MASS)}
\KeywordTok{library}\NormalTok{(caret)}
\CommentTok{#> Loading required package: lattice}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'caret'}
\CommentTok{#> The following objects are masked from 'package:RSNNS':}
\CommentTok{#> }
\CommentTok{#>     confusionMatrix, train}
\CommentTok{# library(diamonds)}

\KeywordTok{head}\NormalTok{(diamonds)}
\CommentTok{#> # A tibble: 6 x 10}
\CommentTok{#>   carat cut       color clarity depth table price     x     y     z}
\CommentTok{#>   <dbl> <ord>     <ord> <ord>   <dbl> <dbl> <int> <dbl> <dbl> <dbl>}
\CommentTok{#> 1 0.23  Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43}
\CommentTok{#> 2 0.21  Premium   E     SI1      59.8    61   326  3.89  3.84  2.31}
\CommentTok{#> 3 0.23  Good      E     VS1      56.9    65   327  4.05  4.07  2.31}
\CommentTok{#> 4 0.290 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63}
\CommentTok{#> 5 0.31  Good      J     SI2      63.3    58   335  4.34  4.35  2.75}
\CommentTok{#> 6 0.24  Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{glimpse}\NormalTok{(diamonds)}
\CommentTok{#> Observations: 53,940}
\CommentTok{#> Variables: 10}
\CommentTok{#> $ carat   <dbl> 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.26, 0.22, ...}
\CommentTok{#> $ cut     <ord> Ideal, Premium, Good, Premium, Good, Very Good, Very G...}
\CommentTok{#> $ color   <ord> E, E, E, I, J, J, I, H, E, H, J, J, F, J, E, E, I, J, ...}
\CommentTok{#> $ clarity <ord> SI2, SI1, VS1, VS2, SI2, VVS2, VVS1, SI1, VS2, VS1, SI...}
\CommentTok{#> $ depth   <dbl> 61.5, 59.8, 56.9, 62.4, 63.3, 62.8, 62.3, 61.9, 65.1, ...}
\CommentTok{#> $ table   <dbl> 55, 61, 65, 58, 58, 57, 57, 55, 61, 61, 55, 56, 61, 54...}
\CommentTok{#> $ price   <int> 326, 326, 327, 334, 335, 336, 336, 337, 337, 338, 339,...}
\CommentTok{#> $ x       <dbl> 3.95, 3.89, 4.05, 4.20, 4.34, 3.94, 3.95, 4.07, 3.87, ...}
\CommentTok{#> $ y       <dbl> 3.98, 3.84, 4.07, 4.23, 4.35, 3.96, 3.98, 4.11, 3.78, ...}
\CommentTok{#> $ z       <dbl> 2.43, 2.31, 2.31, 2.63, 2.75, 2.48, 2.47, 2.53, 2.49, ...}
\end{Highlighting}
\end{Shaded}

The cut, color, and clarity variables are factors, and must be treated as dummy variables in multiple and neural network regressions. Let us start with multiple regression.

\hypertarget{multiple-regression}{%
\section{Multiple Regression}\label{multiple-regression}}

First we ready a Multiple Regression by sampling the rows to randomize the observations, and then create a sample index of 0's and 1's to separate the training and test sets. Note that the depth and table columns (5, 6) are removed because they are linear combinations of the dimensions, x, y, and z. See that the observations in the training and test sets approximate 70\% and 30\% of the total observations, from which we sampled and set the probabilities.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1234567}\NormalTok{)}
\NormalTok{diamonds <-}\StringTok{ }\NormalTok{diamonds[}\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\KeywordTok{nrow}\NormalTok{(diamonds), }\KeywordTok{nrow}\NormalTok{(diamonds)),]}
\NormalTok{d.index =}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\DecValTok{0}\OperatorTok{:}\DecValTok{1}\NormalTok{, }\KeywordTok{nrow}\NormalTok{(diamonds), }\DataTypeTok{prob=}\KeywordTok{c}\NormalTok{(}\FloatTok{0.3}\NormalTok{, }\FloatTok{0.7}\NormalTok{), }\DataTypeTok{rep =} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{d.train <-}\StringTok{ }\NormalTok{diamonds[d.index}\OperatorTok{==}\DecValTok{1}\NormalTok{, }\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{5}\NormalTok{,}\OperatorTok{-}\DecValTok{6}\NormalTok{)]}
\NormalTok{d.test <-}\StringTok{ }\NormalTok{diamonds[d.index}\OperatorTok{==}\DecValTok{0}\NormalTok{, }\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{5}\NormalTok{,}\OperatorTok{-}\DecValTok{6}\NormalTok{)]}
\KeywordTok{dim}\NormalTok{(d.train)}
\CommentTok{#> [1] 37502     8}
\KeywordTok{dim}\NormalTok{(d.test)}
\CommentTok{#> [1] 16438     8}
\end{Highlighting}
\end{Shaded}

Now we move into the next stage with multiple regression via the \texttt{train()} function from the \texttt{caret} library, instead of the regular \texttt{lm()} function. We specify the predictors, the response variable (\texttt{price}), the ``lm'' method, and the cross validation resampling method.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\NormalTok{d.train[,}\OperatorTok{-}\DecValTok{5}\NormalTok{]}
\NormalTok{y <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(d.train[,}\DecValTok{5}\NormalTok{]}\OperatorTok{$}\NormalTok{price)}

\NormalTok{ds.lm <-}\StringTok{ }\NormalTok{caret}\OperatorTok{::}\KeywordTok{train}\NormalTok{(x, y, }\DataTypeTok{method =} \StringTok{"lm"}\NormalTok{,}
                      \DataTypeTok{trainControl =} \KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"cv"}\NormalTok{))}
\CommentTok{#> Warning: Setting row names on a tibble is deprecated.}
\CommentTok{#> Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :}
\CommentTok{#>  extra argument 'trainControl' will be disregarded}
\CommentTok{#> Warning: Setting row names on a tibble is deprecated.}
\CommentTok{#> Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :}
\CommentTok{#>  extra argument 'trainControl' will be disregarded}
\CommentTok{#> Warning: Setting row names on a tibble is deprecated.}
\CommentTok{#> Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :}
\CommentTok{#>  extra argument 'trainControl' will be disregarded}
\CommentTok{#> Warning: Setting row names on a tibble is deprecated.}
\CommentTok{#> Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :}
\CommentTok{#>  extra argument 'trainControl' will be disregarded}
\CommentTok{#> Warning: Setting row names on a tibble is deprecated.}
\CommentTok{#> Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :}
\CommentTok{#>  extra argument 'trainControl' will be disregarded}
\CommentTok{#> Warning: Setting row names on a tibble is deprecated.}
\CommentTok{#> Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :}
\CommentTok{#>  extra argument 'trainControl' will be disregarded}
\CommentTok{#> Warning: Setting row names on a tibble is deprecated.}
\CommentTok{#> Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :}
\CommentTok{#>  extra argument 'trainControl' will be disregarded}
\CommentTok{#> Warning: Setting row names on a tibble is deprecated.}
\CommentTok{#> Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :}
\CommentTok{#>  extra argument 'trainControl' will be disregarded}
\CommentTok{#> Warning: Setting row names on a tibble is deprecated.}
\CommentTok{#> Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :}
\CommentTok{#>  extra argument 'trainControl' will be disregarded}
\CommentTok{#> Warning: Setting row names on a tibble is deprecated.}
\CommentTok{#> Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :}
\CommentTok{#>  extra argument 'trainControl' will be disregarded}
\CommentTok{#> Warning: Setting row names on a tibble is deprecated.}
\CommentTok{#> Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :}
\CommentTok{#>  extra argument 'trainControl' will be disregarded}
\CommentTok{#> Warning: Setting row names on a tibble is deprecated.}
\CommentTok{#> Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :}
\CommentTok{#>  extra argument 'trainControl' will be disregarded}
\CommentTok{#> Warning: Setting row names on a tibble is deprecated.}
\CommentTok{#> Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :}
\CommentTok{#>  extra argument 'trainControl' will be disregarded}
\CommentTok{#> Warning: Setting row names on a tibble is deprecated.}
\CommentTok{#> Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :}
\CommentTok{#>  extra argument 'trainControl' will be disregarded}
\CommentTok{#> Warning: Setting row names on a tibble is deprecated.}
\CommentTok{#> Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :}
\CommentTok{#>  extra argument 'trainControl' will be disregarded}
\CommentTok{#> Warning: Setting row names on a tibble is deprecated.}
\CommentTok{#> Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :}
\CommentTok{#>  extra argument 'trainControl' will be disregarded}
\CommentTok{#> Warning: Setting row names on a tibble is deprecated.}
\CommentTok{#> Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :}
\CommentTok{#>  extra argument 'trainControl' will be disregarded}
\CommentTok{#> Warning: Setting row names on a tibble is deprecated.}
\CommentTok{#> Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :}
\CommentTok{#>  extra argument 'trainControl' will be disregarded}
\CommentTok{#> Warning: Setting row names on a tibble is deprecated.}
\CommentTok{#> Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :}
\CommentTok{#>  extra argument 'trainControl' will be disregarded}
\CommentTok{#> Warning: Setting row names on a tibble is deprecated.}
\CommentTok{#> Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :}
\CommentTok{#>  extra argument 'trainControl' will be disregarded}
\CommentTok{#> Warning: Setting row names on a tibble is deprecated.}
\CommentTok{#> Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :}
\CommentTok{#>  extra argument 'trainControl' will be disregarded}
\CommentTok{#> Warning: Setting row names on a tibble is deprecated.}
\CommentTok{#> Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :}
\CommentTok{#>  extra argument 'trainControl' will be disregarded}
\CommentTok{#> Warning: Setting row names on a tibble is deprecated.}
\CommentTok{#> Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :}
\CommentTok{#>  extra argument 'trainControl' will be disregarded}
\CommentTok{#> Warning: Setting row names on a tibble is deprecated.}
\CommentTok{#> Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :}
\CommentTok{#>  extra argument 'trainControl' will be disregarded}
\CommentTok{#> Warning: Setting row names on a tibble is deprecated.}
\CommentTok{#> Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :}
\CommentTok{#>  extra argument 'trainControl' will be disregarded}
\CommentTok{#> Warning: Setting row names on a tibble is deprecated.}
\CommentTok{#> Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :}
\CommentTok{#>  extra argument 'trainControl' will be disregarded}
\NormalTok{ds.lm                      }
\CommentTok{#> Linear Regression }
\CommentTok{#> }
\CommentTok{#> 37502 samples}
\CommentTok{#>     7 predictor}
\CommentTok{#> }
\CommentTok{#> No pre-processing}
\CommentTok{#> Resampling: Bootstrapped (25 reps) }
\CommentTok{#> Summary of sample sizes: 37502, 37502, 37502, 37502, 37502, 37502, ... }
\CommentTok{#> Resampling results:}
\CommentTok{#> }
\CommentTok{#>   RMSE  Rsquared  MAE}
\CommentTok{#>   1140  0.919     745}
\CommentTok{#> }
\CommentTok{#> Tuning parameter 'intercept' was held constant at a value of TRUE}
\end{Highlighting}
\end{Shaded}

When we call the train(ed) object, we can see the attributes of the training set, resampling, sample sizes, and the results. Note the root mean square error value of 1150. Will that be low enough to take down heavy weight TEAM: Neural Network? Below we visualize the training diamond prices and the predicted prices with \texttt{ggplot()}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(dplyr)}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'dplyr'}
\CommentTok{#> The following object is masked from 'package:MASS':}
\CommentTok{#> }
\CommentTok{#>     select}
\CommentTok{#> The following objects are masked from 'package:stats':}
\CommentTok{#> }
\CommentTok{#>     filter, lag}
\CommentTok{#> The following objects are masked from 'package:base':}
\CommentTok{#> }
\CommentTok{#>     intersect, setdiff, setequal, union}

\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{obs =}\NormalTok{ y, }\DataTypeTok{pred =}\NormalTok{ ds.lm}\OperatorTok{$}\NormalTok{finalModel}\OperatorTok{$}\NormalTok{fitted.values) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ obs, }\DataTypeTok{y =}\NormalTok{ pred)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{alpha=}\FloatTok{0.1}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_abline}\NormalTok{(}\DataTypeTok{color=}\StringTok{"blue"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{title=}\StringTok{"Diamond train price"}\NormalTok{, }\DataTypeTok{x=}\StringTok{"observed"}\NormalTok{, }\DataTypeTok{y=}\StringTok{"predicted"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{regression_950-diamonds_nn_files/figure-latex/unnamed-chunk-7-1} \end{center}

We see from the axis, the predicted prices have some high values compared to the actual prices. Also, there are predicted prices below 0, which cannot be possible in the observed, which will set TEAM: Multiple Regression back a few points.

Next we use \texttt{ggplot()} again to visualize the predicted and observed diamond prices from the test data, which did not train the linear regression model.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# predict on test set}
\NormalTok{ds.lm.p <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(ds.lm, d.test[,}\OperatorTok{-}\DecValTok{5}\NormalTok{], }\DataTypeTok{type=}\StringTok{"raw"}\NormalTok{)}

\CommentTok{# compare observed vs predicted prices in the test set}
\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{obs =}\NormalTok{ d.test[,}\DecValTok{5}\NormalTok{]}\OperatorTok{$}\NormalTok{price, }\DataTypeTok{pred =}\NormalTok{ ds.lm.p) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ obs, }\DataTypeTok{y =}\NormalTok{ pred)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{alpha=}\FloatTok{0.1}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_abline}\NormalTok{(}\DataTypeTok{color=}\StringTok{"blue"}\NormalTok{)}\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\StringTok{"Diamond Test Price"}\NormalTok{, }\DataTypeTok{x=}\StringTok{"observed"}\NormalTok{, }\DataTypeTok{y=}\StringTok{"predicted"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{regression_950-diamonds_nn_files/figure-latex/unnamed-chunk-9-1} \end{center}

Similar to the training prices plot, we see here in the test prices that the model over predicts larger values and also predicted negative price values. In order for the Multiple Regression to win, the Neural Network has to have more wild prediction values.

Lastly, we calculate the root mean square error, by taking the mean of the squared difference between the predicted and observed diamond prices. The resulting RMSE is 1110.843, similar to the RMSE of the training set.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ds.lm.mse <-}\StringTok{ }\NormalTok{(}\DecValTok{1} \OperatorTok{/}\StringTok{ }\KeywordTok{nrow}\NormalTok{(d.test)) }\OperatorTok{*}\StringTok{ }\KeywordTok{sum}\NormalTok{((ds.lm.p }\OperatorTok{-}\StringTok{ }\NormalTok{d.test[,}\DecValTok{5}\NormalTok{])}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\NormalTok{lm.rmse <-}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(ds.lm.mse)}
\NormalTok{lm.rmse}
\CommentTok{#> [1] 1168}
\end{Highlighting}
\end{Shaded}

Below is a detailed output of the model summary, with the coefficients and residuals. Observe how carat is the best predictor, with the highest t value at 191.7, with every increase in 1 carat holding all other variables equal, results in a 10,873 dollar increase in value. As we look at the factor variables, we do not see a reliable increase in coefficients with increases in level value.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(ds.lm)}
\CommentTok{#> }
\CommentTok{#> Call:}
\CommentTok{#> lm(formula = .outcome ~ ., data = dat, trainControl = ..1)}
\CommentTok{#> }
\CommentTok{#> Residuals:}
\CommentTok{#>    Min     1Q Median     3Q    Max }
\CommentTok{#> -21090   -598   -183    378  10778 }
\CommentTok{#> }
\CommentTok{#> Coefficients:}
\CommentTok{#>             Estimate Std. Error t value Pr(>|t|)    }
\CommentTok{#> (Intercept)     3.68      94.63    0.04   0.9690    }
\CommentTok{#> carat       11142.68      57.43  194.02  < 2e-16 ***}
\CommentTok{#> cut.L         767.70      24.31   31.58  < 2e-16 ***}
\CommentTok{#> cut.Q        -336.63      21.41  -15.72  < 2e-16 ***}
\CommentTok{#> cut.C         157.31      18.81    8.36  < 2e-16 ***}
\CommentTok{#> cut^4         -22.81      14.78   -1.54   0.1228    }
\CommentTok{#> color.L     -1950.28      20.66  -94.42  < 2e-16 ***}
\CommentTok{#> color.Q      -665.60      18.82  -35.37  < 2e-16 ***}
\CommentTok{#> color.C      -147.16      17.61   -8.36  < 2e-16 ***}
\CommentTok{#> color^4        44.64      16.20    2.76   0.0059 ** }
\CommentTok{#> color^5       -91.21      15.32   -5.95  2.7e-09 ***}
\CommentTok{#> color^6       -54.74      13.92   -3.93  8.5e-05 ***}
\CommentTok{#> clarity.L    4115.45      36.68  112.19  < 2e-16 ***}
\CommentTok{#> clarity.Q   -1959.71      34.33  -57.09  < 2e-16 ***}
\CommentTok{#> clarity.C     990.60      29.29   33.83  < 2e-16 ***}
\CommentTok{#> clarity^4    -370.82      23.30  -15.92  < 2e-16 ***}
\CommentTok{#> clarity^5     240.60      18.91   12.72  < 2e-16 ***}
\CommentTok{#> clarity^6      -7.99      16.37   -0.49   0.6253    }
\CommentTok{#> clarity^7      80.62      14.48    5.57  2.6e-08 ***}
\CommentTok{#> x           -1400.26      95.70  -14.63  < 2e-16 ***}
\CommentTok{#> y             545.42      94.57    5.77  8.1e-09 ***}
\CommentTok{#> z            -190.86      31.20   -6.12  9.6e-10 ***}
\CommentTok{#> ---}
\CommentTok{#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1}
\CommentTok{#> }
\CommentTok{#> Residual standard error: 1130 on 37480 degrees of freedom}
\CommentTok{#> Multiple R-squared:  0.92,   Adjusted R-squared:  0.92 }
\CommentTok{#> F-statistic: 2.05e+04 on 21 and 37480 DF,  p-value: <2e-16}
\end{Highlighting}
\end{Shaded}

Now we move on to the neural network regression.

\hypertarget{neural-network-1}{%
\section{Neural Network}\label{neural-network-1}}

Because neural networks operate in terms of 0 to 1, or -1 to 1, we must first normalize the price variable to 0 to 1, making the lowest value 0 and the highest value 1. We accomplished this using the \texttt{normalizeData()} function. Save the price output in order to revert the normalization after training the data. Also, we take the factor variables and turn them into numeric labels using toNumericClassLabels(). Below we see the normalized prices before they are split into a training and test set with \texttt{splitForTrainingAndTest()} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{diamonds[,}\DecValTok{3}\NormalTok{] <-}\StringTok{ }\KeywordTok{toNumericClassLabels}\NormalTok{(diamonds[,}\DecValTok{3}\NormalTok{]}\OperatorTok{$}\NormalTok{color)}
\NormalTok{diamonds[,}\DecValTok{4}\NormalTok{] <-}\StringTok{ }\KeywordTok{toNumericClassLabels}\NormalTok{(diamonds[,}\DecValTok{4}\NormalTok{]}\OperatorTok{$}\NormalTok{clarity)}
\NormalTok{prices <-}\StringTok{ }\KeywordTok{normalizeData}\NormalTok{(diamonds[,}\DecValTok{7}\NormalTok{], }\DataTypeTok{type=}\StringTok{"0_1"}\NormalTok{)}
\KeywordTok{head}\NormalTok{(prices)}
\CommentTok{#>        [,1]}
\CommentTok{#> [1,] 0.0841}
\CommentTok{#> [2,] 0.1491}
\CommentTok{#> [3,] 0.0237}
\CommentTok{#> [4,] 0.3247}
\CommentTok{#> [5,] 0.0280}
\CommentTok{#> [6,] 0.0252}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dsplit <-}\StringTok{ }\KeywordTok{splitForTrainingAndTest}\NormalTok{(diamonds[, }\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{2}\NormalTok{,}\OperatorTok{-}\DecValTok{5}\NormalTok{,}\OperatorTok{-}\DecValTok{6}\NormalTok{,}\OperatorTok{-}\DecValTok{7}\NormalTok{,}\OperatorTok{-}\DecValTok{9}\NormalTok{,}\OperatorTok{-}\DecValTok{10}\NormalTok{)], prices, }\DataTypeTok{ratio=}\FloatTok{0.3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Now the Neural Network are ready for the multi-layer perceptron (MLP) regression. We define the training inputs (predictor variables) and targets (prices), the size of the layer (5), the incremented learning parameter (0.1), the max iterations (100 epochs), and also the test input/targets.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# mlp model}
\NormalTok{d.nn <-}\StringTok{ }\KeywordTok{mlp}\NormalTok{(dsplit}\OperatorTok{$}\NormalTok{inputsTrain,}
\NormalTok{            dsplit}\OperatorTok{$}\NormalTok{targetsTrain,}
            \DataTypeTok{size =} \KeywordTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{), }\DataTypeTok{learnFuncParams =} \KeywordTok{c}\NormalTok{(}\FloatTok{0.1}\NormalTok{), }\DataTypeTok{maxit=}\DecValTok{100}\NormalTok{,}
            \DataTypeTok{inputsTest =}\NormalTok{ dsplit}\OperatorTok{$}\NormalTok{inputsTest,}
            \DataTypeTok{targetsTest =}\NormalTok{ dsplit}\OperatorTok{$}\NormalTok{targetsTest,}
            \DataTypeTok{metric =} \StringTok{"RMSE"}\NormalTok{,}
            \DataTypeTok{linout =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

If you spectators have dealt with \texttt{mlp()} before, you know the summary output can be quite lenghty, so it is omitted (we dislike commercials too). We move to the visual description of the MLP model with the iterative sum of square error for the training and test sets. Additionally, we plot the regression error (predicted vs observed) for the training and test prices.

Time for the Neural Network so show off its statistical muscles! First up, we have the iterative sum of square error for each epoch, noting that we specified a maximum of 100 in the MLP model. We see an immediate drop in the SSE with the first few iterations, with the SSE leveling out around 50. The test SSE, in red, fluctuations just above 50 as well. Since the SSE began to plateau, the model fit well but not too well, since we want to avoid over fitting the model. So 100 iterations was a good choice.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# SSE error}
\KeywordTok{plotIterativeError}\NormalTok{(d.nn, }\DataTypeTok{main =} \StringTok{"Diamonds RSNNS-SSE"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{regression_950-diamonds_nn_files/figure-latex/unnamed-chunk-15-1} \end{center}

Second, we observe the regression plot with the fitted (predicted) and target (observed) prices from the training set. The prices fit reasonably well, and we see the red model regression line close to the black (y=x) optimal line. Note that some middle prices were over predicted by the model, and there were no negative prices, unlike the linear regression model.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# regression  errors}
\KeywordTok{plotRegressionError}\NormalTok{(dsplit}\OperatorTok{$}\NormalTok{targetsTrain, d.nn}\OperatorTok{$}\NormalTok{fitted.values,}
                    \DataTypeTok{main =} \StringTok{"Diamonds Training Fit"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{regression_950-diamonds_nn_files/figure-latex/unnamed-chunk-16-1} \end{center}

Third, we look at the predicted and observed prices from the test set. Again the red regression line approximates the optimal black line, and more price values were over predicted by the model. Again, there are no negative predicted prices, a good sign.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plotRegressionError}\NormalTok{(dsplit}\OperatorTok{$}\NormalTok{targetsTest, d.nn}\OperatorTok{$}\NormalTok{fittedTestValues,}
                    \DataTypeTok{main =} \StringTok{"Diamonds Test Fit"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{regression_950-diamonds_nn_files/figure-latex/unnamed-chunk-17-1} \end{center}

Now we calculate the RMSE for the training set, which we get 692.5155. This looks promising for the Neural Network!

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# train set}
\NormalTok{train.pred <-}\StringTok{ }\KeywordTok{denormalizeData}\NormalTok{(d.nn}\OperatorTok{$}\NormalTok{fitted.values,}
                              \KeywordTok{getNormParameters}\NormalTok{(prices))}

\NormalTok{train.obs <-}\StringTok{ }\KeywordTok{denormalizeData}\NormalTok{(dsplit}\OperatorTok{$}\NormalTok{targetsTrain,}
                             \KeywordTok{getNormParameters}\NormalTok{(prices))}

\NormalTok{train.mse <-}\StringTok{ }\NormalTok{(}\DecValTok{1} \OperatorTok{/}\StringTok{ }\KeywordTok{nrow}\NormalTok{(dsplit}\OperatorTok{$}\NormalTok{inputsTrain)) }\OperatorTok{*}\StringTok{ }\KeywordTok{sum}\NormalTok{((train.pred }\OperatorTok{-}\StringTok{ }\NormalTok{train.obs)}\OperatorTok{^}\DecValTok{2}\NormalTok{)}

\NormalTok{rsnns.train.rmse <-}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(train.mse)}
\NormalTok{rsnns.train.rmse}
\CommentTok{#> [1] 739}
\end{Highlighting}
\end{Shaded}

Naturally we want to calculate the RMSE for the test set, but note that in the real world, we would not have the luxury of knowing the real test values. We arrive at 679.5265.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# test set}
\NormalTok{test.pred <-}\StringTok{ }\KeywordTok{denormalizeData}\NormalTok{(d.nn}\OperatorTok{$}\NormalTok{fittedTestValues,}
                             \KeywordTok{getNormParameters}\NormalTok{(prices))}

\NormalTok{test.obs <-}\StringTok{ }\KeywordTok{denormalizeData}\NormalTok{(dsplit}\OperatorTok{$}\NormalTok{targetsTest,}
                            \KeywordTok{getNormParameters}\NormalTok{(prices))}

\NormalTok{test.mse <-}\StringTok{ }\NormalTok{(}\DecValTok{1} \OperatorTok{/}\StringTok{ }\KeywordTok{nrow}\NormalTok{(dsplit}\OperatorTok{$}\NormalTok{inputsTest)) }\OperatorTok{*}\StringTok{ }\KeywordTok{sum}\NormalTok{((test.pred }\OperatorTok{-}\StringTok{ }\NormalTok{test.obs)}\OperatorTok{^}\DecValTok{2}\NormalTok{)}

\NormalTok{rsnns.test.rmse <-}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(test.mse)}
\NormalTok{rsnns.test.rmse}
\CommentTok{#> [1] 751}
\end{Highlighting}
\end{Shaded}

Which model was better in predicting the diamond price? The linear regression model with 10 fold cross validation, or the multi-layer perceptron model with 5 nodes run to 100 iterations? Who won the rumble?

RUMBLE RESULTS

From calculating the two RMSE's from the training and test sets for the two TEAMS, we wrap them in a list. We named the TEAM: Multiple Regression as linear, and the TEAM: Neural Network regression as neural.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# aggregate all rmse}
\NormalTok{d.rmse <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}\DataTypeTok{linear.train =}\NormalTok{ ds.lm}\OperatorTok{$}\NormalTok{results}\OperatorTok{$}\NormalTok{RMSE,}
               \DataTypeTok{linear.test =}\NormalTok{ lm.rmse,}
               \DataTypeTok{neural.train =}\NormalTok{ rsnns.train.rmse,}
               \DataTypeTok{neural.test =}\NormalTok{ rsnns.test.rmse)}
\end{Highlighting}
\end{Shaded}

Below we can evaluate the models from their RMSE values.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{d.rmse}
\CommentTok{#> $linear.train}
\CommentTok{#> [1] 1140}
\CommentTok{#> }
\CommentTok{#> $linear.test}
\CommentTok{#> [1] 1168}
\CommentTok{#> }
\CommentTok{#> $neural.train}
\CommentTok{#> [1] 739}
\CommentTok{#> }
\CommentTok{#> $neural.test}
\CommentTok{#> [1] 751}
\end{Highlighting}
\end{Shaded}

Looking at the training RMSE first, we see a clear difference as the linear RMSE was 66\% larger than the neural RMSE, at 1,152.393 versus 692.5155. Peeking into the test sets, we have a similar 63\% larger linear RMSE than the neural RMSE, with 1,110.843 and 679.5265 respectively. TEAM: Neural Network begins to gain the upper hand in the evaluation round.

One important difference between the two models was the range of the predictions. Recall from both training and test plots that the linear regression model predicted negative price values, whereas the MLP model predicted only positive prices. This is a devastating blow to the Multiple Regression. Also, the over-prediction of prices existed in both models, however the linear regression model over predicted those middle values higher the anticipated maximum price values.

Sometimes the simple models are optimal, and other times more complicated models are better. This time, the neural network model prevailed in predicting diamond prices.

\bibliography{book.bib,packages.bib}


\end{document}
