# Linear Regression with single output

Again, let us preprocess the data first.

```{r}
require(mlbench)
require(mxnet)

data(BostonHousing, package="mlbench")

train.ind <- seq(1, 506, 3)
train.x <- data.matrix(BostonHousing[train.ind, -14])   # remove last column
train.y <- BostonHousing[train.ind, 14]                 # take only the last column
test.x <- data.matrix(BostonHousing[-train.ind, -14])
test.y <- BostonHousing[-train.ind, 14]
```

Although we can use `mx.mlp` again to do regression by changing the `out_activation`, this time we are going to introduce a flexible way to configure neural networks in `mxnet`. The configuration is done by the "Symbol" system in `mxnet`, which takes care of the links among nodes, the activation, dropout ratio, etc. To configure a multi-layer neural network, we can do it in the following way:

```{r}
# Define the input data
data <- mx.symbol.Variable("data")

# A fully connected hidden layer
# data: input source
# num_hidden: number of neurons in this hidden layer
fc1 <- mx.symbol.FullyConnected(data, num_hidden=1)

# Use linear regression for the output layer
lro <- mx.symbol.LinearRegressionOutput(fc1)
```

What matters for a regression task is mainly the last function, this enables the new network to optimize for squared loss. We can now train on this simple data set. In this configuration, we dropped the hidden layer so the input layer is directly connected to the output layer.

next we can make prediction with this structure and other parameters with `mx.model.FeedForward.create`:

```{r}
mx.set.seed(0)
model <- mx.model.FeedForward.create(lro, X=train.x, y=train.y,
                                     ctx=mx.cpu(), num.round=50, array.batch.size=20,
                                     learning.rate=2e-6, momentum=0.9, 
                                     eval.metric=mx.metric.rmse)
```

```{r}
model
```

It is also easy to make prediction and evaluate

```{r}
preds <- predict(model, test.x)
dim(preds)
sqrt(mean((preds - test.y)^2))
```

Currently we have four pre-defined metrics "accuracy", "rmse", "mae" and "rmsle". One might wonder how to customize the evaluation metric. `mxnet` provides the interface for users to define their own metric of interests:

```{r}
demo.metric.mae <- mx.metric.custom("mae", function(label, pred) {
  res <- mean(abs(label-pred))
  return(res)
})
```

This is an example for mean absolute error. We can simply plug it in the training function:

```{r}
mx.set.seed(0)
model <- mx.model.FeedForward.create(lro, X=train.x, y=train.y,
                                     ctx=mx.cpu(), num.round=50, 
                                     array.batch.size=20,
                                     learning.rate=2e-6, momentum=0.9, 
                                     eval.metric = demo.metric.mae)
```


## Regression with multiple outputs

In the previous example, our target is to predict the last column ("medv") in the dataset.
It is also possible to build a regression model with multiple outputs.
This time we use the last two columns as the targets:

```{r}
train.x <- data.matrix(BostonHousing[train.ind, -(13:14)])
train.y <- BostonHousing[train.ind, c(13:14)]
test.x <- data.matrix(BostonHousing[-train.ind, -(13:14)])
test.y <- BostonHousing[-train.ind, c(13:14)]
```

and build a similar network symbol:

```{r}
data <- mx.symbol.Variable("data")
fc2 <- mx.symbol.FullyConnected(data, num_hidden=2)
lro2 <- mx.symbol.LinearRegressionOutput(fc2)
```

We use `mx.io.arrayiter` to build an iter for our training set and train the model using `mx.model.FeedForward.create`:

```{r}
# without the custom MAE function
mx.set.seed(0)
train_iter = mx.io.arrayiter(data = t(train.x), label = t(train.y))

model <- mx.model.FeedForward.create(lro2, X=train_iter,
                                     ctx=mx.cpu(), num.round=50, array.batch.size=20,
                                     learning.rate=2e-6, momentum=0.9)
```

```{r}
mx.set.seed(0)
train_iter = mx.io.arrayiter(data = t(train.x), label = t(train.y))

model <- mx.model.FeedForward.create(lro2, X=train_iter,
                                     ctx=mx.cpu(), num.round=50, array.batch.size=20,
                                     learning.rate=2e-6, momentum=0.9, 
                                     eval.metric = demo.metric.mae)
```


After training, we can see that the dimension of the prediction is the same with our target.

```{r}
preds <- t(predict(model, test.x))

sqrt((preds - test.y)^2)

dim(preds)
dim(test.y)
```
Congratulations! Now you have learnt the basic for using `mxnet`. Please check the other tutorials for advanced features.


preds <- predict(model, test.x)
sqrt(mean((preds - test.y)^2))
