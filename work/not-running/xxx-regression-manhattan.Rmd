# Manhattan
> not running

Source: https://www.jaredlander.com/2017/06/feedforward-networks-with-mxnet-in-r/

```{r}
library(Matrix)
library(dplyr)
library(mxnet)
library(ggplot2)
library(plotly)
library(rbokeh)
library(vtreat)
library(Metrics)
```

Download data from: https://data.world/landeranalytics/nyc-pluto

This dataset is about property lots in Manhattan and includes descriptive information as well as value. The original data are available from NYC Planning and the prepared files seen here at the Lander Analytics data.world repo.

```{r}
dataDir <- "."
data_train <- readr::read_csv(file.path(data_raw_dir, 'manhattan_Train.csv'))
data_validate <- readr::read_csv(file.path(data_raw_dir, 'manhattan_Validate.csv'))
data_test <- readr::read_csv(file.path(data_raw_dir, 'manhattan_Test.csv'))
```

```{r}
# Remove some variables
data_train <- data_train %>% 
    select(-ID, -TotalValue, -Borough, -ZoneDist4, 
           -SchoolDistrict, -Council, -PolicePrct, -HealthArea)
data_validate <- data_validate %>% 
    select(-ID, -TotalValue, -Borough, -ZoneDist4, 
           -SchoolDistrict, -Council, -PolicePrct, -HealthArea)
data_test <- data_test %>% 
    select(-ID, -TotalValue, -Borough, -ZoneDist4, 
           -SchoolDistrict, -Council, -PolicePrct, -HealthArea)
```

```{r}
summary(data_train)
```


Here is a visualization of the class balance. Using `rBokeh`.

```{r}
dataList <- list(data_train, data_validate, data_test)
dataList %>% 
    purrr::map(function(x) figure(width=600 / NROW(dataList), 
                                  height=500, legend_location=NULL) %>% 
                   ly_bar(x=High, color=factor(High), data=x, legend='code')) %>% 
    grid_plot(nrow=1, ncol=NROW(dataList), same_axes=TRUE)
```

```{r}
# The column name for the response
responseName <- 'High'

# The target value for the response
responseTarget <- TRUE

# The remaining columns are predictors
varNames <- setdiff(names(data_train), responseName)
varNames
```


We use `vtreat` to do some automated feature engineering.

```{r}

# build the treatment design
treatmentDesign <- designTreatmentsC(dframe = data_train, varlist=varNames, 
                                     outcomename = responseName, 
                                     outcometarget = responseTarget, 
                                     verbose=TRUE)
```


Then we create train, validate and test matrices.

```{r}
# build design data.frames
dataTrain    <- prepare(treatmentplan=treatmentDesign, dframe=data_train)
dataValidate <- prepare(treatmentplan=treatmentDesign, dframe=data_validate)
dataTest     <- prepare(treatmentplan=treatmentDesign, dframe=data_test)

# use all the level names as predictors
predictorNames <- setdiff(names(dataTrain), responseName)

# training matrices
trainX <- data.matrix(dataTrain[, predictorNames])
trainY <- dataTrain[, responseName]

# validation matrices
validateX <- data.matrix(dataValidate[, predictorNames])
validateY <- dataValidate[, responseName]

# test matrices
testX <- data.matrix(dataTest[, predictorNames])
testY <- dataTest[, responseName]

# Sparse versions for some models
trainX_sparse <- sparse.model.matrix(object=High ~ ., data=dataTrain)
validateX_sparse <- sparse.model.matrix(object=High ~ ., data=dataValidate)
testX_sparse <- sparse.model.matrix(object=High ~ ., data=dataTest)
```

This is a function that allows mxnet to calculate log-loss based on the logloss function from the Metrics package.

```{r}
# log-loss
mx.metric.mlogloss <- mx.metric.custom("mlogloss", function(label, pred){
    return(Metrics::logLoss(label, pred))
})
```


## Network Formulation
We build the model symbolically. We use a feedforward network with two hidden layers. The first hidden layer has 256 units and the second has 128 units. We also use dropout and batch normalization for regularization. The last step is to use a logistic sigmoid (inverse logit) for the logistic regression output.

```{r}
net <- mx.symbol.Variable('data') %>%
    # drop out 20% of predictors
    mx.symbol.Dropout(p=0.2, name='Predictor_Dropout') %>%
    
    # a fully connected layer with 256 units
    mx.symbol.FullyConnected(num_hidden=256, name='fc_1') %>%
    
    # batch normalize the units
    mx.symbol.BatchNorm(name='bn_1') %>%
    
    # use the rectified linear unit (relu) for the activation function
    mx.symbol.Activation(act_type='relu', name='relu_1') %>%
    
    # drop out 50% of the units
    mx.symbol.Dropout(p=0.5, name='dropout_1') %>%
    
    # a fully connected layer with 128 units
    mx.symbol.FullyConnected(num_hidden=128, name='fc_2') %>%
    
    # batch normalize the units
    mx.symbol.BatchNorm(name='bn_2') %>%
    
    # use the rectified linear unit (relu) for the activation function
    mx.symbol.Activation(act_type='relu', name='relu_2') %>%
    
    # drop out 50% of the units
    mx.symbol.Dropout(p=0.5, name='dropout_2') %>%
    
    # fully connect to the output layer which has just the 1 unit
    mx.symbol.FullyConnected(num_hidden=1, name='out') %>%
    
    # use the sigmoid output
    mx.symbol.LogisticRegressionOutput(name='output')
```

## Inspect the Network
By inspecting the symbolic network we see that it is actually just a C++ pointer. We also see its arguments and a visualization.

```{r}
net
```

```{r}
arguments(net)
```


```{r}
graph.viz(net)
```


## Network Training
With the data prepared and the network specified we now train the model. First we set the envinronment variable `MXNET_CPU_WORKER_NTHREADS=4` since this demo is on a laptop with four threads. Using a GPU will speed up the computations. We also set the random seed with mx.set.seed for reproducibility.

We use the Adam optimization algorithm which has an adaptive learning rate which incorporates momentum.

```{r}
# use four CPU threads
Sys.setenv('MXNET_CPU_WORKER_NTHREADS'=4)

# set the random seed
mx.set.seed(1234)

# train the model
mod_net <- mx.model.FeedForward.create(
    symbol            = net,    # the symbolic network
    X                 = trainX, # the predictors
    y                 = trainY, # the response
    optimizer         = "adam", # using the Adam optimization method
    eval.data         = list(data = validateX, label = validateY), # validation data
    ctx               = mx.cpu(), # use the cpu for training
    eval.metric       = mx.metric.mlogloss, # evaluate with log-loss
    num.round         = 50,     # 50 epochs
    learning.rate     = 0.001,   # learning rate
    array.batch.size  = 256,    # batch size
    array.layout      = "rowmajor"  # the data is stored in row major format
)
```

## Predictions
Statisticians call this step prediction while the deep learning field calls it inference which has an entirely different meaning in statistics.

```{r}
preds_net <- predict(mod_net, testX, array.layout = "rowmajor") %>% t
preds_net
```

```{r}
mod_net
```

