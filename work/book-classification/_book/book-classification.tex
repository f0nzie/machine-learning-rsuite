\documentclass[]{book}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={Classification},
            pdfauthor={Alfonso R. Reyes},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{natbib}
\bibliographystyle{apalike}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\providecommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}

  \title{Classification}
    \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
    \author{Alfonso R. Reyes}
    \preauthor{\centering\large\emph}
  \postauthor{\par}
      \predate{\centering\large\emph}
  \postdate{\par}
    \date{2019-09-18}

\usepackage{booktabs}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{prerequisites}{%
\chapter*{Prerequisites}\label{prerequisites}}
\addcontentsline{toc}{chapter}{Prerequisites}

This is a \emph{sample} book written in \textbf{Markdown}. You can use anything that Pandoc's Markdown supports, e.g., a math equation \(a^2 + b^2 = c^2\).

The \textbf{bookdown} package can be installed from CRAN or Github:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"bookdown"}\NormalTok{)}
\CommentTok{# or the development version}
\CommentTok{# devtools::install_github("rstudio/bookdown")}
\end{Highlighting}
\end{Shaded}

Remember each Rmd file contains one and only one chapter, and a chapter is defined by the first-level heading \texttt{\#}.

To compile this example to PDF, you need XeLaTeX. You are recommended to install TinyTeX (which includes XeLaTeX): \url{https://yihui.name/tinytex/}.

\hypertarget{a-gentle-introduction-to-support-vector-machines-using-r}{%
\chapter{A gentle introduction to support vector machines using R}\label{a-gentle-introduction-to-support-vector-machines-using-r}}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

Source: \url{https://eight2late.wordpress.com/2017/02/07/a-gentle-introduction-to-support-vector-machines-using-r/}

Most machine learning algorithms involve minimising an error measure of some kind (this measure is often called an objective function or loss function). For example, the error measure in linear regression problems is the famous mean squared error -- i.e.~the averaged sum of the squared differences between the predicted and actual values. Like the mean squared error, most objective functions depend on all points in the training dataset. In this post, I describe the support vector machine (SVM) approach which focuses instead on finding the optimal separation boundary between datapoints that have different classifications. I'll elaborate on what this means in the next section.

Here's the plan in brief. I'll begin with the rationale behind SVMs using a simple case of a binary (two class) dataset with a simple separation boundary (I'll clarify what ``simple'' means in a minute). Following that, I'll describe how this can be generalised to datasets with more complex boundaries. Finally, I'll work through a couple of examples in R, illustrating the principles behind SVMs. In line with the general philosophy of my ``Gentle Introduction to Data Science Using R'' series, the focus is on developing an intuitive understanding of the algorithm along with a practical demonstration of its use through a toy example.

\hypertarget{the-rationale}{%
\section{The rationale}\label{the-rationale}}

The basic idea behind SVMs is best illustrated by considering a simple case: a set of data points that belong to one of two classes, red and blue, as illustrated in figure 1 below. To make things simpler still, I have assumed that the boundary separating the two classes is a straight line, represented by the solid green line in the diagram. In the technical literature, such datasets are called linearly separable.

\begin{center}\includegraphics[width=0.7\linewidth]{/home/datascience/repos/machine-learning-rsuite/import/assets/svm-fig-1} \end{center}

In the linearly separable case, there is usually a fair amount of freedom in the way a separating line can be drawn. Figure 2 illustrates this point: the two broken green lines are also valid separation boundaries. Indeed, because there is a non-zero distance between the two closest points between categories, there are an infinite number of possible separation lines. This, quite naturally, raises the question as to whether it is possible to choose a separation boundary that is optimal.

\begin{center}\includegraphics[width=0.7\linewidth]{/home/datascience/repos/machine-learning-rsuite/import/assets/svm-fig-2} \end{center}

The short answer is, yes there is. One way to do this is to select a boundary line that maximises the margin, i.e.~the distance between the separation boundary and the points that are closest to it. Such an optimal boundary is illustrated by the black brace in Figure 3. The really cool thing about this criterion is that the location of the separation boundary depends only on the points that are closest to it. This means, unlike other classification methods, the classifier does not depend on any other points in dataset. The directed lines between the boundary and the closest points on either side are called support vectors (these are the solid black lines in figure 3). A direct implication of this is that the fewer the support vectors, the better the generalizability of the boundary.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{/home/datascience/repos/machine-learning-rsuite/import/assets/svm-fig-3} 

}

\caption{Figure 3}\label{fig:unnamed-chunk-5}
\end{figure}

Although the above sounds great, it is of limited practical value because real data sets are seldom (if ever) linearly separable.

So, what can we do when dealing with real (i.e.~non linearly separable) data sets?

A simple approach to tackle small deviations from linear separability is to allow a small number of points (those that are close to the boundary) to be misclassified. The number of possible misclassifications is governed by a free parameter C, which is called the cost. The cost is essentially the penalty associated with making an error: the higher the value of C, the less likely it is that the algorithm will misclassify a point.

This approach -- which is called soft margin classification -- is illustrated in Figure 4. Note the points on the wrong side of the separation boundary. We will demonstrate soft margin SVMs in the next section. (Note: At the risk of belabouring the obvious, the purely linearly separable case discussed in the previous para is simply is a special case of the soft margin classifier.)

\begin{center}\includegraphics[width=0.7\linewidth]{/home/datascience/repos/machine-learning-rsuite/import/assets/svm-fig-4} \end{center}

Real life situations are much more complex and cannot be dealt with using soft margin classifiers. For example, as shown in Figure 5, one could have widely separated clusters of points that belong to the same classes. Such situations, which require the use of multiple (and nonlinear) boundaries, can sometimes be dealt with using a clever approach called the kernel trick.

\begin{center}\includegraphics[width=0.7\linewidth]{/home/datascience/repos/machine-learning-rsuite/import/assets/svm-fig-5} \end{center}

\hypertarget{the-kernel-trick}{%
\section{The kernel trick}\label{the-kernel-trick}}

Recall that in the linearly separable (or soft margin) case, the SVM algorithm works by finding a separation boundary that maximises the margin, which is the distance between the boundary and the points closest to it. The distance here is the usual straight line distance between the boundary and the closest point(s). This is called the Euclidean distance in honour of the great geometer of antiquity. The point to note is that this process results in a separation boundary that is a straight line, which as Figure 5 illustrates, does not always work. In fact in most cases it won't.

So what can we do? To answer this question, we have to take a bit of a detour\ldots{}

What if we were able to generalize the notion of distance in a way that generates nonlinear separation boundaries? It turns out that this is possible. To see how, one has to first understand how the notion of distance can be generalized.

The key properties that any measure of distance must satisfy are:

\begin{verbatim}
Non-negativity â€“ a distance cannot be negative, a point that needs no further explanation I reckon ðŸ™‚
Symmetry â€“ that is, the distance between point A and point B is the same as the distance between point B and point A.
Identityâ€“ the distance between a point and itself is zero.
Triangle inequality â€“ that is the sum of distances between point A and B and points B and C must be less than or equal to the distance between A and C (equality holds only if all three points lie along the same line).
\end{verbatim}

Any mathematical object that displays the above properties is akin to a distance. Such generalized distances are called metrics and the mathematical space in which they live is called a metric space. Metrics are defined using special mathematical functions designed to satisfy the above conditions. These functions are known as kernels.

The essence of the kernel trick lies in mapping the classification problem to a metric space in which the problem is rendered separable via a separation boundary that is simple in the new space, but complex -- as it has to be -- in the original one. Generally, the transformed space has a higher dimensionality, with each of the dimensions being (possibly complex) combinations of the original problem variables. However, this is not necessarily a problem because in practice one doesn't actually mess around with transformations, one just tries different kernels (the transformation being implicit in the kernel) and sees which one does the job. The check is simple: we simply test the predictions resulting from using different kernels against a held out subset of the data (as one would for any machine learning algorithm).

It turns out that a particular function -- called the radial basis function kernel (RBF kernel) -- is very effective in many cases. The RBF kernel is essentially a Gaussian (or Normal) function with the Euclidean distance between pairs of points as the variable (see equation 1 below). The basic rationale behind the RBF kernel is that it creates separation boundaries that it tends to classify points close together (in the Euclidean sense) in the original space in the same way. This is reflected in the fact that the kernel decays (i.e.~drops off to zero) as the Euclidean distance between points increases.

The rate at which a kernel decays is governed by the parameter \(\gamma\) -- the higher the value of \(\gamma\), the more rapid the decay. This serves to illustrate that the RBF kernel is extremely flexible\ldots{}.but the flexibility comes at a price -- the danger of overfitting for large values of \(\gamma\) . One should choose appropriate values of C and \(\gamma\) so as to ensure that the resulting kernel represents the best possible balance between flexibility and accuracy. We'll discuss how this is done in practice later in this article.

Finally, though it is probably obvious, it is worth mentioning that the separation boundaries for arbitrary kernels are also defined through support vectors as in Figure 3. To reiterate a point made earlier, this means that a solution that has fewer support vectors is likely to be more robust than one with many. Why? Because the data points defining support vectors are ones that are most sensitive to noise- therefore the fewer, the better.

There are many other types of kernels, each with their own pros and cons. However, I'll leave these for adventurous readers to explore by themselves. Finally, for a much more detailed\ldots{}.and dare I say, better\ldots{} explanation of the kernel trick, I highly recommend this article by Eric Kim.

\hypertarget{support-vector-machines-in-r}{%
\section{Support vector machines in R}\label{support-vector-machines-in-r}}

In this demo we'll use the svm interface that is implemented in the \texttt{e1071} R package. This interface provides R programmers access to the comprehensive \texttt{libsvm} library written by Chang and Lin. I'll use two toy datasets: the famous iris dataset available with the base R package and the sonar dataset from the mlbench package. I won't describe details of the datasets as they are discussed at length in the documentation that I have linked to. However, it is worth mentioning the reasons why I chose these datasets:

As mentioned earlier, no real life dataset is linearly separable, but the iris dataset is almost so. Consequently, it is a good illustration of using linear SVMs. Although one almost never uses these in practice, I have illustrated their use primarily for pedagogical reasons.
The sonar dataset is a good illustration of the benefits of using RBF kernels in cases where the dataset is hard to visualise (60 variables in this case!). In general, one would almost always use RBF (or other nonlinear) kernels in practice.

With that said, let's get right to it. I assume you have R and RStudio installed. For instructions on how to do this, have a look at the first article in this series. The processing preliminaries -- loading libraries, data and creating training and test datasets are much the same as in my previous articles so I won't dwell on these here. For completeness, however, I'll list all the code so you can run it directly in R or R studio (a complete listing of the code can be found here):

\hypertarget{svm-on-iris-dataset}{%
\section{\texorpdfstring{SVM on \texttt{iris} dataset}{SVM on iris dataset}}\label{svm-on-iris-dataset}}

\hypertarget{training-and-test-datasets}{%
\subsection{Training and test datasets}\label{training-and-test-datasets}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#load required library}
\KeywordTok{library}\NormalTok{(e1071)}

\CommentTok{#load built-in iris dataset}
\KeywordTok{data}\NormalTok{(iris)}

\CommentTok{#set seed to ensure reproducible results}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{42}\NormalTok{)}

\CommentTok{#split into training and test sets}
\NormalTok{iris[, }\StringTok{"train"}\NormalTok{] <-}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(}\KeywordTok{runif}\NormalTok{(}\KeywordTok{nrow}\NormalTok{(iris)) }\OperatorTok{<}\StringTok{ }\FloatTok{0.8}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)}

\CommentTok{#separate training and test sets}
\NormalTok{trainset <-}\StringTok{ }\NormalTok{iris[iris}\OperatorTok{$}\NormalTok{train }\OperatorTok{==}\StringTok{ }\DecValTok{1}\NormalTok{,]}
\NormalTok{testset <-}\StringTok{ }\NormalTok{iris[iris}\OperatorTok{$}\NormalTok{train }\OperatorTok{==}\StringTok{ }\DecValTok{0}\NormalTok{,]}

\CommentTok{#get column index of train flag}
\NormalTok{trainColNum <-}\StringTok{ }\KeywordTok{grep}\NormalTok{(}\StringTok{"train"}\NormalTok{, }\KeywordTok{names}\NormalTok{(trainset))}

\CommentTok{#remove train flag column from train and test sets}
\NormalTok{trainset <-}\StringTok{ }\NormalTok{trainset[,}\OperatorTok{-}\NormalTok{trainColNum]}
\NormalTok{testset <-}\StringTok{ }\NormalTok{testset[,}\OperatorTok{-}\NormalTok{trainColNum]}

\KeywordTok{dim}\NormalTok{(trainset)}
\CommentTok{#> [1] 115   5}
\KeywordTok{dim}\NormalTok{(testset)}
\CommentTok{#> [1] 35  5}
\end{Highlighting}
\end{Shaded}

\hypertarget{build-the-svm-model}{%
\subsection{Build the SVM model}\label{build-the-svm-model}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#get column index of predicted variable in dataset}
\NormalTok{typeColNum <-}\StringTok{ }\KeywordTok{grep}\NormalTok{(}\StringTok{"Species"}\NormalTok{, }\KeywordTok{names}\NormalTok{(iris))}

\CommentTok{#build model â€“ linear kernel and C-classification (soft margin) with default cost (C=1)}
\NormalTok{svm_model <-}\StringTok{ }\KeywordTok{svm}\NormalTok{(Species}\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ trainset, }
                 \DataTypeTok{method =} \StringTok{"C-classification"}\NormalTok{, }
                 \DataTypeTok{kernel =} \StringTok{"linear"}\NormalTok{)}
\NormalTok{svm_model}
\CommentTok{#> }
\CommentTok{#> Call:}
\CommentTok{#> svm(formula = Species ~ ., data = trainset, method = "C-classification", }
\CommentTok{#>     kernel = "linear")}
\CommentTok{#> }
\CommentTok{#> }
\CommentTok{#> Parameters:}
\CommentTok{#>    SVM-Type:  C-classification }
\CommentTok{#>  SVM-Kernel:  linear }
\CommentTok{#>        cost:  1 }
\CommentTok{#>       gamma:  0.25 }
\CommentTok{#> }
\CommentTok{#> Number of Support Vectors:  24}
\end{Highlighting}
\end{Shaded}

The output from the SVM model show that there are 24 support vectors. If desired, these can be examined using the SV variable in the model -- i.e via svm\_model\$SV.

\hypertarget{support-vectors}{%
\subsection{Support Vectors}\label{support-vectors}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# support vectors}
\NormalTok{svm_model}\OperatorTok{$}\NormalTok{SV}
\CommentTok{#>     Sepal.Length Sepal.Width Petal.Length Petal.Width}
\CommentTok{#> 19       -0.2564      1.7668       -1.323      -1.305}
\CommentTok{#> 42       -1.7006     -1.7045       -1.559      -1.305}
\CommentTok{#> 45       -0.9785      1.7668       -1.205      -1.171}
\CommentTok{#> 53        1.1878      0.1469        0.568       0.309}
\CommentTok{#> 55        0.7064     -0.5474        0.390       0.309}
\CommentTok{#> 57        0.4657      0.6097        0.450       0.443}
\CommentTok{#> 58       -1.2192     -1.4730       -0.378      -0.364}
\CommentTok{#> 69        0.3453     -1.9359        0.331       0.309}
\CommentTok{#> 71       -0.0157      0.3783        0.509       0.712}
\CommentTok{#> 73        0.4657     -1.2416        0.568       0.309}
\CommentTok{#> 78        0.9471     -0.0845        0.627       0.578}
\CommentTok{#> 84        0.1046     -0.7788        0.686       0.443}
\CommentTok{#> 85       -0.6174     -0.0845        0.331       0.309}
\CommentTok{#> 86        0.1046      0.8412        0.331       0.443}
\CommentTok{#> 99       -0.9785     -1.2416       -0.555      -0.229}
\CommentTok{#> 107      -1.2192     -1.2416        0.331       0.578}
\CommentTok{#> 111       0.7064      0.3783        0.686       0.981}
\CommentTok{#> 117       0.7064     -0.0845        0.922       0.712}
\CommentTok{#> 124       0.4657     -0.7788        0.568       0.712}
\CommentTok{#> 130       1.5488     -0.0845        1.099       0.443}
\CommentTok{#> 138       0.5860      0.1469        0.922       0.712}
\CommentTok{#> 139       0.1046     -0.0845        0.509       0.712}
\CommentTok{#> 147       0.4657     -1.2416        0.627       0.847}
\CommentTok{#> 150      -0.0157     -0.0845        0.686       0.712}
\end{Highlighting}
\end{Shaded}

The test prediction accuracy indicates that the linear performs quite well on this dataset, confirming that it is indeed near linearly separable. To check performance by class, one can create a confusion matrix as described in my post on random forests. I'll leave this as an exercise for you. Another point is that we have used a soft-margin classification scheme with a cost C=1. You can experiment with this by explicitly changing the value of C. Again, I'll leave this for you an exercise.

\hypertarget{predictions-on-training-model}{%
\subsection{Predictions on training model}\label{predictions-on-training-model}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# training set predictions}
\NormalTok{pred_train <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(svm_model, trainset)}
\KeywordTok{mean}\NormalTok{(pred_train }\OperatorTok{==}\StringTok{ }\NormalTok{trainset}\OperatorTok{$}\NormalTok{Species)}
\CommentTok{#> [1] 0.983}
\CommentTok{# [1] 0.9826087}
\end{Highlighting}
\end{Shaded}

\hypertarget{predictions-on-test-model}{%
\subsection{Predictions on test model}\label{predictions-on-test-model}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# test set predictions}
\NormalTok{pred_test <-}\KeywordTok{predict}\NormalTok{(svm_model, testset)}
\KeywordTok{mean}\NormalTok{(pred_test }\OperatorTok{==}\StringTok{ }\NormalTok{testset}\OperatorTok{$}\NormalTok{Species)}
\CommentTok{#> [1] 0.914}
\CommentTok{# [1] 0.9142857}
\end{Highlighting}
\end{Shaded}

\hypertarget{confusion-matrix-and-accuracy}{%
\subsection{Confusion matrix and Accuracy}\label{confusion-matrix-and-accuracy}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# confusion matrix}
\NormalTok{cm <-}\StringTok{ }\KeywordTok{table}\NormalTok{(pred_test, testset}\OperatorTok{$}\NormalTok{Species)}
\NormalTok{cm}
\CommentTok{#>             }
\CommentTok{#> pred_test    setosa versicolor virginica}
\CommentTok{#>   setosa         18          0         0}
\CommentTok{#>   versicolor      0          5         3}
\CommentTok{#>   virginica       0          0         9}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# accuracy}
\KeywordTok{sum}\NormalTok{(}\KeywordTok{diag}\NormalTok{(cm)) }\OperatorTok{/}\StringTok{ }\KeywordTok{sum}\NormalTok{(cm)}
\CommentTok{#> [1] 0.914}
\end{Highlighting}
\end{Shaded}

\hypertarget{svm-with-radial-basis-function-kernel.-linear}{%
\section{SVM with Radial Basis Function kernel. Linear}\label{svm-with-radial-basis-function-kernel.-linear}}

\hypertarget{training-and-test-sets}{%
\subsection{Training and test sets}\label{training-and-test-sets}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#load required library (assuming e1071 is already loaded)}
\KeywordTok{library}\NormalTok{(mlbench)}

\CommentTok{#load Sonar dataset}
\KeywordTok{data}\NormalTok{(Sonar)}
\CommentTok{#set seed to ensure reproducible results}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{42}\NormalTok{)}
\CommentTok{#split into training and test sets}
\NormalTok{Sonar[, }\StringTok{"train"}\NormalTok{] <-}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(}\KeywordTok{runif}\NormalTok{(}\KeywordTok{nrow}\NormalTok{(Sonar))}\OperatorTok{<}\FloatTok{0.8}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{)}

\CommentTok{#separate training and test sets}
\NormalTok{trainset <-}\StringTok{ }\NormalTok{Sonar[Sonar}\OperatorTok{$}\NormalTok{train}\OperatorTok{==}\DecValTok{1}\NormalTok{,]}
\NormalTok{testset <-}\StringTok{ }\NormalTok{Sonar[Sonar}\OperatorTok{$}\NormalTok{train}\OperatorTok{==}\DecValTok{0}\NormalTok{,]}

\CommentTok{#get column index of train flag}
\NormalTok{trainColNum <-}\StringTok{ }\KeywordTok{grep}\NormalTok{(}\StringTok{"train"}\NormalTok{,}\KeywordTok{names}\NormalTok{(trainset))}
\CommentTok{#remove train flag column from train and test sets}
\NormalTok{trainset <-}\StringTok{ }\NormalTok{trainset[,}\OperatorTok{-}\NormalTok{trainColNum]}
\NormalTok{testset <-}\StringTok{ }\NormalTok{testset[,}\OperatorTok{-}\NormalTok{trainColNum]}

\CommentTok{#get column index of predicted variable in dataset}
\NormalTok{typeColNum <-}\StringTok{ }\KeywordTok{grep}\NormalTok{(}\StringTok{"Class"}\NormalTok{,}\KeywordTok{names}\NormalTok{(Sonar))}
\end{Highlighting}
\end{Shaded}

\hypertarget{predictions-on-training-model-1}{%
\subsection{Predictions on Training model}\label{predictions-on-training-model-1}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#build model â€“ linear kernel and C-classification with default cost (C=1)}
\NormalTok{svm_model <-}\StringTok{ }\KeywordTok{svm}\NormalTok{(Class}\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data=}\NormalTok{trainset, }
                 \DataTypeTok{method=}\StringTok{"C-classification"}\NormalTok{, }
                 \DataTypeTok{kernel=}\StringTok{"linear"}\NormalTok{)}

\CommentTok{#training set predictions}
\NormalTok{pred_train <-}\KeywordTok{predict}\NormalTok{(svm_model,trainset)}
\KeywordTok{mean}\NormalTok{(pred_train}\OperatorTok{==}\NormalTok{trainset}\OperatorTok{$}\NormalTok{Class)}
\CommentTok{#> [1] 0.97}
\end{Highlighting}
\end{Shaded}

\hypertarget{predictions-on-test-model-1}{%
\subsection{Predictions on test model}\label{predictions-on-test-model-1}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#test set predictions}
\NormalTok{pred_test <-}\KeywordTok{predict}\NormalTok{(svm_model,testset)}
\KeywordTok{mean}\NormalTok{(pred_test}\OperatorTok{==}\NormalTok{testset}\OperatorTok{$}\NormalTok{Class)}
\CommentTok{#> [1] 0.605}
\end{Highlighting}
\end{Shaded}

I'll leave you to examine the contents of the model. The important point to note here is that the performance of the model with the test set is quite dismal compared to the previous case. This simply indicates that the linear kernel is not appropriate here. Let's take a look at what happens if we use the RBF kernel with default values for the parameters:

\hypertarget{svm-with-radial-basis-function-kernel.-non-linear}{%
\section{SVM with Radial Basis Function kernel. Non-linear}\label{svm-with-radial-basis-function-kernel.-non-linear}}

\hypertarget{predictions-on-training-model-2}{%
\subsection{Predictions on training model}\label{predictions-on-training-model-2}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#build model: radial kernel, default params}
\NormalTok{svm_model <-}\StringTok{ }\KeywordTok{svm}\NormalTok{(Class}\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data=}\NormalTok{trainset, }
                 \DataTypeTok{method=}\StringTok{"C-classification"}\NormalTok{, }
                 \DataTypeTok{kernel=}\StringTok{"radial"}\NormalTok{)}
\CommentTok{# print params}
\NormalTok{svm_model}\OperatorTok{$}\NormalTok{cost}
\CommentTok{#> [1] 1}
\NormalTok{svm_model}\OperatorTok{$}\NormalTok{gamma}
\CommentTok{#> [1] 0.0167}

\CommentTok{#training set predictions}
\NormalTok{pred_train <-}\KeywordTok{predict}\NormalTok{(svm_model,trainset)}
\KeywordTok{mean}\NormalTok{(pred_train}\OperatorTok{==}\NormalTok{trainset}\OperatorTok{$}\NormalTok{Class)}
\CommentTok{#> [1] 0.988}
\end{Highlighting}
\end{Shaded}

\hypertarget{predictions-on-test-model-2}{%
\subsection{Predictions on test model}\label{predictions-on-test-model-2}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#test set predictions}
\NormalTok{pred_test <-}\KeywordTok{predict}\NormalTok{(svm_model,testset)}
\KeywordTok{mean}\NormalTok{(pred_test}\OperatorTok{==}\NormalTok{testset}\OperatorTok{$}\NormalTok{Class)}
\CommentTok{#> [1] 0.767}
\end{Highlighting}
\end{Shaded}

That's a pretty decent improvement from the linear kernel. Let's see if we can do better by doing some parameter tuning. To do this we first invoke tune.svm and use the parameters it gives us in the call to svm:

\hypertarget{tuning-of-parameters}{%
\subsection{Tuning of parameters}\label{tuning-of-parameters}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# find optimal parameters in a specified range}
\NormalTok{tune_out <-}\StringTok{ }\KeywordTok{tune.svm}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ trainset[,}\OperatorTok{-}\NormalTok{typeColNum], }
                     \DataTypeTok{y =}\NormalTok{ trainset[, typeColNum], }
                     \DataTypeTok{gamma =} \DecValTok{10}\OperatorTok{^}\NormalTok{(}\OperatorTok{-}\DecValTok{3}\OperatorTok{:}\DecValTok{3}\NormalTok{), }
                     \DataTypeTok{cost =} \KeywordTok{c}\NormalTok{(}\FloatTok{0.01}\NormalTok{, }\FloatTok{0.1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{100}\NormalTok{, }\DecValTok{1000}\NormalTok{), }
                     \DataTypeTok{kernel =} \StringTok{"radial"}\NormalTok{)}

\CommentTok{#print best values of cost and gamma}
\NormalTok{tune_out}\OperatorTok{$}\NormalTok{best.parameters}\OperatorTok{$}\NormalTok{cost}
\CommentTok{#> [1] 10}
\NormalTok{tune_out}\OperatorTok{$}\NormalTok{best.parameters}\OperatorTok{$}\NormalTok{gamma}
\CommentTok{#> [1] 0.01}

\CommentTok{#build model}
\NormalTok{svm_model <-}\StringTok{ }\KeywordTok{svm}\NormalTok{(Class}\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ trainset, }
                 \DataTypeTok{method =} \StringTok{"C-classification"}\NormalTok{, }
                 \DataTypeTok{kernel =} \StringTok{"radial"}\NormalTok{, }
                 \DataTypeTok{cost =}\NormalTok{ tune_out}\OperatorTok{$}\NormalTok{best.parameters}\OperatorTok{$}\NormalTok{cost, }
                 \DataTypeTok{gamma =}\NormalTok{ tune_out}\OperatorTok{$}\NormalTok{best.parameters}\OperatorTok{$}\NormalTok{gamma)}
\end{Highlighting}
\end{Shaded}

\hypertarget{prediction-on-training-model-with-new-parameters}{%
\subsection{Prediction on training model with new parameters}\label{prediction-on-training-model-with-new-parameters}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# training set predictions}
\NormalTok{pred_train <-}\KeywordTok{predict}\NormalTok{(svm_model,trainset)}
\KeywordTok{mean}\NormalTok{(pred_train}\OperatorTok{==}\NormalTok{trainset}\OperatorTok{$}\NormalTok{Class)}
\CommentTok{#> [1] 1}
\end{Highlighting}
\end{Shaded}

\hypertarget{prediction-on-test-model-with-new-parameters}{%
\subsection{Prediction on test model with new parameters}\label{prediction-on-test-model-with-new-parameters}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# test set predictions}
\NormalTok{pred_test <-}\KeywordTok{predict}\NormalTok{(svm_model,testset)}
\KeywordTok{mean}\NormalTok{(pred_test}\OperatorTok{==}\NormalTok{testset}\OperatorTok{$}\NormalTok{Class)}
\CommentTok{#> [1] 0.814}
\end{Highlighting}
\end{Shaded}

Which is fairly decent improvement on the un-optimised case.

\hypertarget{wrapping-up}{%
\section{Wrapping up}\label{wrapping-up}}

This bring us to the end of this introductory exploration of SVMs in R. To recap, the distinguishing feature of SVMs in contrast to most other techniques is that they attempt to construct optimal separation boundaries between different categories.

SVMs are quite versatile and have been applied to a wide variety of domains ranging from chemistry to pattern recognition. They are best used in binary classification scenarios. This brings up a question as to where SVMs are to be preferred to other binary classification techniques such as logistic regression. The honest response is, ``it depends'' -- but here are some points to keep in mind when choosing between the two. A general point to keep in mind is that SVM algorithms tend to be expensive both in terms of memory and computation, issues that can start to hurt as the size of the dataset increases.

Given all the above caveats and considerations, the best way to figure out whether an SVM approach will work for your problem may be to do what most machine learning practitioners do: try it out!

\hypertarget{classification-with-svm.-social-network-dataset}{%
\chapter{Classification with SVM. Social Network dataset}\label{classification-with-svm.-social-network-dataset}}

\hypertarget{introduction-1}{%
\section{Introduction}\label{introduction-1}}

\textbf{Source}: \url{https://www.geeksforgeeks.org/classifying-data-using-support-vector-machinessvms-in-r/}

\hypertarget{data-operations}{%
\section{Data Operations}\label{data-operations}}

\hypertarget{load-libraries}{%
\subsection{Load libraries}\label{load-libraries}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load packages}
\KeywordTok{library}\NormalTok{(dplyr)}
\KeywordTok{library}\NormalTok{(caTools) }
\KeywordTok{library}\NormalTok{(e1071) }
\KeywordTok{library}\NormalTok{(ElemStatLearn) }
\end{Highlighting}
\end{Shaded}

\hypertarget{importing-dataset}{%
\subsection{Importing dataset}\label{importing-dataset}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Importing the dataset }
\NormalTok{dataset =}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\KeywordTok{file.path}\NormalTok{(data_raw_dir, }\StringTok{'Social_Network_Ads.csv'}\NormalTok{)) }
\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{glimpse}\NormalTok{(dataset)}
\CommentTok{#> Observations: 400}
\CommentTok{#> Variables: 5}
\CommentTok{#> $ User.ID         <int> 15624510, 15810944, 15668575, 15603246, 158040...}
\CommentTok{#> $ Gender          <fct> Male, Male, Female, Female, Male, Male, Female...}
\CommentTok{#> $ Age             <int> 19, 35, 26, 27, 19, 27, 27, 32, 25, 35, 26, 26...}
\CommentTok{#> $ EstimatedSalary <int> 19000, 20000, 43000, 57000, 76000, 58000, 8400...}
\CommentTok{#> $ Purchased       <int> 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0...}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tibble}\OperatorTok{::}\KeywordTok{as_tibble}\NormalTok{(dataset)}
\CommentTok{#> # A tibble: 400 x 5}
\CommentTok{#>    User.ID Gender   Age EstimatedSalary Purchased}
\CommentTok{#>      <int> <fct>  <int>           <int>     <int>}
\CommentTok{#> 1 15624510 Male      19           19000         0}
\CommentTok{#> 2 15810944 Male      35           20000         0}
\CommentTok{#> 3 15668575 Female    26           43000         0}
\CommentTok{#> 4 15603246 Female    27           57000         0}
\CommentTok{#> 5 15804002 Male      19           76000         0}
\CommentTok{#> 6 15728773 Male      27           58000         0}
\CommentTok{#> # ... with 394 more rows}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Taking columns 3-5 }
\NormalTok{dataset =}\StringTok{ }\NormalTok{dataset[}\DecValTok{3}\OperatorTok{:}\DecValTok{5}\NormalTok{]}
\NormalTok{tibble}\OperatorTok{::}\KeywordTok{as_tibble}\NormalTok{(dataset)}
\CommentTok{#> # A tibble: 400 x 3}
\CommentTok{#>     Age EstimatedSalary Purchased}
\CommentTok{#>   <int>           <int>     <int>}
\CommentTok{#> 1    19           19000         0}
\CommentTok{#> 2    35           20000         0}
\CommentTok{#> 3    26           43000         0}
\CommentTok{#> 4    27           57000         0}
\CommentTok{#> 5    19           76000         0}
\CommentTok{#> 6    27           58000         0}
\CommentTok{#> # ... with 394 more rows}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Encoding the target feature as factor }
\NormalTok{dataset}\OperatorTok{$}\NormalTok{Purchased =}\StringTok{ }\KeywordTok{factor}\NormalTok{(dataset}\OperatorTok{$}\NormalTok{Purchased, }\DataTypeTok{levels =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)) }
\KeywordTok{str}\NormalTok{(dataset)}
\CommentTok{#> 'data.frame':    400 obs. of  3 variables:}
\CommentTok{#>  $ Age            : int  19 35 26 27 19 27 27 32 25 35 ...}
\CommentTok{#>  $ EstimatedSalary: int  19000 20000 43000 57000 76000 58000 84000 150000 33000 65000 ...}
\CommentTok{#>  $ Purchased      : Factor w/ 2 levels "0","1": 1 1 1 1 1 1 1 2 1 1 ...}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Splitting the dataset into the Training set and Test set }
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{) }
\NormalTok{split =}\StringTok{ }\KeywordTok{sample.split}\NormalTok{(dataset}\OperatorTok{$}\NormalTok{Purchased, }\DataTypeTok{SplitRatio =} \FloatTok{0.75}\NormalTok{) }
  
\NormalTok{training_set =}\StringTok{ }\KeywordTok{subset}\NormalTok{(dataset, split }\OperatorTok{==}\StringTok{ }\OtherTok{TRUE}\NormalTok{) }
\NormalTok{test_set =}\StringTok{ }\KeywordTok{subset}\NormalTok{(dataset, split }\OperatorTok{==}\StringTok{ }\OtherTok{FALSE}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{dim}\NormalTok{(training_set)}
\CommentTok{#> [1] 300   3}
\KeywordTok{dim}\NormalTok{(test_set)}
\CommentTok{#> [1] 100   3}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Feature Scaling }
\NormalTok{training_set[}\OperatorTok{-}\DecValTok{3}\NormalTok{] =}\StringTok{ }\KeywordTok{scale}\NormalTok{(training_set[}\OperatorTok{-}\DecValTok{3}\NormalTok{]) }
\NormalTok{test_set[}\OperatorTok{-}\DecValTok{3}\NormalTok{] =}\StringTok{ }\KeywordTok{scale}\NormalTok{(test_set[}\OperatorTok{-}\DecValTok{3}\NormalTok{]) }
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Fitting SVM to the Training set }
\NormalTok{classifier =}\StringTok{ }\KeywordTok{svm}\NormalTok{(}\DataTypeTok{formula =}\NormalTok{ Purchased }\OperatorTok{~}\StringTok{ }\NormalTok{., }
                 \DataTypeTok{data =}\NormalTok{ training_set, }
                 \DataTypeTok{type =} \StringTok{'C-classification'}\NormalTok{, }
                 \DataTypeTok{kernel =} \StringTok{'linear'}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{classifier}
\CommentTok{#> }
\CommentTok{#> Call:}
\CommentTok{#> svm(formula = Purchased ~ ., data = training_set, type = "C-classification", }
\CommentTok{#>     kernel = "linear")}
\CommentTok{#> }
\CommentTok{#> }
\CommentTok{#> Parameters:}
\CommentTok{#>    SVM-Type:  C-classification }
\CommentTok{#>  SVM-Kernel:  linear }
\CommentTok{#>        cost:  1 }
\CommentTok{#>       gamma:  0.5 }
\CommentTok{#> }
\CommentTok{#> Number of Support Vectors:  116}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(classifier)}
\CommentTok{#> }
\CommentTok{#> Call:}
\CommentTok{#> svm(formula = Purchased ~ ., data = training_set, type = "C-classification", }
\CommentTok{#>     kernel = "linear")}
\CommentTok{#> }
\CommentTok{#> }
\CommentTok{#> Parameters:}
\CommentTok{#>    SVM-Type:  C-classification }
\CommentTok{#>  SVM-Kernel:  linear }
\CommentTok{#>        cost:  1 }
\CommentTok{#>       gamma:  0.5 }
\CommentTok{#> }
\CommentTok{#> Number of Support Vectors:  116}
\CommentTok{#> }
\CommentTok{#>  ( 58 58 )}
\CommentTok{#> }
\CommentTok{#> }
\CommentTok{#> Number of Classes:  2 }
\CommentTok{#> }
\CommentTok{#> Levels: }
\CommentTok{#>  0 1}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Predicting the Test set results }
\NormalTok{y_pred =}\StringTok{ }\KeywordTok{predict}\NormalTok{(classifier, }\DataTypeTok{newdata =}\NormalTok{ test_set[}\OperatorTok{-}\DecValTok{3}\NormalTok{]) }
\NormalTok{y_pred}
\CommentTok{#>   2   4   5   9  12  18  19  20  22  29  32  34  35  38  45  46  48  52 }
\CommentTok{#>   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 }
\CommentTok{#>  66  69  74  75  82  84  85  86  87  89 103 104 107 108 109 117 124 126 }
\CommentTok{#>   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0 }
\CommentTok{#> 127 131 134 139 148 154 156 159 162 163 170 175 176 193 199 200 208 213 }
\CommentTok{#>   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   1 }
\CommentTok{#> 224 226 228 229 230 234 236 237 239 241 255 264 265 266 273 274 281 286 }
\CommentTok{#>   1   0   1   0   1   1   1   0   1   1   1   0   1   1   1   1   1   0 }
\CommentTok{#> 292 299 302 305 307 310 316 324 326 332 339 341 343 347 353 363 364 367 }
\CommentTok{#>   1   1   1   0   1   0   0   0   0   1   0   1   0   1   1   0   1   1 }
\CommentTok{#> 368 369 372 373 380 383 389 392 395 400 }
\CommentTok{#>   1   0   1   0   1   1   0   0   0   0 }
\CommentTok{#> Levels: 0 1}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Making the Confusion Matrix }
\NormalTok{cm =}\StringTok{ }\KeywordTok{table}\NormalTok{(test_set[, }\DecValTok{3}\NormalTok{], y_pred) }
\NormalTok{cm}
\CommentTok{#>    y_pred}
\CommentTok{#>      0  1}
\CommentTok{#>   0 57  7}
\CommentTok{#>   1 13 23}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{xtable}\OperatorTok{::}\KeywordTok{xtable}\NormalTok{(cm)}
\CommentTok{#> % latex table generated in R 3.6.0 by xtable 1.8-4 package}
\CommentTok{#> % Wed Sep 18 15:11:17 2019}
\CommentTok{#> \textbackslash{}begin\{table\}[ht]}
\CommentTok{#> \textbackslash{}centering}
\CommentTok{#> \textbackslash{}begin\{tabular\}\{rrr\}}
\CommentTok{#>   \textbackslash{}hline}
\CommentTok{#>  & 0 & 1 \textbackslash{}\textbackslash{} }
\CommentTok{#>   \textbackslash{}hline}
\CommentTok{#> 0 &  57 &   7 \textbackslash{}\textbackslash{} }
\CommentTok{#>   1 &  13 &  23 \textbackslash{}\textbackslash{} }
\CommentTok{#>    \textbackslash{}hline}
\CommentTok{#> \textbackslash{}end\{tabular\}}
\CommentTok{#> \textbackslash{}end\{table\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# installing library ElemStatLearn }
\CommentTok{# library(ElemStatLearn) }
  
\CommentTok{# Plotting the training data set results }
\NormalTok{set =}\StringTok{ }\NormalTok{training_set }
\NormalTok{X1 =}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\KeywordTok{min}\NormalTok{(set[, }\DecValTok{1}\NormalTok{]) }\OperatorTok{-}\StringTok{ }\DecValTok{1}\NormalTok{, }\KeywordTok{max}\NormalTok{(set[, }\DecValTok{1}\NormalTok{]) }\OperatorTok{+}\StringTok{ }\DecValTok{1}\NormalTok{, }\DataTypeTok{by =} \FloatTok{0.01}\NormalTok{) }
\NormalTok{X2 =}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\KeywordTok{min}\NormalTok{(set[, }\DecValTok{2}\NormalTok{]) }\OperatorTok{-}\StringTok{ }\DecValTok{1}\NormalTok{, }\KeywordTok{max}\NormalTok{(set[, }\DecValTok{2}\NormalTok{]) }\OperatorTok{+}\StringTok{ }\DecValTok{1}\NormalTok{, }\DataTypeTok{by =} \FloatTok{0.01}\NormalTok{) }
  
\NormalTok{grid_set =}\StringTok{ }\KeywordTok{expand.grid}\NormalTok{(X1, X2) }
\KeywordTok{colnames}\NormalTok{(grid_set) =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{'Age'}\NormalTok{, }\StringTok{'EstimatedSalary'}\NormalTok{) }
\NormalTok{y_grid =}\StringTok{ }\KeywordTok{predict}\NormalTok{(classifier, }\DataTypeTok{newdata =}\NormalTok{ grid_set) }
  
\KeywordTok{plot}\NormalTok{(set[, }\DecValTok{-3}\NormalTok{], }
     \DataTypeTok{main =} \StringTok{'SVM (Training set)'}\NormalTok{, }
     \DataTypeTok{xlab =} \StringTok{'Age'}\NormalTok{, }\DataTypeTok{ylab =} \StringTok{'Estimated Salary'}\NormalTok{, }
     \DataTypeTok{xlim =} \KeywordTok{range}\NormalTok{(X1), }\DataTypeTok{ylim =} \KeywordTok{range}\NormalTok{(X2)) }
  
\KeywordTok{contour}\NormalTok{(X1, X2, }\KeywordTok{matrix}\NormalTok{(}\KeywordTok{as.numeric}\NormalTok{(y_grid), }\KeywordTok{length}\NormalTok{(X1), }\KeywordTok{length}\NormalTok{(X2)), }\DataTypeTok{add =} \OtherTok{TRUE}\NormalTok{) }
  
\KeywordTok{points}\NormalTok{(grid_set, }\DataTypeTok{pch =} \StringTok{'.'}\NormalTok{, }\DataTypeTok{col =} \KeywordTok{ifelse}\NormalTok{(y_grid }\OperatorTok{==}\StringTok{ }\DecValTok{1}\NormalTok{, }\StringTok{'coral1'}\NormalTok{, }\StringTok{'aquamarine'}\NormalTok{)) }
  
\KeywordTok{points}\NormalTok{(set, }\DataTypeTok{pch =} \DecValTok{21}\NormalTok{, }\DataTypeTok{bg =} \KeywordTok{ifelse}\NormalTok{(set[, }\DecValTok{3}\NormalTok{] }\OperatorTok{==}\StringTok{ }\DecValTok{1}\NormalTok{, }\StringTok{'green4'}\NormalTok{, }\StringTok{'red3'}\NormalTok{)) }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_112-social_networks-SVM_files/figure-latex/plot_training_set-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{set =}\StringTok{ }\NormalTok{test_set }
\NormalTok{X1 =}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\KeywordTok{min}\NormalTok{(set[, }\DecValTok{1}\NormalTok{]) }\OperatorTok{-}\StringTok{ }\DecValTok{1}\NormalTok{, }\KeywordTok{max}\NormalTok{(set[, }\DecValTok{1}\NormalTok{]) }\OperatorTok{+}\StringTok{ }\DecValTok{1}\NormalTok{, }\DataTypeTok{by =} \FloatTok{0.01}\NormalTok{) }
\NormalTok{X2 =}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\KeywordTok{min}\NormalTok{(set[, }\DecValTok{2}\NormalTok{]) }\OperatorTok{-}\StringTok{ }\DecValTok{1}\NormalTok{, }\KeywordTok{max}\NormalTok{(set[, }\DecValTok{2}\NormalTok{]) }\OperatorTok{+}\StringTok{ }\DecValTok{1}\NormalTok{, }\DataTypeTok{by =} \FloatTok{0.01}\NormalTok{) }
  
\NormalTok{grid_set =}\StringTok{ }\KeywordTok{expand.grid}\NormalTok{(X1, X2) }
\KeywordTok{colnames}\NormalTok{(grid_set) =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{'Age'}\NormalTok{, }\StringTok{'EstimatedSalary'}\NormalTok{) }
\NormalTok{y_grid =}\StringTok{ }\KeywordTok{predict}\NormalTok{(classifier, }\DataTypeTok{newdata =}\NormalTok{ grid_set) }
  
\KeywordTok{plot}\NormalTok{(set[, }\DecValTok{-3}\NormalTok{], }\DataTypeTok{main =} \StringTok{'SVM (Test set)'}\NormalTok{, }
     \DataTypeTok{xlab =} \StringTok{'Age'}\NormalTok{, }\DataTypeTok{ylab =} \StringTok{'Estimated Salary'}\NormalTok{, }
     \DataTypeTok{xlim =} \KeywordTok{range}\NormalTok{(X1), }\DataTypeTok{ylim =} \KeywordTok{range}\NormalTok{(X2)) }
  
\KeywordTok{contour}\NormalTok{(X1, X2, }\KeywordTok{matrix}\NormalTok{(}\KeywordTok{as.numeric}\NormalTok{(y_grid), }\KeywordTok{length}\NormalTok{(X1), }\KeywordTok{length}\NormalTok{(X2)), }\DataTypeTok{add =} \OtherTok{TRUE}\NormalTok{) }
  
\KeywordTok{points}\NormalTok{(grid_set, }\DataTypeTok{pch =} \StringTok{'.'}\NormalTok{, }\DataTypeTok{col =} \KeywordTok{ifelse}\NormalTok{(y_grid }\OperatorTok{==}\StringTok{ }\DecValTok{1}\NormalTok{, }\StringTok{'coral1'}\NormalTok{, }\StringTok{'aquamarine'}\NormalTok{)) }
  
\KeywordTok{points}\NormalTok{(set, }\DataTypeTok{pch =} \DecValTok{21}\NormalTok{, }\DataTypeTok{bg =} \KeywordTok{ifelse}\NormalTok{(set[, }\DecValTok{3}\NormalTok{] }\OperatorTok{==}\StringTok{ }\DecValTok{1}\NormalTok{, }\StringTok{'green4'}\NormalTok{, }\StringTok{'red3'}\NormalTok{)) }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_112-social_networks-SVM_files/figure-latex/unnamed-chunk-16-1} \end{center}

\hypertarget{broad-view-of-svm}{%
\chapter{Broad view of SVM}\label{broad-view-of-svm}}

\hypertarget{introduction-2}{%
\section{Introduction}\label{introduction-2}}

Source: \url{http://uc-r.github.io/svm}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# set pseudorandom number generator}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{10}\NormalTok{)}

\CommentTok{# Attach Packages}
\KeywordTok{library}\NormalTok{(tidyverse)    }\CommentTok{# data manipulation and visualization}
\CommentTok{#> Registered S3 methods overwritten by 'ggplot2':}
\CommentTok{#>   method         from }
\CommentTok{#>   [.quosures     rlang}
\CommentTok{#>   c.quosures     rlang}
\CommentTok{#>   print.quosures rlang}
\CommentTok{#> Registered S3 method overwritten by 'rvest':}
\CommentTok{#>   method            from}
\CommentTok{#>   read_xml.response xml2}
\CommentTok{#> -- Attaching packages ----------------------------------------- tidyverse 1.2.1 --}
\CommentTok{#> v ggplot2 3.1.1       v purrr   0.3.2  }
\CommentTok{#> v tibble  2.1.1       v dplyr   0.8.0.1}
\CommentTok{#> v tidyr   0.8.3       v stringr 1.4.0  }
\CommentTok{#> v readr   1.3.1       v forcats 0.4.0}
\CommentTok{#> -- Conflicts -------------------------------------------- tidyverse_conflicts() --}
\CommentTok{#> x dplyr::filter() masks stats::filter()}
\CommentTok{#> x dplyr::lag()    masks stats::lag()}
\KeywordTok{library}\NormalTok{(kernlab)      }\CommentTok{# SVM methodology}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'kernlab'}
\CommentTok{#> The following object is masked from 'package:purrr':}
\CommentTok{#> }
\CommentTok{#>     cross}
\CommentTok{#> The following object is masked from 'package:ggplot2':}
\CommentTok{#> }
\CommentTok{#>     alpha}
\KeywordTok{library}\NormalTok{(e1071)        }\CommentTok{# SVM methodology}
\KeywordTok{library}\NormalTok{(ISLR)         }\CommentTok{# contains example data set "Khan"}
\KeywordTok{library}\NormalTok{(RColorBrewer) }\CommentTok{# customized coloring of plots}
\end{Highlighting}
\end{Shaded}

The data sets used in the tutorial (with the exception of Khan) will be generated using built-in R commands. The Support Vector Machine methodology is sound for any number of dimensions, but becomes difficult to visualize for more than 2. As previously mentioned, SVMs are robust for any number of classes, but we will stick to no more than 3 for the duration of this tutorial.

\hypertarget{maximal-margin-classifier}{%
\section{Maximal Margin Classifier}\label{maximal-margin-classifier}}

If the classes are separable by a linear boundary, we can use a Maximal Margin Classifier to find the classification boundary. To visualize an example of separated data, we generate 40 random observations and assign them to two classes. Upon visual inspection, we can see that infinitely many lines exist that split the two classes.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Construct sample data set - completely separated}
\NormalTok{x <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\KeywordTok{rnorm}\NormalTok{(}\DecValTok{20}\OperatorTok{*}\DecValTok{2}\NormalTok{), }\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{)}
\NormalTok{y <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\KeywordTok{rep}\NormalTok{(}\OperatorTok{-}\DecValTok{1}\NormalTok{,}\DecValTok{10}\NormalTok{), }\KeywordTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{10}\NormalTok{))}
\NormalTok{x[y}\OperatorTok{==}\DecValTok{1}\NormalTok{,] <-}\StringTok{ }\NormalTok{x[y}\OperatorTok{==}\DecValTok{1}\NormalTok{,] }\OperatorTok{+}\StringTok{ }\DecValTok{3}\OperatorTok{/}\DecValTok{2}
\NormalTok{dat <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{x=}\NormalTok{x, }\DataTypeTok{y=}\KeywordTok{as.factor}\NormalTok{(y))}

\CommentTok{# Plot data}
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ dat, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ x}\FloatTok{.2}\NormalTok{, }\DataTypeTok{y =}\NormalTok{ x}\FloatTok{.1}\NormalTok{, }\DataTypeTok{color =}\NormalTok{ y, }\DataTypeTok{shape =}\NormalTok{ y)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{size =} \DecValTok{2}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_color_manual}\NormalTok{(}\DataTypeTok{values=}\KeywordTok{c}\NormalTok{(}\StringTok{"#000000"}\NormalTok{, }\StringTok{"#FF0000"}\NormalTok{)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{legend.position =} \StringTok{"none"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_113-broad_view-SVM_files/figure-latex/unnamed-chunk-3-1} \end{center}

The goal of the maximal margin classifier is to identify the linear boundary that maximizes the total distance between the line and the closest point in each class. We can use the svm() function in the e1071 package to find this boundary.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Fit Support Vector Machine model to data set}
\NormalTok{svmfit <-}\StringTok{ }\KeywordTok{svm}\NormalTok{(y}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data =}\NormalTok{ dat, }\DataTypeTok{kernel =} \StringTok{"linear"}\NormalTok{, }\DataTypeTok{scale =} \OtherTok{FALSE}\NormalTok{)}
\CommentTok{# Plot Results}
\KeywordTok{plot}\NormalTok{(svmfit, dat)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_113-broad_view-SVM_files/figure-latex/unnamed-chunk-4-1} \end{center}

In the plot, points that are represented by an ``X'' are the support vectors, or the points that directly affect the classification line. The points marked with an ``o'' are the other points, which don't affect the calculation of the line. This principle will lay the foundation for support vector machines. The same plot can be generated using the kernlab package, with the following results:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# fit model and produce plot}
\NormalTok{kernfit <-}\StringTok{ }\KeywordTok{ksvm}\NormalTok{(x, y, }\DataTypeTok{type =} \StringTok{"C-svc"}\NormalTok{, }\DataTypeTok{kernel =} \StringTok{'vanilladot'}\NormalTok{)}
\CommentTok{#>  Setting default kernel parameters}
\KeywordTok{plot}\NormalTok{(kernfit, }\DataTypeTok{data =}\NormalTok{ x)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_113-broad_view-SVM_files/figure-latex/unnamed-chunk-5-1} \end{center}

\texttt{kernlab} shows a little more detail than e1071, showing a color gradient that indicates how confidently a new point would be classified based on its features. Just as in the first plot, the support vectors are marked, in this case as filled-in points, while the classes are denoted by different shapes.

\hypertarget{support-vector-classifiers}{%
\section{Support Vector Classifiers}\label{support-vector-classifiers}}

As convenient as the maximal marginal classifier is to understand, most real data sets will not be fully separable by a linear boundary. To handle such data, we must use modified methodology. We simulate a new data set where the classes are more mixed.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Construct sample data set - not completely separated}
\NormalTok{x <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\KeywordTok{rnorm}\NormalTok{(}\DecValTok{20}\OperatorTok{*}\DecValTok{2}\NormalTok{), }\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{)}
\NormalTok{y <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\KeywordTok{rep}\NormalTok{(}\OperatorTok{-}\DecValTok{1}\NormalTok{,}\DecValTok{10}\NormalTok{), }\KeywordTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{10}\NormalTok{))}
\NormalTok{x[y}\OperatorTok{==}\DecValTok{1}\NormalTok{,] <-}\StringTok{ }\NormalTok{x[y}\OperatorTok{==}\DecValTok{1}\NormalTok{,] }\OperatorTok{+}\StringTok{ }\DecValTok{1}
\NormalTok{dat <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{x=}\NormalTok{x, }\DataTypeTok{y=}\KeywordTok{as.factor}\NormalTok{(y))}

\CommentTok{# Plot data set}
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ dat, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ x}\FloatTok{.2}\NormalTok{, }\DataTypeTok{y =}\NormalTok{ x}\FloatTok{.1}\NormalTok{, }\DataTypeTok{color =}\NormalTok{ y, }\DataTypeTok{shape =}\NormalTok{ y)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{size =} \DecValTok{2}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_color_manual}\NormalTok{(}\DataTypeTok{values=}\KeywordTok{c}\NormalTok{(}\StringTok{"#000000"}\NormalTok{, }\StringTok{"#FF0000"}\NormalTok{)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{legend.position =} \StringTok{"none"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_113-broad_view-SVM_files/figure-latex/unnamed-chunk-6-1} \end{center}

Whether the data is separable or not, the \texttt{svm()} command syntax is the same. In the case of data that is not linearly separable, however, the cost = argument takes on real importance. This quantifies the penalty associated with having an observation on the wrong side of the classification boundary. We can plot the fit in the same way as the completely separable case. We first use \texttt{e1071}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Fit Support Vector Machine model to data set}
\NormalTok{svmfit <-}\StringTok{ }\KeywordTok{svm}\NormalTok{(y}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data =}\NormalTok{ dat, }\DataTypeTok{kernel =} \StringTok{"linear"}\NormalTok{, }\DataTypeTok{cost =} \DecValTok{10}\NormalTok{)}
\CommentTok{# Plot Results}
\KeywordTok{plot}\NormalTok{(svmfit, dat)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_113-broad_view-SVM_files/figure-latex/unnamed-chunk-7-1} \end{center}

By upping the cost of misclassification from 10 to 100, you can see the difference in the classification line. We repeat the process of plotting the SVM using the \texttt{kernlab} package:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Fit Support Vector Machine model to data set}
\NormalTok{kernfit <-}\StringTok{ }\KeywordTok{ksvm}\NormalTok{(x,y, }\DataTypeTok{type =} \StringTok{"C-svc"}\NormalTok{, }\DataTypeTok{kernel =} \StringTok{'vanilladot'}\NormalTok{, }\DataTypeTok{C =} \DecValTok{100}\NormalTok{)}
\CommentTok{#>  Setting default kernel parameters}
\CommentTok{# Plot results}
\KeywordTok{plot}\NormalTok{(kernfit, }\DataTypeTok{data =}\NormalTok{ x)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_113-broad_view-SVM_files/figure-latex/unnamed-chunk-8-1} \end{center}

But how do we decide how costly these misclassifications actually are? Instead of specifying a cost up front, we can use the tune() function from e1071 to test various costs and identify which value produces the best fitting model.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# find optimal cost of misclassification}
\NormalTok{tune.out <-}\StringTok{ }\KeywordTok{tune}\NormalTok{(svm, y}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data =}\NormalTok{ dat, }\DataTypeTok{kernel =} \StringTok{"linear"}\NormalTok{,}
                 \DataTypeTok{ranges =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{cost =} \KeywordTok{c}\NormalTok{(}\FloatTok{0.001}\NormalTok{, }\FloatTok{0.01}\NormalTok{, }\FloatTok{0.1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{100}\NormalTok{)))}
\CommentTok{# extract the best model}
\NormalTok{(bestmod <-}\StringTok{ }\NormalTok{tune.out}\OperatorTok{$}\NormalTok{best.model)}
\CommentTok{#> }
\CommentTok{#> Call:}
\CommentTok{#> best.tune(method = svm, train.x = y ~ ., data = dat, ranges = list(cost = c(0.001, }
\CommentTok{#>     0.01, 0.1, 1, 5, 10, 100)), kernel = "linear")}
\CommentTok{#> }
\CommentTok{#> }
\CommentTok{#> Parameters:}
\CommentTok{#>    SVM-Type:  C-classification }
\CommentTok{#>  SVM-Kernel:  linear }
\CommentTok{#>        cost:  0.1 }
\CommentTok{#>       gamma:  0.5 }
\CommentTok{#> }
\CommentTok{#> Number of Support Vectors:  16}
\end{Highlighting}
\end{Shaded}

For our data set, the optimal cost (from amongst the choices we provided) is calculated to be 0.1, which doesn't penalize the model much for misclassified observations. Once this model has been identified, we can construct a table of predicted classes against true classes using the predict() command as follows:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Create a table of misclassified observations}
\NormalTok{ypred <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(bestmod, dat)}
\NormalTok{(misclass <-}\StringTok{ }\KeywordTok{table}\NormalTok{(}\DataTypeTok{predict =}\NormalTok{ ypred, }\DataTypeTok{truth =}\NormalTok{ dat}\OperatorTok{$}\NormalTok{y))}
\CommentTok{#>        truth}
\CommentTok{#> predict -1 1}
\CommentTok{#>      -1  9 3}
\CommentTok{#>      1   1 7}
\end{Highlighting}
\end{Shaded}

Using this support vector classifier, 80\% of the observations were correctly classified, which matches what we see in the plot. If we wanted to test our classifier more rigorously, we could split our data into training and testing sets and then see how our SVC performed with the observations not used to construct the model. We will use this training-testing method later in this tutorial to validate our SVMs.

\hypertarget{support-vector-machines}{%
\section{Support Vector Machines}\label{support-vector-machines}}

Support Vector Classifiers are a subset of the group of classification structures known as Support Vector Machines. Support Vector Machines can construct classification boundaries that are nonlinear in shape. The options for classification structures using the svm() command from the e1071 package are linear, polynomial, radial, and sigmoid. To demonstrate a nonlinear classification boundary, we will construct a new data set.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# construct larger random data set}
\NormalTok{x <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\KeywordTok{rnorm}\NormalTok{(}\DecValTok{200}\OperatorTok{*}\DecValTok{2}\NormalTok{), }\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{)}
\NormalTok{x[}\DecValTok{1}\OperatorTok{:}\DecValTok{100}\NormalTok{,] <-}\StringTok{ }\NormalTok{x[}\DecValTok{1}\OperatorTok{:}\DecValTok{100}\NormalTok{,] }\OperatorTok{+}\StringTok{ }\FloatTok{2.5}
\NormalTok{x[}\DecValTok{101}\OperatorTok{:}\DecValTok{150}\NormalTok{,] <-}\StringTok{ }\NormalTok{x[}\DecValTok{101}\OperatorTok{:}\DecValTok{150}\NormalTok{,] }\OperatorTok{-}\StringTok{ }\FloatTok{2.5}
\NormalTok{y <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\KeywordTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{150}\NormalTok{), }\KeywordTok{rep}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{50}\NormalTok{))}
\NormalTok{dat <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{x=}\NormalTok{x,}\DataTypeTok{y=}\KeywordTok{as.factor}\NormalTok{(y))}

\CommentTok{# Plot data}
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ dat, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ x}\FloatTok{.2}\NormalTok{, }\DataTypeTok{y =}\NormalTok{ x}\FloatTok{.1}\NormalTok{, }\DataTypeTok{color =}\NormalTok{ y, }\DataTypeTok{shape =}\NormalTok{ y)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{size =} \DecValTok{2}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_color_manual}\NormalTok{(}\DataTypeTok{values=}\KeywordTok{c}\NormalTok{(}\StringTok{"#000000"}\NormalTok{, }\StringTok{"#FF0000"}\NormalTok{)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{legend.position =} \StringTok{"none"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_113-broad_view-SVM_files/figure-latex/unnamed-chunk-11-1} \end{center}

Notice that the data is not linearly separable, and furthermore, isn't all clustered together in a single group. There are two sections of class 1 observations with a cluster of class 2 observations in between. To demonstrate the power of SVMs, we'll take 100 random observations from the set and use them to construct our boundary. We set kernel = ``radial'' based on the shape of our data and plot the results.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# set pseudorandom number generator}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\CommentTok{# sample training data and fit model}
\NormalTok{train <-}\StringTok{ }\NormalTok{base}\OperatorTok{::}\KeywordTok{sample}\NormalTok{(}\DecValTok{200}\NormalTok{,}\DecValTok{100}\NormalTok{, }\DataTypeTok{replace =} \OtherTok{FALSE}\NormalTok{)}
\NormalTok{svmfit <-}\StringTok{ }\KeywordTok{svm}\NormalTok{(y}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data =}\NormalTok{ dat[train,], }\DataTypeTok{kernel =} \StringTok{"radial"}\NormalTok{, }\DataTypeTok{gamma =} \DecValTok{1}\NormalTok{, }\DataTypeTok{cost =} \DecValTok{1}\NormalTok{)}
\CommentTok{# plot classifier}
\KeywordTok{plot}\NormalTok{(svmfit, dat)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_113-broad_view-SVM_files/figure-latex/unnamed-chunk-12-1} \end{center}

The same procedure can be run using the kernlab package, which has far more kernel options than the corresponding function in e1071. In addition to the four choices in e1071, this package allows use of a hyperbolic tangent, Laplacian, Bessel, Spline, String, or ANOVA RBF kernel. To fit this data, we set the cost to be the same as it was before, 1.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Fit radial-based SVM in kernlab}
\NormalTok{kernfit <-}\StringTok{ }\KeywordTok{ksvm}\NormalTok{(x[train,],y[train], }\DataTypeTok{type =} \StringTok{"C-svc"}\NormalTok{, }\DataTypeTok{kernel =} \StringTok{'rbfdot'}\NormalTok{, }\DataTypeTok{C =} \DecValTok{1}\NormalTok{, }\DataTypeTok{scaled =} \KeywordTok{c}\NormalTok{())}
\CommentTok{# Plot training data}
\KeywordTok{plot}\NormalTok{(kernfit, }\DataTypeTok{data =}\NormalTok{ x[train,])}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_113-broad_view-SVM_files/figure-latex/unnamed-chunk-13-1} \end{center}

We see that, at least visually, the SVM does a reasonable job of separating the two classes. To fit the model, we used \texttt{cost\ =\ 1}, but as mentioned previously, it isn't usually obvious which cost will produce the optimal classification boundary. We can use the tune() command to try several different values of cost as well as several different values of \(\gamma\), a scaling parameter used to fit nonlinear boundaries.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# tune model to find optimal cost, gamma values}
\NormalTok{tune.out <-}\StringTok{ }\KeywordTok{tune}\NormalTok{(svm, y}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data =}\NormalTok{ dat[train,], }\DataTypeTok{kernel =} \StringTok{"radial"}\NormalTok{,}
                 \DataTypeTok{ranges =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{cost =} \KeywordTok{c}\NormalTok{(}\FloatTok{0.1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{10}\NormalTok{,}\DecValTok{100}\NormalTok{,}\DecValTok{1000}\NormalTok{),}
                 \DataTypeTok{gamma =} \KeywordTok{c}\NormalTok{(}\FloatTok{0.5}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{)))}
\CommentTok{# show best model}
\NormalTok{tune.out}\OperatorTok{$}\NormalTok{best.model}
\CommentTok{#> }
\CommentTok{#> Call:}
\CommentTok{#> best.tune(method = svm, train.x = y ~ ., data = dat[train, ], }
\CommentTok{#>     ranges = list(cost = c(0.1, 1, 10, 100, 1000), gamma = c(0.5, }
\CommentTok{#>         1, 2, 3, 4)), kernel = "radial")}
\CommentTok{#> }
\CommentTok{#> }
\CommentTok{#> Parameters:}
\CommentTok{#>    SVM-Type:  C-classification }
\CommentTok{#>  SVM-Kernel:  radial }
\CommentTok{#>        cost:  1 }
\CommentTok{#>       gamma:  0.5 }
\CommentTok{#> }
\CommentTok{#> Number of Support Vectors:  30}
\end{Highlighting}
\end{Shaded}

The model that reduces the error the most in the training data uses a cost of 1 and \(\gamma\)
value of 0.5. We can now see how well the SVM performs by predicting the class of the 100 testing observations:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# validate model performance}
\NormalTok{(valid <-}\StringTok{ }\KeywordTok{table}\NormalTok{(}\DataTypeTok{true =}\NormalTok{ dat[}\OperatorTok{-}\NormalTok{train,}\StringTok{"y"}\NormalTok{], }\DataTypeTok{pred =} \KeywordTok{predict}\NormalTok{(tune.out}\OperatorTok{$}\NormalTok{best.model,}
                                             \DataTypeTok{newx =}\NormalTok{ dat[}\OperatorTok{-}\NormalTok{train,])))}
\CommentTok{#>     pred}
\CommentTok{#> true  1  2}
\CommentTok{#>    1 55 28}
\CommentTok{#>    2 12  5}
\CommentTok{##     pred}
\CommentTok{## true  1  2}
\CommentTok{##    1 58 19}
\CommentTok{##    2 16  7}
\end{Highlighting}
\end{Shaded}

Our best-fitting model produces 65\% accuracy in identifying classes. For such a complicated shape of observations, this performed reasonably well. We can challenge this method further by adding additional classes of observations.

\hypertarget{svms-for-multiple-classes}{%
\section{SVMs for Multiple Classes}\label{svms-for-multiple-classes}}

The procedure does not change for data sets that involve more than two classes of observations. We construct our data set the same way as we have previously, only now specifying three classes instead of two:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# construct data set}
\NormalTok{x <-}\StringTok{ }\KeywordTok{rbind}\NormalTok{(x, }\KeywordTok{matrix}\NormalTok{(}\KeywordTok{rnorm}\NormalTok{(}\DecValTok{50}\OperatorTok{*}\DecValTok{2}\NormalTok{), }\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{))}
\NormalTok{y <-}\StringTok{ }\KeywordTok{c}\NormalTok{(y, }\KeywordTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{50}\NormalTok{))}
\NormalTok{x[y}\OperatorTok{==}\DecValTok{0}\NormalTok{,}\DecValTok{2}\NormalTok{] <-}\StringTok{ }\NormalTok{x[y}\OperatorTok{==}\DecValTok{0}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OperatorTok{+}\StringTok{ }\FloatTok{2.5}
\NormalTok{dat <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{x=}\NormalTok{x, }\DataTypeTok{y=}\KeywordTok{as.factor}\NormalTok{(y))}
\CommentTok{# plot data set}
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ dat, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ x}\FloatTok{.2}\NormalTok{, }\DataTypeTok{y =}\NormalTok{ x}\FloatTok{.1}\NormalTok{, }\DataTypeTok{color =}\NormalTok{ y, }\DataTypeTok{shape =}\NormalTok{ y)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{size =} \DecValTok{2}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_color_manual}\NormalTok{(}\DataTypeTok{values=}\KeywordTok{c}\NormalTok{(}\StringTok{"#000000"}\NormalTok{,}\StringTok{"#FF0000"}\NormalTok{,}\StringTok{"#00BA00"}\NormalTok{)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{legend.position =} \StringTok{"none"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_113-broad_view-SVM_files/figure-latex/unnamed-chunk-16-1} \end{center}

The commands don't change for the e1071 package. We specify a cost and tuning parameter
\(\gamma\)
and fit a support vector machine. The results and interpretation are similar to two-class classification.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# fit model}
\NormalTok{svmfit <-}\StringTok{ }\KeywordTok{svm}\NormalTok{(y}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data =}\NormalTok{ dat, }\DataTypeTok{kernel =} \StringTok{"radial"}\NormalTok{, }\DataTypeTok{cost =} \DecValTok{10}\NormalTok{, }\DataTypeTok{gamma =} \DecValTok{1}\NormalTok{)}
\CommentTok{# plot results}
\KeywordTok{plot}\NormalTok{(svmfit, dat)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_113-broad_view-SVM_files/figure-latex/unnamed-chunk-17-1} \end{center}

We can check to see how well our model fit the data by using the \texttt{predict()} command, as follows:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#construct table}
\NormalTok{ypred <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(svmfit, dat)}
\NormalTok{(misclass <-}\StringTok{ }\KeywordTok{table}\NormalTok{(}\DataTypeTok{predict =}\NormalTok{ ypred, }\DataTypeTok{truth =}\NormalTok{ dat}\OperatorTok{$}\NormalTok{y))}
\CommentTok{#>        truth}
\CommentTok{#> predict   0   1   2}
\CommentTok{#>       0  38   2   5}
\CommentTok{#>       1   7 145   2}
\CommentTok{#>       2   5   3  43}
\CommentTok{##        truth}
\CommentTok{## predict   0   1   2}
\CommentTok{##       0  38   2   4}
\CommentTok{##       1   8 143   4}
\CommentTok{##       2   4   5  42}
\end{Highlighting}
\end{Shaded}

As shown in the resulting table, 89\% of our training observations were correctly classified. However, since we didn't break our data into training and testing sets, we didn't truly validate our results.

The kernlab package, on the other hand, can fit more than 2 classes, but cannot plot the results. To visualize the results of the ksvm function, we take the steps listed below to create a grid of points, predict the value of each point, and plot the results:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# fit and plot}
\NormalTok{kernfit <-}\StringTok{ }\KeywordTok{ksvm}\NormalTok{(}\KeywordTok{as.matrix}\NormalTok{(dat[,}\DecValTok{2}\OperatorTok{:}\DecValTok{1}\NormalTok{]),dat}\OperatorTok{$}\NormalTok{y, }\DataTypeTok{type =} \StringTok{"C-svc"}\NormalTok{, }\DataTypeTok{kernel =} \StringTok{'rbfdot'}\NormalTok{, }
                \DataTypeTok{C =} \DecValTok{100}\NormalTok{, }\DataTypeTok{scaled =} \KeywordTok{c}\NormalTok{())}

\CommentTok{# Create a fine grid of the feature space}
\NormalTok{x}\FloatTok{.1}\NormalTok{ <-}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\DataTypeTok{from =} \KeywordTok{min}\NormalTok{(dat}\OperatorTok{$}\NormalTok{x}\FloatTok{.1}\NormalTok{), }\DataTypeTok{to =} \KeywordTok{max}\NormalTok{(dat}\OperatorTok{$}\NormalTok{x}\FloatTok{.1}\NormalTok{), }\DataTypeTok{length =} \DecValTok{100}\NormalTok{)}
\NormalTok{x}\FloatTok{.2}\NormalTok{ <-}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\DataTypeTok{from =} \KeywordTok{min}\NormalTok{(dat}\OperatorTok{$}\NormalTok{x}\FloatTok{.2}\NormalTok{), }\DataTypeTok{to =} \KeywordTok{max}\NormalTok{(dat}\OperatorTok{$}\NormalTok{x}\FloatTok{.2}\NormalTok{), }\DataTypeTok{length =} \DecValTok{100}\NormalTok{)}
\NormalTok{x.grid <-}\StringTok{ }\KeywordTok{expand.grid}\NormalTok{(x}\FloatTok{.2}\NormalTok{, x}\FloatTok{.1}\NormalTok{)}

\CommentTok{# Get class predictions over grid}
\NormalTok{pred <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(kernfit, }\DataTypeTok{newdata =}\NormalTok{ x.grid)}

\CommentTok{# Plot the results}
\NormalTok{cols <-}\StringTok{ }\KeywordTok{brewer.pal}\NormalTok{(}\DecValTok{3}\NormalTok{, }\StringTok{"Set1"}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(x.grid, }\DataTypeTok{pch =} \DecValTok{19}\NormalTok{, }\DataTypeTok{col =} \KeywordTok{adjustcolor}\NormalTok{(cols[pred], }\DataTypeTok{alpha.f =} \FloatTok{0.05}\NormalTok{))}

\NormalTok{classes <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(pred, }\DataTypeTok{nrow =} \DecValTok{100}\NormalTok{, }\DataTypeTok{ncol =} \DecValTok{100}\NormalTok{)}
\KeywordTok{contour}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ x}\FloatTok{.2}\NormalTok{, }\DataTypeTok{y =}\NormalTok{ x}\FloatTok{.1}\NormalTok{, }\DataTypeTok{z =}\NormalTok{ classes, }\DataTypeTok{levels =} \DecValTok{1}\OperatorTok{:}\DecValTok{3}\NormalTok{, }\DataTypeTok{labels =} \StringTok{""}\NormalTok{, }\DataTypeTok{add =} \OtherTok{TRUE}\NormalTok{)}

\KeywordTok{points}\NormalTok{(dat[, }\DecValTok{2}\OperatorTok{:}\DecValTok{1}\NormalTok{], }\DataTypeTok{pch =} \DecValTok{19}\NormalTok{, }\DataTypeTok{col =}\NormalTok{ cols[}\KeywordTok{predict}\NormalTok{(kernfit)])}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_113-broad_view-SVM_files/figure-latex/unnamed-chunk-19-1} \end{center}

\hypertarget{application}{%
\section{Application}\label{application}}

The Khan data set contains data on 83 tissue samples with 2308 gene expression measurements on each sample. These were split into 63 training observations and 20 testing observations, and there are four distinct classes in the set. It would be impossible to visualize such data, so we choose the simplest classifier (linear) to construct our model. We will use the svm command from \texttt{e1071} to conduct our analysis.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# fit model}
\NormalTok{dat <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Khan}\OperatorTok{$}\NormalTok{xtrain, }\DataTypeTok{y=}\KeywordTok{as.factor}\NormalTok{(Khan}\OperatorTok{$}\NormalTok{ytrain))}
\NormalTok{(out <-}\StringTok{ }\KeywordTok{svm}\NormalTok{(y}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data =}\NormalTok{ dat, }\DataTypeTok{kernel =} \StringTok{"linear"}\NormalTok{, }\DataTypeTok{cost=}\DecValTok{10}\NormalTok{))}
\CommentTok{#> }
\CommentTok{#> Call:}
\CommentTok{#> svm(formula = y ~ ., data = dat, kernel = "linear", cost = 10)}
\CommentTok{#> }
\CommentTok{#> }
\CommentTok{#> Parameters:}
\CommentTok{#>    SVM-Type:  C-classification }
\CommentTok{#>  SVM-Kernel:  linear }
\CommentTok{#>        cost:  10 }
\CommentTok{#>       gamma:  0.000433 }
\CommentTok{#> }
\CommentTok{#> Number of Support Vectors:  58}
\end{Highlighting}
\end{Shaded}

First of all, we can check how well our model did at classifying the training observations. This is usually high, but again, doesn't validate the model. If the model doesn't do a very good job of classifying the training set, it could be a red flag. In our case, all 63 training observations were correctly classified.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# check model performance on training set}
\KeywordTok{table}\NormalTok{(out}\OperatorTok{$}\NormalTok{fitted, dat}\OperatorTok{$}\NormalTok{y)}
\CommentTok{#>    }
\CommentTok{#>      1  2  3  4}
\CommentTok{#>   1  8  0  0  0}
\CommentTok{#>   2  0 23  0  0}
\CommentTok{#>   3  0  0 12  0}
\CommentTok{#>   4  0  0  0 20}
\end{Highlighting}
\end{Shaded}

To perform validation, we can check how the model performs on the testing set:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# validate model performance}
\NormalTok{dat.te <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{x=}\NormalTok{Khan}\OperatorTok{$}\NormalTok{xtest, }\DataTypeTok{y=}\KeywordTok{as.factor}\NormalTok{(Khan}\OperatorTok{$}\NormalTok{ytest))}
\NormalTok{pred.te <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(out, }\DataTypeTok{newdata=}\NormalTok{dat.te)}
\KeywordTok{table}\NormalTok{(pred.te, dat.te}\OperatorTok{$}\NormalTok{y)}
\CommentTok{#>        }
\CommentTok{#> pred.te 1 2 3 4}
\CommentTok{#>       1 3 0 0 0}
\CommentTok{#>       2 0 6 2 0}
\CommentTok{#>       3 0 0 4 0}
\CommentTok{#>       4 0 0 0 5}
\end{Highlighting}
\end{Shaded}

The model correctly identifies 18 of the 20 testing observations. SVMs and the boundaries they impose are more difficult to interpret at higher dimensions, but these results seem to suggest that our model is a good classifier for the gene data.

\hypertarget{sonar-standalone-model-with-random-forest}{%
\chapter{Sonar Standalone Model with Random Forest}\label{sonar-standalone-model-with-random-forest}}

\textbf{Classification problem}

\hypertarget{introduction-3}{%
\section{Introduction}\label{introduction-3}}

\begin{itemize}
\tightlist
\item
  \texttt{mtry}: Number of variables randomly sampled as candidates at each split.
\item
  \texttt{ntree}: Number of trees to grow.
\end{itemize}

\hypertarget{load-libraries-1}{%
\section{Load libraries}\label{load-libraries-1}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load packages}
\KeywordTok{library}\NormalTok{(caret)}
\KeywordTok{library}\NormalTok{(mlbench)}
\KeywordTok{library}\NormalTok{(randomForest)}
\KeywordTok{library}\NormalTok{(tictoc)}

\CommentTok{# load dataset}
\KeywordTok{data}\NormalTok{(Sonar)}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{7}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{explore-data}{%
\section{Explore data}\label{explore-data}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{glimpse}\NormalTok{(Sonar)}
\CommentTok{#> Observations: 208}
\CommentTok{#> Variables: 61}
\CommentTok{#> $ V1    <dbl> 0.0200, 0.0453, 0.0262, 0.0100, 0.0762, 0.0286, 0.0317, ...}
\CommentTok{#> $ V2    <dbl> 0.0371, 0.0523, 0.0582, 0.0171, 0.0666, 0.0453, 0.0956, ...}
\CommentTok{#> $ V3    <dbl> 0.0428, 0.0843, 0.1099, 0.0623, 0.0481, 0.0277, 0.1321, ...}
\CommentTok{#> $ V4    <dbl> 0.0207, 0.0689, 0.1083, 0.0205, 0.0394, 0.0174, 0.1408, ...}
\CommentTok{#> $ V5    <dbl> 0.0954, 0.1183, 0.0974, 0.0205, 0.0590, 0.0384, 0.1674, ...}
\CommentTok{#> $ V6    <dbl> 0.0986, 0.2583, 0.2280, 0.0368, 0.0649, 0.0990, 0.1710, ...}
\CommentTok{#> $ V7    <dbl> 0.1539, 0.2156, 0.2431, 0.1098, 0.1209, 0.1201, 0.0731, ...}
\CommentTok{#> $ V8    <dbl> 0.1601, 0.3481, 0.3771, 0.1276, 0.2467, 0.1833, 0.1401, ...}
\CommentTok{#> $ V9    <dbl> 0.3109, 0.3337, 0.5598, 0.0598, 0.3564, 0.2105, 0.2083, ...}
\CommentTok{#> $ V10   <dbl> 0.2111, 0.2872, 0.6194, 0.1264, 0.4459, 0.3039, 0.3513, ...}
\CommentTok{#> $ V11   <dbl> 0.1609, 0.4918, 0.6333, 0.0881, 0.4152, 0.2988, 0.1786, ...}
\CommentTok{#> $ V12   <dbl> 0.1582, 0.6552, 0.7060, 0.1992, 0.3952, 0.4250, 0.0658, ...}
\CommentTok{#> $ V13   <dbl> 0.2238, 0.6919, 0.5544, 0.0184, 0.4256, 0.6343, 0.0513, ...}
\CommentTok{#> $ V14   <dbl> 0.0645, 0.7797, 0.5320, 0.2261, 0.4135, 0.8198, 0.3752, ...}
\CommentTok{#> $ V15   <dbl> 0.0660, 0.7464, 0.6479, 0.1729, 0.4528, 1.0000, 0.5419, ...}
\CommentTok{#> $ V16   <dbl> 0.2273, 0.9444, 0.6931, 0.2131, 0.5326, 0.9988, 0.5440, ...}
\CommentTok{#> $ V17   <dbl> 0.3100, 1.0000, 0.6759, 0.0693, 0.7306, 0.9508, 0.5150, ...}
\CommentTok{#> $ V18   <dbl> 0.300, 0.887, 0.755, 0.228, 0.619, 0.902, 0.426, 0.120, ...}
\CommentTok{#> $ V19   <dbl> 0.508, 0.802, 0.893, 0.406, 0.203, 0.723, 0.202, 0.668, ...}
\CommentTok{#> $ V20   <dbl> 0.4797, 0.7818, 0.8619, 0.3973, 0.4636, 0.5122, 0.4233, ...}
\CommentTok{#> $ V21   <dbl> 0.578, 0.521, 0.797, 0.274, 0.415, 0.207, 0.772, 0.783, ...}
\CommentTok{#> $ V22   <dbl> 0.507, 0.405, 0.674, 0.369, 0.429, 0.399, 0.974, 0.535, ...}
\CommentTok{#> $ V23   <dbl> 0.433, 0.396, 0.429, 0.556, 0.573, 0.589, 0.939, 0.681, ...}
\CommentTok{#> $ V24   <dbl> 0.555, 0.391, 0.365, 0.485, 0.540, 0.287, 0.556, 0.917, ...}
\CommentTok{#> $ V25   <dbl> 0.671, 0.325, 0.533, 0.314, 0.316, 0.204, 0.527, 0.761, ...}
\CommentTok{#> $ V26   <dbl> 0.641, 0.320, 0.241, 0.533, 0.229, 0.578, 0.683, 0.822, ...}
\CommentTok{#> $ V27   <dbl> 0.7104, 0.3271, 0.5070, 0.5256, 0.6995, 0.5389, 0.5713, ...}
\CommentTok{#> $ V28   <dbl> 0.8080, 0.2767, 0.8533, 0.2520, 1.0000, 0.3750, 0.5429, ...}
\CommentTok{#> $ V29   <dbl> 0.6791, 0.4423, 0.6036, 0.2090, 0.7262, 0.3411, 0.2177, ...}
\CommentTok{#> $ V30   <dbl> 0.3857, 0.2028, 0.8514, 0.3559, 0.4724, 0.5067, 0.2149, ...}
\CommentTok{#> $ V31   <dbl> 0.131, 0.379, 0.851, 0.626, 0.510, 0.558, 0.581, 0.132, ...}
\CommentTok{#> $ V32   <dbl> 0.2604, 0.2947, 0.5045, 0.7340, 0.5459, 0.4778, 0.6323, ...}
\CommentTok{#> $ V33   <dbl> 0.512, 0.198, 0.186, 0.612, 0.288, 0.330, 0.296, 0.099, ...}
\CommentTok{#> $ V34   <dbl> 0.7547, 0.2341, 0.2709, 0.3497, 0.0981, 0.2198, 0.1873, ...}
\CommentTok{#> $ V35   <dbl> 0.8537, 0.1306, 0.4232, 0.3953, 0.1951, 0.1407, 0.2969, ...}
\CommentTok{#> $ V36   <dbl> 0.851, 0.418, 0.304, 0.301, 0.418, 0.286, 0.516, 0.105, ...}
\CommentTok{#> $ V37   <dbl> 0.669, 0.384, 0.612, 0.541, 0.460, 0.381, 0.615, 0.192, ...}
\CommentTok{#> $ V38   <dbl> 0.6097, 0.1057, 0.6756, 0.8814, 0.3217, 0.4158, 0.4283, ...}
\CommentTok{#> $ V39   <dbl> 0.4943, 0.1840, 0.5375, 0.9857, 0.2828, 0.4054, 0.5479, ...}
\CommentTok{#> $ V40   <dbl> 0.2744, 0.1970, 0.4719, 0.9167, 0.2430, 0.3296, 0.6133, ...}
\CommentTok{#> $ V41   <dbl> 0.0510, 0.1674, 0.4647, 0.6121, 0.1979, 0.2707, 0.5017, ...}
\CommentTok{#> $ V42   <dbl> 0.2834, 0.0583, 0.2587, 0.5006, 0.2444, 0.2650, 0.2377, ...}
\CommentTok{#> $ V43   <dbl> 0.2825, 0.1401, 0.2129, 0.3210, 0.1847, 0.0723, 0.1957, ...}
\CommentTok{#> $ V44   <dbl> 0.4256, 0.1628, 0.2222, 0.3202, 0.0841, 0.1238, 0.1749, ...}
\CommentTok{#> $ V45   <dbl> 0.2641, 0.0621, 0.2111, 0.4295, 0.0692, 0.1192, 0.1304, ...}
\CommentTok{#> $ V46   <dbl> 0.1386, 0.0203, 0.0176, 0.3654, 0.0528, 0.1089, 0.0597, ...}
\CommentTok{#> $ V47   <dbl> 0.1051, 0.0530, 0.1348, 0.2655, 0.0357, 0.0623, 0.1124, ...}
\CommentTok{#> $ V48   <dbl> 0.1343, 0.0742, 0.0744, 0.1576, 0.0085, 0.0494, 0.1047, ...}
\CommentTok{#> $ V49   <dbl> 0.0383, 0.0409, 0.0130, 0.0681, 0.0230, 0.0264, 0.0507, ...}
\CommentTok{#> $ V50   <dbl> 0.0324, 0.0061, 0.0106, 0.0294, 0.0046, 0.0081, 0.0159, ...}
\CommentTok{#> $ V51   <dbl> 0.0232, 0.0125, 0.0033, 0.0241, 0.0156, 0.0104, 0.0195, ...}
\CommentTok{#> $ V52   <dbl> 0.0027, 0.0084, 0.0232, 0.0121, 0.0031, 0.0045, 0.0201, ...}
\CommentTok{#> $ V53   <dbl> 0.0065, 0.0089, 0.0166, 0.0036, 0.0054, 0.0014, 0.0248, ...}
\CommentTok{#> $ V54   <dbl> 0.0159, 0.0048, 0.0095, 0.0150, 0.0105, 0.0038, 0.0131, ...}
\CommentTok{#> $ V55   <dbl> 0.0072, 0.0094, 0.0180, 0.0085, 0.0110, 0.0013, 0.0070, ...}
\CommentTok{#> $ V56   <dbl> 0.0167, 0.0191, 0.0244, 0.0073, 0.0015, 0.0089, 0.0138, ...}
\CommentTok{#> $ V57   <dbl> 0.0180, 0.0140, 0.0316, 0.0050, 0.0072, 0.0057, 0.0092, ...}
\CommentTok{#> $ V58   <dbl> 0.0084, 0.0049, 0.0164, 0.0044, 0.0048, 0.0027, 0.0143, ...}
\CommentTok{#> $ V59   <dbl> 0.0090, 0.0052, 0.0095, 0.0040, 0.0107, 0.0051, 0.0036, ...}
\CommentTok{#> $ V60   <dbl> 0.0032, 0.0044, 0.0078, 0.0117, 0.0094, 0.0062, 0.0103, ...}
\CommentTok{#> $ Class <fct> R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R,...}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tibble}\OperatorTok{::}\KeywordTok{as_tibble}\NormalTok{(Sonar)}
\CommentTok{#> # A tibble: 208 x 61}
\CommentTok{#>       V1     V2     V3     V4     V5     V6    V7    V8     V9   V10    V11}
\CommentTok{#>    <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl> <dbl> <dbl>  <dbl> <dbl>  <dbl>}
\CommentTok{#> 1 0.02   0.0371 0.0428 0.0207 0.0954 0.0986 0.154 0.160 0.311  0.211 0.161 }
\CommentTok{#> 2 0.0453 0.0523 0.0843 0.0689 0.118  0.258  0.216 0.348 0.334  0.287 0.492 }
\CommentTok{#> 3 0.0262 0.0582 0.110  0.108  0.0974 0.228  0.243 0.377 0.560  0.619 0.633 }
\CommentTok{#> 4 0.01   0.0171 0.0623 0.0205 0.0205 0.0368 0.110 0.128 0.0598 0.126 0.0881}
\CommentTok{#> 5 0.0762 0.0666 0.0481 0.0394 0.059  0.0649 0.121 0.247 0.356  0.446 0.415 }
\CommentTok{#> 6 0.0286 0.0453 0.0277 0.0174 0.0384 0.099  0.120 0.183 0.210  0.304 0.299 }
\CommentTok{#> # ... with 202 more rows, and 50 more variables: V12 <dbl>, V13 <dbl>,}
\CommentTok{#> #   V14 <dbl>, V15 <dbl>, V16 <dbl>, V17 <dbl>, V18 <dbl>, V19 <dbl>,}
\CommentTok{#> #   V20 <dbl>, V21 <dbl>, V22 <dbl>, V23 <dbl>, V24 <dbl>, V25 <dbl>,}
\CommentTok{#> #   V26 <dbl>, V27 <dbl>, V28 <dbl>, V29 <dbl>, V30 <dbl>, V31 <dbl>,}
\CommentTok{#> #   V32 <dbl>, V33 <dbl>, V34 <dbl>, V35 <dbl>, V36 <dbl>, V37 <dbl>,}
\CommentTok{#> #   V38 <dbl>, V39 <dbl>, V40 <dbl>, V41 <dbl>, V42 <dbl>, V43 <dbl>,}
\CommentTok{#> #   V44 <dbl>, V45 <dbl>, V46 <dbl>, V47 <dbl>, V48 <dbl>, V49 <dbl>,}
\CommentTok{#> #   V50 <dbl>, V51 <dbl>, V52 <dbl>, V53 <dbl>, V54 <dbl>, V55 <dbl>,}
\CommentTok{#> #   V56 <dbl>, V57 <dbl>, V58 <dbl>, V59 <dbl>, V60 <dbl>, Class <fct>}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# create 80%/20% for training and validation datasets}
\NormalTok{validationIndex <-}\StringTok{ }\KeywordTok{createDataPartition}\NormalTok{(Sonar}\OperatorTok{$}\NormalTok{Class, }\DataTypeTok{p=}\FloatTok{0.80}\NormalTok{, }\DataTypeTok{list=}\OtherTok{FALSE}\NormalTok{)}
\NormalTok{validation <-}\StringTok{ }\NormalTok{Sonar[}\OperatorTok{-}\NormalTok{validationIndex,]}
\NormalTok{training   <-}\StringTok{ }\NormalTok{Sonar[validationIndex,]}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{tic}\NormalTok{()}
\CommentTok{# train a model and summarize model}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{7}\NormalTok{)}
\NormalTok{trainControl <-}\StringTok{ }\KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method=}\StringTok{"repeatedcv"}\NormalTok{, }\DataTypeTok{number=}\DecValTok{10}\NormalTok{, }\DataTypeTok{repeats=}\DecValTok{3}\NormalTok{)}
\NormalTok{fit.rf <-}\StringTok{ }\KeywordTok{train}\NormalTok{(Class}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{training, }
                \DataTypeTok{method =} \StringTok{"rf"}\NormalTok{, }
                \DataTypeTok{metric =} \StringTok{"Accuracy"}\NormalTok{, }
                \DataTypeTok{trControl =}\NormalTok{ trainControl, }
                \DataTypeTok{ntree =} \DecValTok{2000}\NormalTok{)}
\KeywordTok{toc}\NormalTok{()}
\CommentTok{#> 118.619 sec elapsed}
\KeywordTok{print}\NormalTok{(fit.rf)}
\CommentTok{#> Random Forest }
\CommentTok{#> }
\CommentTok{#> 167 samples}
\CommentTok{#>  60 predictor}
\CommentTok{#>   2 classes: 'M', 'R' }
\CommentTok{#> }
\CommentTok{#> No pre-processing}
\CommentTok{#> Resampling: Cross-Validated (10 fold, repeated 3 times) }
\CommentTok{#> Summary of sample sizes: 150, 150, 150, 151, 151, 150, ... }
\CommentTok{#> Resampling results across tuning parameters:}
\CommentTok{#> }
\CommentTok{#>   mtry  Accuracy  Kappa}
\CommentTok{#>    2    0.845     0.682}
\CommentTok{#>   31    0.828     0.651}
\CommentTok{#>   60    0.808     0.611}
\CommentTok{#> }
\CommentTok{#> Accuracy was used to select the optimal model using the largest value.}
\CommentTok{#> The final value used for the model was mtry = 2.}
\KeywordTok{print}\NormalTok{(fit.rf}\OperatorTok{$}\NormalTok{finalModel)}
\CommentTok{#> }
\CommentTok{#> Call:}
\CommentTok{#>  randomForest(x = x, y = y, ntree = 2000, mtry = param$mtry) }
\CommentTok{#>                Type of random forest: classification}
\CommentTok{#>                      Number of trees: 2000}
\CommentTok{#> No. of variables tried at each split: 2}
\CommentTok{#> }
\CommentTok{#>         OOB estimate of  error rate: 14.4%}
\CommentTok{#> Confusion matrix:}
\CommentTok{#>    M  R class.error}
\CommentTok{#> M 84  5      0.0562}
\CommentTok{#> R 19 59      0.2436}
\end{Highlighting}
\end{Shaded}

\begin{quote}
Accuracy: 85.26\% at mtry=2
\end{quote}

\hypertarget{apply-tuning-parameters-for-final-model}{%
\section{Apply tuning parameters for final model}\label{apply-tuning-parameters-for-final-model}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# create standalone model using all training data}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{7}\NormalTok{)}
\NormalTok{finalModel <-}\StringTok{ }\KeywordTok{randomForest}\NormalTok{(Class}\OperatorTok{~}\NormalTok{., training, }\DataTypeTok{mtry=}\DecValTok{2}\NormalTok{, }\DataTypeTok{ntree=}\DecValTok{2000}\NormalTok{)}

\CommentTok{# make a predictions on "new data" using the final model}
\NormalTok{finalPredictions <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(finalModel, validation[,}\DecValTok{1}\OperatorTok{:}\DecValTok{60}\NormalTok{])}
\KeywordTok{confusionMatrix}\NormalTok{(finalPredictions, validation}\OperatorTok{$}\NormalTok{Class)}
\CommentTok{#> Confusion Matrix and Statistics}
\CommentTok{#> }
\CommentTok{#>           Reference}
\CommentTok{#> Prediction  M  R}
\CommentTok{#>          M 20  4}
\CommentTok{#>          R  2 15}
\CommentTok{#>                                         }
\CommentTok{#>                Accuracy : 0.854         }
\CommentTok{#>                  95% CI : (0.708, 0.944)}
\CommentTok{#>     No Information Rate : 0.537         }
\CommentTok{#>     P-Value [Acc > NIR] : 1.88e-05      }
\CommentTok{#>                                         }
\CommentTok{#>                   Kappa : 0.704         }
\CommentTok{#>                                         }
\CommentTok{#>  Mcnemar's Test P-Value : 0.683         }
\CommentTok{#>                                         }
\CommentTok{#>             Sensitivity : 0.909         }
\CommentTok{#>             Specificity : 0.789         }
\CommentTok{#>          Pos Pred Value : 0.833         }
\CommentTok{#>          Neg Pred Value : 0.882         }
\CommentTok{#>              Prevalence : 0.537         }
\CommentTok{#>          Detection Rate : 0.488         }
\CommentTok{#>    Detection Prevalence : 0.585         }
\CommentTok{#>       Balanced Accuracy : 0.849         }
\CommentTok{#>                                         }
\CommentTok{#>        'Positive' Class : M             }
\CommentTok{#> }
\end{Highlighting}
\end{Shaded}

\begin{quote}
Accuracy: 82.93\%
\end{quote}

\hypertarget{save-model}{%
\section{Save model}\label{save-model}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# save the model to disk}
\KeywordTok{saveRDS}\NormalTok{(finalModel, }\KeywordTok{file.path}\NormalTok{(model_out_dir, }\StringTok{"sonar-finalModel.rds"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\hypertarget{use-the-saved-model}{%
\section{Use the saved model}\label{use-the-saved-model}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load the model}
\NormalTok{superModel <-}\StringTok{ }\KeywordTok{readRDS}\NormalTok{(}\KeywordTok{file.path}\NormalTok{(model_out_dir, }\StringTok{"sonar-finalModel.rds"}\NormalTok{))}
\KeywordTok{print}\NormalTok{(superModel)}
\CommentTok{#> }
\CommentTok{#> Call:}
\CommentTok{#>  randomForest(formula = Class ~ ., data = training, mtry = 2,      ntree = 2000) }
\CommentTok{#>                Type of random forest: classification}
\CommentTok{#>                      Number of trees: 2000}
\CommentTok{#> No. of variables tried at each split: 2}
\CommentTok{#> }
\CommentTok{#>         OOB estimate of  error rate: 16.2%}
\CommentTok{#> Confusion matrix:}
\CommentTok{#>    M  R class.error}
\CommentTok{#> M 81  8      0.0899}
\CommentTok{#> R 19 59      0.2436}
\end{Highlighting}
\end{Shaded}

\hypertarget{make-prediction-with-new-data}{%
\section{Make prediction with new data}\label{make-prediction-with-new-data}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# make a predictions on "new data" using the final model}
\NormalTok{finalPredictions <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(superModel, validation[,}\DecValTok{1}\OperatorTok{:}\DecValTok{60}\NormalTok{])}
\KeywordTok{confusionMatrix}\NormalTok{(finalPredictions, validation}\OperatorTok{$}\NormalTok{Class)}
\CommentTok{#> Confusion Matrix and Statistics}
\CommentTok{#> }
\CommentTok{#>           Reference}
\CommentTok{#> Prediction  M  R}
\CommentTok{#>          M 20  4}
\CommentTok{#>          R  2 15}
\CommentTok{#>                                         }
\CommentTok{#>                Accuracy : 0.854         }
\CommentTok{#>                  95% CI : (0.708, 0.944)}
\CommentTok{#>     No Information Rate : 0.537         }
\CommentTok{#>     P-Value [Acc > NIR] : 1.88e-05      }
\CommentTok{#>                                         }
\CommentTok{#>                   Kappa : 0.704         }
\CommentTok{#>                                         }
\CommentTok{#>  Mcnemar's Test P-Value : 0.683         }
\CommentTok{#>                                         }
\CommentTok{#>             Sensitivity : 0.909         }
\CommentTok{#>             Specificity : 0.789         }
\CommentTok{#>          Pos Pred Value : 0.833         }
\CommentTok{#>          Neg Pred Value : 0.882         }
\CommentTok{#>              Prevalence : 0.537         }
\CommentTok{#>          Detection Rate : 0.488         }
\CommentTok{#>    Detection Prevalence : 0.585         }
\CommentTok{#>       Balanced Accuracy : 0.849         }
\CommentTok{#>                                         }
\CommentTok{#>        'Positive' Class : M             }
\CommentTok{#> }
\end{Highlighting}
\end{Shaded}

\hypertarget{glass-classification}{%
\chapter{Glass classification}\label{glass-classification}}

\url{https://cran.r-project.org/web/packages/e1071/vignettes/svmdoc.pdf}

In this example, we use the glass data from the UCI Repository of Machine
Learning Databases for classification. The task is to predict the type of a glass
on basis of its chemical analysis. We start by splitting the data into a train and
test set:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(caret)}
\CommentTok{#> Loading required package: lattice}
\CommentTok{#> Loading required package: ggplot2}
\CommentTok{#> Registered S3 methods overwritten by 'ggplot2':}
\CommentTok{#>   method         from }
\CommentTok{#>   [.quosures     rlang}
\CommentTok{#>   c.quosures     rlang}
\CommentTok{#>   print.quosures rlang}
\KeywordTok{library}\NormalTok{(e1071)}
\KeywordTok{library}\NormalTok{(rpart)}

\KeywordTok{data}\NormalTok{(Glass, }\DataTypeTok{package=}\StringTok{"mlbench"}\NormalTok{)}
\KeywordTok{str}\NormalTok{(Glass)}
\CommentTok{#> 'data.frame':    214 obs. of  10 variables:}
\CommentTok{#>  $ RI  : num  1.52 1.52 1.52 1.52 1.52 ...}
\CommentTok{#>  $ Na  : num  13.6 13.9 13.5 13.2 13.3 ...}
\CommentTok{#>  $ Mg  : num  4.49 3.6 3.55 3.69 3.62 3.61 3.6 3.61 3.58 3.6 ...}
\CommentTok{#>  $ Al  : num  1.1 1.36 1.54 1.29 1.24 1.62 1.14 1.05 1.37 1.36 ...}
\CommentTok{#>  $ Si  : num  71.8 72.7 73 72.6 73.1 ...}
\CommentTok{#>  $ K   : num  0.06 0.48 0.39 0.57 0.55 0.64 0.58 0.57 0.56 0.57 ...}
\CommentTok{#>  $ Ca  : num  8.75 7.83 7.78 8.22 8.07 8.07 8.17 8.24 8.3 8.4 ...}
\CommentTok{#>  $ Ba  : num  0 0 0 0 0 0 0 0 0 0 ...}
\CommentTok{#>  $ Fe  : num  0 0 0 0 0 0.26 0 0 0 0.11 ...}
\CommentTok{#>  $ Type: Factor w/ 6 levels "1","2","3","5",..: 1 1 1 1 1 1 1 1 1 1 ...}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## split data into a train and test set}
\NormalTok{index <-}\StringTok{ }\DecValTok{1}\OperatorTok{:}\KeywordTok{nrow}\NormalTok{(Glass)}
\NormalTok{testindex <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(index, }\KeywordTok{trunc}\NormalTok{(}\KeywordTok{length}\NormalTok{(index)}\OperatorTok{/}\DecValTok{3}\NormalTok{))}
\NormalTok{testset  <-}\StringTok{ }\NormalTok{Glass[testindex,]}
\NormalTok{trainset <-}\StringTok{ }\NormalTok{Glass[}\OperatorTok{-}\NormalTok{testindex,]}
\end{Highlighting}
\end{Shaded}

Both for the SVM and the partitioning tree (via \texttt{rpart()}), we fit the model and
try to predict the test set values:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## svm}
\NormalTok{svm.model <-}\StringTok{ }\KeywordTok{svm}\NormalTok{(Type }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ trainset, }\DataTypeTok{cost =} \DecValTok{100}\NormalTok{, }\DataTypeTok{gamma =} \DecValTok{1}\NormalTok{)}
\NormalTok{svm.pred  <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(svm.model, testset[,}\OperatorTok{-}\DecValTok{10}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

(The dependent variable, Type, has column number 10. cost is a general penalizing
parameter for C-classification and \texttt{gamma} is the radial basis function-specific
\texttt{kernel} parameter.)

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## rpart}
\NormalTok{rpart.model <-}\StringTok{ }\KeywordTok{rpart}\NormalTok{(Type }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ trainset)}
\NormalTok{rpart.pred <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(rpart.model, testset[,}\OperatorTok{-}\DecValTok{10}\NormalTok{], }\DataTypeTok{type =} \StringTok{"class"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

A cross-tabulation of the true versus the predicted values yields:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## compute svm confusion matrix}
\KeywordTok{table}\NormalTok{(}\DataTypeTok{pred =}\NormalTok{ svm.pred, }\DataTypeTok{true =}\NormalTok{ testset[,}\DecValTok{10}\NormalTok{])}
\CommentTok{#>     true}
\CommentTok{#> pred  1  2  3  5  6  7}
\CommentTok{#>    1 20  3  3  0  0  0}
\CommentTok{#>    2  6 13  5  4  2  4}
\CommentTok{#>    3  2  1  0  0  0  0}
\CommentTok{#>    5  0  0  0  1  0  0}
\CommentTok{#>    6  0  0  0  0  0  0}
\CommentTok{#>    7  0  0  0  0  0  7}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## compute rpart confusion matrix}
\KeywordTok{table}\NormalTok{(}\DataTypeTok{pred =}\NormalTok{ rpart.pred, }\DataTypeTok{true =}\NormalTok{ testset[,}\DecValTok{10}\NormalTok{])}
\CommentTok{#>     true}
\CommentTok{#> pred  1  2  3  5  6  7}
\CommentTok{#>    1 22  0  3  0  0  0}
\CommentTok{#>    2  5 12  4  0  0  0}
\CommentTok{#>    3  0  2  1  0  0  0}
\CommentTok{#>    5  0  2  0  5  2  1}
\CommentTok{#>    6  0  0  0  0  0  0}
\CommentTok{#>    7  1  1  0  0  0 10}
\end{Highlighting}
\end{Shaded}

\hypertarget{comparison-test-sets}{%
\subsection{Comparison test sets}\label{comparison-test-sets}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{confusionMatrix}\NormalTok{(svm.pred, testset}\OperatorTok{$}\NormalTok{Type)}
\CommentTok{#> Confusion Matrix and Statistics}
\CommentTok{#> }
\CommentTok{#>           Reference}
\CommentTok{#> Prediction  1  2  3  5  6  7}
\CommentTok{#>          1 20  3  3  0  0  0}
\CommentTok{#>          2  6 13  5  4  2  4}
\CommentTok{#>          3  2  1  0  0  0  0}
\CommentTok{#>          5  0  0  0  1  0  0}
\CommentTok{#>          6  0  0  0  0  0  0}
\CommentTok{#>          7  0  0  0  0  0  7}
\CommentTok{#> }
\CommentTok{#> Overall Statistics}
\CommentTok{#>                                         }
\CommentTok{#>                Accuracy : 0.577         }
\CommentTok{#>                  95% CI : (0.454, 0.694)}
\CommentTok{#>     No Information Rate : 0.394         }
\CommentTok{#>     P-Value [Acc > NIR] : 0.00137       }
\CommentTok{#>                                         }
\CommentTok{#>                   Kappa : 0.413         }
\CommentTok{#>                                         }
\CommentTok{#>  Mcnemar's Test P-Value : NA            }
\CommentTok{#> }
\CommentTok{#> Statistics by Class:}
\CommentTok{#> }
\CommentTok{#>                      Class: 1 Class: 2 Class: 3 Class: 5 Class: 6 Class: 7}
\CommentTok{#> Sensitivity             0.714    0.765   0.0000   0.2000   0.0000   0.6364}
\CommentTok{#> Specificity             0.860    0.611   0.9524   1.0000   1.0000   1.0000}
\CommentTok{#> Pos Pred Value          0.769    0.382   0.0000   1.0000      NaN   1.0000}
\CommentTok{#> Neg Pred Value          0.822    0.892   0.8824   0.9429   0.9718   0.9375}
\CommentTok{#> Prevalence              0.394    0.239   0.1127   0.0704   0.0282   0.1549}
\CommentTok{#> Detection Rate          0.282    0.183   0.0000   0.0141   0.0000   0.0986}
\CommentTok{#> Detection Prevalence    0.366    0.479   0.0423   0.0141   0.0000   0.0986}
\CommentTok{#> Balanced Accuracy       0.787    0.688   0.4762   0.6000   0.5000   0.8182}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{confusionMatrix}\NormalTok{(rpart.pred, testset}\OperatorTok{$}\NormalTok{Type)}
\CommentTok{#> Confusion Matrix and Statistics}
\CommentTok{#> }
\CommentTok{#>           Reference}
\CommentTok{#> Prediction  1  2  3  5  6  7}
\CommentTok{#>          1 22  0  3  0  0  0}
\CommentTok{#>          2  5 12  4  0  0  0}
\CommentTok{#>          3  0  2  1  0  0  0}
\CommentTok{#>          5  0  2  0  5  2  1}
\CommentTok{#>          6  0  0  0  0  0  0}
\CommentTok{#>          7  1  1  0  0  0 10}
\CommentTok{#> }
\CommentTok{#> Overall Statistics}
\CommentTok{#>                                         }
\CommentTok{#>                Accuracy : 0.704         }
\CommentTok{#>                  95% CI : (0.584, 0.807)}
\CommentTok{#>     No Information Rate : 0.394         }
\CommentTok{#>     P-Value [Acc > NIR] : 1.23e-07      }
\CommentTok{#>                                         }
\CommentTok{#>                   Kappa : 0.605         }
\CommentTok{#>                                         }
\CommentTok{#>  Mcnemar's Test P-Value : NA            }
\CommentTok{#> }
\CommentTok{#> Statistics by Class:}
\CommentTok{#> }
\CommentTok{#>                      Class: 1 Class: 2 Class: 3 Class: 5 Class: 6 Class: 7}
\CommentTok{#> Sensitivity             0.786    0.706   0.1250   1.0000   0.0000    0.909}
\CommentTok{#> Specificity             0.930    0.833   0.9683   0.9242   1.0000    0.967}
\CommentTok{#> Pos Pred Value          0.880    0.571   0.3333   0.5000      NaN    0.833}
\CommentTok{#> Neg Pred Value          0.870    0.900   0.8971   1.0000   0.9718    0.983}
\CommentTok{#> Prevalence              0.394    0.239   0.1127   0.0704   0.0282    0.155}
\CommentTok{#> Detection Rate          0.310    0.169   0.0141   0.0704   0.0000    0.141}
\CommentTok{#> Detection Prevalence    0.352    0.296   0.0423   0.1408   0.0000    0.169}
\CommentTok{#> Balanced Accuracy       0.858    0.770   0.5466   0.9621   0.5000    0.938}
\end{Highlighting}
\end{Shaded}

\hypertarget{comparison-with-resamples}{%
\subsection{Comparison with resamples}\label{comparison-with-resamples}}

Finally, we compare the performance of the two methods by computing the
respective accuracy rates and the kappa indices (as computed by \texttt{classAgreement()}
also contained in package \texttt{e1071}). In Table 1, we summarize the results
of 10 replications---Support Vector Machines show better results.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1234567}\NormalTok{)}

\CommentTok{# SVM}
\NormalTok{fit.svm <-}\StringTok{ }\KeywordTok{train}\NormalTok{(Type }\OperatorTok{~}\NormalTok{., }\DataTypeTok{data =}\NormalTok{ trainset, }
                 \DataTypeTok{method =} \StringTok{"svmRadial"}\NormalTok{)}

\CommentTok{# Random Forest}
\NormalTok{fit.rpart <-}\StringTok{ }\KeywordTok{train}\NormalTok{(Type }\OperatorTok{~}\NormalTok{., }\DataTypeTok{data =}\NormalTok{ trainset, }
                \DataTypeTok{method=}\StringTok{"rpart"}\NormalTok{)}

\CommentTok{# collect resamples}
\NormalTok{results <-}\StringTok{ }\KeywordTok{resamples}\NormalTok{(}\KeywordTok{list}\NormalTok{(}\DataTypeTok{svm =}\NormalTok{ fit.svm, }
                          \DataTypeTok{rpart  =}\NormalTok{ fit.rpart))}

\KeywordTok{summary}\NormalTok{(results)}
\CommentTok{#> }
\CommentTok{#> Call:}
\CommentTok{#> summary.resamples(object = results)}
\CommentTok{#> }
\CommentTok{#> Models: svm, rpart }
\CommentTok{#> Number of resamples: 25 }
\CommentTok{#> }
\CommentTok{#> Accuracy }
\CommentTok{#>        Min. 1st Qu. Median  Mean 3rd Qu.  Max. NA's}
\CommentTok{#> svm   0.510   0.565  0.600 0.599   0.625 0.704    0}
\CommentTok{#> rpart 0.462   0.519  0.554 0.558   0.600 0.660    0}
\CommentTok{#> }
\CommentTok{#> Kappa }
\CommentTok{#>        Min. 1st Qu. Median  Mean 3rd Qu.  Max. NA's}
\CommentTok{#> svm   0.267   0.376  0.406 0.410   0.446 0.559    0}
\CommentTok{#> rpart 0.135   0.299  0.358 0.363   0.443 0.545    0}
\end{Highlighting}
\end{Shaded}

\hypertarget{ozone-svm}{%
\chapter{Ozone SVM}\label{ozone-svm}}

\url{https://cran.r-project.org/web/packages/e1071/vignettes/svmdoc.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(e1071)}
\KeywordTok{library}\NormalTok{(rpart)}

\KeywordTok{data}\NormalTok{(Ozone, }\DataTypeTok{package=}\StringTok{"mlbench"}\NormalTok{)}
\CommentTok{## split data into a train and test set}
\NormalTok{index <-}\StringTok{ }\DecValTok{1}\OperatorTok{:}\KeywordTok{nrow}\NormalTok{(Ozone)}
\NormalTok{testindex <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(index, }\KeywordTok{trunc}\NormalTok{(}\KeywordTok{length}\NormalTok{(index)}\OperatorTok{/}\DecValTok{3}\NormalTok{))}
\NormalTok{testset <-}\StringTok{ }\KeywordTok{na.omit}\NormalTok{(Ozone[testindex,}\OperatorTok{-}\DecValTok{3}\NormalTok{])}
\NormalTok{trainset <-}\StringTok{ }\KeywordTok{na.omit}\NormalTok{(Ozone[}\OperatorTok{-}\NormalTok{testindex,}\OperatorTok{-}\DecValTok{3}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## svm}
\NormalTok{svm.model <-}\StringTok{ }\KeywordTok{svm}\NormalTok{(V4 }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ trainset, }\DataTypeTok{cost =} \DecValTok{1000}\NormalTok{, }\DataTypeTok{gamma =} \FloatTok{0.0001}\NormalTok{)}
\NormalTok{svm.pred  <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(svm.model, testset[,}\OperatorTok{-}\DecValTok{3}\NormalTok{])}
\KeywordTok{crossprod}\NormalTok{(svm.pred }\OperatorTok{-}\StringTok{ }\NormalTok{testset[,}\DecValTok{3}\NormalTok{]) }\OperatorTok{/}\StringTok{ }\KeywordTok{length}\NormalTok{(testindex)}
\CommentTok{#>      [,1]}
\CommentTok{#> [1,] 10.7}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## rpart}
\NormalTok{rpart.model <-}\StringTok{ }\KeywordTok{rpart}\NormalTok{(V4 }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ trainset)}
\NormalTok{rpart.pred  <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(rpart.model, testset[,}\OperatorTok{-}\DecValTok{3}\NormalTok{])}
\KeywordTok{crossprod}\NormalTok{(rpart.pred }\OperatorTok{-}\StringTok{ }\NormalTok{testset[,}\DecValTok{3}\NormalTok{]) }\OperatorTok{/}\StringTok{ }\KeywordTok{length}\NormalTok{(testindex)}
\CommentTok{#>      [,1]}
\CommentTok{#> [1,] 11.9}
\end{Highlighting}
\end{Shaded}

\hypertarget{a-gentle-introduction-to-support-vector-machines-using-r-1}{%
\chapter{A gentle introduction to support vector machines using R}\label{a-gentle-introduction-to-support-vector-machines-using-r-1}}

\url{https://eight2late.wordpress.com/2017/02/07/a-gentle-introduction-to-support-vector-machines-using-r/}

\hypertarget{support-vector-machines-in-r-1}{%
\section{Support vector machines in R}\label{support-vector-machines-in-r-1}}

In this demo we'll use the svm interface that is implemented in the \texttt{e1071} R package. This interface provides R programmers access to the comprehensive \texttt{libsvm} library written by Chang and Lin. I'll use two toy datasets: the famous iris dataset available with the base R package and the sonar dataset from the mlbench package. I won't describe details of the datasets as they are discussed at length in the documentation that I have linked to. However, it is worth mentioning the reasons why I chose these datasets:

As mentioned earlier, no real life dataset is linearly separable, but the iris dataset is almost so. Consequently, it is a good illustration of using linear SVMs. Although one almost never uses these in practice, I have illustrated their use primarily for pedagogical reasons.
The sonar dataset is a good illustration of the benefits of using RBF kernels in cases where the dataset is hard to visualise (60 variables in this case!). In general, one would almost always use RBF (or other nonlinear) kernels in practice.

With that said, let's get right to it. I assume you have R and RStudio installed. For instructions on how to do this, have a look at the first article in this series. The processing preliminaries -- loading libraries, data and creating training and test datasets are much the same as in my previous articles so I won't dwell on these here. For completeness, however, I'll list all the code so you can run it directly in R or R studio (a complete listing of the code can be found here):

\hypertarget{svm-on-iris-dataset-1}{%
\section{\texorpdfstring{SVM on \texttt{iris} dataset}{SVM on iris dataset}}\label{svm-on-iris-dataset-1}}

\hypertarget{training-and-test-datasets-1}{%
\subsection{Training and test datasets}\label{training-and-test-datasets-1}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#load required library}
\KeywordTok{library}\NormalTok{(e1071)}

\CommentTok{#load built-in iris dataset}
\KeywordTok{data}\NormalTok{(iris)}

\CommentTok{#set seed to ensure reproducible results}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{42}\NormalTok{)}

\CommentTok{#split into training and test sets}
\NormalTok{iris[, }\StringTok{"train"}\NormalTok{] <-}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(}\KeywordTok{runif}\NormalTok{(}\KeywordTok{nrow}\NormalTok{(iris)) }\OperatorTok{<}\StringTok{ }\FloatTok{0.8}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)}

\CommentTok{#separate training and test sets}
\NormalTok{trainset <-}\StringTok{ }\NormalTok{iris[iris}\OperatorTok{$}\NormalTok{train }\OperatorTok{==}\StringTok{ }\DecValTok{1}\NormalTok{,]}
\NormalTok{testset <-}\StringTok{ }\NormalTok{iris[iris}\OperatorTok{$}\NormalTok{train }\OperatorTok{==}\StringTok{ }\DecValTok{0}\NormalTok{,]}

\CommentTok{#get column index of train flag}
\NormalTok{trainColNum <-}\StringTok{ }\KeywordTok{grep}\NormalTok{(}\StringTok{"train"}\NormalTok{, }\KeywordTok{names}\NormalTok{(trainset))}

\CommentTok{#remove train flag column from train and test sets}
\NormalTok{trainset <-}\StringTok{ }\NormalTok{trainset[,}\OperatorTok{-}\NormalTok{trainColNum]}
\NormalTok{testset <-}\StringTok{ }\NormalTok{testset[,}\OperatorTok{-}\NormalTok{trainColNum]}

\KeywordTok{dim}\NormalTok{(trainset)}
\CommentTok{#> [1] 115   5}
\KeywordTok{dim}\NormalTok{(testset)}
\CommentTok{#> [1] 35  5}
\end{Highlighting}
\end{Shaded}

\hypertarget{build-the-svm-model-1}{%
\subsection{Build the SVM model}\label{build-the-svm-model-1}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#get column index of predicted variable in dataset}
\NormalTok{typeColNum <-}\StringTok{ }\KeywordTok{grep}\NormalTok{(}\StringTok{"Species"}\NormalTok{, }\KeywordTok{names}\NormalTok{(iris))}

\CommentTok{#build model â€“ linear kernel and C-classification (soft margin) with default cost (C=1)}
\NormalTok{svm_model <-}\StringTok{ }\KeywordTok{svm}\NormalTok{(Species}\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ trainset, }
                 \DataTypeTok{method =} \StringTok{"C-classification"}\NormalTok{, }
                 \DataTypeTok{kernel =} \StringTok{"linear"}\NormalTok{)}
\NormalTok{svm_model}
\CommentTok{#> }
\CommentTok{#> Call:}
\CommentTok{#> svm(formula = Species ~ ., data = trainset, method = "C-classification", }
\CommentTok{#>     kernel = "linear")}
\CommentTok{#> }
\CommentTok{#> }
\CommentTok{#> Parameters:}
\CommentTok{#>    SVM-Type:  C-classification }
\CommentTok{#>  SVM-Kernel:  linear }
\CommentTok{#>        cost:  1 }
\CommentTok{#>       gamma:  0.25 }
\CommentTok{#> }
\CommentTok{#> Number of Support Vectors:  24}
\end{Highlighting}
\end{Shaded}

The output from the SVM model show that there are 24 support vectors. If desired, these can be examined using the SV variable in the model -- i.e via svm\_model\$SV.

\hypertarget{support-vectors-1}{%
\subsection{Support Vectors}\label{support-vectors-1}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# support vectors}
\NormalTok{svm_model}\OperatorTok{$}\NormalTok{SV}
\CommentTok{#>     Sepal.Length Sepal.Width Petal.Length Petal.Width}
\CommentTok{#> 19       -0.2564      1.7668       -1.323      -1.305}
\CommentTok{#> 42       -1.7006     -1.7045       -1.559      -1.305}
\CommentTok{#> 45       -0.9785      1.7668       -1.205      -1.171}
\CommentTok{#> 53        1.1878      0.1469        0.568       0.309}
\CommentTok{#> 55        0.7064     -0.5474        0.390       0.309}
\CommentTok{#> 57        0.4657      0.6097        0.450       0.443}
\CommentTok{#> 58       -1.2192     -1.4730       -0.378      -0.364}
\CommentTok{#> 69        0.3453     -1.9359        0.331       0.309}
\CommentTok{#> 71       -0.0157      0.3783        0.509       0.712}
\CommentTok{#> 73        0.4657     -1.2416        0.568       0.309}
\CommentTok{#> 78        0.9471     -0.0845        0.627       0.578}
\CommentTok{#> 84        0.1046     -0.7788        0.686       0.443}
\CommentTok{#> 85       -0.6174     -0.0845        0.331       0.309}
\CommentTok{#> 86        0.1046      0.8412        0.331       0.443}
\CommentTok{#> 99       -0.9785     -1.2416       -0.555      -0.229}
\CommentTok{#> 107      -1.2192     -1.2416        0.331       0.578}
\CommentTok{#> 111       0.7064      0.3783        0.686       0.981}
\CommentTok{#> 117       0.7064     -0.0845        0.922       0.712}
\CommentTok{#> 124       0.4657     -0.7788        0.568       0.712}
\CommentTok{#> 130       1.5488     -0.0845        1.099       0.443}
\CommentTok{#> 138       0.5860      0.1469        0.922       0.712}
\CommentTok{#> 139       0.1046     -0.0845        0.509       0.712}
\CommentTok{#> 147       0.4657     -1.2416        0.627       0.847}
\CommentTok{#> 150      -0.0157     -0.0845        0.686       0.712}
\end{Highlighting}
\end{Shaded}

The test prediction accuracy indicates that the linear performs quite well on this dataset, confirming that it is indeed near linearly separable. To check performance by class, one can create a confusion matrix as described in my post on random forests. I'll leave this as an exercise for you. Another point is that we have used a soft-margin classification scheme with a cost C=1. You can experiment with this by explicitly changing the value of C. Again, I'll leave this for you an exercise.

\hypertarget{predictions-on-training-model-3}{%
\subsection{Predictions on training model}\label{predictions-on-training-model-3}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# training set predictions}
\NormalTok{pred_train <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(svm_model, trainset)}
\KeywordTok{mean}\NormalTok{(pred_train }\OperatorTok{==}\StringTok{ }\NormalTok{trainset}\OperatorTok{$}\NormalTok{Species)}
\CommentTok{#> [1] 0.983}
\CommentTok{# [1] 0.9826087}
\end{Highlighting}
\end{Shaded}

\hypertarget{predictions-on-test-model-3}{%
\subsection{Predictions on test model}\label{predictions-on-test-model-3}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# test set predictions}
\NormalTok{pred_test <-}\KeywordTok{predict}\NormalTok{(svm_model, testset)}
\KeywordTok{mean}\NormalTok{(pred_test }\OperatorTok{==}\StringTok{ }\NormalTok{testset}\OperatorTok{$}\NormalTok{Species)}
\CommentTok{#> [1] 0.914}
\CommentTok{# [1] 0.9142857}
\end{Highlighting}
\end{Shaded}

\hypertarget{confusion-matrix-and-accuracy-1}{%
\subsection{Confusion matrix and Accuracy}\label{confusion-matrix-and-accuracy-1}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# confusion matrix}
\NormalTok{cm <-}\StringTok{ }\KeywordTok{table}\NormalTok{(pred_test, testset}\OperatorTok{$}\NormalTok{Species)}
\NormalTok{cm}
\CommentTok{#>             }
\CommentTok{#> pred_test    setosa versicolor virginica}
\CommentTok{#>   setosa         18          0         0}
\CommentTok{#>   versicolor      0          5         3}
\CommentTok{#>   virginica       0          0         9}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# accuracy}
\KeywordTok{sum}\NormalTok{(}\KeywordTok{diag}\NormalTok{(cm)) }\OperatorTok{/}\StringTok{ }\KeywordTok{sum}\NormalTok{(cm)}
\CommentTok{#> [1] 0.914}
\end{Highlighting}
\end{Shaded}

\hypertarget{svm-with-radial-basis-function-kernel.-linear-1}{%
\section{SVM with Radial Basis Function kernel. Linear}\label{svm-with-radial-basis-function-kernel.-linear-1}}

\hypertarget{training-and-test-sets-1}{%
\subsection{Training and test sets}\label{training-and-test-sets-1}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#load required library (assuming e1071 is already loaded)}
\KeywordTok{library}\NormalTok{(mlbench)}

\CommentTok{#load Sonar dataset}
\KeywordTok{data}\NormalTok{(Sonar)}
\CommentTok{#set seed to ensure reproducible results}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{42}\NormalTok{)}
\CommentTok{#split into training and test sets}
\NormalTok{Sonar[, }\StringTok{"train"}\NormalTok{] <-}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(}\KeywordTok{runif}\NormalTok{(}\KeywordTok{nrow}\NormalTok{(Sonar))}\OperatorTok{<}\FloatTok{0.8}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{)}

\CommentTok{#separate training and test sets}
\NormalTok{trainset <-}\StringTok{ }\NormalTok{Sonar[Sonar}\OperatorTok{$}\NormalTok{train}\OperatorTok{==}\DecValTok{1}\NormalTok{,]}
\NormalTok{testset <-}\StringTok{ }\NormalTok{Sonar[Sonar}\OperatorTok{$}\NormalTok{train}\OperatorTok{==}\DecValTok{0}\NormalTok{,]}

\CommentTok{#get column index of train flag}
\NormalTok{trainColNum <-}\StringTok{ }\KeywordTok{grep}\NormalTok{(}\StringTok{"train"}\NormalTok{,}\KeywordTok{names}\NormalTok{(trainset))}
\CommentTok{#remove train flag column from train and test sets}
\NormalTok{trainset <-}\StringTok{ }\NormalTok{trainset[,}\OperatorTok{-}\NormalTok{trainColNum]}
\NormalTok{testset <-}\StringTok{ }\NormalTok{testset[,}\OperatorTok{-}\NormalTok{trainColNum]}

\CommentTok{#get column index of predicted variable in dataset}
\NormalTok{typeColNum <-}\StringTok{ }\KeywordTok{grep}\NormalTok{(}\StringTok{"Class"}\NormalTok{,}\KeywordTok{names}\NormalTok{(Sonar))}
\end{Highlighting}
\end{Shaded}

\hypertarget{predictions-on-training-model-4}{%
\subsection{Predictions on Training model}\label{predictions-on-training-model-4}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#build model â€“ linear kernel and C-classification with default cost (C=1)}
\NormalTok{svm_model <-}\StringTok{ }\KeywordTok{svm}\NormalTok{(Class}\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data=}\NormalTok{trainset, }
                 \DataTypeTok{method=}\StringTok{"C-classification"}\NormalTok{, }
                 \DataTypeTok{kernel=}\StringTok{"linear"}\NormalTok{)}

\CommentTok{#training set predictions}
\NormalTok{pred_train <-}\KeywordTok{predict}\NormalTok{(svm_model,trainset)}
\KeywordTok{mean}\NormalTok{(pred_train}\OperatorTok{==}\NormalTok{trainset}\OperatorTok{$}\NormalTok{Class)}
\CommentTok{#> [1] 0.97}
\end{Highlighting}
\end{Shaded}

\hypertarget{predictions-on-test-model-4}{%
\subsection{Predictions on test model}\label{predictions-on-test-model-4}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#test set predictions}
\NormalTok{pred_test <-}\KeywordTok{predict}\NormalTok{(svm_model,testset)}
\KeywordTok{mean}\NormalTok{(pred_test}\OperatorTok{==}\NormalTok{testset}\OperatorTok{$}\NormalTok{Class)}
\CommentTok{#> [1] 0.605}
\end{Highlighting}
\end{Shaded}

I'll leave you to examine the contents of the model. The important point to note here is that the performance of the model with the test set is quite dismal compared to the previous case. This simply indicates that the linear kernel is not appropriate here. Let's take a look at what happens if we use the RBF kernel with default values for the parameters:

\hypertarget{svm-with-radial-basis-function-kernel.-non-linear-1}{%
\section{SVM with Radial Basis Function kernel. Non-linear}\label{svm-with-radial-basis-function-kernel.-non-linear-1}}

\hypertarget{predictions-on-training-model-5}{%
\subsection{Predictions on training model}\label{predictions-on-training-model-5}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#build model: radial kernel, default params}
\NormalTok{svm_model <-}\StringTok{ }\KeywordTok{svm}\NormalTok{(Class}\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data=}\NormalTok{trainset, }
                 \DataTypeTok{method=}\StringTok{"C-classification"}\NormalTok{, }
                 \DataTypeTok{kernel=}\StringTok{"radial"}\NormalTok{)}
\CommentTok{# print params}
\NormalTok{svm_model}\OperatorTok{$}\NormalTok{cost}
\CommentTok{#> [1] 1}
\NormalTok{svm_model}\OperatorTok{$}\NormalTok{gamma}
\CommentTok{#> [1] 0.0167}

\CommentTok{#training set predictions}
\NormalTok{pred_train <-}\KeywordTok{predict}\NormalTok{(svm_model,trainset)}
\KeywordTok{mean}\NormalTok{(pred_train}\OperatorTok{==}\NormalTok{trainset}\OperatorTok{$}\NormalTok{Class)}
\CommentTok{#> [1] 0.988}
\end{Highlighting}
\end{Shaded}

\hypertarget{predictions-on-test-model-5}{%
\subsection{Predictions on test model}\label{predictions-on-test-model-5}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#test set predictions}
\NormalTok{pred_test <-}\KeywordTok{predict}\NormalTok{(svm_model,testset)}
\KeywordTok{mean}\NormalTok{(pred_test}\OperatorTok{==}\NormalTok{testset}\OperatorTok{$}\NormalTok{Class)}
\CommentTok{#> [1] 0.767}
\end{Highlighting}
\end{Shaded}

That's a pretty decent improvement from the linear kernel. Let's see if we can do better by doing some parameter tuning. To do this we first invoke tune.svm and use the parameters it gives us in the call to svm:

\hypertarget{tuning-of-parameters-1}{%
\subsection{Tuning of parameters}\label{tuning-of-parameters-1}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# find optimal parameters in a specified range}
\NormalTok{tune_out <-}\StringTok{ }\KeywordTok{tune.svm}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ trainset[,}\OperatorTok{-}\NormalTok{typeColNum], }
                     \DataTypeTok{y =}\NormalTok{ trainset[, typeColNum], }
                     \DataTypeTok{gamma =} \DecValTok{10}\OperatorTok{^}\NormalTok{(}\OperatorTok{-}\DecValTok{3}\OperatorTok{:}\DecValTok{3}\NormalTok{), }
                     \DataTypeTok{cost =} \KeywordTok{c}\NormalTok{(}\FloatTok{0.01}\NormalTok{, }\FloatTok{0.1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{100}\NormalTok{, }\DecValTok{1000}\NormalTok{), }
                     \DataTypeTok{kernel =} \StringTok{"radial"}\NormalTok{)}

\CommentTok{#print best values of cost and gamma}
\NormalTok{tune_out}\OperatorTok{$}\NormalTok{best.parameters}\OperatorTok{$}\NormalTok{cost}
\CommentTok{#> [1] 10}
\NormalTok{tune_out}\OperatorTok{$}\NormalTok{best.parameters}\OperatorTok{$}\NormalTok{gamma}
\CommentTok{#> [1] 0.01}

\CommentTok{#build model}
\NormalTok{svm_model <-}\StringTok{ }\KeywordTok{svm}\NormalTok{(Class}\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ trainset, }
                 \DataTypeTok{method =} \StringTok{"C-classification"}\NormalTok{, }
                 \DataTypeTok{kernel =} \StringTok{"radial"}\NormalTok{, }
                 \DataTypeTok{cost =}\NormalTok{ tune_out}\OperatorTok{$}\NormalTok{best.parameters}\OperatorTok{$}\NormalTok{cost, }
                 \DataTypeTok{gamma =}\NormalTok{ tune_out}\OperatorTok{$}\NormalTok{best.parameters}\OperatorTok{$}\NormalTok{gamma)}
\end{Highlighting}
\end{Shaded}

\hypertarget{prediction-on-training-model-with-new-parameters-1}{%
\subsection{Prediction on training model with new parameters}\label{prediction-on-training-model-with-new-parameters-1}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# training set predictions}
\NormalTok{pred_train <-}\KeywordTok{predict}\NormalTok{(svm_model,trainset)}
\KeywordTok{mean}\NormalTok{(pred_train}\OperatorTok{==}\NormalTok{trainset}\OperatorTok{$}\NormalTok{Class)}
\CommentTok{#> [1] 1}
\end{Highlighting}
\end{Shaded}

\hypertarget{prediction-on-test-model-with-new-parameters-1}{%
\subsection{Prediction on test model with new parameters}\label{prediction-on-test-model-with-new-parameters-1}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# test set predictions}
\NormalTok{pred_test <-}\KeywordTok{predict}\NormalTok{(svm_model,testset)}
\KeywordTok{mean}\NormalTok{(pred_test}\OperatorTok{==}\NormalTok{testset}\OperatorTok{$}\NormalTok{Class)}
\CommentTok{#> [1] 0.814}
\end{Highlighting}
\end{Shaded}

Which is fairly decent improvement on the un-optimised case.

\hypertarget{wrapping-up-1}{%
\section{Wrapping up}\label{wrapping-up-1}}

This bring us to the end of this introductory exploration of SVMs in R. To recap, the distinguishing feature of SVMs in contrast to most other techniques is that they attempt to construct optimal separation boundaries between different categories.

SVMs are quite versatile and have been applied to a wide variety of domains ranging from chemistry to pattern recognition. They are best used in binary classification scenarios. This brings up a question as to where SVMs are to be preferred to other binary classification techniques such as logistic regression. The honest response is, ``it depends'' -- but here are some points to keep in mind when choosing between the two. A general point to keep in mind is that SVM algorithms tend to be expensive both in terms of memory and computation, issues that can start to hurt as the size of the dataset increases.

Given all the above caveats and considerations, the best way to figure out whether an SVM approach will work for your problem may be to do what most machine learning practitioners do: try it out!

\hypertarget{sms-spam.-naive-bayes.-classification}{%
\chapter{SMS spam. Naive Bayes. Classification}\label{sms-spam.-naive-bayes.-classification}}

Dataset: \url{https://github.com/stedy/Machine-Learning-with-R-datasets/blob/master/sms_spam.csv}

Instructions: Machine Learning with R. Page 104.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tictoc)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sms_raw <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\KeywordTok{file.path}\NormalTok{(data_raw_dir, }\StringTok{"sms_spam.csv"}\NormalTok{), }\DataTypeTok{stringsAsFactors =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str}\NormalTok{(sms_raw)}
\CommentTok{#> 'data.frame':    5574 obs. of  2 variables:}
\CommentTok{#>  $ type: chr  "ham" "ham" "spam" "ham" ...}
\CommentTok{#>  $ text: chr  "Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat..." "Ok lar... Joking wif u oni..." "Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question("| __truncated__ "U dun say so early hor... U c already then say..." ...}
\end{Highlighting}
\end{Shaded}

\hypertarget{convert-type-to-a-factor}{%
\subsection{convert type to a factor}\label{convert-type-to-a-factor}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sms_raw}\OperatorTok{$}\NormalTok{type <-}\StringTok{ }\KeywordTok{factor}\NormalTok{(sms_raw}\OperatorTok{$}\NormalTok{type)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str}\NormalTok{(sms_raw}\OperatorTok{$}\NormalTok{type)}
\CommentTok{#>  Factor w/ 2 levels "ham","spam": 1 1 2 1 1 2 1 1 2 2 ...}
\end{Highlighting}
\end{Shaded}

How many email of type ham or spam:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{table}\NormalTok{(sms_raw}\OperatorTok{$}\NormalTok{type)}
\CommentTok{#> }
\CommentTok{#>  ham spam }
\CommentTok{#> 4827  747}
\end{Highlighting}
\end{Shaded}

Create the corpus:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tm)}
\CommentTok{#> Loading required package: NLP}

\NormalTok{sms_corpus <-}\StringTok{ }\KeywordTok{VCorpus}\NormalTok{(}\KeywordTok{VectorSource}\NormalTok{(sms_raw}\OperatorTok{$}\NormalTok{text))}
\KeywordTok{print}\NormalTok{(sms_corpus)}
\CommentTok{#> <<VCorpus>>}
\CommentTok{#> Metadata:  corpus specific: 0, document level (indexed): 0}
\CommentTok{#> Content:  documents: 5574}
\end{Highlighting}
\end{Shaded}

Let's see a couple of documents:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{inspect}\NormalTok{(sms_corpus[}\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{])}
\CommentTok{#> <<VCorpus>>}
\CommentTok{#> Metadata:  corpus specific: 0, document level (indexed): 0}
\CommentTok{#> Content:  documents: 2}
\CommentTok{#> }
\CommentTok{#> [[1]]}
\CommentTok{#> <<PlainTextDocument>>}
\CommentTok{#> Metadata:  7}
\CommentTok{#> Content:  chars: 111}
\CommentTok{#> }
\CommentTok{#> [[2]]}
\CommentTok{#> <<PlainTextDocument>>}
\CommentTok{#> Metadata:  7}
\CommentTok{#> Content:  chars: 29}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# show some text}
\KeywordTok{as.character}\NormalTok{(sms_corpus[[}\DecValTok{1}\NormalTok{]])}
\CommentTok{#> [1] "Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat..."}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# show three documents}
\KeywordTok{lapply}\NormalTok{(sms_corpus[}\DecValTok{1}\OperatorTok{:}\DecValTok{3}\NormalTok{], as.character)}
\CommentTok{#> $`1`}
\CommentTok{#> [1] "Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat..."}
\CommentTok{#> }
\CommentTok{#> $`2`}
\CommentTok{#> [1] "Ok lar... Joking wif u oni..."}
\CommentTok{#> }
\CommentTok{#> $`3`}
\CommentTok{#> [1] "Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's"}
\end{Highlighting}
\end{Shaded}

\hypertarget{some-conversion}{%
\section{Some conversion}\label{some-conversion}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# convert to lowercase}
\NormalTok{sms_corpus_clean <-}\StringTok{ }\KeywordTok{tm_map}\NormalTok{(sms_corpus, }\KeywordTok{content_transformer}\NormalTok{(tolower))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{as.character}\NormalTok{(sms_corpus[[}\DecValTok{1}\NormalTok{]])}
\CommentTok{#> [1] "Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat..."}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# converted to lowercase}
\KeywordTok{as.character}\NormalTok{(sms_corpus_clean[[}\DecValTok{1}\NormalTok{]])}
\CommentTok{#> [1] "go until jurong point, crazy.. available only in bugis n great world la e buffet... cine there got amore wat..."}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# remove numbers}
\NormalTok{sms_corpus_clean <-}\StringTok{ }\KeywordTok{tm_map}\NormalTok{(sms_corpus_clean, removeNumbers)}
\end{Highlighting}
\end{Shaded}

What transformations are available

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# what transformations are available}
\KeywordTok{getTransformations}\NormalTok{()}
\CommentTok{#> [1] "removeNumbers"     "removePunctuation" "removeWords"      }
\CommentTok{#> [4] "stemDocument"      "stripWhitespace"}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# remove stop words}
\NormalTok{sms_corpus_clean <-}\StringTok{ }\KeywordTok{tm_map}\NormalTok{(sms_corpus_clean, removeWords, }\KeywordTok{stopwords}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# remove punctuation}
\NormalTok{sms_corpus_clean <-}\StringTok{ }\KeywordTok{tm_map}\NormalTok{(sms_corpus_clean, removePunctuation)}
\end{Highlighting}
\end{Shaded}

Stemming:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(SnowballC)}
\KeywordTok{wordStem}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"learn"}\NormalTok{, }\StringTok{"learned"}\NormalTok{, }\StringTok{"learning"}\NormalTok{, }\StringTok{"learns"}\NormalTok{))}
\CommentTok{#> [1] "learn" "learn" "learn" "learn"}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# stemming corpus}
\NormalTok{sms_corpus_clean <-}\StringTok{ }\KeywordTok{tm_map}\NormalTok{(sms_corpus_clean, stemDocument)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# remove white spaces}
\NormalTok{sms_corpus_clean <-}\StringTok{ }\KeywordTok{tm_map}\NormalTok{(sms_corpus_clean, stripWhitespace)}
\end{Highlighting}
\end{Shaded}

Show what we've got so far

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# show what we've got so far}
\KeywordTok{lapply}\NormalTok{(sms_corpus[}\DecValTok{1}\OperatorTok{:}\DecValTok{3}\NormalTok{], as.character)}
\CommentTok{#> $`1`}
\CommentTok{#> [1] "Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat..."}
\CommentTok{#> }
\CommentTok{#> $`2`}
\CommentTok{#> [1] "Ok lar... Joking wif u oni..."}
\CommentTok{#> }
\CommentTok{#> $`3`}
\CommentTok{#> [1] "Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's"}

\KeywordTok{lapply}\NormalTok{(sms_corpus_clean[}\DecValTok{1}\OperatorTok{:}\DecValTok{3}\NormalTok{], as.character)}
\CommentTok{#> $`1`}
\CommentTok{#> [1] "go jurong point crazi avail bugi n great world la e buffet cine got amor wat"}
\CommentTok{#> }
\CommentTok{#> $`2`}
\CommentTok{#> [1] "ok lar joke wif u oni"}
\CommentTok{#> }
\CommentTok{#> $`3`}
\CommentTok{#> [1] "free entri wkli comp win fa cup final tkts st may text fa receiv entri questionstd txt ratetc appli s"}
\end{Highlighting}
\end{Shaded}

\hypertarget{convert-to-document-term-matrix-dtm}{%
\section{Convert to Document Term Matrix (dtm}\label{convert-to-document-term-matrix-dtm}}

)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sms_dtm <-}\StringTok{ }\KeywordTok{DocumentTermMatrix}\NormalTok{(sms_corpus_clean)}
\NormalTok{sms_dtm}
\CommentTok{#> <<DocumentTermMatrix (documents: 5574, terms: 6592)>>}
\CommentTok{#> Non-/sparse entries: 42608/36701200}
\CommentTok{#> Sparsity           : 100%}
\CommentTok{#> Maximal term length: 40}
\CommentTok{#> Weighting          : term frequency (tf)}
\end{Highlighting}
\end{Shaded}

\hypertarget{split-in-training-and-test-datasets}{%
\section{split in training and test datasets}\label{split-in-training-and-test-datasets}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sms_dtm_train <-}\StringTok{ }\NormalTok{sms_dtm[}\DecValTok{1}\OperatorTok{:}\DecValTok{4169}\NormalTok{, ]}
\NormalTok{sms_dtm_test  <-}\StringTok{ }\NormalTok{sms_dtm[}\DecValTok{4170}\OperatorTok{:}\DecValTok{5559}\NormalTok{, ]}
\end{Highlighting}
\end{Shaded}

\hypertarget{separate-the-labels}{%
\subsection{separate the labels}\label{separate-the-labels}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sms_train_labels <-}\StringTok{ }\NormalTok{sms_raw[}\DecValTok{1}\OperatorTok{:}\DecValTok{4169}\NormalTok{, ]}\OperatorTok{$}\NormalTok{type}
\NormalTok{sms_test_labels  <-}\StringTok{ }\NormalTok{sms_raw[}\DecValTok{4170}\OperatorTok{:}\DecValTok{5559}\NormalTok{, ]}\OperatorTok{$}\NormalTok{type}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{prop.table}\NormalTok{(}\KeywordTok{table}\NormalTok{(sms_train_labels))}
\CommentTok{#> sms_train_labels}
\CommentTok{#>   ham  spam }
\CommentTok{#> 0.865 0.135}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{prop.table}\NormalTok{(}\KeywordTok{table}\NormalTok{(sms_test_labels))}
\CommentTok{#> sms_test_labels}
\CommentTok{#>  ham spam }
\CommentTok{#> 0.87 0.13}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# convert dtm to matrix}
\NormalTok{sms_mat_train <-}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{(}\KeywordTok{t}\NormalTok{(sms_dtm_train))}
\NormalTok{dtm.rs <-}\StringTok{ }\KeywordTok{sort}\NormalTok{(}\KeywordTok{rowSums}\NormalTok{(sms_mat_train), }\DataTypeTok{decreasing=}\OtherTok{TRUE}\NormalTok{)}

\CommentTok{# dataframe with word-frequency}
\NormalTok{dtm.df <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{word =} \KeywordTok{names}\NormalTok{(dtm.rs), }\DataTypeTok{freq =} \KeywordTok{as.integer}\NormalTok{(dtm.rs),}
                     \DataTypeTok{stringsAsFactors =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{plot-wordcloud}{%
\section{plot wordcloud}\label{plot-wordcloud}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(wordcloud)}
\CommentTok{#> Loading required package: RColorBrewer}
\KeywordTok{wordcloud}\NormalTok{(sms_corpus_clean, }\DataTypeTok{min.freq =} \DecValTok{50}\NormalTok{, }\DataTypeTok{random.order =} \OtherTok{FALSE}\NormalTok{)}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): tone could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): also could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): look could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): start could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): smile could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): urgent could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): use could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): someth could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): place could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): gud could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): guy could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): custom could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): next could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): person could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): someon could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): tonight could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): went could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): around could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): talk could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): chat could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): money could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): collect could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): mani could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): per could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): soon could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): gonna could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): plan could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): alway could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): nice could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): check could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): dun could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): special could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): told could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): box could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): lot could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): shop could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): hello could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): hour could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): mean could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): month could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): guarante could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): peopl could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): happen could not be fit on page. It will not be plotted.}
\CommentTok{#> Warning in wordcloud(sms_corpus_clean, min.freq = 50, random.order =}
\CommentTok{#> FALSE): thk could not be fit on page. It will not be plotted.}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_225-sms_spam-tm-nb_files/figure-latex/unnamed-chunk-29-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spam <-}\StringTok{ }\KeywordTok{subset}\NormalTok{(sms_raw, type }\OperatorTok{==}\StringTok{ "spam"}\NormalTok{)}
\NormalTok{ham  <-}\StringTok{ }\KeywordTok{subset}\NormalTok{(sms_raw, type }\OperatorTok{==}\StringTok{ "ham"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Words related to \textbf{spam}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{wordcloud}\NormalTok{(spam}\OperatorTok{$}\NormalTok{text, }\DataTypeTok{max.words =} \DecValTok{40}\NormalTok{, }\DataTypeTok{scale =} \KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\FloatTok{0.5}\NormalTok{))}
\CommentTok{#> Warning in tm_map.SimpleCorpus(corpus, tm::removePunctuation):}
\CommentTok{#> transformation drops documents}
\CommentTok{#> Warning in tm_map.SimpleCorpus(corpus, function(x) tm::removeWords(x,}
\CommentTok{#> tm::stopwords())): transformation drops documents}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_225-sms_spam-tm-nb_files/figure-latex/unnamed-chunk-31-1} \end{center}

Words related to \textbf{ham}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{wordcloud}\NormalTok{(ham}\OperatorTok{$}\NormalTok{text, }\DataTypeTok{max.words =} \DecValTok{40}\NormalTok{, }\DataTypeTok{scale =} \KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\FloatTok{0.5}\NormalTok{))}
\CommentTok{#> Warning in tm_map.SimpleCorpus(corpus, tm::removePunctuation):}
\CommentTok{#> transformation drops documents}
\CommentTok{#> Warning in tm_map.SimpleCorpus(corpus, function(x) tm::removeWords(x,}
\CommentTok{#> tm::stopwords())): transformation drops documents}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_225-sms_spam-tm-nb_files/figure-latex/unnamed-chunk-32-1} \end{center}

\hypertarget{limit-frequent-words}{%
\section{Limit Frequent words}\label{limit-frequent-words}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# words that appear at least in 5 messages}
\NormalTok{sms_freq_words <-}\StringTok{ }\KeywordTok{findFreqTerms}\NormalTok{(sms_dtm_train, }\DecValTok{6}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str}\NormalTok{(sms_freq_words)}
\CommentTok{#>  chr [1:997] "abiola" "abl" "abt" "accept" "access" "account" "across" ...}
\end{Highlighting}
\end{Shaded}

\hypertarget{get-only-frequent-words}{%
\subsection{get only frequent words}\label{get-only-frequent-words}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sms_dtm_freq_train<-}\StringTok{ }\NormalTok{sms_dtm_train[ , sms_freq_words]}
\NormalTok{sms_dtm_freq_test <-}\StringTok{ }\NormalTok{sms_dtm_test[ , sms_freq_words]}
\end{Highlighting}
\end{Shaded}

\hypertarget{function-to-change-value-to-yesno}{%
\subsection{function to change value to Yes/No}\label{function-to-change-value-to-yesno}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{convert_counts <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x) \{}
\NormalTok{    x <-}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(x }\OperatorTok{>}\StringTok{ }\DecValTok{0}\NormalTok{, }\StringTok{"Yes"}\NormalTok{, }\StringTok{"No"}\NormalTok{)}
\NormalTok{  \}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# change from number to Yes/No}
\CommentTok{# also the result returns a matrix}
\NormalTok{sms_train <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(sms_dtm_freq_train, }\DataTypeTok{MARGIN =} \DecValTok{2}\NormalTok{,}
\NormalTok{                                       convert_counts)}
\NormalTok{sms_test  <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(sms_dtm_freq_test, }\DataTypeTok{MARGIN =} \DecValTok{2}\NormalTok{,}
\NormalTok{                                      convert_counts)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# matrix of}
\CommentTok{# 4169 documents as rows}
\CommentTok{# 1159 terms as columns}
\KeywordTok{dim}\NormalTok{(sms_train)}
\CommentTok{#> [1] 4169  997}
\KeywordTok{length}\NormalTok{(sms_train_labels)}
\CommentTok{#> [1] 4169}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# this is how the matrix looks}
\NormalTok{sms_train[}\DecValTok{1}\OperatorTok{:}\DecValTok{10}\NormalTok{, }\DecValTok{10}\OperatorTok{:}\DecValTok{15}\NormalTok{]}
\CommentTok{#>     Terms}
\CommentTok{#> Docs add  address admir advanc aft  afternoon}
\CommentTok{#>   1  "No" "No"    "No"  "No"   "No" "No"     }
\CommentTok{#>   2  "No" "No"    "No"  "No"   "No" "No"     }
\CommentTok{#>   3  "No" "No"    "No"  "No"   "No" "No"     }
\CommentTok{#>   4  "No" "No"    "No"  "No"   "No" "No"     }
\CommentTok{#>   5  "No" "No"    "No"  "No"   "No" "No"     }
\CommentTok{#>   6  "No" "No"    "No"  "No"   "No" "No"     }
\CommentTok{#>   7  "No" "No"    "No"  "No"   "No" "No"     }
\CommentTok{#>   8  "No" "No"    "No"  "No"   "No" "No"     }
\CommentTok{#>   9  "No" "No"    "No"  "No"   "No" "No"     }
\CommentTok{#>   10 "No" "No"    "No"  "No"   "No" "No"}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(e1071)}
\NormalTok{sms_classifier <-}\StringTok{ }\KeywordTok{naiveBayes}\NormalTok{(sms_train, sms_train_labels)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{tic}\NormalTok{()}
\NormalTok{sms_test_pred <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(sms_classifier, sms_test)}
\KeywordTok{toc}\NormalTok{()}
\CommentTok{#> 20.346 sec elapsed}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(gmodels)}
\KeywordTok{CrossTable}\NormalTok{(sms_test_pred, sms_test_labels,}
    \DataTypeTok{prop.chisq =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{prop.t =} \OtherTok{FALSE}\NormalTok{,}
    \DataTypeTok{dnn =} \KeywordTok{c}\NormalTok{(}\StringTok{'predicted'}\NormalTok{, }\StringTok{'actual'}\NormalTok{))}
\CommentTok{#> }
\CommentTok{#>  }
\CommentTok{#>    Cell Contents}
\CommentTok{#> |-------------------------|}
\CommentTok{#> |                       N |}
\CommentTok{#> |           N / Row Total |}
\CommentTok{#> |           N / Col Total |}
\CommentTok{#> |-------------------------|}
\CommentTok{#> }
\CommentTok{#>  }
\CommentTok{#> Total Observations in Table:  1390 }
\CommentTok{#> }
\CommentTok{#>  }
\CommentTok{#>              | actual }
\CommentTok{#>    predicted |       ham |      spam | Row Total | }
\CommentTok{#> -------------|-----------|-----------|-----------|}
\CommentTok{#>          ham |      1202 |        21 |      1223 | }
\CommentTok{#>              |     0.983 |     0.017 |     0.880 | }
\CommentTok{#>              |     0.994 |     0.116 |           | }
\CommentTok{#> -------------|-----------|-----------|-----------|}
\CommentTok{#>         spam |         7 |       160 |       167 | }
\CommentTok{#>              |     0.042 |     0.958 |     0.120 | }
\CommentTok{#>              |     0.006 |     0.884 |           | }
\CommentTok{#> -------------|-----------|-----------|-----------|}
\CommentTok{#> Column Total |      1209 |       181 |      1390 | }
\CommentTok{#>              |     0.870 |     0.130 |           | }
\CommentTok{#> -------------|-----------|-----------|-----------|}
\CommentTok{#> }
\CommentTok{#> }
\end{Highlighting}
\end{Shaded}

\begin{quote}
Misclassified:
20+9 (frequency = 5)
25+7 (freq=4)
23+7 (freq=3)
25+8 (freq=2)
21+7 (freq=6)
\end{quote}

\begin{quote}
Decreasing the minimum word frequency doesn't make the model better.
\end{quote}

\hypertarget{improve-model-performance}{%
\section{Improve model performance}\label{improve-model-performance}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sms_classifier2 <-}\StringTok{ }\KeywordTok{naiveBayes}\NormalTok{(sms_train, sms_train_labels, }
                              \DataTypeTok{laplace =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{tic}\NormalTok{()}
\NormalTok{sms_test_pred2 <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(sms_classifier2, sms_test)}
\KeywordTok{toc}\NormalTok{()}
\CommentTok{#> 20.755 sec elapsed}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{CrossTable}\NormalTok{(sms_test_pred2, sms_test_labels,}
    \DataTypeTok{prop.chisq =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{prop.t =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{prop.r =} \OtherTok{FALSE}\NormalTok{,}
    \DataTypeTok{dnn =} \KeywordTok{c}\NormalTok{(}\StringTok{'predicted'}\NormalTok{, }\StringTok{'actual'}\NormalTok{))}
\CommentTok{#> }
\CommentTok{#>  }
\CommentTok{#>    Cell Contents}
\CommentTok{#> |-------------------------|}
\CommentTok{#> |                       N |}
\CommentTok{#> |           N / Col Total |}
\CommentTok{#> |-------------------------|}
\CommentTok{#> }
\CommentTok{#>  }
\CommentTok{#> Total Observations in Table:  1390 }
\CommentTok{#> }
\CommentTok{#>  }
\CommentTok{#>              | actual }
\CommentTok{#>    predicted |       ham |      spam | Row Total | }
\CommentTok{#> -------------|-----------|-----------|-----------|}
\CommentTok{#>          ham |      1203 |        28 |      1231 | }
\CommentTok{#>              |     0.995 |     0.155 |           | }
\CommentTok{#> -------------|-----------|-----------|-----------|}
\CommentTok{#>         spam |         6 |       153 |       159 | }
\CommentTok{#>              |     0.005 |     0.845 |           | }
\CommentTok{#> -------------|-----------|-----------|-----------|}
\CommentTok{#> Column Total |      1209 |       181 |      1390 | }
\CommentTok{#>              |     0.870 |     0.130 |           | }
\CommentTok{#> -------------|-----------|-----------|-----------|}
\CommentTok{#> }
\CommentTok{#> }
\end{Highlighting}
\end{Shaded}

\begin{quote}
Misclassified: 28+7
\end{quote}

\hypertarget{classification-tree-vehicle-example}{%
\chapter{Classification Tree: Vehicle example}\label{classification-tree-vehicle-example}}

\begin{itemize}
\tightlist
\item
  Dataset: Vehicle (mlbench)
\item
  Instructions: book ``Applied Predictive Modeling Techniques'', Lewis, N.D.
\end{itemize}

\hypertarget{load-packages}{%
\section{Load packages}\label{load-packages}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tree)}
\KeywordTok{library}\NormalTok{(mlbench)}

\KeywordTok{data}\NormalTok{(Vehicle)}
\KeywordTok{str}\NormalTok{(Vehicle)}
\CommentTok{#> 'data.frame':    846 obs. of  19 variables:}
\CommentTok{#>  $ Comp        : num  95 91 104 93 85 107 97 90 86 93 ...}
\CommentTok{#>  $ Circ        : num  48 41 50 41 44 57 43 43 34 44 ...}
\CommentTok{#>  $ D.Circ      : num  83 84 106 82 70 106 73 66 62 98 ...}
\CommentTok{#>  $ Rad.Ra      : num  178 141 209 159 205 172 173 157 140 197 ...}
\CommentTok{#>  $ Pr.Axis.Ra  : num  72 57 66 63 103 50 65 65 61 62 ...}
\CommentTok{#>  $ Max.L.Ra    : num  10 9 10 9 52 6 6 9 7 11 ...}
\CommentTok{#>  $ Scat.Ra     : num  162 149 207 144 149 255 153 137 122 183 ...}
\CommentTok{#>  $ Elong       : num  42 45 32 46 45 26 42 48 54 36 ...}
\CommentTok{#>  $ Pr.Axis.Rect: num  20 19 23 19 19 28 19 18 17 22 ...}
\CommentTok{#>  $ Max.L.Rect  : num  159 143 158 143 144 169 143 146 127 146 ...}
\CommentTok{#>  $ Sc.Var.Maxis: num  176 170 223 160 241 280 176 162 141 202 ...}
\CommentTok{#>  $ Sc.Var.maxis: num  379 330 635 309 325 957 361 281 223 505 ...}
\CommentTok{#>  $ Ra.Gyr      : num  184 158 220 127 188 264 172 164 112 152 ...}
\CommentTok{#>  $ Skew.Maxis  : num  70 72 73 63 127 85 66 67 64 64 ...}
\CommentTok{#>  $ Skew.maxis  : num  6 9 14 6 9 5 13 3 2 4 ...}
\CommentTok{#>  $ Kurt.maxis  : num  16 14 9 10 11 9 1 3 14 14 ...}
\CommentTok{#>  $ Kurt.Maxis  : num  187 189 188 199 180 181 200 193 200 195 ...}
\CommentTok{#>  $ Holl.Ra     : num  197 199 196 207 183 183 204 202 208 204 ...}
\CommentTok{#>  $ Class       : Factor w/ 4 levels "bus","opel","saab",..: 4 4 3 4 1 1 1 4 4 3 ...}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(Vehicle[}\DecValTok{1}\NormalTok{])}
\CommentTok{#>       Comp      }
\CommentTok{#>  Min.   : 73.0  }
\CommentTok{#>  1st Qu.: 87.0  }
\CommentTok{#>  Median : 93.0  }
\CommentTok{#>  Mean   : 93.7  }
\CommentTok{#>  3rd Qu.:100.0  }
\CommentTok{#>  Max.   :119.0}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(Vehicle[}\DecValTok{2}\NormalTok{])}
\CommentTok{#>       Circ     }
\CommentTok{#>  Min.   :33.0  }
\CommentTok{#>  1st Qu.:40.0  }
\CommentTok{#>  Median :44.0  }
\CommentTok{#>  Mean   :44.9  }
\CommentTok{#>  3rd Qu.:49.0  }
\CommentTok{#>  Max.   :59.0}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{attributes}\NormalTok{(Vehicle}\OperatorTok{$}\NormalTok{Class)}
\CommentTok{#> $levels}
\CommentTok{#> [1] "bus"  "opel" "saab" "van" }
\CommentTok{#> }
\CommentTok{#> $class}
\CommentTok{#> [1] "factor"}
\end{Highlighting}
\end{Shaded}

\hypertarget{prepare-data}{%
\section{Prepare data}\label{prepare-data}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{107}\NormalTok{)}
\NormalTok{N =}\StringTok{ }\KeywordTok{nrow}\NormalTok{(Vehicle)}
\NormalTok{train <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\NormalTok{N, }\DecValTok{500}\NormalTok{, }\OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# training and test sets}
\NormalTok{trainset <-}\StringTok{ }\NormalTok{Vehicle[train,]}
\NormalTok{testset  <-}\StringTok{ }\NormalTok{Vehicle[}\OperatorTok{-}\NormalTok{train,]}
\end{Highlighting}
\end{Shaded}

\hypertarget{estimate-the-decision-tree}{%
\section{Estimate the decision tree}\label{estimate-the-decision-tree}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit <-}\StringTok{ }\KeywordTok{tree}\NormalTok{(Class }\OperatorTok{~}\NormalTok{., }\DataTypeTok{data =}\NormalTok{ trainset, }\DataTypeTok{split =} \StringTok{"deviance"}\NormalTok{)}
\NormalTok{fit}
\CommentTok{#> node), split, n, deviance, yval, (yprob)}
\CommentTok{#>       * denotes terminal node}
\CommentTok{#> }
\CommentTok{#>   1) root 500 1000 opel ( 0 0 0 0 )  }
\CommentTok{#>     2) Elong < 41.5 215  500 saab ( 0 0 0 0 )  }
\CommentTok{#>       4) Max.L.Ra < 7.5 51   50 bus ( 1 0 0 0 )  }
\CommentTok{#>         8) Comp < 93.5 12   20 bus ( 0 0 0 0 )  }
\CommentTok{#>          16) Pr.Axis.Ra < 67.5 7    8 saab ( 0 0 1 0 ) *}
\CommentTok{#>          17) Pr.Axis.Ra > 67.5 5    0 bus ( 1 0 0 0 ) *}
\CommentTok{#>         9) Comp > 93.5 39    9 bus ( 1 0 0 0 ) *}
\CommentTok{#>       5) Max.L.Ra > 7.5 164  200 opel ( 0 1 0 0 )  }
\CommentTok{#>        10) Sc.Var.maxis < 723 149  200 saab ( 0 0 1 0 )  }
\CommentTok{#>          20) Comp < 109.5 137  200 opel ( 0 1 0 0 ) *}
\CommentTok{#>          21) Comp > 109.5 12    0 saab ( 0 0 1 0 ) *}
\CommentTok{#>        11) Sc.Var.maxis > 723 15    7 opel ( 0 1 0 0 ) *}
\CommentTok{#>     3) Elong > 41.5 285  700 van ( 0 0 0 0 )  }
\CommentTok{#>       6) Sc.Var.maxis < 305.5 116  200 van ( 0 0 0 1 )  }
\CommentTok{#>        12) Max.L.Rect < 128.5 40   90 saab ( 0 0 0 0 )  }
\CommentTok{#>          24) Scat.Ra < 120.5 15   30 van ( 0 0 0 1 ) *}
\CommentTok{#>          25) Scat.Ra > 120.5 25   30 saab ( 0 0 1 0 ) *}
\CommentTok{#>        13) Max.L.Rect > 128.5 76   90 van ( 0 0 0 1 )  }
\CommentTok{#>          26) Max.L.Rect < 138.5 38   60 van ( 0 0 0 1 )  }
\CommentTok{#>            52) Circ < 37.5 17   10 van ( 0 0 0 1 ) *}
\CommentTok{#>            53) Circ > 37.5 21   40 opel ( 0 0 0 0 ) *}
\CommentTok{#>          27) Max.L.Rect > 138.5 38   20 van ( 0 0 0 1 ) *}
\CommentTok{#>       7) Sc.Var.maxis > 305.5 169  400 bus ( 0 0 0 0 )  }
\CommentTok{#>        14) Max.L.Ra < 8.5 116  200 bus ( 1 0 0 0 )  }
\CommentTok{#>          28) D.Circ < 76.5 97  100 bus ( 1 0 0 0 )  }
\CommentTok{#>            56) Skew.maxis < 10.5 87   70 bus ( 1 0 0 0 )  }
\CommentTok{#>             112) Max.L.Rect < 134.5 12   20 bus ( 0 0 0 0 ) *}
\CommentTok{#>             113) Max.L.Rect > 134.5 75   20 bus ( 1 0 0 0 ) *}
\CommentTok{#>            57) Skew.maxis > 10.5 10   20 opel ( 0 0 0 0 ) *}
\CommentTok{#>          29) D.Circ > 76.5 19   30 opel ( 0 1 0 0 ) *}
\CommentTok{#>        15) Max.L.Ra > 8.5 53   20 van ( 0 0 0 1 ) *}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# fit <- tree(Class ~., data = Vehicle[train,], split ="deviance")}
\CommentTok{# fit}
\end{Highlighting}
\end{Shaded}

\begin{quote}
We use deviance as the splitting criteria, a common alternative is to use
split=``gini''.
\end{quote}

\begin{quote}
At each branch of the tree (after root) we see in order:
1. The branch number (e.g.~in this case 1,2,14 and 15);
2. the split (e.g.~Elong \textless{} 41.5);
3. the number of samples going along that split (e.g.~229);
4. the deviance associated with that split (e.g.~489.1);
5. the predicted class (e.g.~opel);
6. the associated probabilities (e.g. ( 0.222707 0.410480 0.366812 0.000000
);
7. and for a terminal node (or leaf), the symbol "*".
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(fit)}
\CommentTok{#> }
\CommentTok{#> Classification tree:}
\CommentTok{#> tree(formula = Class ~ ., data = trainset, split = "deviance")}
\CommentTok{#> Variables actually used in tree construction:}
\CommentTok{#>  [1] "Elong"        "Max.L.Ra"     "Comp"         "Pr.Axis.Ra"  }
\CommentTok{#>  [5] "Sc.Var.maxis" "Max.L.Rect"   "Scat.Ra"      "Circ"        }
\CommentTok{#>  [9] "D.Circ"       "Skew.maxis"  }
\CommentTok{#> Number of terminal nodes:  16 }
\CommentTok{#> Residual mean deviance:  0.943 = 456 / 484 }
\CommentTok{#> Misclassification error rate: 0.252 = 126 / 500}
\end{Highlighting}
\end{Shaded}

\begin{quote}
Notice that summary(fit) shows:
1. The type of tree, in this case a Classification tree;
2. the formula used to fit the tree;
3. the variables used to fit the tree;
4. the number of terminal nodes in this case 15;
5. the residual mean deviance - 0.9381;
6. the misclassification error rate 0.232 or 23.2\%.
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(fit); }\KeywordTok{text}\NormalTok{(fit)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_226-vehicle-mlbench-tree_files/figure-latex/unnamed-chunk-7-1} \end{center}

\hypertarget{assess-model}{%
\section{Assess model}\label{assess-model}}

Unfortunately, classification trees have a tendency to overfit the data. One
approach to reduce this risk is to use cross-validation. For each hold out
sample we fit the model and note at what level the tree gives the best results
(using deviance or the misclassification rate). Then we hold out a different
sample and repeat. This can be carried out using the \texttt{cv.tree()} function.
We use a leave-one-out cross-validation using the misclassification rate and
deviance (\texttt{FUN=prune.misclass}, followed by \texttt{FUN=prune.tree}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitM.cv <-}\StringTok{ }\KeywordTok{cv.tree}\NormalTok{(fit, }\DataTypeTok{K=}\DecValTok{346}\NormalTok{, }\DataTypeTok{FUN =}\NormalTok{ prune.misclass)}
\NormalTok{fitP.cv <-}\StringTok{ }\KeywordTok{cv.tree}\NormalTok{(fit, }\DataTypeTok{K=}\DecValTok{346}\NormalTok{, }\DataTypeTok{FUN =}\NormalTok{ prune.tree)}
\end{Highlighting}
\end{Shaded}

The results are plotted out side by side in Figure 1.2. The jagged lines
shows where the minimum deviance / misclassification occurred with the
cross-validated tree. Since the cross validated misclassification and deviance
both reach their minimum close to the number of branches in the original
fitted tree there is little to be gained from pruning this tree

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\KeywordTok{plot}\NormalTok{(fitM.cv)}
\KeywordTok{plot}\NormalTok{(fitP.cv)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_226-vehicle-mlbench-tree_files/figure-latex/plot-fit-1} \end{center}

\hypertarget{make-predictions}{%
\section{Make predictions}\label{make-predictions}}

We use the validation data set and the fitted decision tree to predict vehicle
classes; then we display the confusion matrix and calculate the error rate of
the fitted tree. Overall, the model has an error rate of 32\%.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{testLabels <-}\StringTok{ }\NormalTok{Vehicle}\OperatorTok{$}\NormalTok{Class[}\OperatorTok{-}\NormalTok{train]}
\NormalTok{testLabels}
\CommentTok{#>   [1] van  bus  bus  van  van  bus  bus  saab opel bus  van  saab van  saab}
\CommentTok{#>  [15] saab van  saab opel van  saab saab saab bus  bus  saab opel bus  opel}
\CommentTok{#>  [29] bus  opel van  opel opel saab saab bus  bus  bus  van  van  saab opel}
\CommentTok{#>  [43] bus  opel van  opel saab bus  van  bus  opel van  saab bus  opel bus }
\CommentTok{#>  [57] opel opel van  bus  van  saab opel bus  van  saab opel opel saab saab}
\CommentTok{#>  [71] saab opel bus  van  bus  opel bus  saab bus  bus  bus  opel opel van }
\CommentTok{#>  [85] saab bus  bus  bus  van  saab opel van  van  bus  bus  opel bus  opel}
\CommentTok{#>  [99] saab opel bus  opel bus  saab van  van  saab saab bus  van  opel van }
\CommentTok{#> [113] saab opel saab saab van  van  van  van  bus  bus  opel bus  bus  van }
\CommentTok{#> [127] saab bus  opel bus  bus  bus  bus  opel van  saab saab bus  opel van }
\CommentTok{#> [141] bus  saab bus  van  bus  opel van  saab opel saab opel van  saab van }
\CommentTok{#> [155] saab opel bus  van  bus  saab saab opel opel bus  bus  opel van  van }
\CommentTok{#> [169] bus  van  van  saab bus  saab opel saab opel bus  bus  bus  saab bus }
\CommentTok{#> [183] opel opel saab saab saab van  van  opel opel van  van  opel bus  saab}
\CommentTok{#> [197] bus  van  opel opel bus  bus  bus  opel saab opel van  bus  opel opel}
\CommentTok{#> [211] saab opel bus  opel opel opel van  opel van  saab saab van  saab saab}
\CommentTok{#> [225] saab saab van  van  van  saab bus  van  van  bus  saab opel saab saab}
\CommentTok{#> [239] opel saab saab saab saab van  saab opel bus  saab bus  opel opel opel}
\CommentTok{#> [253] saab bus  van  opel saab opel bus  bus  saab van  opel bus  saab van }
\CommentTok{#> [267] opel saab saab saab saab van  opel bus  bus  bus  opel saab saab saab}
\CommentTok{#> [281] van  saab bus  opel saab van  opel bus  saab saab opel opel van  saab}
\CommentTok{#> [295] bus  opel bus  van  van  opel bus  bus  saab bus  van  saab bus  van }
\CommentTok{#> [309] saab van  opel bus  bus  opel saab opel bus  bus  saab van  saab saab}
\CommentTok{#> [323] bus  opel opel opel bus  saab bus  van  bus  van  saab opel saab van }
\CommentTok{#> [337] opel opel van  bus  saab saab van  saab opel saab}
\CommentTok{#> Levels: bus opel saab van}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Confusion Matrix}
\NormalTok{pred <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(fit, }\DataTypeTok{newdata =}\NormalTok{ testset)}
\CommentTok{# find column whih has the maximum of all rows }
\NormalTok{pred.class <-}\StringTok{ }\KeywordTok{colnames}\NormalTok{(pred)[}\KeywordTok{max.col}\NormalTok{(pred, }\DataTypeTok{ties.method =} \KeywordTok{c}\NormalTok{(}\StringTok{"random"}\NormalTok{))]}
\NormalTok{cm <-}\StringTok{ }\KeywordTok{table}\NormalTok{(testLabels, pred.class, }
      \DataTypeTok{dnn =} \KeywordTok{c}\NormalTok{(}\StringTok{"Observed Class"}\NormalTok{, }\StringTok{"Predicted Class"}\NormalTok{))}
\NormalTok{cm}
\CommentTok{#>               Predicted Class}
\CommentTok{#> Observed Class bus opel saab van}
\CommentTok{#>           bus   85    1    1   5}
\CommentTok{#>           opel   3   70   10   2}
\CommentTok{#>           saab   7   67   14   7}
\CommentTok{#>           van    1    4    5  64}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Sensitivity}
\KeywordTok{sum}\NormalTok{(}\KeywordTok{diag}\NormalTok{(cm)) }\OperatorTok{/}\StringTok{ }\KeywordTok{sum}\NormalTok{(cm)}
\CommentTok{#> [1] 0.673}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# pred <- predict(fit, newdata = Vehicle[-train,])}
\CommentTok{# pred.class <- colnames(pred)[max.col(pred, ties.method = c("random"))]}
\CommentTok{# table(Vehicle$Class[-train], pred.class, }
\CommentTok{#       dnn = c("Observed Class", "Predicted Class"))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{error_rate =}\StringTok{ }\NormalTok{(}\DecValTok{1} \OperatorTok{-}\StringTok{ }\KeywordTok{sum}\NormalTok{(pred.class }\OperatorTok{==}\StringTok{ }\NormalTok{testset) }\OperatorTok{/}\StringTok{ }\KeywordTok{nrow}\NormalTok{(testset))}
\KeywordTok{round}\NormalTok{(error_rate, }\DecValTok{3}\NormalTok{)}
\CommentTok{#> [1] 0.327}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# error_rate = (1 - sum(pred.class == Vehicle$Class[-train])/346)}
\CommentTok{# round(error_rate,3)}
\end{Highlighting}
\end{Shaded}

\hypertarget{bike-sharing-demand}{%
\chapter{Bike sharing demand}\label{bike-sharing-demand}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#loading the required libraries}
\KeywordTok{library}\NormalTok{(rpart)}
\KeywordTok{library}\NormalTok{(rattle)}
\CommentTok{#> Rattle: A free graphical interface for data science with R.}
\CommentTok{#> Version 5.2.0 Copyright (c) 2006-2018 Togaware Pty Ltd.}
\CommentTok{#> Type 'rattle()' to shake, rattle, and roll your data.}
\KeywordTok{library}\NormalTok{(rpart.plot)}
\KeywordTok{library}\NormalTok{(RColorBrewer)}
\KeywordTok{library}\NormalTok{(randomForest)}
\CommentTok{#> randomForest 4.6-14}
\CommentTok{#> Type rfNews() to see new features/changes/bug fixes.}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'randomForest'}
\CommentTok{#> The following object is masked from 'package:rattle':}
\CommentTok{#> }
\CommentTok{#>     importance}
\KeywordTok{library}\NormalTok{(corrplot)}
\CommentTok{#> corrplot 0.84 loaded}
\KeywordTok{library}\NormalTok{(dplyr)}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'dplyr'}
\CommentTok{#> The following object is masked from 'package:randomForest':}
\CommentTok{#> }
\CommentTok{#>     combine}
\CommentTok{#> The following objects are masked from 'package:stats':}
\CommentTok{#> }
\CommentTok{#>     filter, lag}
\CommentTok{#> The following objects are masked from 'package:base':}
\CommentTok{#> }
\CommentTok{#>     intersect, setdiff, setequal, union}
\KeywordTok{library}\NormalTok{(tictoc)}
\end{Highlighting}
\end{Shaded}

Source: \url{https://www.analyticsvidhya.com/blog/2015/06/solution-kaggle-competition-bike-sharing-demand/}

\hypertarget{hypothesis-generation}{%
\section{Hypothesis Generation}\label{hypothesis-generation}}

Before exploring the data to understand the relationship between variables, I'd recommend you to focus on hypothesis generation first. Now, this might sound counter-intuitive for solving a data science problem, but if there is one thing I have learnt over years, it is this. Before exploring data, you should spend some time thinking about the business problem, gaining the domain knowledge and may be gaining first hand experience of the problem (only if I could travel to North America!)

How does it help? This practice usually helps you form better features later on, which are not biased by the data available in the dataset. At this stage, you are expected to posses structured thinking i.e.~a thinking process which takes into consideration all the possible aspects of a particular problem.

Here are some of the hypothesis which I thought could influence the demand of bikes:

\begin{itemize}
\item
  \textbf{Hourly trend}: There must be high demand during office timings. Early morning and late evening can have different trend (cyclist) and low demand during 10:00 pm to 4:00 am.
\item
  \textbf{Daily Trend}: Registered users demand more bike on weekdays as compared to weekend or holiday.
\item
  \textbf{Rain}: The demand of bikes will be lower on a rainy day as compared to a sunny day. Similarly, higher humidity will cause to lower the demand and vice versa.
\item
  \textbf{Temperature}: Would high or low temperature encourage or disencourage bike riding?
\item
  \textbf{Pollution}: If the pollution level in a city starts soaring, people may start using Bike (it may be influenced by government / company policies or increased awareness).
\item
  \textbf{Time}: Total demand should have higher contribution of registered user as compared to casual because registered user base would increase over time.
\item
  \textbf{Traffic}: It can be positively correlated with Bike demand. Higher traffic may force people to use bike as compared to other road transport medium like car, taxi etc
\end{itemize}

\hypertarget{understanding-the-data-set}{%
\section{Understanding the Data Set}\label{understanding-the-data-set}}

The dataset shows hourly rental data for two years (2011 and 2012). The training data set is for the \textbf{first 19 days of each month}. The test dataset is from \textbf{20th day to month's end}. We are required to predict the total count of bikes rented during each hour covered by the test set.

In the training data set, they have separately given bike demand by registered, casual users and sum of both is given as count.

Training data set has 12 variables (see below) and Test has 9 (excluding registered, casual and count).

\hypertarget{independent-variables}{%
\subsection{Independent variables}\label{independent-variables}}

\begin{verbatim}
datetime:   date and hour in "mm/dd/yyyy hh:mm" format
season:     Four categories-> 1 = spring, 2 = summer, 3 = fall, 4 = winter
holiday:    whether the day is a holiday or not (1/0)
workingday: whether the day is neither a weekend nor holiday (1/0)
weather:    Four Categories of weather
            1-> Clear, Few clouds, Partly cloudy, Partly cloudy
            2-> Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist
            3-> Light Snow and Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds
            4-> Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog
temp:       hourly temperature in Celsius
atemp:      "feels like" temperature in Celsius
humidity:   relative humidity
windspeed:  wind speed
\end{verbatim}

\hypertarget{dependent-variables}{%
\subsection{Dependent variables}\label{dependent-variables}}

\begin{verbatim}
registered: number of registered user
casual:     number of non-registered user
count:      number of total rentals (registered + casual)
\end{verbatim}

\hypertarget{importing-the-dataset-and-data-exploration}{%
\section{Importing the dataset and Data Exploration}\label{importing-the-dataset-and-data-exploration}}

For this solution, I have used R (R Studio 0.99.442) in Windows Environment.

Below are the steps to import and perform data exploration. If you are new to this concept, you can refer this guide on Data Exploration in R

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Import Train and Test Data Set
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# https://www.kaggle.com/c/bike-sharing-demand/data}
\NormalTok{train =}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\KeywordTok{file.path}\NormalTok{(data_raw_dir, }\StringTok{"bike_train.csv"}\NormalTok{))}
\NormalTok{test =}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\KeywordTok{file.path}\NormalTok{(data_raw_dir, }\StringTok{"bike_test.csv"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{glimpse}\NormalTok{(train)}
\CommentTok{#> Observations: 10,886}
\CommentTok{#> Variables: 12}
\CommentTok{#> $ datetime   <fct> 2011-01-01 00:00:00, 2011-01-01 01:00:00, 2011-01-0...}
\CommentTok{#> $ season     <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...}
\CommentTok{#> $ holiday    <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...}
\CommentTok{#> $ workingday <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...}
\CommentTok{#> $ weather    <int> 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, ...}
\CommentTok{#> $ temp       <dbl> 9.84, 9.02, 9.02, 9.84, 9.84, 9.84, 9.02, 8.20, 9.8...}
\CommentTok{#> $ atemp      <dbl> 14.4, 13.6, 13.6, 14.4, 14.4, 12.9, 13.6, 12.9, 14....}
\CommentTok{#> $ humidity   <int> 81, 80, 80, 75, 75, 75, 80, 86, 75, 76, 76, 81, 77,...}
\CommentTok{#> $ windspeed  <dbl> 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 17, 19, 19, 20, 19, 2...}
\CommentTok{#> $ casual     <int> 3, 8, 5, 3, 0, 0, 2, 1, 1, 8, 12, 26, 29, 47, 35, 4...}
\CommentTok{#> $ registered <int> 13, 32, 27, 10, 1, 1, 0, 2, 7, 6, 24, 30, 55, 47, 7...}
\CommentTok{#> $ count      <int> 16, 40, 32, 13, 1, 1, 2, 3, 8, 14, 36, 56, 84, 94, ...}
\KeywordTok{glimpse}\NormalTok{(test)}
\CommentTok{#> Observations: 6,493}
\CommentTok{#> Variables: 9}
\CommentTok{#> $ datetime   <fct> 2011-01-20 00:00:00, 2011-01-20 01:00:00, 2011-01-2...}
\CommentTok{#> $ season     <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...}
\CommentTok{#> $ holiday    <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...}
\CommentTok{#> $ workingday <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...}
\CommentTok{#> $ weather    <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 2, 2, 2, 2, 2, ...}
\CommentTok{#> $ temp       <dbl> 10.66, 10.66, 10.66, 10.66, 10.66, 9.84, 9.02, 9.02...}
\CommentTok{#> $ atemp      <dbl> 11.4, 13.6, 13.6, 12.9, 12.9, 11.4, 10.6, 10.6, 10....}
\CommentTok{#> $ humidity   <int> 56, 56, 56, 56, 56, 60, 60, 55, 55, 52, 48, 45, 42,...}
\CommentTok{#> $ windspeed  <dbl> 26, 0, 0, 11, 11, 15, 15, 15, 19, 15, 20, 11, 0, 7,...}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Combine both Train and Test Data set (to understand the distribution of independent variable together).
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# add variables to test dataset before merging}
\NormalTok{test}\OperatorTok{$}\NormalTok{registered=}\DecValTok{0}
\NormalTok{test}\OperatorTok{$}\NormalTok{casual=}\DecValTok{0}
\NormalTok{test}\OperatorTok{$}\NormalTok{count=}\DecValTok{0}

\NormalTok{data =}\StringTok{ }\KeywordTok{rbind}\NormalTok{(train,test)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Variable Type Identification
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str}\NormalTok{(data)}
\CommentTok{#> 'data.frame':    17379 obs. of  12 variables:}
\CommentTok{#>  $ datetime  : Factor w/ 17379 levels "2011-01-01 00:00:00",..: 1 2 3 4 5 6 7 8 9 10 ...}
\CommentTok{#>  $ season    : int  1 1 1 1 1 1 1 1 1 1 ...}
\CommentTok{#>  $ holiday   : int  0 0 0 0 0 0 0 0 0 0 ...}
\CommentTok{#>  $ workingday: int  0 0 0 0 0 0 0 0 0 0 ...}
\CommentTok{#>  $ weather   : int  1 1 1 1 1 2 1 1 1 1 ...}
\CommentTok{#>  $ temp      : num  9.84 9.02 9.02 9.84 9.84 ...}
\CommentTok{#>  $ atemp     : num  14.4 13.6 13.6 14.4 14.4 ...}
\CommentTok{#>  $ humidity  : int  81 80 80 75 75 75 80 86 75 76 ...}
\CommentTok{#>  $ windspeed : num  0 0 0 0 0 ...}
\CommentTok{#>  $ casual    : num  3 8 5 3 0 0 2 1 1 8 ...}
\CommentTok{#>  $ registered: num  13 32 27 10 1 1 0 2 7 6 ...}
\CommentTok{#>  $ count     : num  16 40 32 13 1 1 2 3 8 14 ...}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Find missing values in the dataset if any
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{table}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(data))}
\CommentTok{#> }
\CommentTok{#>  FALSE }
\CommentTok{#> 208548}
\end{Highlighting}
\end{Shaded}

\begin{quote}
No NAs in the dataset.
\end{quote}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Understand the distribution of numerical variables and generate a frequency table for numeric variables. Analyze the distribution.
\end{enumerate}

\hypertarget{histograms}{%
\subsection{histograms}\label{histograms}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# histograms each attribute}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{4}\NormalTok{))}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{2}\OperatorTok{:}\DecValTok{9}\NormalTok{) \{}
    \KeywordTok{hist}\NormalTok{(data[,i], }\DataTypeTok{main =} \KeywordTok{names}\NormalTok{(data)[i])}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_227-bike_sharing_demand-rf_files/figure-latex/unnamed-chunk-8-1} \end{center}

\hypertarget{density-plots}{%
\subsection{density plots}\label{density-plots}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# density plot for each attribute}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{4}\NormalTok{))}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{2}\OperatorTok{:}\DecValTok{9}\NormalTok{) \{}
    \KeywordTok{plot}\NormalTok{(}\KeywordTok{density}\NormalTok{(data[,i]), }\DataTypeTok{main=}\KeywordTok{names}\NormalTok{(data)[i])}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_227-bike_sharing_demand-rf_files/figure-latex/unnamed-chunk-9-1} \end{center}

\hypertarget{boxplots}{%
\subsection{boxplots}\label{boxplots}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# boxplots for each attribute}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{4}\NormalTok{))}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{2}\OperatorTok{:}\DecValTok{9}\NormalTok{) \{}
    \KeywordTok{boxplot}\NormalTok{(data[,i], }\DataTypeTok{main=}\KeywordTok{names}\NormalTok{(data)[i])}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_227-bike_sharing_demand-rf_files/figure-latex/unnamed-chunk-10-1} \end{center}

\hypertarget{unique-values-of-discrete-variables}{%
\subsection{Unique values of discrete variables}\label{unique-values-of-discrete-variables}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# the discrete variables in this case are integers}
\NormalTok{ints <-}\StringTok{ }\KeywordTok{unlist}\NormalTok{(}\KeywordTok{lapply}\NormalTok{(data, is.integer))}
\KeywordTok{names}\NormalTok{(data)[ints]}
\CommentTok{#> [1] "season"     "holiday"    "workingday" "weather"    "humidity"}
\end{Highlighting}
\end{Shaded}

\begin{quote}
Humidity should not be an integer or discrete variable; it is a continuous or numeric variable.
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# convert humidity to numeric}
\NormalTok{data}\OperatorTok{$}\NormalTok{humidity <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(data}\OperatorTok{$}\NormalTok{humidity)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# list unique values of integer variables}
\NormalTok{ints <-}\StringTok{ }\KeywordTok{unlist}\NormalTok{(}\KeywordTok{lapply}\NormalTok{(data, is.integer))}
\NormalTok{int_vars <-}\StringTok{ }\KeywordTok{names}\NormalTok{(data)[ints]}

\KeywordTok{sapply}\NormalTok{(int_vars, }\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{unique}\NormalTok{(data[x]))}
\CommentTok{#> $season.season}
\CommentTok{#> [1] 1 2 3 4}
\CommentTok{#> }
\CommentTok{#> $holiday.holiday}
\CommentTok{#> [1] 0 1}
\CommentTok{#> }
\CommentTok{#> $workingday.workingday}
\CommentTok{#> [1] 0 1}
\CommentTok{#> }
\CommentTok{#> $weather.weather}
\CommentTok{#> [1] 1 2 3 4}
\end{Highlighting}
\end{Shaded}

\hypertarget{inferences}{%
\subsection{Inferences}\label{inferences}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The variables \texttt{season}, \texttt{holiday}, \texttt{workingday} and \texttt{weather} are discrete (integer).
\item
  Activity is even through all seasons.
\item
  Most of the activitity happens during non-holidays.
\item
  Activity doubles during the working days.
\item
  Activity happens mostly during clear (1) weather.
\item
  temp, atemp and humidity are continuous variables (numeric).
\end{enumerate}

\hypertarget{hypothesis-testing-using-multivariate-analysis}{%
\section{Hypothesis Testing (using multivariate analysis)}\label{hypothesis-testing-using-multivariate-analysis}}

Till now, we have got a fair understanding of the data set. Now, let's test the hypothesis which we had generated earlier. Here I have added some additional hypothesis from the dataset. Let's test them one by one:

\hypertarget{hourly-trend}{%
\subsection{Hourly trend}\label{hourly-trend}}

\begin{quote}
\emph{There must be high demand during office timings. Early morning and late evening can have different trend (cyclist) and low demand during 10:00 pm to 4:00 am.}
\end{quote}

We don't have the variable `hour' with us. But we can extract it using the datetime column.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(data}\OperatorTok{$}\NormalTok{datetime)}
\CommentTok{#> [1] 2011-01-01 00:00:00 2011-01-01 01:00:00 2011-01-01 02:00:00}
\CommentTok{#> [4] 2011-01-01 03:00:00 2011-01-01 04:00:00 2011-01-01 05:00:00}
\CommentTok{#> 17379 Levels: 2011-01-01 00:00:00 2011-01-01 01:00:00 ... 2012-12-31 23:00:00}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{(data}\OperatorTok{$}\NormalTok{datetime)}
\CommentTok{#> [1] "factor"}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# show hour and day from the variable datetime}
\KeywordTok{head}\NormalTok{(}\KeywordTok{substr}\NormalTok{(data}\OperatorTok{$}\NormalTok{datetime, }\DecValTok{12}\NormalTok{, }\DecValTok{13}\NormalTok{))  }\CommentTok{# hour}
\CommentTok{#> [1] "00" "01" "02" "03" "04" "05"}
\KeywordTok{head}\NormalTok{(}\KeywordTok{substr}\NormalTok{(data}\OperatorTok{$}\NormalTok{datetime, }\DecValTok{9}\NormalTok{, }\DecValTok{10}\NormalTok{))   }\CommentTok{# day}
\CommentTok{#> [1] "01" "01" "01" "01" "01" "01"}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# extracting hour}
\NormalTok{data}\OperatorTok{$}\NormalTok{hour =}\StringTok{ }\KeywordTok{substr}\NormalTok{(data}\OperatorTok{$}\NormalTok{datetime,}\DecValTok{12}\NormalTok{,}\DecValTok{13}\NormalTok{)}
\NormalTok{data}\OperatorTok{$}\NormalTok{hour =}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(data}\OperatorTok{$}\NormalTok{hour)}
\KeywordTok{head}\NormalTok{(data}\OperatorTok{$}\NormalTok{hour)}
\CommentTok{#> [1] 00 01 02 03 04 05}
\CommentTok{#> 24 Levels: 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 ... 23}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{### dividing again in train and test}
\CommentTok{# the train dataset is for the first 19 days}
\NormalTok{train =}\StringTok{ }\NormalTok{data[}\KeywordTok{as.integer}\NormalTok{(}\KeywordTok{substr}\NormalTok{(data}\OperatorTok{$}\NormalTok{datetime, }\DecValTok{9}\NormalTok{, }\DecValTok{10}\NormalTok{)) }\OperatorTok{<}\StringTok{ }\DecValTok{20}\NormalTok{,]}

\CommentTok{# the test dataset is from day 20 to the end of the month}
\NormalTok{test =}\StringTok{ }\NormalTok{data[}\KeywordTok{as.integer}\NormalTok{(}\KeywordTok{substr}\NormalTok{(data}\OperatorTok{$}\NormalTok{datetime, }\DecValTok{9}\NormalTok{, }\DecValTok{10}\NormalTok{)) }\OperatorTok{>}\StringTok{ }\DecValTok{19}\NormalTok{,]}
\end{Highlighting}
\end{Shaded}

\hypertarget{boxplot-count-vs-hour-in-training-set}{%
\subsection{boxplot count vs hour in training set}\label{boxplot-count-vs-hour-in-training-set}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{boxplot}\NormalTok{(train}\OperatorTok{$}\NormalTok{count }\OperatorTok{~}\StringTok{ }\NormalTok{train}\OperatorTok{$}\NormalTok{hour, }\DataTypeTok{xlab=}\StringTok{"hour"}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{"count of users"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_227-bike_sharing_demand-rf_files/figure-latex/unnamed-chunk-19-1} \end{center}

\begin{quote}
Rides increase from 6 am to 6pm, during office hours.
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# casual users}
\NormalTok{casual <-}\StringTok{ }\NormalTok{data[data}\OperatorTok{$}\NormalTok{casual }\OperatorTok{>}\StringTok{ }\DecValTok{0}\NormalTok{, ]}
\NormalTok{registered <-}\StringTok{ }\NormalTok{data[data}\OperatorTok{$}\NormalTok{registered }\OperatorTok{>}\StringTok{ }\DecValTok{0}\NormalTok{, ]}

\KeywordTok{dim}\NormalTok{(casual)}
\CommentTok{#> [1] 9900   13}
\KeywordTok{dim}\NormalTok{(registered)}
\CommentTok{#> [1] 10871    13}
\end{Highlighting}
\end{Shaded}

\hypertarget{boxplot-hourly-casual-vs-registered-users-in-the-training-set}{%
\subsection{Boxplot hourly: casual vs registered users in the training set}\label{boxplot-hourly-casual-vs-registered-users-in-the-training-set}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# by hour: casual vs registered users}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\KeywordTok{boxplot}\NormalTok{(train}\OperatorTok{$}\NormalTok{casual }\OperatorTok{~}\StringTok{ }\NormalTok{train}\OperatorTok{$}\NormalTok{hour, }\DataTypeTok{xlab=}\StringTok{"hour"}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{"casual users"}\NormalTok{)}
\KeywordTok{boxplot}\NormalTok{(train}\OperatorTok{$}\NormalTok{registered }\OperatorTok{~}\StringTok{ }\NormalTok{train}\OperatorTok{$}\NormalTok{hour, }\DataTypeTok{xlab=}\StringTok{"hour"}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{"registered users"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_227-bike_sharing_demand-rf_files/figure-latex/unnamed-chunk-21-1} \end{center}

\begin{quote}
Casual and Registered users have different distributions. Casual users tend to rent more during office hours.
\end{quote}

\hypertarget{outliers-in-the-training-set}{%
\subsection{outliers in the training set}\label{outliers-in-the-training-set}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\KeywordTok{boxplot}\NormalTok{(train}\OperatorTok{$}\NormalTok{count }\OperatorTok{~}\StringTok{ }\NormalTok{train}\OperatorTok{$}\NormalTok{hour, }\DataTypeTok{xlab=}\StringTok{"hour"}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{"count of users"}\NormalTok{)}
\KeywordTok{boxplot}\NormalTok{(}\KeywordTok{log}\NormalTok{(train}\OperatorTok{$}\NormalTok{count) }\OperatorTok{~}\StringTok{ }\NormalTok{train}\OperatorTok{$}\NormalTok{hour,}\DataTypeTok{xlab=}\StringTok{"hour"}\NormalTok{,}\DataTypeTok{ylab=}\StringTok{"log(count)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_227-bike_sharing_demand-rf_files/figure-latex/unnamed-chunk-22-1} \end{center}

\hypertarget{daily-trend}{%
\subsection{Daily trend}\label{daily-trend}}

\begin{quote}
\emph{Registered users demand more bike on weekdays as compared to weekend or holiday.}
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# extracting days of week}
\NormalTok{date <-}\StringTok{ }\KeywordTok{substr}\NormalTok{(data}\OperatorTok{$}\NormalTok{datetime, }\DecValTok{1}\NormalTok{, }\DecValTok{10}\NormalTok{)}
\NormalTok{days <-}\StringTok{ }\KeywordTok{weekdays}\NormalTok{(}\KeywordTok{as.Date}\NormalTok{(date))}
\NormalTok{data}\OperatorTok{$}\NormalTok{day <-}\StringTok{ }\NormalTok{days}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# split the dataset again at day 20 of the month, before and after}
\NormalTok{train =}\StringTok{ }\NormalTok{data[}\KeywordTok{as.integer}\NormalTok{(}\KeywordTok{substr}\NormalTok{(data}\OperatorTok{$}\NormalTok{datetime,}\DecValTok{9}\NormalTok{,}\DecValTok{10}\NormalTok{)) }\OperatorTok{<}\StringTok{ }\DecValTok{20}\NormalTok{,]}
\NormalTok{test  =}\StringTok{ }\NormalTok{data[}\KeywordTok{as.integer}\NormalTok{(}\KeywordTok{substr}\NormalTok{(data}\OperatorTok{$}\NormalTok{datetime,}\DecValTok{9}\NormalTok{,}\DecValTok{10}\NormalTok{)) }\OperatorTok{>}\StringTok{ }\DecValTok{19}\NormalTok{,]}
\end{Highlighting}
\end{Shaded}

\hypertarget{boxplot-daily-trend-casual-vs-registered-users-training-set}{%
\subsection{Boxplot daily trend: casual vs registered users, training set}\label{boxplot-daily-trend-casual-vs-registered-users-training-set}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# creating boxplots for rentals with different variables to see the variation}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\KeywordTok{boxplot}\NormalTok{(train}\OperatorTok{$}\NormalTok{casual }\OperatorTok{~}\StringTok{ }\NormalTok{train}\OperatorTok{$}\NormalTok{day, }\DataTypeTok{xlab=}\StringTok{"day"}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{"casual users"}\NormalTok{)}
\KeywordTok{boxplot}\NormalTok{(train}\OperatorTok{$}\NormalTok{registered }\OperatorTok{~}\StringTok{ }\NormalTok{train}\OperatorTok{$}\NormalTok{day, }\DataTypeTok{xlab=}\StringTok{"day"}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{"registered users"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_227-bike_sharing_demand-rf_files/figure-latex/unnamed-chunk-25-1} \end{center}

\begin{quote}
Demand of casual users increases during the weekend, contrary of registered users.
\end{quote}

\hypertarget{rain}{%
\subsection{Rain}\label{rain}}

\begin{quote}
\emph{The demand of bikes will be lower on a rainy day as compared to a sunny day. Similarly, higher humidity will cause to lower the demand and vice versa.}
\end{quote}

We use the variable weather (1 to 4) to analyze riding under rain conditions.

\hypertarget{boxplot-of-rain-effect-on-bike-riding-training-set}{%
\subsubsection{Boxplot of rain effect on bike riding, training set}\label{boxplot-of-rain-effect-on-bike-riding-training-set}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\KeywordTok{boxplot}\NormalTok{(train}\OperatorTok{$}\NormalTok{casual }\OperatorTok{~}\StringTok{ }\NormalTok{train}\OperatorTok{$}\NormalTok{weather, }\DataTypeTok{xlab=}\StringTok{"day"}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{"casual users"}\NormalTok{)}
\KeywordTok{boxplot}\NormalTok{(train}\OperatorTok{$}\NormalTok{registered }\OperatorTok{~}\StringTok{ }\NormalTok{train}\OperatorTok{$}\NormalTok{weather, }\DataTypeTok{xlab=}\StringTok{"day"}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{"registered users"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_227-bike_sharing_demand-rf_files/figure-latex/unnamed-chunk-26-1} \end{center}

\begin{quote}
Registered used tend to ride even with rain.
\end{quote}

\hypertarget{temperature}{%
\subsection{Temperature}\label{temperature}}

\begin{quote}
\emph{Would high or low temperature encourage or disencourage bike riding?}
\end{quote}

\hypertarget{boxplot-of-temperature-effect-training-set}{%
\subsubsection{boxplot of temperature effect, training set}\label{boxplot-of-temperature-effect-training-set}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\KeywordTok{boxplot}\NormalTok{(train}\OperatorTok{$}\NormalTok{casual }\OperatorTok{~}\StringTok{ }\NormalTok{train}\OperatorTok{$}\NormalTok{temp, }\DataTypeTok{xlab=}\StringTok{"temp"}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{"casual users"}\NormalTok{)}
\KeywordTok{boxplot}\NormalTok{(train}\OperatorTok{$}\NormalTok{registered }\OperatorTok{~}\StringTok{ }\NormalTok{train}\OperatorTok{$}\NormalTok{temp, }\DataTypeTok{xlab=}\StringTok{"temp"}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{"registered users"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_227-bike_sharing_demand-rf_files/figure-latex/unnamed-chunk-27-1} \end{center}

\begin{quote}
Casual users tend to ride with milder temperatures while registered users ride even at low temperatures.
\end{quote}

\hypertarget{correlation}{%
\subsection{Correlation}\label{correlation}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sub =}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(train}\OperatorTok{$}\NormalTok{registered, train}\OperatorTok{$}\NormalTok{casual, train}\OperatorTok{$}\NormalTok{count, train}\OperatorTok{$}\NormalTok{temp,}
\NormalTok{                 train}\OperatorTok{$}\NormalTok{humidity, train}\OperatorTok{$}\NormalTok{atemp, train}\OperatorTok{$}\NormalTok{windspeed)}
\KeywordTok{cor}\NormalTok{(sub)}
\CommentTok{#>                  train.registered train.casual train.count train.temp}
\CommentTok{#> train.registered           1.0000       0.4972       0.971     0.3186}
\CommentTok{#> train.casual               0.4972       1.0000       0.690     0.4671}
\CommentTok{#> train.count                0.9709       0.6904       1.000     0.3945}
\CommentTok{#> train.temp                 0.3186       0.4671       0.394     1.0000}
\CommentTok{#> train.humidity            -0.2655      -0.3482      -0.317    -0.0649}
\CommentTok{#> train.atemp                0.3146       0.4621       0.390     0.9849}
\CommentTok{#> train.windspeed            0.0911       0.0923       0.101    -0.0179}
\CommentTok{#>                  train.humidity train.atemp train.windspeed}
\CommentTok{#> train.registered        -0.2655      0.3146          0.0911}
\CommentTok{#> train.casual            -0.3482      0.4621          0.0923}
\CommentTok{#> train.count             -0.3174      0.3898          0.1014}
\CommentTok{#> train.temp              -0.0649      0.9849         -0.0179}
\CommentTok{#> train.humidity           1.0000     -0.0435         -0.3186}
\CommentTok{#> train.atemp             -0.0435      1.0000         -0.0575}
\CommentTok{#> train.windspeed         -0.3186     -0.0575          1.0000}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# do not show the diagonal}
\KeywordTok{corrplot}\NormalTok{(}\KeywordTok{cor}\NormalTok{(sub), }\DataTypeTok{diag =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_227-bike_sharing_demand-rf_files/figure-latex/unnamed-chunk-29-1} \end{center}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  correlation between \texttt{casual} and atemp, temp.
\item
  Strong correlation between temp and atemp.
\end{enumerate}

\hypertarget{activity-by-year}{%
\subsection{Activity by year}\label{activity-by-year}}

\hypertarget{year-extraction}{%
\subsubsection{Year extraction}\label{year-extraction}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# extracting year}
\NormalTok{data}\OperatorTok{$}\NormalTok{year =}\StringTok{ }\KeywordTok{substr}\NormalTok{(data}\OperatorTok{$}\NormalTok{datetime, }\DecValTok{1}\NormalTok{, }\DecValTok{4}\NormalTok{)}
\NormalTok{data}\OperatorTok{$}\NormalTok{year =}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(data}\OperatorTok{$}\NormalTok{year)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# ignore the division of data again and again, this could have been done together also}
\NormalTok{train =}\StringTok{ }\NormalTok{data[}\KeywordTok{as.integer}\NormalTok{(}\KeywordTok{substr}\NormalTok{(data}\OperatorTok{$}\NormalTok{datetime,}\DecValTok{9}\NormalTok{,}\DecValTok{10}\NormalTok{)) }\OperatorTok{<}\StringTok{ }\DecValTok{20}\NormalTok{,]}
\NormalTok{test =}\StringTok{ }\NormalTok{data[}\KeywordTok{as.integer}\NormalTok{(}\KeywordTok{substr}\NormalTok{(data}\OperatorTok{$}\NormalTok{datetime,}\DecValTok{9}\NormalTok{,}\DecValTok{10}\NormalTok{)) }\OperatorTok{>}\StringTok{ }\DecValTok{19}\NormalTok{,]}
\end{Highlighting}
\end{Shaded}

\hypertarget{trend-by-year-training-set}{%
\subsubsection{Trend by year, training set}\label{trend-by-year-training-set}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\CommentTok{# again some boxplots with different variables}
\CommentTok{# these boxplots give important information about the dependent variable with respect to the independent variables}
\KeywordTok{boxplot}\NormalTok{(train}\OperatorTok{$}\NormalTok{casual }\OperatorTok{~}\StringTok{ }\NormalTok{train}\OperatorTok{$}\NormalTok{year, }\DataTypeTok{xlab=}\StringTok{"year"}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{"casual users"}\NormalTok{)}
\KeywordTok{boxplot}\NormalTok{(train}\OperatorTok{$}\NormalTok{registered }\OperatorTok{~}\StringTok{ }\NormalTok{train}\OperatorTok{$}\NormalTok{year, }\DataTypeTok{xlab=}\StringTok{"year"}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{"registered users"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_227-bike_sharing_demand-rf_files/figure-latex/unnamed-chunk-32-1} \end{center}

\begin{quote}
Activity increased in 2012.
\end{quote}

\hypertarget{trend-by-windspeed-training-set}{%
\subsubsection{trend by windspeed, training set}\label{trend-by-windspeed-training-set}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\KeywordTok{boxplot}\NormalTok{(train}\OperatorTok{$}\NormalTok{casual }\OperatorTok{~}\StringTok{ }\NormalTok{train}\OperatorTok{$}\NormalTok{windspeed, }\DataTypeTok{xlab=}\StringTok{"windspeed"}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{"casual users"}\NormalTok{)}
\KeywordTok{boxplot}\NormalTok{(train}\OperatorTok{$}\NormalTok{registered }\OperatorTok{~}\StringTok{ }\NormalTok{train}\OperatorTok{$}\NormalTok{windspeed, }\DataTypeTok{xlab=}\StringTok{"windspeed"}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{"registered users"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_227-bike_sharing_demand-rf_files/figure-latex/unnamed-chunk-33-1} \end{center}

\begin{quote}
Casual users ride even with stron winds.
\end{quote}

\hypertarget{trend-by-humidity-training-set}{%
\subsubsection{trend by humidity, training set}\label{trend-by-humidity-training-set}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\KeywordTok{boxplot}\NormalTok{(train}\OperatorTok{$}\NormalTok{casual }\OperatorTok{~}\StringTok{ }\NormalTok{train}\OperatorTok{$}\NormalTok{humidity, }\DataTypeTok{xlab=}\StringTok{"humidity"}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{"casual users"}\NormalTok{)}
\KeywordTok{boxplot}\NormalTok{(train}\OperatorTok{$}\NormalTok{registered }\OperatorTok{~}\StringTok{ }\NormalTok{train}\OperatorTok{$}\NormalTok{humidity, }\DataTypeTok{xlab=}\StringTok{"humidity"}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{"registered users"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_227-bike_sharing_demand-rf_files/figure-latex/unnamed-chunk-34-1} \end{center}

\begin{quote}
Casual users prefer not to ride with humid weather.
\end{quote}

\hypertarget{feature-engineering}{%
\section{Feature Engineering}\label{feature-engineering}}

\hypertarget{prepare-data-1}{%
\subsection{Prepare data}\label{prepare-data-1}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# factoring some variables from integer}
\NormalTok{data}\OperatorTok{$}\NormalTok{season     <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(data}\OperatorTok{$}\NormalTok{season)}
\NormalTok{data}\OperatorTok{$}\NormalTok{weather    <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(data}\OperatorTok{$}\NormalTok{weather)}
\NormalTok{data}\OperatorTok{$}\NormalTok{holiday    <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(data}\OperatorTok{$}\NormalTok{holiday)}
\NormalTok{data}\OperatorTok{$}\NormalTok{workingday <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(data}\OperatorTok{$}\NormalTok{workingday)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# new column}
\NormalTok{data}\OperatorTok{$}\NormalTok{hour <-}\StringTok{ }\KeywordTok{as.integer}\NormalTok{(data}\OperatorTok{$}\NormalTok{hour)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# created this variable to divide a day into parts, but did not finally use it}
\NormalTok{data}\OperatorTok{$}\NormalTok{day_part <-}\StringTok{ }\DecValTok{0}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# split in training and test sets again}
\NormalTok{train <-}\StringTok{ }\NormalTok{data[}\KeywordTok{as.integer}\NormalTok{(}\KeywordTok{substr}\NormalTok{(data}\OperatorTok{$}\NormalTok{datetime, }\DecValTok{9}\NormalTok{, }\DecValTok{10}\NormalTok{)) }\OperatorTok{<}\StringTok{ }\DecValTok{20}\NormalTok{,]}
\NormalTok{test  <-}\StringTok{ }\NormalTok{data[}\KeywordTok{as.integer}\NormalTok{(}\KeywordTok{substr}\NormalTok{(data}\OperatorTok{$}\NormalTok{datetime, }\DecValTok{9}\NormalTok{, }\DecValTok{10}\NormalTok{)) }\OperatorTok{>}\StringTok{ }\DecValTok{19}\NormalTok{,]}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# combine the sets}
\NormalTok{data <-}\StringTok{ }\KeywordTok{rbind}\NormalTok{(train, test)}
\end{Highlighting}
\end{Shaded}

\hypertarget{build-hour-bins}{%
\subsection{Build hour bins}\label{build-hour-bins}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# for registered users}
\NormalTok{d =}\StringTok{ }\KeywordTok{rpart}\NormalTok{(registered }\OperatorTok{~}\StringTok{ }\NormalTok{hour, }\DataTypeTok{data =}\NormalTok{ train)}
\KeywordTok{fancyRpartPlot}\NormalTok{(d)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_227-bike_sharing_demand-rf_files/figure-latex/model-rpart-registered-hour-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# for casual users}
\NormalTok{d =}\StringTok{ }\KeywordTok{rpart}\NormalTok{(casual }\OperatorTok{~}\StringTok{ }\NormalTok{hour, }\DataTypeTok{data =}\NormalTok{ train)}
\KeywordTok{fancyRpartPlot}\NormalTok{(d)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_227-bike_sharing_demand-rf_files/figure-latex/model-rpart-casual-hour-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Assign the timings according to tree}
\CommentTok{# fill the hour bins}
\NormalTok{data =}\StringTok{ }\KeywordTok{rbind}\NormalTok{(train,test)}

\CommentTok{# create hour buckets for registered users}
\CommentTok{# 0,1,2,3,4,5,6,7 < 7.5}
\CommentTok{# 22,23,24 >=22}
\CommentTok{# 10,11,12,13,14,15,16,17: h>=9.5 & h<18}
\CommentTok{# h<9.5 & h<8.5 : 8}
\CommentTok{# h<9.5 & h>=8.5 : 9}
\CommentTok{# h>=20: 20,21}
\CommentTok{# h < 20: 18,19}

\NormalTok{data}\OperatorTok{$}\NormalTok{dp_reg =}\StringTok{ }\DecValTok{0}
\NormalTok{data}\OperatorTok{$}\NormalTok{dp_reg[data}\OperatorTok{$}\NormalTok{hour }\OperatorTok{<}\StringTok{ }\DecValTok{8}\NormalTok{] =}\StringTok{ }\DecValTok{1}
\NormalTok{data}\OperatorTok{$}\NormalTok{dp_reg[data}\OperatorTok{$}\NormalTok{hour }\OperatorTok{>=}\StringTok{ }\DecValTok{22}\NormalTok{] =}\StringTok{ }\DecValTok{2}
\NormalTok{data}\OperatorTok{$}\NormalTok{dp_reg[data}\OperatorTok{$}\NormalTok{hour }\OperatorTok{>}\StringTok{ }\DecValTok{9} \OperatorTok{&}\StringTok{ }\NormalTok{data}\OperatorTok{$}\NormalTok{hour }\OperatorTok{<}\StringTok{ }\DecValTok{18}\NormalTok{] =}\StringTok{ }\DecValTok{3}
\NormalTok{data}\OperatorTok{$}\NormalTok{dp_reg[data}\OperatorTok{$}\NormalTok{hour }\OperatorTok{==}\StringTok{ }\DecValTok{8}\NormalTok{] =}\StringTok{ }\DecValTok{4}
\NormalTok{data}\OperatorTok{$}\NormalTok{dp_reg[data}\OperatorTok{$}\NormalTok{hour }\OperatorTok{==}\StringTok{ }\DecValTok{9}\NormalTok{] =}\StringTok{ }\DecValTok{5}
\NormalTok{data}\OperatorTok{$}\NormalTok{dp_reg[data}\OperatorTok{$}\NormalTok{hour }\OperatorTok{==}\StringTok{ }\DecValTok{20} \OperatorTok{|}\StringTok{ }\NormalTok{data}\OperatorTok{$}\NormalTok{hour }\OperatorTok{==}\StringTok{ }\DecValTok{21}\NormalTok{] =}\StringTok{ }\DecValTok{6}
\NormalTok{data}\OperatorTok{$}\NormalTok{dp_reg[data}\OperatorTok{$}\NormalTok{hour }\OperatorTok{==}\StringTok{ }\DecValTok{19} \OperatorTok{|}\StringTok{ }\NormalTok{data}\OperatorTok{$}\NormalTok{hour }\OperatorTok{==}\StringTok{ }\DecValTok{18}\NormalTok{] =}\StringTok{ }\DecValTok{7}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# casual users}
\CommentTok{# h<11, h<8.5: 0,1,2,3,4,5,6,7,8}
\CommentTok{# h>=8.5 & h<11: 9, 10 }
\CommentTok{# h >=11 & h>=21: 21,22,23,24}
\CommentTok{# h >=11 & h<21: 11,12,13,14,15,16,17,18,19,20}
\NormalTok{data}\OperatorTok{$}\NormalTok{dp_cas =}\StringTok{ }\DecValTok{0}
\NormalTok{data}\OperatorTok{$}\NormalTok{dp_cas[data}\OperatorTok{$}\NormalTok{hour }\OperatorTok{<}\StringTok{ }\DecValTok{11} \OperatorTok{&}\StringTok{ }\NormalTok{data}\OperatorTok{$}\NormalTok{hour }\OperatorTok{>=}\StringTok{ }\DecValTok{8}\NormalTok{] =}\StringTok{ }\DecValTok{1}
\NormalTok{data}\OperatorTok{$}\NormalTok{dp_cas[data}\OperatorTok{$}\NormalTok{hour }\OperatorTok{==}\StringTok{ }\DecValTok{9} \OperatorTok{|}\StringTok{ }\NormalTok{data}\OperatorTok{$}\NormalTok{hour }\OperatorTok{==}\StringTok{ }\DecValTok{10}\NormalTok{] =}\StringTok{ }\DecValTok{2}
\NormalTok{data}\OperatorTok{$}\NormalTok{dp_cas[data}\OperatorTok{$}\NormalTok{hour }\OperatorTok{>=}\StringTok{ }\DecValTok{11} \OperatorTok{&}\StringTok{ }\NormalTok{data}\OperatorTok{$}\NormalTok{hour }\OperatorTok{<}\StringTok{ }\DecValTok{21}\NormalTok{] =}\StringTok{ }\DecValTok{3}
\NormalTok{data}\OperatorTok{$}\NormalTok{dp_cas[data}\OperatorTok{$}\NormalTok{hour }\OperatorTok{>=}\StringTok{ }\DecValTok{21}\NormalTok{] =}\StringTok{ }\DecValTok{4}
\end{Highlighting}
\end{Shaded}

\hypertarget{temperature-bins}{%
\subsection{Temperature bins}\label{temperature-bins}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# partition the data by temperature, registered users}
\NormalTok{f =}\StringTok{ }\KeywordTok{rpart}\NormalTok{(registered }\OperatorTok{~}\StringTok{ }\NormalTok{temp, }\DataTypeTok{data=}\NormalTok{train)}
\KeywordTok{fancyRpartPlot}\NormalTok{(f)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_227-bike_sharing_demand-rf_files/figure-latex/model-rpart-temperature-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# partition the data by temperature,, casual users}
\NormalTok{f=}\KeywordTok{rpart}\NormalTok{(casual }\OperatorTok{~}\StringTok{ }\NormalTok{temp, }\DataTypeTok{data=}\NormalTok{train)}
\KeywordTok{fancyRpartPlot}\NormalTok{(f)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_227-bike_sharing_demand-rf_files/figure-latex/unnamed-chunk-42-1} \end{center}

\hypertarget{assign-temperature-ranges-accoding-to-trees}{%
\subsubsection{Assign temperature ranges accoding to trees}\label{assign-temperature-ranges-accoding-to-trees}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data}\OperatorTok{$}\NormalTok{temp_reg =}\StringTok{ }\DecValTok{0}
\NormalTok{data}\OperatorTok{$}\NormalTok{temp_reg[data}\OperatorTok{$}\NormalTok{temp }\OperatorTok{<}\StringTok{ }\DecValTok{13}\NormalTok{] =}\StringTok{ }\DecValTok{1}
\NormalTok{data}\OperatorTok{$}\NormalTok{temp_reg[data}\OperatorTok{$}\NormalTok{temp }\OperatorTok{>=}\StringTok{ }\DecValTok{13} \OperatorTok{&}\StringTok{ }\NormalTok{data}\OperatorTok{$}\NormalTok{temp }\OperatorTok{<}\StringTok{ }\DecValTok{23}\NormalTok{] =}\StringTok{ }\DecValTok{2}
\NormalTok{data}\OperatorTok{$}\NormalTok{temp_reg[data}\OperatorTok{$}\NormalTok{temp }\OperatorTok{>=}\StringTok{ }\DecValTok{23} \OperatorTok{&}\StringTok{ }\NormalTok{data}\OperatorTok{$}\NormalTok{temp }\OperatorTok{<}\StringTok{ }\DecValTok{30}\NormalTok{] =}\StringTok{ }\DecValTok{3}
\NormalTok{data}\OperatorTok{$}\NormalTok{temp_reg[data}\OperatorTok{$}\NormalTok{temp }\OperatorTok{>=}\StringTok{ }\DecValTok{30}\NormalTok{] =}\StringTok{ }\DecValTok{4}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data}\OperatorTok{$}\NormalTok{temp_cas =}\StringTok{ }\DecValTok{0}
\NormalTok{data}\OperatorTok{$}\NormalTok{temp_cas[data}\OperatorTok{$}\NormalTok{temp }\OperatorTok{<}\StringTok{ }\DecValTok{15}\NormalTok{] =}\StringTok{ }\DecValTok{1}
\NormalTok{data}\OperatorTok{$}\NormalTok{temp_cas[data}\OperatorTok{$}\NormalTok{temp }\OperatorTok{>=}\StringTok{ }\DecValTok{15} \OperatorTok{&}\StringTok{ }\NormalTok{data}\OperatorTok{$}\NormalTok{temp }\OperatorTok{<}\StringTok{ }\DecValTok{23}\NormalTok{] =}\StringTok{ }\DecValTok{2}
\NormalTok{data}\OperatorTok{$}\NormalTok{temp_cas[data}\OperatorTok{$}\NormalTok{temp }\OperatorTok{>=}\StringTok{ }\DecValTok{23} \OperatorTok{&}\StringTok{ }\NormalTok{data}\OperatorTok{$}\NormalTok{temp }\OperatorTok{<}\StringTok{ }\DecValTok{30}\NormalTok{] =}\StringTok{ }\DecValTok{3}
\NormalTok{data}\OperatorTok{$}\NormalTok{temp_cas[data}\OperatorTok{$}\NormalTok{temp }\OperatorTok{>=}\StringTok{ }\DecValTok{30}\NormalTok{] =}\StringTok{ }\DecValTok{4}
\end{Highlighting}
\end{Shaded}

\hypertarget{year-bins-by-quarter}{%
\subsection{Year bins by quarter}\label{year-bins-by-quarter}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# add new variable with the month number}
\NormalTok{data}\OperatorTok{$}\NormalTok{month <-}\StringTok{ }\KeywordTok{substr}\NormalTok{(data}\OperatorTok{$}\NormalTok{datetime, }\DecValTok{6}\NormalTok{, }\DecValTok{7}\NormalTok{)}
\NormalTok{data}\OperatorTok{$}\NormalTok{month <-}\StringTok{ }\KeywordTok{as.integer}\NormalTok{(data}\OperatorTok{$}\NormalTok{month)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# bin by quarter manually}
\NormalTok{data}\OperatorTok{$}\NormalTok{year_part[data}\OperatorTok{$}\NormalTok{year}\OperatorTok{==}\StringTok{'2011'}\NormalTok{]                =}\StringTok{ }\DecValTok{1}
\NormalTok{data}\OperatorTok{$}\NormalTok{year_part[data}\OperatorTok{$}\NormalTok{year}\OperatorTok{==}\StringTok{'2011'} \OperatorTok{&}\StringTok{ }\NormalTok{data}\OperatorTok{$}\NormalTok{month}\OperatorTok{>}\DecValTok{3}\NormalTok{] =}\StringTok{ }\DecValTok{2}
\NormalTok{data}\OperatorTok{$}\NormalTok{year_part[data}\OperatorTok{$}\NormalTok{year}\OperatorTok{==}\StringTok{'2011'} \OperatorTok{&}\StringTok{ }\NormalTok{data}\OperatorTok{$}\NormalTok{month}\OperatorTok{>}\DecValTok{6}\NormalTok{] =}\StringTok{ }\DecValTok{3}
\NormalTok{data}\OperatorTok{$}\NormalTok{year_part[data}\OperatorTok{$}\NormalTok{year}\OperatorTok{==}\StringTok{'2011'} \OperatorTok{&}\StringTok{ }\NormalTok{data}\OperatorTok{$}\NormalTok{month}\OperatorTok{>}\DecValTok{9}\NormalTok{] =}\StringTok{ }\DecValTok{4}
\NormalTok{data}\OperatorTok{$}\NormalTok{year_part[data}\OperatorTok{$}\NormalTok{year}\OperatorTok{==}\StringTok{'2012'}\NormalTok{]                =}\StringTok{ }\DecValTok{5}
\NormalTok{data}\OperatorTok{$}\NormalTok{year_part[data}\OperatorTok{$}\NormalTok{year}\OperatorTok{==}\StringTok{'2012'} \OperatorTok{&}\StringTok{ }\NormalTok{data}\OperatorTok{$}\NormalTok{month}\OperatorTok{>}\DecValTok{3}\NormalTok{] =}\StringTok{ }\DecValTok{6}
\NormalTok{data}\OperatorTok{$}\NormalTok{year_part[data}\OperatorTok{$}\NormalTok{year}\OperatorTok{==}\StringTok{'2012'} \OperatorTok{&}\StringTok{ }\NormalTok{data}\OperatorTok{$}\NormalTok{month}\OperatorTok{>}\DecValTok{6}\NormalTok{] =}\StringTok{ }\DecValTok{7}
\NormalTok{data}\OperatorTok{$}\NormalTok{year_part[data}\OperatorTok{$}\NormalTok{year}\OperatorTok{==}\StringTok{'2012'} \OperatorTok{&}\StringTok{ }\NormalTok{data}\OperatorTok{$}\NormalTok{month}\OperatorTok{>}\DecValTok{9}\NormalTok{] =}\StringTok{ }\DecValTok{8}
\KeywordTok{table}\NormalTok{(data}\OperatorTok{$}\NormalTok{year_part)}
\CommentTok{#> }
\CommentTok{#>    1    2    3    4    5    6    7    8 }
\CommentTok{#> 2067 2183 2192 2203 2176 2182 2208 2168}
\end{Highlighting}
\end{Shaded}

\hypertarget{day-type}{%
\subsection{Day Type}\label{day-type}}

Created a variable having categories like ``weekday'', ``weekend'' and ``holiday''.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# creating another variable day_type which may affect our accuracy as weekends and weekdays are important in deciding rentals}
\NormalTok{data}\OperatorTok{$}\NormalTok{day_type =}\StringTok{ }\DecValTok{0}
\NormalTok{data}\OperatorTok{$}\NormalTok{day_type[data}\OperatorTok{$}\NormalTok{holiday}\OperatorTok{==}\DecValTok{0} \OperatorTok{&}\StringTok{ }\NormalTok{data}\OperatorTok{$}\NormalTok{workingday}\OperatorTok{==}\DecValTok{0}\NormalTok{] =}\StringTok{ "weekend"}
\NormalTok{data}\OperatorTok{$}\NormalTok{day_type[data}\OperatorTok{$}\NormalTok{holiday}\OperatorTok{==}\DecValTok{1}\NormalTok{]                      =}\StringTok{ "holiday"}
\NormalTok{data}\OperatorTok{$}\NormalTok{day_type[data}\OperatorTok{$}\NormalTok{holiday}\OperatorTok{==}\DecValTok{0} \OperatorTok{&}\StringTok{ }\NormalTok{data}\OperatorTok{$}\NormalTok{workingday}\OperatorTok{==}\DecValTok{1}\NormalTok{] =}\StringTok{ "working day"}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# split dataset again}
\NormalTok{train =}\StringTok{ }\NormalTok{data[}\KeywordTok{as.integer}\NormalTok{(}\KeywordTok{substr}\NormalTok{(data}\OperatorTok{$}\NormalTok{datetime,}\DecValTok{9}\NormalTok{,}\DecValTok{10}\NormalTok{)) }\OperatorTok{<}\StringTok{ }\DecValTok{20}\NormalTok{,]}
\NormalTok{test =}\StringTok{ }\NormalTok{data[}\KeywordTok{as.integer}\NormalTok{(}\KeywordTok{substr}\NormalTok{(data}\OperatorTok{$}\NormalTok{datetime,}\DecValTok{9}\NormalTok{,}\DecValTok{10}\NormalTok{)) }\OperatorTok{>}\StringTok{ }\DecValTok{19}\NormalTok{,]}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\KeywordTok{boxplot}\NormalTok{(train}\OperatorTok{$}\NormalTok{casual }\OperatorTok{~}\StringTok{ }\NormalTok{train}\OperatorTok{$}\NormalTok{dp_cas, }\DataTypeTok{xlab =} \StringTok{"day partition"}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{"casual users"}\NormalTok{)}
\KeywordTok{boxplot}\NormalTok{(train}\OperatorTok{$}\NormalTok{registered }\OperatorTok{~}\StringTok{ }\NormalTok{train}\OperatorTok{$}\NormalTok{dp_reg, }\DataTypeTok{xlab =} \StringTok{"day partition"}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{"registered users"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_227-bike_sharing_demand-rf_files/figure-latex/unnamed-chunk-49-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\KeywordTok{boxplot}\NormalTok{(train}\OperatorTok{$}\NormalTok{casual }\OperatorTok{~}\StringTok{ }\NormalTok{train}\OperatorTok{$}\NormalTok{day_type, }\DataTypeTok{xlab =} \StringTok{"day type"}\NormalTok{, }
        \DataTypeTok{ylab=}\StringTok{"casual users"}\NormalTok{, }\DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{900}\NormalTok{))}
\KeywordTok{boxplot}\NormalTok{(train}\OperatorTok{$}\NormalTok{registered }\OperatorTok{~}\StringTok{ }\NormalTok{train}\OperatorTok{$}\NormalTok{day_type, }\DataTypeTok{xlab =} \StringTok{"day type"}\NormalTok{, }
        \DataTypeTok{ylab=}\StringTok{"registered users"}\NormalTok{, }\DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{900}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_227-bike_sharing_demand-rf_files/figure-latex/unnamed-chunk-50-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\KeywordTok{boxplot}\NormalTok{(train}\OperatorTok{$}\NormalTok{casual }\OperatorTok{~}\StringTok{ }\NormalTok{train}\OperatorTok{$}\NormalTok{year_part, }\DataTypeTok{xlab =} \StringTok{"year partition, quarter"}\NormalTok{, }
        \DataTypeTok{ylab=}\StringTok{"casual users"}\NormalTok{, }\DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{900}\NormalTok{))}
\KeywordTok{boxplot}\NormalTok{(train}\OperatorTok{$}\NormalTok{registered }\OperatorTok{~}\StringTok{ }\NormalTok{train}\OperatorTok{$}\NormalTok{year_part, }\DataTypeTok{xlab =} \StringTok{"year partition, quarter"}\NormalTok{, }
        \DataTypeTok{ylab=}\StringTok{"registered users"}\NormalTok{, }\DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{900}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_227-bike_sharing_demand-rf_files/figure-latex/unnamed-chunk-51-1} \end{center}

\hypertarget{temperatures}{%
\subsection{Temperatures}\label{temperatures}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\KeywordTok{boxplot}\NormalTok{(train}\OperatorTok{$}\NormalTok{casual }\OperatorTok{~}\StringTok{ }\NormalTok{train}\OperatorTok{$}\NormalTok{temp, }\DataTypeTok{xlab =} \StringTok{"temperature"}\NormalTok{, }
        \DataTypeTok{ylab=}\StringTok{"casual users"}\NormalTok{, }\DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{900}\NormalTok{))}
\KeywordTok{boxplot}\NormalTok{(train}\OperatorTok{$}\NormalTok{registered }\OperatorTok{~}\StringTok{ }\NormalTok{train}\OperatorTok{$}\NormalTok{temp, }\DataTypeTok{xlab =} \StringTok{"temperature"}\NormalTok{, }
        \DataTypeTok{ylab=}\StringTok{"registered users"}\NormalTok{, }\DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{900}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_227-bike_sharing_demand-rf_files/figure-latex/unnamed-chunk-52-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(train}\OperatorTok{$}\NormalTok{temp, train}\OperatorTok{$}\NormalTok{count)}
\NormalTok{data <-}\StringTok{ }\KeywordTok{rbind}\NormalTok{(train, test)}
\CommentTok{# data$month <- substr(data$datetime, 6, 7)}
\CommentTok{# data$month <- as.integer(data$month)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_227-bike_sharing_demand-rf_files/figure-latex/unnamed-chunk-53-1} \end{center}

\hypertarget{imputting-missing-data-to-wind-speed}{%
\subsection{Imputting missing data to wind speed}\label{imputting-missing-data-to-wind-speed}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# dividing total data depending on windspeed to impute/predict the missing values}
\KeywordTok{table}\NormalTok{(data}\OperatorTok{$}\NormalTok{windspeed }\OperatorTok{==}\StringTok{ }\DecValTok{0}\NormalTok{)}
\CommentTok{#> }
\CommentTok{#> FALSE  TRUE }
\CommentTok{#> 15199  2180}
    \CommentTok{# FALSE  TRUE }
    \CommentTok{# 15199  2180 }

\NormalTok{k =}\StringTok{ }\NormalTok{data}\OperatorTok{$}\NormalTok{windspeed }\OperatorTok{==}\StringTok{ }\DecValTok{0}

\NormalTok{wind_}\DecValTok{0}\NormalTok{ =}\StringTok{ }\KeywordTok{subset}\NormalTok{(data, k)    }\CommentTok{# windspeed is zero}
\NormalTok{wind_}\DecValTok{1}\NormalTok{ =}\StringTok{ }\KeywordTok{subset}\NormalTok{(data, }\OperatorTok{!}\NormalTok{k)   }\CommentTok{# windspeed not zero}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{tic}\NormalTok{()}
\CommentTok{# predicting missing values in windspeed using a random forest model}
\CommentTok{# this is a different approach to impute missing values rather than }
\CommentTok{# just using the mean or median or some other statistic for imputation}

\KeywordTok{set.seed}\NormalTok{(}\DecValTok{415}\NormalTok{)}
\NormalTok{fit <-}\StringTok{ }\KeywordTok{randomForest}\NormalTok{(windspeed }\OperatorTok{~}\StringTok{ }\NormalTok{season }\OperatorTok{+}\StringTok{ }\NormalTok{weather }\OperatorTok{+}\StringTok{ }\NormalTok{humidity }\OperatorTok{+}\StringTok{ }\NormalTok{month }\OperatorTok{+}\StringTok{ }\NormalTok{temp }\OperatorTok{+}\StringTok{ }
\StringTok{                        }\NormalTok{year }\OperatorTok{+}\StringTok{ }\NormalTok{atemp, }
                    \DataTypeTok{data =}\NormalTok{ wind_}\DecValTok{1}\NormalTok{, }
                    \DataTypeTok{importance =} \OtherTok{TRUE}\NormalTok{, }
                    \DataTypeTok{ntree =} \DecValTok{250}\NormalTok{)}

\NormalTok{pred =}\StringTok{ }\KeywordTok{predict}\NormalTok{(fit, wind_}\DecValTok{0}\NormalTok{)}
\NormalTok{wind_}\DecValTok{0}\OperatorTok{$}\NormalTok{windspeed =}\StringTok{ }\NormalTok{pred       }\CommentTok{# fill with wind speed predictions}
\KeywordTok{toc}\NormalTok{()}
\CommentTok{#> 63.95 sec elapsed}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# recompose the whole dataset}
\NormalTok{data =}\StringTok{ }\KeywordTok{rbind}\NormalTok{(wind_}\DecValTok{0}\NormalTok{, wind_}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# how many zero values now?}
\KeywordTok{sum}\NormalTok{(data}\OperatorTok{$}\NormalTok{windspeed }\OperatorTok{==}\StringTok{ }\DecValTok{0}\NormalTok{)}
\CommentTok{#> [1] 0}
\end{Highlighting}
\end{Shaded}

\hypertarget{weekend-variable}{%
\subsection{Weekend variable}\label{weekend-variable}}

Created a separate variable for weekend (0/1)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data}\OperatorTok{$}\NormalTok{weekend =}\StringTok{ }\DecValTok{0}
\NormalTok{data}\OperatorTok{$}\NormalTok{weekend[data}\OperatorTok{$}\NormalTok{day}\OperatorTok{==}\StringTok{"Sunday"} \OperatorTok{|}\StringTok{ }\NormalTok{data}\OperatorTok{$}\NormalTok{day}\OperatorTok{==}\StringTok{"Saturday"}\NormalTok{ ] =}\StringTok{ }\DecValTok{1}
\end{Highlighting}
\end{Shaded}

\hypertarget{model-building}{%
\section{Model Building}\label{model-building}}

As this was our first attempt, we applied decision tree, conditional inference tree and random forest algorithms and found that random forest is performing the best. You can also go with regression, boosted regression, neural network and find which one is working well for you.

Before executing the random forest model code, I have followed following steps:

Convert discrete variables into factor (weather, season, hour, holiday, working day, month, day)

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str}\NormalTok{(data)}
\CommentTok{#> 'data.frame':    17379 obs. of  24 variables:}
\CommentTok{#>  $ datetime  : Factor w/ 17379 levels "2011-01-01 00:00:00",..: 1 2 3 4 5 7 8 9 10 65 ...}
\CommentTok{#>  $ season    : Factor w/ 4 levels "1","2","3","4": 1 1 1 1 1 1 1 1 1 1 ...}
\CommentTok{#>  $ holiday   : Factor w/ 2 levels "0","1": 1 1 1 1 1 1 1 1 1 1 ...}
\CommentTok{#>  $ workingday: Factor w/ 2 levels "0","1": 1 1 1 1 1 1 1 1 1 2 ...}
\CommentTok{#>  $ weather   : Factor w/ 4 levels "1","2","3","4": 1 1 1 1 1 1 1 1 1 1 ...}
\CommentTok{#>  $ temp      : num  9.84 9.02 9.02 9.84 9.84 ...}
\CommentTok{#>  $ atemp     : num  14.4 13.6 13.6 14.4 14.4 ...}
\CommentTok{#>  $ humidity  : num  81 80 80 75 75 80 86 75 76 47 ...}
\CommentTok{#>  $ windspeed : num  9.03 9.05 9.05 9.15 9.15 ...}
\CommentTok{#>  $ casual    : num  3 8 5 3 0 2 1 1 8 8 ...}
\CommentTok{#>  $ registered: num  13 32 27 10 1 0 2 7 6 102 ...}
\CommentTok{#>  $ count     : num  16 40 32 13 1 2 3 8 14 110 ...}
\CommentTok{#>  $ hour      : int  1 2 3 4 5 7 8 9 10 20 ...}
\CommentTok{#>  $ day       : chr  "Saturday" "Saturday" "Saturday" "Saturday" ...}
\CommentTok{#>  $ year      : Factor w/ 2 levels "2011","2012": 1 1 1 1 1 1 1 1 1 1 ...}
\CommentTok{#>  $ day_part  : num  0 0 0 0 0 0 0 0 0 0 ...}
\CommentTok{#>  $ dp_reg    : num  1 1 1 1 1 1 4 5 3 6 ...}
\CommentTok{#>  $ dp_cas    : num  0 0 0 0 0 0 1 2 2 3 ...}
\CommentTok{#>  $ temp_reg  : num  1 1 1 1 1 1 1 1 2 1 ...}
\CommentTok{#>  $ temp_cas  : num  1 1 1 1 1 1 1 1 1 1 ...}
\CommentTok{#>  $ month     : int  1 1 1 1 1 1 1 1 1 1 ...}
\CommentTok{#>  $ year_part : num  1 1 1 1 1 1 1 1 1 1 ...}
\CommentTok{#>  $ day_type  : chr  "weekend" "weekend" "weekend" "weekend" ...}
\CommentTok{#>  $ weekend   : num  1 1 1 1 1 1 1 1 1 0 ...}
\end{Highlighting}
\end{Shaded}

\hypertarget{convert-variables-to-factors}{%
\subsection{Convert variables to factors}\label{convert-variables-to-factors}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# converting all relevant categorical variables into factors to feed to our random forest model}
\NormalTok{data}\OperatorTok{$}\NormalTok{season     =}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(data}\OperatorTok{$}\NormalTok{season)}
\NormalTok{data}\OperatorTok{$}\NormalTok{holiday    =}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(data}\OperatorTok{$}\NormalTok{holiday)}
\NormalTok{data}\OperatorTok{$}\NormalTok{workingday =}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(data}\OperatorTok{$}\NormalTok{workingday)}
\NormalTok{data}\OperatorTok{$}\NormalTok{weather    =}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(data}\OperatorTok{$}\NormalTok{weather)}
\NormalTok{data}\OperatorTok{$}\NormalTok{hour       =}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(data}\OperatorTok{$}\NormalTok{hour)}
\NormalTok{data}\OperatorTok{$}\NormalTok{month      =}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(data}\OperatorTok{$}\NormalTok{month)}
\NormalTok{data}\OperatorTok{$}\NormalTok{day_part   =}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(data}\OperatorTok{$}\NormalTok{dp_cas)}
\NormalTok{data}\OperatorTok{$}\NormalTok{day_type   =}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(data}\OperatorTok{$}\NormalTok{dp_reg)}
\NormalTok{data}\OperatorTok{$}\NormalTok{day        =}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(data}\OperatorTok{$}\NormalTok{day)}
\NormalTok{data}\OperatorTok{$}\NormalTok{temp_cas   =}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(data}\OperatorTok{$}\NormalTok{temp_cas)}
\NormalTok{data}\OperatorTok{$}\NormalTok{temp_reg   =}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(data}\OperatorTok{$}\NormalTok{temp_reg)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str}\NormalTok{(data)}
\CommentTok{#> 'data.frame':    17379 obs. of  24 variables:}
\CommentTok{#>  $ datetime  : Factor w/ 17379 levels "2011-01-01 00:00:00",..: 1 2 3 4 5 7 8 9 10 65 ...}
\CommentTok{#>  $ season    : Factor w/ 4 levels "1","2","3","4": 1 1 1 1 1 1 1 1 1 1 ...}
\CommentTok{#>  $ holiday   : Factor w/ 2 levels "0","1": 1 1 1 1 1 1 1 1 1 1 ...}
\CommentTok{#>  $ workingday: Factor w/ 2 levels "0","1": 1 1 1 1 1 1 1 1 1 2 ...}
\CommentTok{#>  $ weather   : Factor w/ 4 levels "1","2","3","4": 1 1 1 1 1 1 1 1 1 1 ...}
\CommentTok{#>  $ temp      : num  9.84 9.02 9.02 9.84 9.84 ...}
\CommentTok{#>  $ atemp     : num  14.4 13.6 13.6 14.4 14.4 ...}
\CommentTok{#>  $ humidity  : num  81 80 80 75 75 80 86 75 76 47 ...}
\CommentTok{#>  $ windspeed : num  9.03 9.05 9.05 9.15 9.15 ...}
\CommentTok{#>  $ casual    : num  3 8 5 3 0 2 1 1 8 8 ...}
\CommentTok{#>  $ registered: num  13 32 27 10 1 0 2 7 6 102 ...}
\CommentTok{#>  $ count     : num  16 40 32 13 1 2 3 8 14 110 ...}
\CommentTok{#>  $ hour      : Factor w/ 24 levels "1","2","3","4",..: 1 2 3 4 5 7 8 9 10 20 ...}
\CommentTok{#>  $ day       : Factor w/ 7 levels "Friday","Monday",..: 3 3 3 3 3 3 3 3 3 2 ...}
\CommentTok{#>  $ year      : Factor w/ 2 levels "2011","2012": 1 1 1 1 1 1 1 1 1 1 ...}
\CommentTok{#>  $ day_part  : Factor w/ 5 levels "0","1","2","3",..: 1 1 1 1 1 1 2 3 3 4 ...}
\CommentTok{#>  $ dp_reg    : num  1 1 1 1 1 1 4 5 3 6 ...}
\CommentTok{#>  $ dp_cas    : num  0 0 0 0 0 0 1 2 2 3 ...}
\CommentTok{#>  $ temp_reg  : Factor w/ 4 levels "1","2","3","4": 1 1 1 1 1 1 1 1 2 1 ...}
\CommentTok{#>  $ temp_cas  : Factor w/ 4 levels "1","2","3","4": 1 1 1 1 1 1 1 1 1 1 ...}
\CommentTok{#>  $ month     : Factor w/ 12 levels "1","2","3","4",..: 1 1 1 1 1 1 1 1 1 1 ...}
\CommentTok{#>  $ year_part : num  1 1 1 1 1 1 1 1 1 1 ...}
\CommentTok{#>  $ day_type  : Factor w/ 7 levels "1","2","3","4",..: 1 1 1 1 1 1 4 5 3 6 ...}
\CommentTok{#>  $ weekend   : num  1 1 1 1 1 1 1 1 1 0 ...}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\item
  As we know that dependent variables have natural outliers so we will predict log of dependent variables.
\item
  Predict bike demand registered and casual users separately.
  \(y1 = \log(casual+1)\) and \(y2 = \log(registered+1)\), Here we have added 1 to deal with zero values in the casual and registered columns.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# separate again as train and test set}
\NormalTok{train =}\StringTok{ }\NormalTok{data[}\KeywordTok{as.integer}\NormalTok{(}\KeywordTok{substr}\NormalTok{(data}\OperatorTok{$}\NormalTok{datetime, }\DecValTok{9}\NormalTok{, }\DecValTok{10}\NormalTok{)) }\OperatorTok{<}\StringTok{ }\DecValTok{20}\NormalTok{,]}
\NormalTok{test =}\StringTok{ }\NormalTok{data[}\KeywordTok{as.integer}\NormalTok{(}\KeywordTok{substr}\NormalTok{(data}\OperatorTok{$}\NormalTok{datetime, }\DecValTok{9}\NormalTok{, }\DecValTok{10}\NormalTok{)) }\OperatorTok{>}\StringTok{ }\DecValTok{19}\NormalTok{,]}
\end{Highlighting}
\end{Shaded}

\hypertarget{log-transform}{%
\subsection{Log transform}\label{log-transform}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# log transformation for some skewed variables, }
\CommentTok{# which can be seen from their distribution}
\NormalTok{train}\OperatorTok{$}\NormalTok{reg1   =}\StringTok{ }\NormalTok{train}\OperatorTok{$}\NormalTok{registered }\OperatorTok{+}\StringTok{ }\DecValTok{1}
\NormalTok{train}\OperatorTok{$}\NormalTok{cas1   =}\StringTok{ }\NormalTok{train}\OperatorTok{$}\NormalTok{casual }\OperatorTok{+}\StringTok{ }\DecValTok{1}
\NormalTok{train}\OperatorTok{$}\NormalTok{logcas =}\StringTok{ }\KeywordTok{log}\NormalTok{(train}\OperatorTok{$}\NormalTok{cas1)}
\NormalTok{train}\OperatorTok{$}\NormalTok{logreg =}\StringTok{ }\KeywordTok{log}\NormalTok{(train}\OperatorTok{$}\NormalTok{reg1)}
\NormalTok{test}\OperatorTok{$}\NormalTok{logreg  =}\StringTok{ }\DecValTok{0}
\NormalTok{test}\OperatorTok{$}\NormalTok{logcas  =}\StringTok{ }\DecValTok{0}
\end{Highlighting}
\end{Shaded}

\hypertarget{plot-by-weather-by-season}{%
\subsubsection{Plot by weather, by season}\label{plot-by-weather-by-season}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# cartesian plot}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\KeywordTok{boxplot}\NormalTok{(train}\OperatorTok{$}\NormalTok{registered }\OperatorTok{~}\StringTok{ }\NormalTok{train}\OperatorTok{$}\NormalTok{weather, }\DataTypeTok{xlab=}\StringTok{"weather"}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{"registered users"}\NormalTok{)}
\KeywordTok{boxplot}\NormalTok{(train}\OperatorTok{$}\NormalTok{registered }\OperatorTok{~}\StringTok{ }\NormalTok{train}\OperatorTok{$}\NormalTok{season, }\DataTypeTok{xlab=}\StringTok{"season"}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{"registered users"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_227-bike_sharing_demand-rf_files/figure-latex/unnamed-chunk-63-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# semilog plot}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\KeywordTok{boxplot}\NormalTok{(train}\OperatorTok{$}\NormalTok{logreg }\OperatorTok{~}\StringTok{ }\NormalTok{train}\OperatorTok{$}\NormalTok{weather, }\DataTypeTok{xlab =} \StringTok{"weather"}\NormalTok{)}
\KeywordTok{boxplot}\NormalTok{(train}\OperatorTok{$}\NormalTok{logreg }\OperatorTok{~}\StringTok{ }\NormalTok{train}\OperatorTok{$}\NormalTok{season, }\DataTypeTok{xlab =} \StringTok{"season"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_227-bike_sharing_demand-rf_files/figure-latex/unnamed-chunk-64-1} \end{center}

\hypertarget{predicting-for-registered-and-casual-users-test-dataset}{%
\subsection{Predicting for registered and casual users, test dataset}\label{predicting-for-registered-and-casual-users-test-dataset}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{tic}\NormalTok{()}
\CommentTok{# final model building using random forest}
\CommentTok{# note that we build different models for predicting for }
\CommentTok{# registered and casual users}
\CommentTok{# this was seen as giving best result after a lot of experimentation}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{415}\NormalTok{)}
\NormalTok{fit1 <-}\StringTok{ }\KeywordTok{randomForest}\NormalTok{(logreg }\OperatorTok{~}\StringTok{ }\NormalTok{hour }\OperatorTok{+}\StringTok{ }\NormalTok{workingday }\OperatorTok{+}\StringTok{ }\NormalTok{day }\OperatorTok{+}\StringTok{ }\NormalTok{holiday }\OperatorTok{+}\StringTok{ }\NormalTok{day_type }\OperatorTok{+}
\StringTok{                         }\NormalTok{temp_reg }\OperatorTok{+}\StringTok{ }\NormalTok{humidity }\OperatorTok{+}\StringTok{ }\NormalTok{atemp }\OperatorTok{+}\StringTok{ }\NormalTok{windspeed }\OperatorTok{+}\StringTok{ }\NormalTok{season }\OperatorTok{+}\StringTok{ }
\StringTok{                         }\NormalTok{weather }\OperatorTok{+}\StringTok{ }\NormalTok{dp_reg }\OperatorTok{+}\StringTok{ }\NormalTok{weekend }\OperatorTok{+}\StringTok{ }\NormalTok{year }\OperatorTok{+}\StringTok{ }\NormalTok{year_part, }
                     \DataTypeTok{data =}\NormalTok{ train, }
                     \DataTypeTok{importance =} \OtherTok{TRUE}\NormalTok{, }
                     \DataTypeTok{ntree =} \DecValTok{250}\NormalTok{)}

\NormalTok{pred1 =}\StringTok{ }\KeywordTok{predict}\NormalTok{(fit1, test)}
\NormalTok{test}\OperatorTok{$}\NormalTok{logreg =}\StringTok{ }\NormalTok{pred1}
\KeywordTok{toc}\NormalTok{()}
\CommentTok{#> 160.988 sec elapsed}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# casual users}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{415}\NormalTok{)}
\NormalTok{fit2 <-}\StringTok{ }\KeywordTok{randomForest}\NormalTok{(logcas }\OperatorTok{~}\StringTok{ }\NormalTok{hour }\OperatorTok{+}\StringTok{ }\NormalTok{day_type }\OperatorTok{+}\StringTok{ }\NormalTok{day }\OperatorTok{+}\StringTok{ }\NormalTok{humidity }\OperatorTok{+}\StringTok{ }\NormalTok{atemp }\OperatorTok{+}\StringTok{ }
\StringTok{                         }\NormalTok{temp_cas }\OperatorTok{+}\StringTok{ }\NormalTok{windspeed }\OperatorTok{+}\StringTok{ }\NormalTok{season }\OperatorTok{+}\StringTok{ }\NormalTok{weather }\OperatorTok{+}\StringTok{ }\NormalTok{holiday }\OperatorTok{+}
\StringTok{                         }\NormalTok{workingday }\OperatorTok{+}\StringTok{ }\NormalTok{dp_cas }\OperatorTok{+}\StringTok{ }\NormalTok{weekend }\OperatorTok{+}\StringTok{ }\NormalTok{year }\OperatorTok{+}\StringTok{ }\NormalTok{year_part, }
                     \DataTypeTok{data =}\NormalTok{ train, }\DataTypeTok{importance =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{ntree =} \DecValTok{250}\NormalTok{)}

\NormalTok{pred2 =}\StringTok{ }\KeywordTok{predict}\NormalTok{(fit2, test)}
\NormalTok{test}\OperatorTok{$}\NormalTok{logcas =}\StringTok{ }\NormalTok{pred2}
\end{Highlighting}
\end{Shaded}

\hypertarget{preparing-and-exporting-results}{%
\subsection{Preparing and exporting results}\label{preparing-and-exporting-results}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# creating the final submission file}
\CommentTok{# reverse log conversion}
\NormalTok{test}\OperatorTok{$}\NormalTok{registered <-}\StringTok{ }\KeywordTok{exp}\NormalTok{(test}\OperatorTok{$}\NormalTok{logreg) }\OperatorTok{-}\StringTok{ }\DecValTok{1}
\NormalTok{test}\OperatorTok{$}\NormalTok{casual     <-}\StringTok{ }\KeywordTok{exp}\NormalTok{(test}\OperatorTok{$}\NormalTok{logcas) }\OperatorTok{-}\StringTok{ }\DecValTok{1}
\NormalTok{test}\OperatorTok{$}\NormalTok{count      <-}\StringTok{ }\NormalTok{test}\OperatorTok{$}\NormalTok{casual }\OperatorTok{+}\StringTok{ }\NormalTok{test}\OperatorTok{$}\NormalTok{registered}

\NormalTok{r <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{datetime =}\NormalTok{ test}\OperatorTok{$}\NormalTok{datetime, }
                \DataTypeTok{casual =}\NormalTok{ test}\OperatorTok{$}\NormalTok{casual, }
                \DataTypeTok{registered =}\NormalTok{ test}\OperatorTok{$}\NormalTok{registered)}

\KeywordTok{print}\NormalTok{(}\KeywordTok{sum}\NormalTok{(r}\OperatorTok{$}\NormalTok{casual))}
\CommentTok{#> [1] 205804}
\KeywordTok{print}\NormalTok{(}\KeywordTok{sum}\NormalTok{(r}\OperatorTok{$}\NormalTok{registered))}
\CommentTok{#> [1] 962834}

\NormalTok{s <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{datetime =}\NormalTok{ test}\OperatorTok{$}\NormalTok{datetime, }\DataTypeTok{count =}\NormalTok{ test}\OperatorTok{$}\NormalTok{count)}
\KeywordTok{write.csv}\NormalTok{(s, }\DataTypeTok{file =}\KeywordTok{file.path}\NormalTok{(data_out_dir, }\StringTok{"bike-submit.csv"}\NormalTok{), }\DataTypeTok{row.names =} \OtherTok{FALSE}\NormalTok{)}

\CommentTok{# sum(cas+reg) = 1168638}
\CommentTok{# month number now is correct}
\end{Highlighting}
\end{Shaded}

After following the steps mentioned above, you can score 0.38675 on Kaggle leaderboard i.e.~top 5 percentile of total participants. As you might have seen, we have not applied any extraordinary science in getting to this level. But, the real competition starts here. I would like to see, if I can improve this further by use of more features and some more advanced modeling techniques.

\hypertarget{end-notes}{%
\section{End Notes}\label{end-notes}}

In this article, we have looked at structured approach of problem solving and how this method can help you to improve performance. I would recommend you to generate hypothesis before you deep dive in the data set as this technique will not limit your thought process. You can improve your performance by applying advanced techniques (or ensemble methods) and understand your data trend better.

You can find the complete solution here : \href{https://github.com/adityashrm21/Kaggle/blob/master/Bike_Sharing_Demand.R}{GitHub Link}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# this is the older submission. months were incomplete}
\NormalTok{old <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\DataTypeTok{file =} \KeywordTok{file.path}\NormalTok{(data_raw_dir, }\StringTok{"bike-submit-old.csv"}\NormalTok{))}
\KeywordTok{sum}\NormalTok{(old}\OperatorTok{$}\NormalTok{count)}
\CommentTok{#> [1] 1164829}
\end{Highlighting}
\end{Shaded}

\hypertarget{breast-cancer-wisconsin}{%
\chapter{Breast Cancer Wisconsin}\label{breast-cancer-wisconsin}}

Source: \url{https://shiring.github.io/machine_learning/2017/01/15/rfe_ga_post}

\hypertarget{read-and-process-the-data}{%
\section{Read and process the data}\label{read-and-process-the-data}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bc_data <-}\StringTok{ }\KeywordTok{read.table}\NormalTok{(}\KeywordTok{file.path}\NormalTok{(data_raw_dir, }\StringTok{"breast-cancer-wisconsin.data"}\NormalTok{), }
                      \DataTypeTok{header =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{sep =} \StringTok{","}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# assign the column names}
\KeywordTok{colnames}\NormalTok{(bc_data) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"sample_code_number"}\NormalTok{, }\StringTok{"clump_thickness"}\NormalTok{, }
                       \StringTok{"uniformity_of_cell_size"}\NormalTok{, }\StringTok{"uniformity_of_cell_shape"}\NormalTok{,}
                       \StringTok{"marginal_adhesion"}\NormalTok{, }\StringTok{"single_epithelial_cell_size"}\NormalTok{, }
                       \StringTok{"bare_nuclei"}\NormalTok{, }\StringTok{"bland_chromatin"}\NormalTok{, }\StringTok{"normal_nucleoli"}\NormalTok{, }
                       \StringTok{"mitosis"}\NormalTok{, }\StringTok{"classes"}\NormalTok{)}

\CommentTok{# change classes from numeric to character}
\NormalTok{bc_data}\OperatorTok{$}\NormalTok{classes <-}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(bc_data}\OperatorTok{$}\NormalTok{classes }\OperatorTok{==}\StringTok{ "2"}\NormalTok{, }\StringTok{"benign"}\NormalTok{,}
                          \KeywordTok{ifelse}\NormalTok{(bc_data}\OperatorTok{$}\NormalTok{classes }\OperatorTok{==}\StringTok{ "4"}\NormalTok{, }\StringTok{"malignant"}\NormalTok{, }\OtherTok{NA}\NormalTok{))}

\CommentTok{# if query sign make NA}
\NormalTok{bc_data[bc_data }\OperatorTok{==}\StringTok{ "?"}\NormalTok{] <-}\StringTok{ }\OtherTok{NA}

\CommentTok{# how many NAs are in the data}
\KeywordTok{length}\NormalTok{(}\KeywordTok{which}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(bc_data)))}
\CommentTok{#> [1] 16}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{names}\NormalTok{(bc_data)}
\CommentTok{#>  [1] "sample_code_number"          "clump_thickness"            }
\CommentTok{#>  [3] "uniformity_of_cell_size"     "uniformity_of_cell_shape"   }
\CommentTok{#>  [5] "marginal_adhesion"           "single_epithelial_cell_size"}
\CommentTok{#>  [7] "bare_nuclei"                 "bland_chromatin"            }
\CommentTok{#>  [9] "normal_nucleoli"             "mitosis"                    }
\CommentTok{#> [11] "classes"}
\end{Highlighting}
\end{Shaded}

\hypertarget{missing-data}{%
\subsection{Missing data}\label{missing-data}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# impute missing data}
\KeywordTok{library}\NormalTok{(mice)}
\CommentTok{#> Loading required package: lattice}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'mice'}
\CommentTok{#> The following objects are masked from 'package:base':}
\CommentTok{#> }
\CommentTok{#>     cbind, rbind}

\CommentTok{# skip these columns: sample_code_number and classes}
\CommentTok{# convert to numeric}
\NormalTok{bc_data[,}\DecValTok{2}\OperatorTok{:}\DecValTok{10}\NormalTok{] <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(bc_data[, }\DecValTok{2}\OperatorTok{:}\DecValTok{10}\NormalTok{], }\DecValTok{2}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{as.character}\NormalTok{(x)))}

\CommentTok{# impute but mute}
\NormalTok{dataset_impute <-}\StringTok{ }\KeywordTok{mice}\NormalTok{(bc_data[, }\DecValTok{2}\OperatorTok{:}\DecValTok{10}\NormalTok{],  }\DataTypeTok{print =} \OtherTok{FALSE}\NormalTok{)}

\CommentTok{# bind "classes" with the rest. skip "sample_code_number"}
\NormalTok{bc_data <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(bc_data[, }\DecValTok{11}\NormalTok{, }\DataTypeTok{drop =} \OtherTok{FALSE}\NormalTok{], }
\NormalTok{                 mice}\OperatorTok{::}\KeywordTok{complete}\NormalTok{(dataset_impute, }\DataTypeTok{action =}\DecValTok{1}\NormalTok{))}

\NormalTok{bc_data}\OperatorTok{$}\NormalTok{classes <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(bc_data}\OperatorTok{$}\NormalTok{classes)}

\CommentTok{# how many benign and malignant cases are there?}
\KeywordTok{summary}\NormalTok{(bc_data}\OperatorTok{$}\NormalTok{classes)}
\CommentTok{#>    benign malignant }
\CommentTok{#>       458       241}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# confirm NAs have been removed}
\KeywordTok{length}\NormalTok{(}\KeywordTok{which}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(bc_data)))}
\CommentTok{#> [1] 0}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str}\NormalTok{(bc_data)}
\CommentTok{#> 'data.frame':    699 obs. of  10 variables:}
\CommentTok{#>  $ classes                    : Factor w/ 2 levels "benign","malignant": 1 1 1 1 1 2 1 1 1 1 ...}
\CommentTok{#>  $ clump_thickness            : num  5 5 3 6 4 8 1 2 2 4 ...}
\CommentTok{#>  $ uniformity_of_cell_size    : num  1 4 1 8 1 10 1 1 1 2 ...}
\CommentTok{#>  $ uniformity_of_cell_shape   : num  1 4 1 8 1 10 1 2 1 1 ...}
\CommentTok{#>  $ marginal_adhesion          : num  1 5 1 1 3 8 1 1 1 1 ...}
\CommentTok{#>  $ single_epithelial_cell_size: num  2 7 2 3 2 7 2 2 2 2 ...}
\CommentTok{#>  $ bare_nuclei                : num  1 10 2 4 1 10 10 1 1 1 ...}
\CommentTok{#>  $ bland_chromatin            : num  3 3 3 3 3 9 3 3 1 2 ...}
\CommentTok{#>  $ normal_nucleoli            : num  1 2 1 7 1 7 1 1 1 1 ...}
\CommentTok{#>  $ mitosis                    : num  1 1 1 1 1 1 1 1 5 1 ...}
\end{Highlighting}
\end{Shaded}

\hypertarget{principal-component-analysis-pca}{%
\section{Principal Component Analysis (PCA)}\label{principal-component-analysis-pca}}

To get an idea about the dimensionality and variance of the datasets, I am first looking at PCA plots for samples and features. The first two principal components (PCs) show the two components that explain the majority of variation in the data.

After defining my custom \texttt{ggplot2} theme, I am creating a function that performs the PCA (using the \texttt{pcaGoPromoter} package), calculates ellipses of the data points (with the \texttt{ellipse} package) and produces the plot with \texttt{ggplot2}. Some of the features in datasets 2 and 3 are not very distinct and overlap in the PCA plots, therefore I am also plotting hierarchical clustering dendrograms.

\hypertarget{theme}{%
\subsubsection{theme}\label{theme}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plotting theme}

\KeywordTok{library}\NormalTok{(ggplot2)}
\CommentTok{#> Registered S3 methods overwritten by 'ggplot2':}
\CommentTok{#>   method         from }
\CommentTok{#>   [.quosures     rlang}
\CommentTok{#>   c.quosures     rlang}
\CommentTok{#>   print.quosures rlang}

\NormalTok{my_theme <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(}\DataTypeTok{base_size =} \DecValTok{12}\NormalTok{, }\DataTypeTok{base_family =} \StringTok{"sans"}\NormalTok{)\{}
  \KeywordTok{theme_minimal}\NormalTok{(}\DataTypeTok{base_size =}\NormalTok{ base_size, }\DataTypeTok{base_family =}\NormalTok{ base_family) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}
    \DataTypeTok{axis.text =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{size =} \DecValTok{12}\NormalTok{),}
    \DataTypeTok{axis.text.x =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{angle =} \DecValTok{0}\NormalTok{, }\DataTypeTok{vjust =} \FloatTok{0.5}\NormalTok{, }\DataTypeTok{hjust =} \FloatTok{0.5}\NormalTok{),}
    \DataTypeTok{axis.title =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{size =} \DecValTok{14}\NormalTok{),}
    \DataTypeTok{panel.grid.major =} \KeywordTok{element_line}\NormalTok{(}\DataTypeTok{color =} \StringTok{"grey"}\NormalTok{),}
    \DataTypeTok{panel.grid.minor =} \KeywordTok{element_blank}\NormalTok{(),}
    \DataTypeTok{panel.background =} \KeywordTok{element_rect}\NormalTok{(}\DataTypeTok{fill =} \StringTok{"aliceblue"}\NormalTok{),}
    \DataTypeTok{strip.background =} \KeywordTok{element_rect}\NormalTok{(}\DataTypeTok{fill =} \StringTok{"navy"}\NormalTok{, }\DataTypeTok{color =} \StringTok{"navy"}\NormalTok{, }\DataTypeTok{size =} \DecValTok{1}\NormalTok{),}
    \DataTypeTok{strip.text =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{face =} \StringTok{"bold"}\NormalTok{, }\DataTypeTok{size =} \DecValTok{12}\NormalTok{, }\DataTypeTok{color =} \StringTok{"white"}\NormalTok{),}
    \DataTypeTok{legend.position =} \StringTok{"right"}\NormalTok{,}
    \DataTypeTok{legend.justification =} \StringTok{"top"}\NormalTok{, }
    \DataTypeTok{legend.background =} \KeywordTok{element_blank}\NormalTok{(),}
    \DataTypeTok{panel.border =} \KeywordTok{element_rect}\NormalTok{(}\DataTypeTok{color =} \StringTok{"grey"}\NormalTok{, }\DataTypeTok{fill =} \OtherTok{NA}\NormalTok{, }\DataTypeTok{size =} \FloatTok{0.5}\NormalTok{)}
\NormalTok{  )}
\NormalTok{\}}

\KeywordTok{theme_set}\NormalTok{(}\KeywordTok{my_theme}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\hypertarget{pca-function}{%
\subsubsection{PCA function}\label{pca-function}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# function for PCA plotting}
\KeywordTok{library}\NormalTok{(pcaGoPromoter)                  }\CommentTok{# install from BioConductor}
\CommentTok{#> Loading required package: ellipse}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'ellipse'}
\CommentTok{#> The following object is masked from 'package:graphics':}
\CommentTok{#> }
\CommentTok{#>     pairs}
\CommentTok{#> Loading required package: Biostrings}
\CommentTok{#> Loading required package: BiocGenerics}
\CommentTok{#> Loading required package: parallel}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'BiocGenerics'}
\CommentTok{#> The following objects are masked from 'package:parallel':}
\CommentTok{#> }
\CommentTok{#>     clusterApply, clusterApplyLB, clusterCall, clusterEvalQ,}
\CommentTok{#>     clusterExport, clusterMap, parApply, parCapply, parLapply,}
\CommentTok{#>     parLapplyLB, parRapply, parSapply, parSapplyLB}
\CommentTok{#> The following objects are masked from 'package:mice':}
\CommentTok{#> }
\CommentTok{#>     cbind, rbind}
\CommentTok{#> The following objects are masked from 'package:stats':}
\CommentTok{#> }
\CommentTok{#>     IQR, mad, sd, var, xtabs}
\CommentTok{#> The following objects are masked from 'package:base':}
\CommentTok{#> }
\CommentTok{#>     anyDuplicated, append, as.data.frame, basename, cbind,}
\CommentTok{#>     colnames, dirname, do.call, duplicated, eval, evalq, Filter,}
\CommentTok{#>     Find, get, grep, grepl, intersect, is.unsorted, lapply, Map,}
\CommentTok{#>     mapply, match, mget, order, paste, pmax, pmax.int, pmin,}
\CommentTok{#>     pmin.int, Position, rank, rbind, Reduce, rownames, sapply,}
\CommentTok{#>     setdiff, sort, table, tapply, union, unique, unsplit, which,}
\CommentTok{#>     which.max, which.min}
\CommentTok{#> Loading required package: S4Vectors}
\CommentTok{#> Loading required package: stats4}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'S4Vectors'}
\CommentTok{#> The following object is masked from 'package:base':}
\CommentTok{#> }
\CommentTok{#>     expand.grid}
\CommentTok{#> Loading required package: IRanges}
\CommentTok{#> Loading required package: XVector}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'Biostrings'}
\CommentTok{#> The following object is masked from 'package:base':}
\CommentTok{#> }
\CommentTok{#>     strsplit}
\KeywordTok{library}\NormalTok{(ellipse)}

\NormalTok{pca_func <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(data, groups, title, }\DataTypeTok{print_ellipse =} \OtherTok{TRUE}\NormalTok{) \{}
  
  \CommentTok{# perform pca and extract scores for all principal components: PC1:PC9}
\NormalTok{  pcaOutput <-}\StringTok{ }\KeywordTok{pca}\NormalTok{(data, }\DataTypeTok{printDropped =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{scale =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{center =} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{  pcaOutput2 <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(pcaOutput}\OperatorTok{$}\NormalTok{scores)}
  
  \CommentTok{# define groups for plotting. will group the classes}
\NormalTok{  pcaOutput2}\OperatorTok{$}\NormalTok{groups <-}\StringTok{ }\NormalTok{groups}
  
  \CommentTok{# when plotting samples calculate ellipses for plotting }
  \CommentTok{# (when plotting features, there are no replicates)}
  \ControlFlowTok{if}\NormalTok{ (print_ellipse) \{}
    \CommentTok{# group and summarize by classes: benign, malignant}
    \CommentTok{# centroids w/3 columns: groups, PC1, PC2}
\NormalTok{    centroids <-}\StringTok{ }\KeywordTok{aggregate}\NormalTok{(}\KeywordTok{cbind}\NormalTok{(PC1, PC2) }\OperatorTok{~}\StringTok{ }\NormalTok{groups, pcaOutput2, mean)}
    \CommentTok{# bind for the two groups (classes)}
    \CommentTok{# conf.rgn w/3 columns: groups, PC1, PC2}
\NormalTok{    conf.rgn  <-}\StringTok{ }\KeywordTok{do.call}\NormalTok{(rbind, }\KeywordTok{lapply}\NormalTok{(}\KeywordTok{unique}\NormalTok{(pcaOutput2}\OperatorTok{$}\NormalTok{groups), }\ControlFlowTok{function}\NormalTok{(t)}
      \KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{groups =} \KeywordTok{as.character}\NormalTok{(t),}
                 \CommentTok{# ellipse data for PC1 and PC2}
                 \KeywordTok{ellipse}\NormalTok{(}\KeywordTok{cov}\NormalTok{(pcaOutput2[pcaOutput2}\OperatorTok{$}\NormalTok{groups }\OperatorTok{==}\StringTok{ }\NormalTok{t, }\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{]),}
                       \DataTypeTok{centre =} \KeywordTok{as.matrix}\NormalTok{(centroids[centroids}\OperatorTok{$}\NormalTok{groups }\OperatorTok{==}\StringTok{ }\NormalTok{t, }\DecValTok{2}\OperatorTok{:}\DecValTok{3}\NormalTok{]),}
                       \DataTypeTok{level =} \FloatTok{0.95}\NormalTok{),}
                 \DataTypeTok{stringsAsFactors =} \OtherTok{FALSE}\NormalTok{)))}
    
\NormalTok{    plot <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ pcaOutput2, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ PC1, }\DataTypeTok{y =}\NormalTok{ PC2, }
                                          \DataTypeTok{group =}\NormalTok{ groups, }
                                          \DataTypeTok{color =}\NormalTok{ groups)) }\OperatorTok{+}\StringTok{ }
\StringTok{      }\KeywordTok{geom_polygon}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ conf.rgn, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{fill =}\NormalTok{ groups), }\DataTypeTok{alpha =} \FloatTok{0.2}\NormalTok{) }\OperatorTok{+}\StringTok{ }\CommentTok{# ellipses}
\StringTok{      }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{size =} \DecValTok{2}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{0.6}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{      }\KeywordTok{scale_color_brewer}\NormalTok{(}\DataTypeTok{palette =} \StringTok{"Set1"}\NormalTok{) }\OperatorTok{+}
\StringTok{      }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{title =}\NormalTok{ title,}
           \DataTypeTok{color =} \StringTok{""}\NormalTok{,}
           \DataTypeTok{fill =} \StringTok{""}\NormalTok{,}
           \DataTypeTok{x =} \KeywordTok{paste0}\NormalTok{(}\StringTok{"PC1: "}\NormalTok{, }\KeywordTok{round}\NormalTok{(pcaOutput}\OperatorTok{$}\NormalTok{pov[}\DecValTok{1}\NormalTok{], }\DataTypeTok{digits =} \DecValTok{2}\NormalTok{) }\OperatorTok{*}\StringTok{ }\DecValTok{100}\NormalTok{, }
                      \StringTok{"% variance"}\NormalTok{),}
           \DataTypeTok{y =} \KeywordTok{paste0}\NormalTok{(}\StringTok{"PC2: "}\NormalTok{, }\KeywordTok{round}\NormalTok{(pcaOutput}\OperatorTok{$}\NormalTok{pov[}\DecValTok{2}\NormalTok{], }\DataTypeTok{digits =} \DecValTok{2}\NormalTok{) }\OperatorTok{*}\StringTok{ }\DecValTok{100}\NormalTok{, }
                      \StringTok{"% variance"}\NormalTok{))}
    
\NormalTok{  \} }\ControlFlowTok{else}\NormalTok{ \{}
    
    \CommentTok{# if < 10 groups (e.g. the predictor classes) have colors from RColorBrewer}
    \ControlFlowTok{if}\NormalTok{ (}\KeywordTok{length}\NormalTok{(}\KeywordTok{unique}\NormalTok{(pcaOutput2}\OperatorTok{$}\NormalTok{groups)) }\OperatorTok{<=}\StringTok{ }\DecValTok{10}\NormalTok{) \{}
      
\NormalTok{      plot <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ pcaOutput2, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ PC1, }\DataTypeTok{y =}\NormalTok{ PC2, }
                                            \DataTypeTok{group =}\NormalTok{ groups, }
                                            \DataTypeTok{color =}\NormalTok{ groups)) }\OperatorTok{+}\StringTok{ }
\StringTok{        }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{size =} \DecValTok{2}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{0.6}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{        }\KeywordTok{scale_color_brewer}\NormalTok{(}\DataTypeTok{palette =} \StringTok{"Set1"}\NormalTok{) }\OperatorTok{+}
\StringTok{        }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{title =}\NormalTok{ title,}
             \DataTypeTok{color =} \StringTok{""}\NormalTok{,}
             \DataTypeTok{fill =} \StringTok{""}\NormalTok{,}
             \DataTypeTok{x =} \KeywordTok{paste0}\NormalTok{(}\StringTok{"PC1: "}\NormalTok{, }\KeywordTok{round}\NormalTok{(pcaOutput}\OperatorTok{$}\NormalTok{pov[}\DecValTok{1}\NormalTok{], }\DataTypeTok{digits =} \DecValTok{2}\NormalTok{) }\OperatorTok{*}\StringTok{ }\DecValTok{100}\NormalTok{, }
                        \StringTok{"% variance"}\NormalTok{),}
             \DataTypeTok{y =} \KeywordTok{paste0}\NormalTok{(}\StringTok{"PC2: "}\NormalTok{, }\KeywordTok{round}\NormalTok{(pcaOutput}\OperatorTok{$}\NormalTok{pov[}\DecValTok{2}\NormalTok{], }\DataTypeTok{digits =} \DecValTok{2}\NormalTok{) }\OperatorTok{*}\StringTok{ }\DecValTok{100}\NormalTok{, }
                        \StringTok{"% variance"}\NormalTok{))}
      
\NormalTok{    \} }\ControlFlowTok{else}\NormalTok{ \{}
      \CommentTok{# otherwise use the default rainbow colors}
\NormalTok{      plot <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ pcaOutput2, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ PC1, }\DataTypeTok{y =}\NormalTok{ PC2, }
                                            \DataTypeTok{group =}\NormalTok{ groups, }\DataTypeTok{color =}\NormalTok{ groups)) }\OperatorTok{+}\StringTok{ }
\StringTok{        }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{size =} \DecValTok{2}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{0.6}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{        }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{title =}\NormalTok{ title,}
             \DataTypeTok{color =} \StringTok{""}\NormalTok{,}
             \DataTypeTok{fill =} \StringTok{""}\NormalTok{,}
             \DataTypeTok{x =} \KeywordTok{paste0}\NormalTok{(}\StringTok{"PC1: "}\NormalTok{, }\KeywordTok{round}\NormalTok{(pcaOutput}\OperatorTok{$}\NormalTok{pov[}\DecValTok{1}\NormalTok{], }\DataTypeTok{digits =} \DecValTok{2}\NormalTok{) }\OperatorTok{*}\StringTok{ }\DecValTok{100}\NormalTok{, }
                        \StringTok{"% variance"}\NormalTok{),}
             \DataTypeTok{y =} \KeywordTok{paste0}\NormalTok{(}\StringTok{"PC2: "}\NormalTok{, }\KeywordTok{round}\NormalTok{(pcaOutput}\OperatorTok{$}\NormalTok{pov[}\DecValTok{2}\NormalTok{], }\DataTypeTok{digits =} \DecValTok{2}\NormalTok{) }\OperatorTok{*}\StringTok{ }\DecValTok{100}\NormalTok{, }
                        \StringTok{"% variance"}\NormalTok{))}
\NormalTok{    \}}
\NormalTok{  \}}
  
  \KeywordTok{return}\NormalTok{(plot)}
  
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(gridExtra)}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'gridExtra'}
\CommentTok{#> The following object is masked from 'package:BiocGenerics':}
\CommentTok{#> }
\CommentTok{#>     combine}
\KeywordTok{library}\NormalTok{(grid)}

\CommentTok{# plot all data. one row is a feature}
\NormalTok{p1 <-}\StringTok{ }\KeywordTok{pca_func}\NormalTok{(}\DataTypeTok{data =} \KeywordTok{t}\NormalTok{(bc_data[, }\DecValTok{2}\OperatorTok{:}\DecValTok{10}\NormalTok{]), }
               \DataTypeTok{groups =} \KeywordTok{as.character}\NormalTok{(bc_data}\OperatorTok{$}\NormalTok{classes), }
               \DataTypeTok{title =} \StringTok{"Breast cancer dataset 1: Samples"}\NormalTok{)}

\CommentTok{# plot features only. features as columns}
\NormalTok{p2 <-}\StringTok{ }\KeywordTok{pca_func}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ bc_data[, }\DecValTok{2}\OperatorTok{:}\DecValTok{10}\NormalTok{], }
               \DataTypeTok{groups =} \KeywordTok{as.character}\NormalTok{(}\KeywordTok{colnames}\NormalTok{(bc_data[, }\DecValTok{2}\OperatorTok{:}\DecValTok{10}\NormalTok{])), }
               \DataTypeTok{title =} \StringTok{"Breast cancer dataset 1: Features"}\NormalTok{, }\DataTypeTok{print_ellipse =} \OtherTok{FALSE}\NormalTok{)}

\KeywordTok{grid.arrange}\NormalTok{(p1, p2, }\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_229-breast_cancer_ga-sglander_files/figure-latex/plot-pca-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{h_}\DecValTok{1}\NormalTok{ <-}\StringTok{ }\KeywordTok{hclust}\NormalTok{(}\KeywordTok{dist}\NormalTok{(}\KeywordTok{t}\NormalTok{(bc_data[, }\DecValTok{2}\OperatorTok{:}\DecValTok{10}\NormalTok{]), }\DataTypeTok{method =} \StringTok{"euclidean"}\NormalTok{), }\DataTypeTok{method =} \StringTok{"complete"}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(h_}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_229-breast_cancer_ga-sglander_files/figure-latex/plot-dendrogram-1} \end{center}

\hypertarget{density-plots-vs-class}{%
\subsection{density plots vs class}\label{density-plots-vs-class}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# density plot showing the feature vs classes}
\KeywordTok{library}\NormalTok{(tidyr)}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'tidyr'}
\CommentTok{#> The following object is masked from 'package:S4Vectors':}
\CommentTok{#> }
\CommentTok{#>     expand}
\CommentTok{#> The following object is masked from 'package:mice':}
\CommentTok{#> }
\CommentTok{#>     complete}

\CommentTok{# gather data. from column clump_thickness to mitosis}
\NormalTok{bc_data_gather <-}\StringTok{ }\NormalTok{bc_data }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{gather}\NormalTok{(measure, value, clump_thickness}\OperatorTok{:}\NormalTok{mitosis)}

\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ bc_data_gather, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ value, }\DataTypeTok{fill =}\NormalTok{ classes, }\DataTypeTok{color =}\NormalTok{ classes)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_density}\NormalTok{(}\DataTypeTok{alpha =} \FloatTok{0.3}\NormalTok{, }\DataTypeTok{size =} \DecValTok{1}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_rug}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_fill_brewer}\NormalTok{(}\DataTypeTok{palette =} \StringTok{"Set1"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_color_brewer}\NormalTok{(}\DataTypeTok{palette =} \StringTok{"Set1"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{facet_wrap}\NormalTok{( }\OperatorTok{~}\StringTok{ }\NormalTok{measure, }\DataTypeTok{scales =} \StringTok{"free_y"}\NormalTok{, }\DataTypeTok{ncol =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_229-breast_cancer_ga-sglander_files/figure-latex/unnamed-chunk-9-1} \end{center}

\hypertarget{feature-importance}{%
\section{Feature importance}\label{feature-importance}}

To get an idea about the feature's respective importances, I'm running Random Forest models with 10 x 10 cross validation using the \texttt{caret} package. If I wanted to use feature importance to select features for modeling, I would need to perform it on the training data instead of on the complete dataset. But here, I only want to use it to get acquainted with my data. I am again defining a function that estimates the feature importance and produces a plot.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(caret)}
\CommentTok{# library(doParallel) # parallel processing}
\CommentTok{# registerDoParallel()}

\CommentTok{# prepare training scheme}
\NormalTok{control <-}\StringTok{ }\KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"repeatedcv"}\NormalTok{, }\DataTypeTok{number =} \DecValTok{10}\NormalTok{, }\DataTypeTok{repeats =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{feature_imp <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(model, title) \{}
  \CommentTok{# estimate variable importance}
\NormalTok{  importance <-}\StringTok{ }\KeywordTok{varImp}\NormalTok{(model, }\DataTypeTok{scale =} \OtherTok{TRUE}\NormalTok{)}
  \CommentTok{# prepare dataframes for plotting}
\NormalTok{  importance_df_}\DecValTok{1}\NormalTok{ <-}\StringTok{ }\NormalTok{importance}\OperatorTok{$}\NormalTok{importance}
\NormalTok{  importance_df_}\DecValTok{1}\OperatorTok{$}\NormalTok{group <-}\StringTok{ }\KeywordTok{rownames}\NormalTok{(importance_df_}\DecValTok{1}\NormalTok{)}
  
\NormalTok{  importance_df_}\DecValTok{2}\NormalTok{ <-}\StringTok{ }\NormalTok{importance_df_}\DecValTok{1}
\NormalTok{  importance_df_}\DecValTok{2}\OperatorTok{$}\NormalTok{Overall <-}\StringTok{ }\DecValTok{0}
\NormalTok{  importance_df <-}\StringTok{ }\KeywordTok{rbind}\NormalTok{(importance_df_}\DecValTok{1}\NormalTok{, importance_df_}\DecValTok{2}\NormalTok{)}
  
\NormalTok{  plot <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{() }\OperatorTok{+}
\StringTok{    }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ importance_df_}\DecValTok{1}\NormalTok{, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Overall, }
                                           \DataTypeTok{y =}\NormalTok{ group, }
                                           \DataTypeTok{color =}\NormalTok{ group), }\DataTypeTok{size =} \DecValTok{2}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{geom_path}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ importance_df, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Overall, }
                                        \DataTypeTok{y =}\NormalTok{ group, }
                                        \DataTypeTok{color =}\NormalTok{ group, }
                                        \DataTypeTok{group =}\NormalTok{ group), }\DataTypeTok{size =} \DecValTok{1}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{legend.position =} \StringTok{"none"}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{labs}\NormalTok{(}
      \DataTypeTok{x =} \StringTok{"Importance"}\NormalTok{,}
      \DataTypeTok{y =} \StringTok{""}\NormalTok{,}
      \DataTypeTok{title =}\NormalTok{ title,}
      \DataTypeTok{subtitle =} \StringTok{"Scaled feature importance"}\NormalTok{,}
      \DataTypeTok{caption =} \StringTok{"}\CharTok{\textbackslash{}n}\StringTok{Determined with Random Forest and}
\StringTok{      repeated cross validation (10 repeats, 10 times)"}
\NormalTok{    )}
  \KeywordTok{return}\NormalTok{(plot)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# train the model}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{27}\NormalTok{)}
\NormalTok{imp_}\DecValTok{1}\NormalTok{ <-}\StringTok{ }\KeywordTok{train}\NormalTok{(classes }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ bc_data, }\DataTypeTok{method =} \StringTok{"rf"}\NormalTok{, }
               \DataTypeTok{preProcess =} \KeywordTok{c}\NormalTok{(}\StringTok{"scale"}\NormalTok{, }\StringTok{"center"}\NormalTok{), }
               \DataTypeTok{trControl =}\NormalTok{ control)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p1 <-}\StringTok{ }\KeywordTok{feature_imp}\NormalTok{(imp_}\DecValTok{1}\NormalTok{, }\DataTypeTok{title =} \StringTok{"Breast cancer dataset 1"}\NormalTok{)}
\NormalTok{p1}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_229-breast_cancer_ga-sglander_files/figure-latex/plot-featureImportance-1} \end{center}

\hypertarget{feature-selection}{%
\section{Feature Selection}\label{feature-selection}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  By correlation
\item
  By Recursive Feature Elimination
\item
  By Genetic Algorithm
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{27}\NormalTok{)}
\NormalTok{bc_data_index <-}\StringTok{ }\KeywordTok{createDataPartition}\NormalTok{(bc_data}\OperatorTok{$}\NormalTok{classes, }\DataTypeTok{p =} \FloatTok{0.7}\NormalTok{, }\DataTypeTok{list =} \OtherTok{FALSE}\NormalTok{)}
\NormalTok{bc_data_train <-}\StringTok{ }\NormalTok{bc_data[bc_data_index, ]}
\NormalTok{bc_data_test  <-}\StringTok{ }\NormalTok{bc_data[}\OperatorTok{-}\NormalTok{bc_data_index, ]}
\end{Highlighting}
\end{Shaded}

\hypertarget{correlation-1}{%
\subsection{Correlation}\label{correlation-1}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(corrplot)}
\CommentTok{#> corrplot 0.84 loaded}

\CommentTok{# calculate correlation matrix}
\NormalTok{corMatMy <-}\StringTok{ }\KeywordTok{cor}\NormalTok{(bc_data_train[, }\DecValTok{-1}\NormalTok{])}
\KeywordTok{corrplot}\NormalTok{(corMatMy, }\DataTypeTok{order =} \StringTok{"hclust"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_229-breast_cancer_ga-sglander_files/figure-latex/plot-correlation-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Apply correlation filter at 0.70,}
\NormalTok{highlyCor <-}\StringTok{ }\KeywordTok{colnames}\NormalTok{(bc_data_train[, }\DecValTok{-1}\NormalTok{])[}\KeywordTok{findCorrelation}\NormalTok{(corMatMy, }
                                                           \DataTypeTok{cutoff =} \FloatTok{0.7}\NormalTok{, }
                                                           \DataTypeTok{verbose =} \OtherTok{TRUE}\NormalTok{)]}
\CommentTok{#> Compare row 2  and column  3 with corr  0.9 }
\CommentTok{#>   Means:  0.709 vs 0.595 so flagging column 2 }
\CommentTok{#> Compare row 3  and column  7 with corr  0.737 }
\CommentTok{#>   Means:  0.674 vs 0.572 so flagging column 3 }
\CommentTok{#> All correlations <= 0.7}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# which variables are flagged for removal?}
\NormalTok{highlyCor}
\CommentTok{#> [1] "uniformity_of_cell_size"  "uniformity_of_cell_shape"}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# then we remove these variables}
\NormalTok{bc_data_cor <-}\StringTok{ }\NormalTok{bc_data_train[, }\KeywordTok{which}\NormalTok{(}\OperatorTok{!}\KeywordTok{colnames}\NormalTok{(bc_data_train) }\OperatorTok{%in%}\StringTok{ }\NormalTok{highlyCor)]}
\KeywordTok{names}\NormalTok{(bc_data_cor)}
\CommentTok{#> [1] "classes"                     "clump_thickness"            }
\CommentTok{#> [3] "marginal_adhesion"           "single_epithelial_cell_size"}
\CommentTok{#> [5] "bare_nuclei"                 "bland_chromatin"            }
\CommentTok{#> [7] "normal_nucleoli"             "mitosis"}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# confirm features were removed}
\NormalTok{outersect <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x, y) \{}
  \KeywordTok{sort}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\KeywordTok{setdiff}\NormalTok{(x, y),}
         \KeywordTok{setdiff}\NormalTok{(y, x)))}
\NormalTok{\}}
 
\KeywordTok{outersect}\NormalTok{(}\KeywordTok{names}\NormalTok{(bc_data_cor), }\KeywordTok{names}\NormalTok{(bc_data_train))}
\CommentTok{#> [1] "uniformity_of_cell_shape" "uniformity_of_cell_size"}
\end{Highlighting}
\end{Shaded}

\begin{quote}
Four features removed
\end{quote}

\hypertarget{recursive-feature-elimination-rfe}{%
\subsection{Recursive Feature Elimination (RFE)}\label{recursive-feature-elimination-rfe}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# ensure the results are repeatable}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{7}\NormalTok{)}

\CommentTok{# define the control using a random forest selection function with cross validation}
\NormalTok{control <-}\StringTok{ }\KeywordTok{rfeControl}\NormalTok{(}\DataTypeTok{functions =}\NormalTok{ rfFuncs, }\DataTypeTok{method =} \StringTok{"cv"}\NormalTok{, }\DataTypeTok{number =} \DecValTok{10}\NormalTok{)}

\CommentTok{# run the RFE algorithm}
\NormalTok{results_}\DecValTok{1}\NormalTok{ <-}\StringTok{ }\KeywordTok{rfe}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ bc_data_train[, }\DecValTok{-1}\NormalTok{], }
                 \DataTypeTok{y =}\NormalTok{ bc_data_train}\OperatorTok{$}\NormalTok{classes, }
                 \DataTypeTok{sizes =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{9}\NormalTok{), }
                 \DataTypeTok{rfeControl =}\NormalTok{ control)}

\CommentTok{# chosen features}
\KeywordTok{predictors}\NormalTok{(results_}\DecValTok{1}\NormalTok{)}
\CommentTok{#> [1] "bare_nuclei"                 "clump_thickness"            }
\CommentTok{#> [3] "normal_nucleoli"             "uniformity_of_cell_size"    }
\CommentTok{#> [5] "uniformity_of_cell_shape"    "single_epithelial_cell_size"}
\CommentTok{#> [7] "bland_chromatin"             "marginal_adhesion"}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# subset the chosen features}
\NormalTok{sel_cols <-}\StringTok{ }\KeywordTok{which}\NormalTok{(}\KeywordTok{colnames}\NormalTok{(bc_data_train) }\OperatorTok{%in%}\StringTok{ }\KeywordTok{predictors}\NormalTok{(results_}\DecValTok{1}\NormalTok{))}
\NormalTok{bc_data_rfe <-}\StringTok{ }\NormalTok{bc_data_train[, }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, sel_cols)]}
\KeywordTok{names}\NormalTok{(bc_data_rfe)}
\CommentTok{#> [1] "classes"                     "clump_thickness"            }
\CommentTok{#> [3] "uniformity_of_cell_size"     "uniformity_of_cell_shape"   }
\CommentTok{#> [5] "marginal_adhesion"           "single_epithelial_cell_size"}
\CommentTok{#> [7] "bare_nuclei"                 "bland_chromatin"            }
\CommentTok{#> [9] "normal_nucleoli"}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# confirm features removed by RFE}
\KeywordTok{outersect}\NormalTok{(}\KeywordTok{names}\NormalTok{(bc_data_rfe), }\KeywordTok{names}\NormalTok{(bc_data_train))}
\CommentTok{#> [1] "mitosis"}
\end{Highlighting}
\end{Shaded}

\begin{quote}
No features removed with RFE
\end{quote}

\hypertarget{genetic-algorithm-ga}{%
\subsection{Genetic Algorithm (GA)}\label{genetic-algorithm-ga}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(dplyr)}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'dplyr'}
\CommentTok{#> The following object is masked from 'package:gridExtra':}
\CommentTok{#> }
\CommentTok{#>     combine}
\CommentTok{#> The following objects are masked from 'package:Biostrings':}
\CommentTok{#> }
\CommentTok{#>     collapse, intersect, setdiff, setequal, union}
\CommentTok{#> The following object is masked from 'package:XVector':}
\CommentTok{#> }
\CommentTok{#>     slice}
\CommentTok{#> The following objects are masked from 'package:IRanges':}
\CommentTok{#> }
\CommentTok{#>     collapse, desc, intersect, setdiff, slice, union}
\CommentTok{#> The following objects are masked from 'package:S4Vectors':}
\CommentTok{#> }
\CommentTok{#>     first, intersect, rename, setdiff, setequal, union}
\CommentTok{#> The following objects are masked from 'package:BiocGenerics':}
\CommentTok{#> }
\CommentTok{#>     combine, intersect, setdiff, union}
\CommentTok{#> The following objects are masked from 'package:stats':}
\CommentTok{#> }
\CommentTok{#>     filter, lag}
\CommentTok{#> The following objects are masked from 'package:base':}
\CommentTok{#> }
\CommentTok{#>     intersect, setdiff, setequal, union}

\NormalTok{ga_ctrl <-}\StringTok{ }\KeywordTok{gafsControl}\NormalTok{(}\DataTypeTok{functions =}\NormalTok{ rfGA, }\CommentTok{# Assess fitness with RF}
                       \DataTypeTok{method =} \StringTok{"cv"}\NormalTok{,    }\CommentTok{# 10 fold cross validation}
                       \DataTypeTok{genParallel =} \OtherTok{TRUE}\NormalTok{, }\CommentTok{# Use parallel programming}
                       \DataTypeTok{allowParallel =} \OtherTok{TRUE}\NormalTok{)}

\NormalTok{lev <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"malignant"}\NormalTok{, }\StringTok{"benign"}\NormalTok{)     }\CommentTok{# Set the levels}

\KeywordTok{set.seed}\NormalTok{(}\DecValTok{27}\NormalTok{)}
\NormalTok{model_}\DecValTok{1}\NormalTok{ <-}\StringTok{ }\KeywordTok{gafs}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ bc_data_train[, }\DecValTok{-1}\NormalTok{], }\DataTypeTok{y =}\NormalTok{ bc_data_train}\OperatorTok{$}\NormalTok{classes,}
                   \DataTypeTok{iters =} \DecValTok{10}\NormalTok{, }\CommentTok{# generations of algorithm}
                   \DataTypeTok{popSize =} \DecValTok{5}\NormalTok{, }\CommentTok{# population size for each generation}
                   \DataTypeTok{levels =}\NormalTok{ lev,}
                   \DataTypeTok{gafsControl =}\NormalTok{ ga_ctrl)}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'recipes'}
\CommentTok{#> The following object is masked from 'package:stats':}
\CommentTok{#> }
\CommentTok{#>     step}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(model_}\DecValTok{1}\NormalTok{) }\CommentTok{# Plot mean fitness (AUC) by generation}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_229-breast_cancer_ga-sglander_files/figure-latex/plot-auc-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# features}
\NormalTok{model_}\DecValTok{1}\OperatorTok{$}\NormalTok{ga}\OperatorTok{$}\NormalTok{final}
\CommentTok{#> [1] "clump_thickness"          "uniformity_of_cell_size" }
\CommentTok{#> [3] "uniformity_of_cell_shape" "marginal_adhesion"       }
\CommentTok{#> [5] "bare_nuclei"              "bland_chromatin"         }
\CommentTok{#> [7] "normal_nucleoli"}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# select features}
\NormalTok{sel_cols_ga <-}\StringTok{ }\KeywordTok{which}\NormalTok{(}\KeywordTok{colnames}\NormalTok{(bc_data_train) }\OperatorTok{%in%}\StringTok{ }\NormalTok{model_}\DecValTok{1}\OperatorTok{$}\NormalTok{ga}\OperatorTok{$}\NormalTok{final)}
\NormalTok{bc_data_ga <-}\StringTok{ }\NormalTok{bc_data_train[, }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, sel_cols_ga)]}
\KeywordTok{names}\NormalTok{(bc_data_ga)}
\CommentTok{#> [1] "classes"                  "clump_thickness"         }
\CommentTok{#> [3] "uniformity_of_cell_size"  "uniformity_of_cell_shape"}
\CommentTok{#> [5] "marginal_adhesion"        "bare_nuclei"             }
\CommentTok{#> [7] "bland_chromatin"          "normal_nucleoli"}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# features removed GA}
\KeywordTok{outersect}\NormalTok{(}\KeywordTok{names}\NormalTok{(bc_data_ga), }\KeywordTok{names}\NormalTok{(bc_data_train))}
\CommentTok{#> [1] "mitosis"                     "single_epithelial_cell_size"}
\end{Highlighting}
\end{Shaded}

\begin{quote}
Two features removed with GA.
\end{quote}

\hypertarget{model-comparison}{%
\section{Model comparison}\label{model-comparison}}

\hypertarget{using-all-features}{%
\subsection{Using all features}\label{using-all-features}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{27}\NormalTok{)}
\NormalTok{model_bc_data_all <-}\StringTok{ }\KeywordTok{train}\NormalTok{(classes }\OperatorTok{~}\StringTok{ }\NormalTok{.,}
                           \DataTypeTok{data =}\NormalTok{ bc_data_train,}
                           \DataTypeTok{method =} \StringTok{"rf"}\NormalTok{,}
                           \DataTypeTok{preProcess =} \KeywordTok{c}\NormalTok{(}\StringTok{"scale"}\NormalTok{, }\StringTok{"center"}\NormalTok{),}
                           \DataTypeTok{trControl =} \KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"repeatedcv"}\NormalTok{, }
                                                    \DataTypeTok{number =} \DecValTok{5}\NormalTok{, }\DataTypeTok{repeats =} \DecValTok{10}\NormalTok{,}
                                                    \DataTypeTok{verboseIter =} \OtherTok{FALSE}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# confusion matrix }
\NormalTok{cm_all_}\DecValTok{1}\NormalTok{ <-}\StringTok{ }\KeywordTok{confusionMatrix}\NormalTok{(}\KeywordTok{predict}\NormalTok{(model_bc_data_all, bc_data_test[, }\DecValTok{-1}\NormalTok{]), bc_data_test}\OperatorTok{$}\NormalTok{classes)}
\NormalTok{cm_all_}\DecValTok{1}
\CommentTok{#> Confusion Matrix and Statistics}
\CommentTok{#> }
\CommentTok{#>            Reference}
\CommentTok{#> Prediction  benign malignant}
\CommentTok{#>   benign       131         2}
\CommentTok{#>   malignant      6        70}
\CommentTok{#>                                         }
\CommentTok{#>                Accuracy : 0.962         }
\CommentTok{#>                  95% CI : (0.926, 0.983)}
\CommentTok{#>     No Information Rate : 0.656         }
\CommentTok{#>     P-Value [Acc > NIR] : <2e-16        }
\CommentTok{#>                                         }
\CommentTok{#>                   Kappa : 0.916         }
\CommentTok{#>                                         }
\CommentTok{#>  Mcnemar's Test P-Value : 0.289         }
\CommentTok{#>                                         }
\CommentTok{#>             Sensitivity : 0.956         }
\CommentTok{#>             Specificity : 0.972         }
\CommentTok{#>          Pos Pred Value : 0.985         }
\CommentTok{#>          Neg Pred Value : 0.921         }
\CommentTok{#>              Prevalence : 0.656         }
\CommentTok{#>          Detection Rate : 0.627         }
\CommentTok{#>    Detection Prevalence : 0.636         }
\CommentTok{#>       Balanced Accuracy : 0.964         }
\CommentTok{#>                                         }
\CommentTok{#>        'Positive' Class : benign        }
\CommentTok{#> }
\end{Highlighting}
\end{Shaded}

\hypertarget{compare-selection-methods}{%
\subsection{Compare selection methods}\label{compare-selection-methods}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# compare features selected by the three methods}
\KeywordTok{library}\NormalTok{(gplots)}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'gplots'}
\CommentTok{#> The following object is masked from 'package:IRanges':}
\CommentTok{#> }
\CommentTok{#>     space}
\CommentTok{#> The following object is masked from 'package:S4Vectors':}
\CommentTok{#> }
\CommentTok{#>     space}
\CommentTok{#> The following object is masked from 'package:stats':}
\CommentTok{#> }
\CommentTok{#>     lowess}

\NormalTok{venn_list <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}\DataTypeTok{cor =} \KeywordTok{colnames}\NormalTok{(bc_data_cor)[}\OperatorTok{-}\DecValTok{1}\NormalTok{],}
                  \DataTypeTok{rfe =} \KeywordTok{colnames}\NormalTok{(bc_data_rfe)[}\OperatorTok{-}\DecValTok{1}\NormalTok{],}
                  \DataTypeTok{ga  =} \KeywordTok{colnames}\NormalTok{(bc_data_ga)[}\OperatorTok{-}\DecValTok{1}\NormalTok{])}

\NormalTok{venn <-}\StringTok{ }\KeywordTok{venn}\NormalTok{(venn_list)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_229-breast_cancer_ga-sglander_files/figure-latex/unnamed-chunk-20-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{venn}
\CommentTok{#>     num cor rfe ga}
\CommentTok{#> 000   0   0   0  0}
\CommentTok{#> 001   0   0   0  1}
\CommentTok{#> 010   0   0   1  0}
\CommentTok{#> 011   2   0   1  1}
\CommentTok{#> 100   1   1   0  0}
\CommentTok{#> 101   0   1   0  1}
\CommentTok{#> 110   1   1   1  0}
\CommentTok{#> 111   5   1   1  1}
\CommentTok{#> attr(,"intersections")}
\CommentTok{#> attr(,"intersections")$`cor:rfe:ga`}
\CommentTok{#> [1] "clump_thickness"   "marginal_adhesion" "bare_nuclei"      }
\CommentTok{#> [4] "bland_chromatin"   "normal_nucleoli"  }
\CommentTok{#> }
\CommentTok{#> attr(,"intersections")$cor}
\CommentTok{#> [1] "mitosis"}
\CommentTok{#> }
\CommentTok{#> attr(,"intersections")$`rfe:ga`}
\CommentTok{#> [1] "uniformity_of_cell_size"  "uniformity_of_cell_shape"}
\CommentTok{#> }
\CommentTok{#> attr(,"intersections")$`cor:rfe`}
\CommentTok{#> [1] "single_epithelial_cell_size"}
\CommentTok{#> }
\CommentTok{#> attr(,"class")}
\CommentTok{#> [1] "venn"}
\end{Highlighting}
\end{Shaded}

\begin{quote}
4 out of 10 features were chosen by all three methods; the biggest overlap is seen between GA and RFE with 7 features. RFE and GA both retained 8 features for modeling, compared to only 5 based on the correlation method.
\end{quote}

\hypertarget{correlation-2}{%
\subsection{Correlation}\label{correlation-2}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# correlation}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{127}\NormalTok{)}
\NormalTok{model_bc_data_cor <-}\StringTok{ }\KeywordTok{train}\NormalTok{(classes }\OperatorTok{~}\StringTok{ }\NormalTok{.,}
                 \DataTypeTok{data =}\NormalTok{ bc_data_cor,}
                 \DataTypeTok{method =} \StringTok{"rf"}\NormalTok{,}
                 \DataTypeTok{preProcess =} \KeywordTok{c}\NormalTok{(}\StringTok{"scale"}\NormalTok{, }\StringTok{"center"}\NormalTok{),}
                 \DataTypeTok{trControl =} \KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"repeatedcv"}\NormalTok{, }\DataTypeTok{number =} \DecValTok{5}\NormalTok{, }\DataTypeTok{repeats =} \DecValTok{10}\NormalTok{, }\DataTypeTok{verboseIter =} \OtherTok{FALSE}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cm_cor_}\DecValTok{1}\NormalTok{ <-}\StringTok{ }\KeywordTok{confusionMatrix}\NormalTok{(}\KeywordTok{predict}\NormalTok{(model_bc_data_cor, bc_data_test[, }\DecValTok{-1}\NormalTok{]), bc_data_test}\OperatorTok{$}\NormalTok{classes)}
\NormalTok{cm_cor_}\DecValTok{1}
\CommentTok{#> Confusion Matrix and Statistics}
\CommentTok{#> }
\CommentTok{#>            Reference}
\CommentTok{#> Prediction  benign malignant}
\CommentTok{#>   benign       130         4}
\CommentTok{#>   malignant      7        68}
\CommentTok{#>                                         }
\CommentTok{#>                Accuracy : 0.947         }
\CommentTok{#>                  95% CI : (0.908, 0.973)}
\CommentTok{#>     No Information Rate : 0.656         }
\CommentTok{#>     P-Value [Acc > NIR] : <2e-16        }
\CommentTok{#>                                         }
\CommentTok{#>                   Kappa : 0.885         }
\CommentTok{#>                                         }
\CommentTok{#>  Mcnemar's Test P-Value : 0.546         }
\CommentTok{#>                                         }
\CommentTok{#>             Sensitivity : 0.949         }
\CommentTok{#>             Specificity : 0.944         }
\CommentTok{#>          Pos Pred Value : 0.970         }
\CommentTok{#>          Neg Pred Value : 0.907         }
\CommentTok{#>              Prevalence : 0.656         }
\CommentTok{#>          Detection Rate : 0.622         }
\CommentTok{#>    Detection Prevalence : 0.641         }
\CommentTok{#>       Balanced Accuracy : 0.947         }
\CommentTok{#>                                         }
\CommentTok{#>        'Positive' Class : benign        }
\CommentTok{#> }
\end{Highlighting}
\end{Shaded}

\hypertarget{recursive-feature-elimination}{%
\subsection{Recursive Feature Elimination}\label{recursive-feature-elimination}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{127}\NormalTok{)}
\NormalTok{model_bc_data_rfe <-}\StringTok{ }\KeywordTok{train}\NormalTok{(classes }\OperatorTok{~}\StringTok{ }\NormalTok{.,}
                           \DataTypeTok{data =}\NormalTok{ bc_data_rfe,}
                           \DataTypeTok{method =} \StringTok{"rf"}\NormalTok{,}
                           \DataTypeTok{preProcess =} \KeywordTok{c}\NormalTok{(}\StringTok{"scale"}\NormalTok{, }\StringTok{"center"}\NormalTok{),}
                           \DataTypeTok{trControl =} \KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"repeatedcv"}\NormalTok{, }
                                                    \DataTypeTok{number =} \DecValTok{5}\NormalTok{, }\DataTypeTok{repeats =} \DecValTok{10}\NormalTok{, }
                                                    \DataTypeTok{verboseIter =} \OtherTok{FALSE}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cm_rfe_}\DecValTok{1}\NormalTok{ <-}\StringTok{ }\KeywordTok{confusionMatrix}\NormalTok{(}\KeywordTok{predict}\NormalTok{(model_bc_data_rfe, bc_data_test[, }\DecValTok{-1}\NormalTok{]), bc_data_test}\OperatorTok{$}\NormalTok{classes)}
\NormalTok{cm_rfe_}\DecValTok{1}
\CommentTok{#> Confusion Matrix and Statistics}
\CommentTok{#> }
\CommentTok{#>            Reference}
\CommentTok{#> Prediction  benign malignant}
\CommentTok{#>   benign       130         3}
\CommentTok{#>   malignant      7        69}
\CommentTok{#>                                         }
\CommentTok{#>                Accuracy : 0.952         }
\CommentTok{#>                  95% CI : (0.914, 0.977)}
\CommentTok{#>     No Information Rate : 0.656         }
\CommentTok{#>     P-Value [Acc > NIR] : <2e-16        }
\CommentTok{#>                                         }
\CommentTok{#>                   Kappa : 0.895         }
\CommentTok{#>                                         }
\CommentTok{#>  Mcnemar's Test P-Value : 0.343         }
\CommentTok{#>                                         }
\CommentTok{#>             Sensitivity : 0.949         }
\CommentTok{#>             Specificity : 0.958         }
\CommentTok{#>          Pos Pred Value : 0.977         }
\CommentTok{#>          Neg Pred Value : 0.908         }
\CommentTok{#>              Prevalence : 0.656         }
\CommentTok{#>          Detection Rate : 0.622         }
\CommentTok{#>    Detection Prevalence : 0.636         }
\CommentTok{#>       Balanced Accuracy : 0.954         }
\CommentTok{#>                                         }
\CommentTok{#>        'Positive' Class : benign        }
\CommentTok{#> }
\end{Highlighting}
\end{Shaded}

\hypertarget{ga}{%
\subsection{GA}\label{ga}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{127}\NormalTok{)}
\NormalTok{model_bc_data_ga <-}\StringTok{ }\KeywordTok{train}\NormalTok{(classes }\OperatorTok{~}\StringTok{ }\NormalTok{.,}
                           \DataTypeTok{data =}\NormalTok{ bc_data_ga,}
                           \DataTypeTok{method =} \StringTok{"rf"}\NormalTok{,}
                           \DataTypeTok{preProcess =} \KeywordTok{c}\NormalTok{(}\StringTok{"scale"}\NormalTok{, }\StringTok{"center"}\NormalTok{),}
                           \DataTypeTok{trControl =} \KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"repeatedcv"}\NormalTok{, }
                                                    \DataTypeTok{number =} \DecValTok{5}\NormalTok{, }\DataTypeTok{repeats =} \DecValTok{10}\NormalTok{, }
                                                    \DataTypeTok{verboseIter =} \OtherTok{FALSE}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cm_ga_}\DecValTok{1}\NormalTok{ <-}\StringTok{ }\KeywordTok{confusionMatrix}\NormalTok{(}\KeywordTok{predict}\NormalTok{(model_bc_data_ga, bc_data_test[, }\DecValTok{-1}\NormalTok{]), bc_data_test}\OperatorTok{$}\NormalTok{classes)}
\NormalTok{cm_ga_}\DecValTok{1}
\CommentTok{#> Confusion Matrix and Statistics}
\CommentTok{#> }
\CommentTok{#>            Reference}
\CommentTok{#> Prediction  benign malignant}
\CommentTok{#>   benign       131         2}
\CommentTok{#>   malignant      6        70}
\CommentTok{#>                                         }
\CommentTok{#>                Accuracy : 0.962         }
\CommentTok{#>                  95% CI : (0.926, 0.983)}
\CommentTok{#>     No Information Rate : 0.656         }
\CommentTok{#>     P-Value [Acc > NIR] : <2e-16        }
\CommentTok{#>                                         }
\CommentTok{#>                   Kappa : 0.916         }
\CommentTok{#>                                         }
\CommentTok{#>  Mcnemar's Test P-Value : 0.289         }
\CommentTok{#>                                         }
\CommentTok{#>             Sensitivity : 0.956         }
\CommentTok{#>             Specificity : 0.972         }
\CommentTok{#>          Pos Pred Value : 0.985         }
\CommentTok{#>          Neg Pred Value : 0.921         }
\CommentTok{#>              Prevalence : 0.656         }
\CommentTok{#>          Detection Rate : 0.627         }
\CommentTok{#>    Detection Prevalence : 0.636         }
\CommentTok{#>       Balanced Accuracy : 0.964         }
\CommentTok{#>                                         }
\CommentTok{#>        'Positive' Class : benign        }
\CommentTok{#> }
\end{Highlighting}
\end{Shaded}

\hypertarget{create-comparison-tables}{%
\section{Create comparison tables}\label{create-comparison-tables}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# take "overall" variable only from Confusion Matrix}
\NormalTok{overall <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{dataset =} \DecValTok{1}\NormalTok{, }
           \DataTypeTok{model =} \KeywordTok{rep}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"all"}\NormalTok{, }\StringTok{"cor"}\NormalTok{, }\StringTok{"rfe"}\NormalTok{, }\StringTok{"ga"}\NormalTok{), }\DecValTok{1}\NormalTok{),}
           \KeywordTok{rbind}\NormalTok{(cm_all_}\DecValTok{1}\OperatorTok{$}\NormalTok{overall,}
\NormalTok{                 cm_cor_}\DecValTok{1}\OperatorTok{$}\NormalTok{overall,}
\NormalTok{                 cm_rfe_}\DecValTok{1}\OperatorTok{$}\NormalTok{overall,}
\NormalTok{                 cm_ga_}\DecValTok{1}\OperatorTok{$}\NormalTok{overall)}
\NormalTok{)}

\CommentTok{# convert to tidy data}
\KeywordTok{library}\NormalTok{(tidyr)}
\NormalTok{overall_gather <-}\StringTok{ }\NormalTok{overall[, }\DecValTok{1}\OperatorTok{:}\DecValTok{4}\NormalTok{] }\OperatorTok{%>%}\StringTok{     }\CommentTok{# take the first columns:}
\StringTok{  }\KeywordTok{gather}\NormalTok{(measure, value, Accuracy}\OperatorTok{:}\NormalTok{Kappa) }\CommentTok{# dataset, model, Accuracy and Kappa}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# take "byClass" variable only from Confusion Matrix}
\NormalTok{byClass <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{dataset =} \DecValTok{1}\NormalTok{,}
                      \DataTypeTok{model =} \KeywordTok{rep}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"all"}\NormalTok{, }\StringTok{"cor"}\NormalTok{, }\StringTok{"rfe"}\NormalTok{, }\StringTok{"ga"}\NormalTok{), }\DecValTok{1}\NormalTok{),}
                      \KeywordTok{rbind}\NormalTok{(cm_all_}\DecValTok{1}\OperatorTok{$}\NormalTok{byClass,}
\NormalTok{                      cm_cor_}\DecValTok{1}\OperatorTok{$}\NormalTok{byClass,}
\NormalTok{                      cm_rfe_}\DecValTok{1}\OperatorTok{$}\NormalTok{byClass,}
\NormalTok{                      cm_ga_}\DecValTok{1}\OperatorTok{$}\NormalTok{byClass)}
\NormalTok{)}

\CommentTok{# convert to tidy data}
\NormalTok{byClass_gather <-}\StringTok{ }\NormalTok{byClass[, }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{4}\NormalTok{, }\DecValTok{7}\NormalTok{)] }\OperatorTok{%>%}\StringTok{      }\CommentTok{# select columns: dataset, model}
\StringTok{  }\KeywordTok{gather}\NormalTok{(measure, value, Sensitivity}\OperatorTok{:}\NormalTok{Precision) }\CommentTok{# Sensitiv, Specific, Precis}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# join the two tables}
\NormalTok{overall_byClass_gather <-}\StringTok{ }\KeywordTok{rbind}\NormalTok{(overall_gather, byClass_gather)}
\NormalTok{overall_byClass_gather <-}\StringTok{ }\KeywordTok{within}\NormalTok{(}
\NormalTok{  overall_byClass_gather, model <-}\StringTok{ }\KeywordTok{factor}\NormalTok{(model, }
                                          \DataTypeTok{levels =} \KeywordTok{c}\NormalTok{(}\StringTok{"all"}\NormalTok{, }\StringTok{"cor"}\NormalTok{, }\StringTok{"rfe"}\NormalTok{, }\StringTok{"ga"}\NormalTok{)))  }
                                          \CommentTok{# convert to factor}

\KeywordTok{ggplot}\NormalTok{(overall_byClass_gather, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ model, }\DataTypeTok{y =}\NormalTok{ value, }\DataTypeTok{color =}\NormalTok{ measure, }
                                   \DataTypeTok{shape =}\NormalTok{ measure, }\DataTypeTok{group =}\NormalTok{ measure)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{size =} \DecValTok{4}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{0.8}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_path}\NormalTok{(}\DataTypeTok{alpha =} \FloatTok{0.7}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_colour_brewer}\NormalTok{(}\DataTypeTok{palette =} \StringTok{"Set1"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{facet_grid}\NormalTok{(dataset }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{scales =} \StringTok{"free_y"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}
    \DataTypeTok{x =} \StringTok{"Feature Selection method"}\NormalTok{,}
    \DataTypeTok{y =} \StringTok{"Value"}\NormalTok{,}
    \DataTypeTok{color =} \StringTok{""}\NormalTok{,}
    \DataTypeTok{shape =} \StringTok{""}\NormalTok{,}
    \DataTypeTok{title =} \StringTok{"Comparison of feature selection methods"}\NormalTok{,}
    \DataTypeTok{subtitle =} \StringTok{"in three breast cancer datasets"}\NormalTok{,}
    \DataTypeTok{caption =} \StringTok{"}\CharTok{\textbackslash{}n}\StringTok{Breast Cancer Wisconsin (Diagnostic) Data Sets: 1, 2 & 3}
\StringTok{    Street et al., 1993;}
\StringTok{    all: no feature selection}
\StringTok{    cor: features with correlation > 0.7 removed}
\StringTok{    rfe: Recursive Feature Elimination}
\StringTok{    ga: Genetic Algorithm"}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_229-breast_cancer_ga-sglander_files/figure-latex/unnamed-chunk-27-1} \end{center}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Less accurate: selection of features by correlation
\item
  More accurate: genetic algorithm
\item
  Including all features is more accurate to removing features by correlation.
\end{enumerate}

\hypertarget{notes}{%
\section{Notes}\label{notes}}

\texttt{pcaGoPromoter} is a BioConductor package. Its dependencies are \texttt{BioGenerics}, \texttt{AnnotationDbi} and \texttt{BioStrings}, which at their turn require \texttt{DBI} and \texttt{RSQLite} packages from CRAN. Install first those from CRAN, and then move to install \texttt{pcaGoPromoter}.

\hypertarget{titanic-with-naive-bayes-classifier}{%
\chapter{Titanic with Naive-Bayes Classifier}\label{titanic-with-naive-bayes-classifier}}

The Titanic dataset in R is a table for about 2200 passengers summarised according to four factors -- economic status ranging from 1st class, 2nd class, 3rd class and crew; gender which is either male or female; Age category which is either Child or Adult and whether the type of passenger survived. For each combination of Age, Gender, Class and Survived status, the table gives the number of passengers who fall into the combination. We will use the Naive Bayes Technique to classify such passengers and check how well it performs.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#Getting started with Naive Bayes}
\CommentTok{#Install the package}
\CommentTok{#install.packages(â€œe1071â€)}
\CommentTok{#Loading the library}
\KeywordTok{library}\NormalTok{(e1071)}

\CommentTok{#Next load the Titanic dataset}
\KeywordTok{data}\NormalTok{(}\StringTok{"Titanic"}\NormalTok{)}
\CommentTok{#Save into a data frame and view it}
\NormalTok{Titanic_df =}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(Titanic)}
\end{Highlighting}
\end{Shaded}

We see that there are 32 observations which represent all possible combinations of Class, Sex, Age and Survived with their frequency. Since it is summarised, this table is not suitable for modelling purposes. We need to expand the table into individual rows. Let's create a repeating sequence of rows based on the frequencies in the table

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#Creating data from table}
\NormalTok{repeating_sequence=}\KeywordTok{rep.int}\NormalTok{(}\KeywordTok{seq_len}\NormalTok{(}\KeywordTok{nrow}\NormalTok{(Titanic_df)), Titanic_df}\OperatorTok{$}\NormalTok{Freq) }\CommentTok{#This will repeat each combination equal to the frequency of each combination}

\CommentTok{# Create the dataset by row repetition created}
\NormalTok{Titanic_dataset=Titanic_df[repeating_sequence,]}

\CommentTok{# We no longer need the frequency, drop the feature}
\NormalTok{Titanic_dataset}\OperatorTok{$}\NormalTok{Freq=}\OtherTok{NULL}
\end{Highlighting}
\end{Shaded}

The data is now ready for Naive Bayes to process. Let's fit the model

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Fitting the Naive Bayes model}
\NormalTok{Naive_Bayes_Model=}\KeywordTok{naiveBayes}\NormalTok{(Survived }\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{Titanic_dataset)}

\CommentTok{# What does the model say? Print the model summary}
\NormalTok{Naive_Bayes_Model}
\CommentTok{#> }
\CommentTok{#> Naive Bayes Classifier for Discrete Predictors}
\CommentTok{#> }
\CommentTok{#> Call:}
\CommentTok{#> naiveBayes.default(x = X, y = Y, laplace = laplace)}
\CommentTok{#> }
\CommentTok{#> A-priori probabilities:}
\CommentTok{#> Y}
\CommentTok{#>    No   Yes }
\CommentTok{#> 0.677 0.323 }
\CommentTok{#> }
\CommentTok{#> Conditional probabilities:}
\CommentTok{#>      Class}
\CommentTok{#> Y        1st    2nd    3rd   Crew}
\CommentTok{#>   No  0.0819 0.1121 0.3544 0.4517}
\CommentTok{#>   Yes 0.2855 0.1660 0.2504 0.2982}
\CommentTok{#> }
\CommentTok{#>      Sex}
\CommentTok{#> Y       Male Female}
\CommentTok{#>   No  0.9154 0.0846}
\CommentTok{#>   Yes 0.5162 0.4838}
\CommentTok{#> }
\CommentTok{#>      Age}
\CommentTok{#> Y      Child  Adult}
\CommentTok{#>   No  0.0349 0.9651}
\CommentTok{#>   Yes 0.0802 0.9198}
\end{Highlighting}
\end{Shaded}

The model creates the conditional probability for each feature separately. We also have the a-priori probabilities which indicates the distribution of our data. Let's calculate how we perform on the data.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Prediction on the dataset}
\NormalTok{NB_Predictions=}\KeywordTok{predict}\NormalTok{(Naive_Bayes_Model,Titanic_dataset)}
\CommentTok{# Confusion matrix to check accuracy}
\KeywordTok{table}\NormalTok{(NB_Predictions,Titanic_dataset}\OperatorTok{$}\NormalTok{Survived)}
\CommentTok{#>               }
\CommentTok{#> NB_Predictions   No  Yes}
\CommentTok{#>            No  1364  362}
\CommentTok{#>            Yes  126  349}
\end{Highlighting}
\end{Shaded}

We have the results! We are able to classify 1364 out of 1490 ``No'' cases correctly and 349 out of 711 ``Yes'' cases correctly. This means the ability of Naive Bayes algorithm to predict ``No'' cases is about 91.5\% but it falls down to only 49\% of the ``Yes'' cases resulting in an overall accuracy of 77.8\%

\hypertarget{can-we-do-any-better}{%
\chapter{Can we Do any Better?}\label{can-we-do-any-better}}

Naive Bayes is a parametric algorithm which implies that you cannot perform differently in different runs as long as the data remains the same. We will, however, learn another implementation of Naive Bayes algorithm using the `mlr' package. Assuming the same session is going on for the readers, I will install and load the package and start fitting a model

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Getting started with Naive Bayes in mlr}
\CommentTok{# install.packages(â€œmlrâ€)}
\CommentTok{# Loading the library}
\KeywordTok{library}\NormalTok{(mlr)}
\CommentTok{#> Loading required package: ParamHelpers}
\CommentTok{#> Registered S3 methods overwritten by 'ggplot2':}
\CommentTok{#>   method         from }
\CommentTok{#>   [.quosures     rlang}
\CommentTok{#>   c.quosures     rlang}
\CommentTok{#>   print.quosures rlang}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'mlr'}
\CommentTok{#> The following object is masked from 'package:e1071':}
\CommentTok{#> }
\CommentTok{#>     impute}
\end{Highlighting}
\end{Shaded}

The mlr package consists of a lot of models and works by creating tasks and learners which are then trained. Let's create a classification task using the titanic dataset and fit a model with the naive bayes algorithm.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Create a classification task for learning on Titanic Dataset and specify the target feature}
\NormalTok{task =}\StringTok{ }\KeywordTok{makeClassifTask}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ Titanic_dataset, }\DataTypeTok{target =} \StringTok{"Survived"}\NormalTok{)}

\CommentTok{# Initialize the Naive Bayes classifier}
\NormalTok{selected_model =}\StringTok{ }\KeywordTok{makeLearner}\NormalTok{(}\StringTok{"classif.naiveBayes"}\NormalTok{)}

\CommentTok{# Train the model}
\NormalTok{NB_mlr =}\StringTok{ }\KeywordTok{train}\NormalTok{(selected_model, task)}
\end{Highlighting}
\end{Shaded}

The summary of the model which was printed in e3071 package is stored in learner model. Let's print it and compare

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Read the model learned  }
\NormalTok{NB_mlr}\OperatorTok{$}\NormalTok{learner.model}
\CommentTok{#> }
\CommentTok{#> Naive Bayes Classifier for Discrete Predictors}
\CommentTok{#> }
\CommentTok{#> Call:}
\CommentTok{#> naiveBayes.default(x = X, y = Y, laplace = laplace)}
\CommentTok{#> }
\CommentTok{#> A-priori probabilities:}
\CommentTok{#> Y}
\CommentTok{#>    No   Yes }
\CommentTok{#> 0.677 0.323 }
\CommentTok{#> }
\CommentTok{#> Conditional probabilities:}
\CommentTok{#>      Class}
\CommentTok{#> Y        1st    2nd    3rd   Crew}
\CommentTok{#>   No  0.0819 0.1121 0.3544 0.4517}
\CommentTok{#>   Yes 0.2855 0.1660 0.2504 0.2982}
\CommentTok{#> }
\CommentTok{#>      Sex}
\CommentTok{#> Y       Male Female}
\CommentTok{#>   No  0.9154 0.0846}
\CommentTok{#>   Yes 0.5162 0.4838}
\CommentTok{#> }
\CommentTok{#>      Age}
\CommentTok{#> Y      Child  Adult}
\CommentTok{#>   No  0.0349 0.9651}
\CommentTok{#>   Yes 0.0802 0.9198}
\end{Highlighting}
\end{Shaded}

The a-priori probabilities and the conditional probabilities for the model are similar to the one calculated by e3071 package as was expected. This means that our predictions will also be the same.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Predict on the dataset without passing the target feature}
\NormalTok{predictions_mlr =}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(}\KeywordTok{predict}\NormalTok{(NB_mlr, }\DataTypeTok{newdata =}\NormalTok{ Titanic_dataset[,}\DecValTok{1}\OperatorTok{:}\DecValTok{3}\NormalTok{]))}

\CommentTok{## Confusion matrix to check accuracy}
\KeywordTok{table}\NormalTok{(predictions_mlr[,}\DecValTok{1}\NormalTok{],Titanic_dataset}\OperatorTok{$}\NormalTok{Survived)}
\CommentTok{#>      }
\CommentTok{#>         No  Yes}
\CommentTok{#>   No  1364  362}
\CommentTok{#>   Yes  126  349}
\end{Highlighting}
\end{Shaded}

As we see, the predictions are exactly same. The only way to improve is to have more features or more data. Perhaps, if we have more features such as the exact age, size of family, number of parents in the ship and siblings then we may arrive at a better model using Naive Bayes. In essence, Naive Bayes has an advantage of a strong foundation build and is very robust. I know of the `caret' package which also consists of Naive Bayes function but it will also give us the same predictions and probability.

\hypertarget{building-a-naive-bayes-classifier-in-r}{%
\chapter{Building a Naive Bayes Classifier in R}\label{building-a-naive-bayes-classifier-in-r}}

\url{https://www.machinelearningplus.com/predictive-modeling/how-naive-bayes-algorithm-works-with-example-and-full-code/}

\hypertarget{building-a-naive-bayes-classifier-in-r-1}{%
\section{8. Building a Naive Bayes Classifier in R}\label{building-a-naive-bayes-classifier-in-r-1}}

Understanding Naive Bayes was the (slightly) tricky part. Implementing it is fairly straightforward.

In R, Naive Bayes classifier is implemented in packages such as \texttt{e1071}, \texttt{klaR} and \texttt{bnlearn}. In Python, it is implemented in \texttt{scikit-learn}.

For sake of demonstration, let's use the standard iris dataset to predict the Species of flower using 4 different features: Sepal.Length, Sepal.Width, Petal.Length, Petal.Width

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Import Data}
\NormalTok{training <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{'https://raw.githubusercontent.com/selva86/datasets/master/iris_train.csv'}\NormalTok{)}
\NormalTok{test <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{'https://raw.githubusercontent.com/selva86/datasets/master/iris_test.csv'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The training data is now contained in training and test data in test dataframe. Lets load the klaR package and build the naive bayes model.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Using klaR for Naive Bayes}
\KeywordTok{library}\NormalTok{(klaR)}
\CommentTok{#> Loading required package: MASS}
\NormalTok{nb_mod <-}\StringTok{ }\KeywordTok{NaiveBayes}\NormalTok{(Species }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data=}\NormalTok{training)}
\NormalTok{pred <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(nb_mod, test)}
\end{Highlighting}
\end{Shaded}

Lets see the confusion matrix.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Confusion Matrix}
\NormalTok{tab <-}\StringTok{ }\KeywordTok{table}\NormalTok{(pred}\OperatorTok{$}\NormalTok{class, test}\OperatorTok{$}\NormalTok{Species)}
\NormalTok{caret}\OperatorTok{::}\KeywordTok{confusionMatrix}\NormalTok{(tab)  }
\CommentTok{#> Registered S3 methods overwritten by 'ggplot2':}
\CommentTok{#>   method         from }
\CommentTok{#>   [.quosures     rlang}
\CommentTok{#>   c.quosures     rlang}
\CommentTok{#>   print.quosures rlang}
\CommentTok{#> Confusion Matrix and Statistics}
\CommentTok{#> }
\CommentTok{#>             }
\CommentTok{#>              setosa versicolor virginica}
\CommentTok{#>   setosa         15          0         0}
\CommentTok{#>   versicolor      0         11         0}
\CommentTok{#>   virginica       0          4        15}
\CommentTok{#> }
\CommentTok{#> Overall Statistics}
\CommentTok{#>                                         }
\CommentTok{#>                Accuracy : 0.911         }
\CommentTok{#>                  95% CI : (0.788, 0.975)}
\CommentTok{#>     No Information Rate : 0.333         }
\CommentTok{#>     P-Value [Acc > NIR] : 8.47e-16      }
\CommentTok{#>                                         }
\CommentTok{#>                   Kappa : 0.867         }
\CommentTok{#>                                         }
\CommentTok{#>  Mcnemar's Test P-Value : NA            }
\CommentTok{#> }
\CommentTok{#> Statistics by Class:}
\CommentTok{#> }
\CommentTok{#>                      Class: setosa Class: versicolor Class: virginica}
\CommentTok{#> Sensitivity                  1.000             0.733            1.000}
\CommentTok{#> Specificity                  1.000             1.000            0.867}
\CommentTok{#> Pos Pred Value               1.000             1.000            0.789}
\CommentTok{#> Neg Pred Value               1.000             0.882            1.000}
\CommentTok{#> Prevalence                   0.333             0.333            0.333}
\CommentTok{#> Detection Rate               0.333             0.244            0.333}
\CommentTok{#> Detection Prevalence         0.333             0.244            0.422}
\CommentTok{#> Balanced Accuracy            1.000             0.867            0.933}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Plot density of each feature using nb_mod}
\NormalTok{opar =}\StringTok{ }\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{), }\DataTypeTok{mar=}\KeywordTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{))}
\KeywordTok{plot}\NormalTok{(nb_mod, }\DataTypeTok{main=}\StringTok{""}\NormalTok{)  }
\KeywordTok{par}\NormalTok{(opar)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_252b-iris-naive_bayes_files/figure-latex/unnamed-chunk-5-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Plot the Confusion Matrix}
\KeywordTok{library}\NormalTok{(ggplot2)}
\NormalTok{test}\OperatorTok{$}\NormalTok{pred <-}\StringTok{ }\NormalTok{pred}\OperatorTok{$}\NormalTok{class}
\KeywordTok{ggplot}\NormalTok{(test, }\KeywordTok{aes}\NormalTok{(Species, pred, }\DataTypeTok{color =}\NormalTok{ Species)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_jitter}\NormalTok{(}\DataTypeTok{width =} \FloatTok{0.2}\NormalTok{, }\DataTypeTok{height =} \FloatTok{0.1}\NormalTok{, }\DataTypeTok{size=}\DecValTok{2}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{title=}\StringTok{"Confusion Matrix"}\NormalTok{, }
       \DataTypeTok{subtitle=}\StringTok{"Predicted vs. Observed from Iris dataset"}\NormalTok{, }
       \DataTypeTok{y=}\StringTok{"Predicted"}\NormalTok{, }
       \DataTypeTok{x=}\StringTok{"Truth"}\NormalTok{,}
       \DataTypeTok{caption=}\StringTok{"machinelearningplus.com"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{classification_252b-iris-naive_bayes_files/figure-latex/unnamed-chunk-6-1} \end{center}

\hypertarget{logistic-regression.-diabetes}{%
\chapter{Logistic Regression. Diabetes}\label{logistic-regression.-diabetes}}

\hypertarget{introduction-4}{%
\section{Introduction}\label{introduction-4}}

Source: \url{https://github.com/AntoineGuillot2/Logistic-Regression-R/blob/master/script.R}
Source: \url{http://enhancedatascience.com/2017/04/26/r-basics-logistic-regression-with-r/}
Data: \url{https://www.kaggle.com/uciml/pima-indians-diabetes-database}

The goal of logistic regression is to predict whether an outcome will be positive (aka 1) or negative (i.e: 0). Some real life example could be:

\begin{itemize}
\tightlist
\item
  Will Emmanuel Macron win the French Presidential election or will he lose?
\item
  Does Mr.X has this illness or not?
\item
  Will this visitor click on my link or not?
\end{itemize}

So, logistic regression can be used in a lot of binary classification cases and will often be run before more advanced methods. For this tutorial, we will use the diabetes detection dataset from Kaggle.

This dataset contains data from Pima Indians Women such as the number of pregnancies, the blood pressure, the skin thickness, \ldots{} the goal of the tutorial is to be able to detect diabetes using only these measures.

\hypertarget{exploring-the-data}{%
\section{Exploring the data}\label{exploring-the-data}}

As usual, first, let's take a look at our data. You can download the data here then please put the file diabetes.csv in your working directory. With the summary function, we can easily summarise the different variables:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(ggplot2)}
\CommentTok{#> Registered S3 methods overwritten by 'ggplot2':}
\CommentTok{#>   method         from }
\CommentTok{#>   [.quosures     rlang}
\CommentTok{#>   c.quosures     rlang}
\CommentTok{#>   print.quosures rlang}
\KeywordTok{library}\NormalTok{(data.table)}

\NormalTok{DiabetesData <-}\StringTok{ }\KeywordTok{data.table}\NormalTok{(}\KeywordTok{read.csv}\NormalTok{(}\KeywordTok{file.path}\NormalTok{(data_raw_dir, }\StringTok{'diabetes.csv'}\NormalTok{)))}

\CommentTok{# Quick data summary}
\KeywordTok{summary}\NormalTok{(DiabetesData)}
\CommentTok{#>   Pregnancies       Glucose    BloodPressure   SkinThickness }
\CommentTok{#>  Min.   : 0.00   Min.   :  0   Min.   :  0.0   Min.   : 0.0  }
\CommentTok{#>  1st Qu.: 1.00   1st Qu.: 99   1st Qu.: 62.0   1st Qu.: 0.0  }
\CommentTok{#>  Median : 3.00   Median :117   Median : 72.0   Median :23.0  }
\CommentTok{#>  Mean   : 3.85   Mean   :121   Mean   : 69.1   Mean   :20.5  }
\CommentTok{#>  3rd Qu.: 6.00   3rd Qu.:140   3rd Qu.: 80.0   3rd Qu.:32.0  }
\CommentTok{#>  Max.   :17.00   Max.   :199   Max.   :122.0   Max.   :99.0  }
\CommentTok{#>     Insulin         BMI       DiabetesPedigreeFunction      Age      }
\CommentTok{#>  Min.   :  0   Min.   : 0.0   Min.   :0.078            Min.   :21.0  }
\CommentTok{#>  1st Qu.:  0   1st Qu.:27.3   1st Qu.:0.244            1st Qu.:24.0  }
\CommentTok{#>  Median : 30   Median :32.0   Median :0.372            Median :29.0  }
\CommentTok{#>  Mean   : 80   Mean   :32.0   Mean   :0.472            Mean   :33.2  }
\CommentTok{#>  3rd Qu.:127   3rd Qu.:36.6   3rd Qu.:0.626            3rd Qu.:41.0  }
\CommentTok{#>  Max.   :846   Max.   :67.1   Max.   :2.420            Max.   :81.0  }
\CommentTok{#>     Outcome     }
\CommentTok{#>  Min.   :0.000  }
\CommentTok{#>  1st Qu.:0.000  }
\CommentTok{#>  Median :0.000  }
\CommentTok{#>  Mean   :0.349  }
\CommentTok{#>  3rd Qu.:1.000  }
\CommentTok{#>  Max.   :1.000}
\end{Highlighting}
\end{Shaded}

The mean of the outcome is 0.35 which shows an imbalance between the classes. However, the imbalance should not be too strong to be a problem.

To understand the relationship between variables, a Scatter Plot Matrix will be used. To plot it, the GGally package was used.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Scatter plot matrix}
\KeywordTok{library}\NormalTok{(GGally)}
\CommentTok{#> Registered S3 method overwritten by 'GGally':}
\CommentTok{#>   method from   }
\CommentTok{#>   +.gg   ggplot2}
\KeywordTok{ggpairs}\NormalTok{(DiabetesData, }\DataTypeTok{lower =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{continuous=}\StringTok{'smooth'}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{meta_137-logistic_regression-diabetes_files/figure-latex/plot-matrix-1} \end{center}

The correlations between explanatory variables do not seem too strong. Hence the model is not likely to suffer from multicollinearity. All explanatory variable are correlated with the Outcome. At first sight, glucose rate is the most important factor to detect the outcome.

\hypertarget{logistic-regression-with-r}{%
\section{Logistic regression with R}\label{logistic-regression-with-r}}

After variable exploration, a first model can be fitted using the glm function. With stargazer, it is easy to get nice output in ASCII or even Latex.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# first model: all features}
\NormalTok{glm1 =}\StringTok{ }\KeywordTok{glm}\NormalTok{(Outcome}\OperatorTok{~}\NormalTok{., }
\NormalTok{           DiabetesData, }
           \DataTypeTok{family =} \KeywordTok{binomial}\NormalTok{(}\DataTypeTok{link=}\StringTok{"logit"}\NormalTok{))}

\KeywordTok{summary}\NormalTok{(glm1)}
\CommentTok{#> }
\CommentTok{#> Call:}
\CommentTok{#> glm(formula = Outcome ~ ., family = binomial(link = "logit"), }
\CommentTok{#>     data = DiabetesData)}
\CommentTok{#> }
\CommentTok{#> Deviance Residuals: }
\CommentTok{#>    Min      1Q  Median      3Q     Max  }
\CommentTok{#> -2.557  -0.727  -0.416   0.727   2.930  }
\CommentTok{#> }
\CommentTok{#> Coefficients:}
\CommentTok{#>                           Estimate Std. Error z value Pr(>|z|)    }
\CommentTok{#> (Intercept)              -8.404696   0.716636  -11.73  < 2e-16 ***}
\CommentTok{#> Pregnancies               0.123182   0.032078    3.84  0.00012 ***}
\CommentTok{#> Glucose                   0.035164   0.003709    9.48  < 2e-16 ***}
\CommentTok{#> BloodPressure            -0.013296   0.005234   -2.54  0.01107 *  }
\CommentTok{#> SkinThickness             0.000619   0.006899    0.09  0.92852    }
\CommentTok{#> Insulin                  -0.001192   0.000901   -1.32  0.18607    }
\CommentTok{#> BMI                       0.089701   0.015088    5.95  2.8e-09 ***}
\CommentTok{#> DiabetesPedigreeFunction  0.945180   0.299147    3.16  0.00158 ** }
\CommentTok{#> Age                       0.014869   0.009335    1.59  0.11119    }
\CommentTok{#> ---}
\CommentTok{#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1}
\CommentTok{#> }
\CommentTok{#> (Dispersion parameter for binomial family taken to be 1)}
\CommentTok{#> }
\CommentTok{#>     Null deviance: 993.48  on 767  degrees of freedom}
\CommentTok{#> Residual deviance: 723.45  on 759  degrees of freedom}
\CommentTok{#> AIC: 741.4}
\CommentTok{#> }
\CommentTok{#> Number of Fisher Scoring iterations: 5}
\KeywordTok{require}\NormalTok{(stargazer)}
\CommentTok{#> Loading required package: stargazer}
\CommentTok{#> }
\CommentTok{#> Please cite as:}
\CommentTok{#>  Hlavac, Marek (2018). stargazer: Well-Formatted Regression and Summary Statistics Tables.}
\CommentTok{#>  R package version 5.2.2. https://CRAN.R-project.org/package=stargazer}
\KeywordTok{stargazer}\NormalTok{(glm1,}\DataTypeTok{type=}\StringTok{'text'}\NormalTok{)}
\CommentTok{#> }
\CommentTok{#> ====================================================}
\CommentTok{#>                              Dependent variable:    }
\CommentTok{#>                          ---------------------------}
\CommentTok{#>                                    Outcome          }
\CommentTok{#> ----------------------------------------------------}
\CommentTok{#> Pregnancies                       0.123***          }
\CommentTok{#>                                    (0.032)          }
\CommentTok{#>                                                     }
\CommentTok{#> Glucose                           0.035***          }
\CommentTok{#>                                    (0.004)          }
\CommentTok{#>                                                     }
\CommentTok{#> BloodPressure                     -0.013**          }
\CommentTok{#>                                    (0.005)          }
\CommentTok{#>                                                     }
\CommentTok{#> SkinThickness                       0.001           }
\CommentTok{#>                                    (0.007)          }
\CommentTok{#>                                                     }
\CommentTok{#> Insulin                            -0.001           }
\CommentTok{#>                                    (0.001)          }
\CommentTok{#>                                                     }
\CommentTok{#> BMI                               0.090***          }
\CommentTok{#>                                    (0.015)          }
\CommentTok{#>                                                     }
\CommentTok{#> DiabetesPedigreeFunction          0.945***          }
\CommentTok{#>                                    (0.299)          }
\CommentTok{#>                                                     }
\CommentTok{#> Age                                 0.015           }
\CommentTok{#>                                    (0.009)          }
\CommentTok{#>                                                     }
\CommentTok{#> Constant                          -8.400***         }
\CommentTok{#>                                    (0.717)          }
\CommentTok{#>                                                     }
\CommentTok{#> ----------------------------------------------------}
\CommentTok{#> Observations                         768            }
\CommentTok{#> Log Likelihood                    -362.000          }
\CommentTok{#> Akaike Inf. Crit.                  741.000          }
\CommentTok{#> ====================================================}
\CommentTok{#> Note:                    *p<0.1; **p<0.05; ***p<0.01}
\end{Highlighting}
\end{Shaded}

The overall model is significant. As expected the glucose rate has the lowest p-value of all the variables. However, Age, Insulin and Skin Thickness are not good predictors of Diabetes.

\hypertarget{a-second-model}{%
\section{A second model}\label{a-second-model}}

Since some variables are not significant, removing them is a good way to improve model robustness. In the second model, SkinThickness, Insulin, and Age are removed.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# second model: selected features}
\NormalTok{glm2 =}\StringTok{ }\KeywordTok{glm}\NormalTok{(Outcome}\OperatorTok{~}\NormalTok{., }
         \DataTypeTok{data =}\NormalTok{ DiabetesData[,}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{3}\NormalTok{,}\DecValTok{6}\OperatorTok{:}\DecValTok{7}\NormalTok{,}\DecValTok{9}\NormalTok{), }\DataTypeTok{with=}\NormalTok{F], }
         \DataTypeTok{family =} \KeywordTok{binomial}\NormalTok{(}\DataTypeTok{link=}\StringTok{"logit"}\NormalTok{))}

\KeywordTok{summary}\NormalTok{(glm2)}
\CommentTok{#> }
\CommentTok{#> Call:}
\CommentTok{#> glm(formula = Outcome ~ ., family = binomial(link = "logit"), }
\CommentTok{#>     data = DiabetesData[, c(1:3, 6:7, 9), with = F])}
\CommentTok{#> }
\CommentTok{#> Deviance Residuals: }
\CommentTok{#>    Min      1Q  Median      3Q     Max  }
\CommentTok{#> -2.793  -0.736  -0.419   0.725   2.955  }
\CommentTok{#> }
\CommentTok{#> Coefficients:}
\CommentTok{#>                          Estimate Std. Error z value Pr(>|z|)    }
\CommentTok{#> (Intercept)              -7.95495    0.67582  -11.77  < 2e-16 ***}
\CommentTok{#> Pregnancies               0.15349    0.02784    5.51  3.5e-08 ***}
\CommentTok{#> Glucose                   0.03466    0.00339   10.21  < 2e-16 ***}
\CommentTok{#> BloodPressure            -0.01201    0.00503   -2.39    0.017 *  }
\CommentTok{#> BMI                       0.08483    0.01412    6.01  1.9e-09 ***}
\CommentTok{#> DiabetesPedigreeFunction  0.91063    0.29403    3.10    0.002 ** }
\CommentTok{#> ---}
\CommentTok{#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1}
\CommentTok{#> }
\CommentTok{#> (Dispersion parameter for binomial family taken to be 1)}
\CommentTok{#> }
\CommentTok{#>     Null deviance: 993.48  on 767  degrees of freedom}
\CommentTok{#> Residual deviance: 728.56  on 762  degrees of freedom}
\CommentTok{#> AIC: 740.6}
\CommentTok{#> }
\CommentTok{#> Number of Fisher Scoring iterations: 5}
\end{Highlighting}
\end{Shaded}

\hypertarget{classification-rate-and-confusion-matrix}{%
\section{Classification rate and confusion matrix}\label{classification-rate-and-confusion-matrix}}

Now that we have our model, let's access its performance.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Correctly classified observations}
\KeywordTok{mean}\NormalTok{((glm2}\OperatorTok{$}\NormalTok{fitted.values}\OperatorTok{>}\FloatTok{0.5}\NormalTok{)}\OperatorTok{==}\NormalTok{DiabetesData}\OperatorTok{$}\NormalTok{Outcome)}
\CommentTok{#> [1] 0.775}
\end{Highlighting}
\end{Shaded}

Around 77.4\% of all observations are correctly classified. Due to class imbalance, we need to go further with a confusion matrix.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{###Confusion matrix count}
\NormalTok{RP=}\KeywordTok{sum}\NormalTok{((glm2}\OperatorTok{$}\NormalTok{fitted.values}\OperatorTok{>=}\FloatTok{0.5}\NormalTok{)}\OperatorTok{==}\NormalTok{DiabetesData}\OperatorTok{$}\NormalTok{Outcome }\OperatorTok{&}\StringTok{ }\NormalTok{DiabetesData}\OperatorTok{$}\NormalTok{Outcome}\OperatorTok{==}\DecValTok{1}\NormalTok{)}
\NormalTok{FP=}\KeywordTok{sum}\NormalTok{((glm2}\OperatorTok{$}\NormalTok{fitted.values}\OperatorTok{>=}\FloatTok{0.5}\NormalTok{)}\OperatorTok{!=}\NormalTok{DiabetesData}\OperatorTok{$}\NormalTok{Outcome }\OperatorTok{&}\StringTok{ }\NormalTok{DiabetesData}\OperatorTok{$}\NormalTok{Outcome}\OperatorTok{==}\DecValTok{0}\NormalTok{)}
\NormalTok{RN=}\KeywordTok{sum}\NormalTok{((glm2}\OperatorTok{$}\NormalTok{fitted.values}\OperatorTok{>=}\FloatTok{0.5}\NormalTok{)}\OperatorTok{==}\NormalTok{DiabetesData}\OperatorTok{$}\NormalTok{Outcome }\OperatorTok{&}\StringTok{ }\NormalTok{DiabetesData}\OperatorTok{$}\NormalTok{Outcome}\OperatorTok{==}\DecValTok{0}\NormalTok{)}
\NormalTok{FN=}\KeywordTok{sum}\NormalTok{((glm2}\OperatorTok{$}\NormalTok{fitted.values}\OperatorTok{>=}\FloatTok{0.5}\NormalTok{)}\OperatorTok{!=}\NormalTok{DiabetesData}\OperatorTok{$}\NormalTok{Outcome }\OperatorTok{&}\StringTok{ }\NormalTok{DiabetesData}\OperatorTok{$}\NormalTok{Outcome}\OperatorTok{==}\DecValTok{1}\NormalTok{)}
\NormalTok{confMat<-}\KeywordTok{matrix}\NormalTok{(}\KeywordTok{c}\NormalTok{(RP,FP,FN,RN),}\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{)}
\KeywordTok{colnames}\NormalTok{(confMat)<-}\KeywordTok{c}\NormalTok{(}\StringTok{"Pred Diabetes"}\NormalTok{,}\StringTok{'Pred no diabetes'}\NormalTok{)}
\KeywordTok{rownames}\NormalTok{(confMat)<-}\KeywordTok{c}\NormalTok{(}\StringTok{"Real Diabetes"}\NormalTok{,}\StringTok{'Real no diabetes'}\NormalTok{)}
\NormalTok{confMat}
\CommentTok{#>                  Pred Diabetes Pred no diabetes}
\CommentTok{#> Real Diabetes              154              114}
\CommentTok{#> Real no diabetes            59              441}
\end{Highlighting}
\end{Shaded}

The model is good to detect people who do not have diabetes. However, its performance on ill people is not great (only 154 out of 268 have been correctly classified).

You can also get the percentage of Real/False Positive/Negative:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Confusion matrix proportion}
\NormalTok{RPR=RP}\OperatorTok{/}\KeywordTok{sum}\NormalTok{(DiabetesData}\OperatorTok{$}\NormalTok{Outcome}\OperatorTok{==}\DecValTok{1}\NormalTok{)}\OperatorTok{*}\DecValTok{100}
\NormalTok{FNR=FN}\OperatorTok{/}\KeywordTok{sum}\NormalTok{(DiabetesData}\OperatorTok{$}\NormalTok{Outcome}\OperatorTok{==}\DecValTok{1}\NormalTok{)}\OperatorTok{*}\DecValTok{100}
\NormalTok{FPR=FP}\OperatorTok{/}\KeywordTok{sum}\NormalTok{(DiabetesData}\OperatorTok{$}\NormalTok{Outcome}\OperatorTok{==}\DecValTok{0}\NormalTok{)}\OperatorTok{*}\DecValTok{100}
\NormalTok{RNR=RN}\OperatorTok{/}\KeywordTok{sum}\NormalTok{(DiabetesData}\OperatorTok{$}\NormalTok{Outcome}\OperatorTok{==}\DecValTok{0}\NormalTok{)}\OperatorTok{*}\DecValTok{100}
\NormalTok{confMat<-}\KeywordTok{matrix}\NormalTok{(}\KeywordTok{c}\NormalTok{(RPR,FPR,FNR,RNR),}\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{)}
\KeywordTok{colnames}\NormalTok{(confMat)<-}\KeywordTok{c}\NormalTok{(}\StringTok{"Pred Diabetes"}\NormalTok{,}\StringTok{'Pred no diabetes'}\NormalTok{)}
\KeywordTok{rownames}\NormalTok{(confMat)<-}\KeywordTok{c}\NormalTok{(}\StringTok{"Real Diabetes"}\NormalTok{,}\StringTok{'Real no diabetes'}\NormalTok{)}
\NormalTok{confMat}
\CommentTok{#>                  Pred Diabetes Pred no diabetes}
\CommentTok{#> Real Diabetes             57.5             42.5}
\CommentTok{#> Real no diabetes          11.8             88.2}
\end{Highlighting}
\end{Shaded}

And here is the matrix, 57.5\% of people with diabetes are correctly classified. A way to improve the false negative rate would lower the detection threshold. However, as a consequence, the false positive rate would increase.

\hypertarget{plots-and-decision-boundaries}{%
\section{Plots and decision boundaries}\label{plots-and-decision-boundaries}}

The two strongest predictors of the outcome are Glucose rate and BMI. High glucose rate and high BMI are strong indicators of Diabetes.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Plot and decision boundaries}
\NormalTok{DiabetesData}\OperatorTok{$}\NormalTok{Predicted <-}\StringTok{ }\NormalTok{glm2}\OperatorTok{$}\NormalTok{fitted.values}

\KeywordTok{ggplot}\NormalTok{(DiabetesData, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ BMI, }\DataTypeTok{y =}\NormalTok{ Glucose, }\DataTypeTok{color =}\NormalTok{ Predicted }\OperatorTok{>}\StringTok{ }\FloatTok{0.5}\NormalTok{)) }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{size=}\DecValTok{2}\NormalTok{, }\DataTypeTok{alpha=}\FloatTok{0.5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{meta_137-logistic_regression-diabetes_files/figure-latex/plot-bmi-vs-glucose-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(DiabetesData, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{BMI, }\DataTypeTok{y =}\NormalTok{ Glucose, }\DataTypeTok{color=}\NormalTok{Outcome }\OperatorTok{==}\StringTok{ }\NormalTok{(Predicted }\OperatorTok{>}\StringTok{ }\FloatTok{0.5}\NormalTok{))) }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{size=}\DecValTok{2}\NormalTok{, }\DataTypeTok{alpha=}\FloatTok{0.5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{meta_137-logistic_regression-diabetes_files/figure-latex/unnamed-chunk-3-1} \end{center}

We can also plot both BMI and glucose against the outcomes, the other variables are taken at their mean level.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{range}\NormalTok{(DiabetesData}\OperatorTok{$}\NormalTok{BMI)}
\CommentTok{#> [1]  0.0 67.1}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# BMI vs predicted}
\NormalTok{BMI_plot =}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{BMI =}\NormalTok{ ((}\KeywordTok{min}\NormalTok{(DiabetesData}\OperatorTok{$}\NormalTok{BMI}\DecValTok{-2}\NormalTok{)}\OperatorTok{*}\DecValTok{100}\NormalTok{)}\OperatorTok{:}
\StringTok{                               }\NormalTok{(}\KeywordTok{max}\NormalTok{(DiabetesData}\OperatorTok{$}\NormalTok{BMI}\OperatorTok{+}\DecValTok{2}\NormalTok{)}\OperatorTok{*}\DecValTok{100}\NormalTok{))}\OperatorTok{/}\DecValTok{100}\NormalTok{,}
                    \DataTypeTok{Glucose =} \KeywordTok{mean}\NormalTok{(DiabetesData}\OperatorTok{$}\NormalTok{Glucose),}
                    \DataTypeTok{Pregnancies =} \KeywordTok{mean}\NormalTok{(DiabetesData}\OperatorTok{$}\NormalTok{Pregnancies),}
                    \DataTypeTok{BloodPressure =} \KeywordTok{mean}\NormalTok{(DiabetesData}\OperatorTok{$}\NormalTok{BloodPressure),}
                    \DataTypeTok{DiabetesPedigreeFunction =} \KeywordTok{mean}\NormalTok{(DiabetesData}\OperatorTok{$}\NormalTok{DiabetesPedigreeFunction))}

\NormalTok{BMI_plot}\OperatorTok{$}\NormalTok{Predicted =}\StringTok{ }\KeywordTok{predict}\NormalTok{(glm2, }\DataTypeTok{newdata =}\NormalTok{ BMI_plot, }\DataTypeTok{type =} \StringTok{'response'}\NormalTok{)}
\KeywordTok{ggplot}\NormalTok{(BMI_plot, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ BMI, }\DataTypeTok{y =}\NormalTok{ Predicted)) }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{geom_line}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{meta_137-logistic_regression-diabetes_files/figure-latex/bmi-vs-predicted-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{range}\NormalTok{(BMI_plot}\OperatorTok{$}\NormalTok{BMI)}
\CommentTok{#> [1] -2.0 69.1}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{range}\NormalTok{(DiabetesData}\OperatorTok{$}\NormalTok{Glucose)}
\CommentTok{#> [1]   0 199}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Glucose vs predicted}
\NormalTok{Glucose_plot=}\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{Glucose =} 
\NormalTok{                        ((}\KeywordTok{min}\NormalTok{(DiabetesData}\OperatorTok{$}\NormalTok{Glucose}\DecValTok{-2}\NormalTok{)}\OperatorTok{*}\DecValTok{100}\NormalTok{)}\OperatorTok{:}
\StringTok{                             }\NormalTok{(}\KeywordTok{max}\NormalTok{(DiabetesData}\OperatorTok{$}\NormalTok{Glucose}\OperatorTok{+}\DecValTok{2}\NormalTok{)}\OperatorTok{*}\DecValTok{100}\NormalTok{))}\OperatorTok{/}\DecValTok{100}\NormalTok{,}
                    \DataTypeTok{BMI=}\KeywordTok{mean}\NormalTok{(DiabetesData}\OperatorTok{$}\NormalTok{BMI),}
                    \DataTypeTok{Pregnancies=}\KeywordTok{mean}\NormalTok{(DiabetesData}\OperatorTok{$}\NormalTok{Pregnancies),}
                    \DataTypeTok{BloodPressure=}\KeywordTok{mean}\NormalTok{(DiabetesData}\OperatorTok{$}\NormalTok{BloodPressure),}
                    \DataTypeTok{DiabetesPedigreeFunction=}\KeywordTok{mean}\NormalTok{(DiabetesData}\OperatorTok{$}\NormalTok{DiabetesPedigreeFunction))}

\NormalTok{Glucose_plot}\OperatorTok{$}\NormalTok{Predicted =}\StringTok{ }\KeywordTok{predict}\NormalTok{(glm2, }\DataTypeTok{newdata =}\NormalTok{ Glucose_plot, }\DataTypeTok{type =} \StringTok{'response'}\NormalTok{)}
\KeywordTok{ggplot}\NormalTok{(Glucose_plot, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Glucose, }\DataTypeTok{y =}\NormalTok{ Predicted)) }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{geom_line}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{meta_137-logistic_regression-diabetes_files/figure-latex/glucose-vs-predicted-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{range}\NormalTok{(Glucose_plot}\OperatorTok{$}\NormalTok{Glucose)}
\CommentTok{#> [1]  -2 201}
\end{Highlighting}
\end{Shaded}

\bibliography{book.bib,packages.bib}


\end{document}
