[
["index.html", "Machine Learning Meta Chapter 1 Prerequisites", " Machine Learning Meta Alfonso R. Reyes 2019-09-10 Chapter 1 Prerequisites This is a sample book written in Markdown. You can use anything that Pandoc’s Markdown supports, e.g., a math equation \\(a^2 + b^2 = c^2\\). The bookdown package can be installed from CRAN or Github: install.packages(&quot;bookdown&quot;) # or the development version # devtools::install_github(&quot;rstudio/bookdown&quot;) Remember each Rmd file contains one and only one chapter, and a chapter is defined by the first-level heading #. To compile this example to PDF, you need XeLaTeX. You are recommended to install TinyTeX (which includes XeLaTeX): https://yihui.name/tinytex/. "],
["intro.html", "Chapter 2 Introduction", " Chapter 2 Introduction You can label chapter and section titles using {#label} after them, e.g., we can reference Chapter 2. If you do not manually label them, there will be automatic labels anyway, e.g., Chapter ??. Figures and tables with captions will be placed in figure and table environments, respectively. par(mar = c(4, 4, .1, .1)) plot(pressure, type = &#39;b&#39;, pch = 19) Figure 2.1: Here is a nice figure! Reference a figure by its code chunk label with the fig: prefix, e.g., see Figure 2.1. Similarly, you can reference tables generated from knitr::kable(), e.g., see Table 2.1. knitr::kable( head(iris, 20), caption = &#39;Here is a nice table!&#39;, booktabs = TRUE ) Table 2.1: Here is a nice table! Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa 4.6 3.4 1.4 0.3 setosa 5.0 3.4 1.5 0.2 setosa 4.4 2.9 1.4 0.2 setosa 4.9 3.1 1.5 0.1 setosa 5.4 3.7 1.5 0.2 setosa 4.8 3.4 1.6 0.2 setosa 4.8 3.0 1.4 0.1 setosa 4.3 3.0 1.1 0.1 setosa 5.8 4.0 1.2 0.2 setosa 5.7 4.4 1.5 0.4 setosa 5.4 3.9 1.3 0.4 setosa 5.1 3.5 1.4 0.3 setosa 5.7 3.8 1.7 0.3 setosa 5.1 3.8 1.5 0.3 setosa You can write citations, too. For example, we are using the bookdown package (Xie 2019) in this sample book, which was built on top of R Markdown and knitr (Xie 2015). References "],
["references.html", "References", " References "],
["pca-prcomp-vs-princomp.html", "Chapter 3 PCA: prcomp vs princomp 3.1 General methods for principal component analysis 3.2 prcomp() and princomp() functions 3.3 factoextra 3.4 demo dataset 3.5 Compute PCA in R using prcomp() 3.6 Plots: quality and contribution 3.7 Access to the PCA results", " Chapter 3 PCA: prcomp vs princomp http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/118-principal-component-analysis-in-r-prcomp-vs-princomp/ 3.1 General methods for principal component analysis There are two general methods to perform PCA in R : Spectral decomposition which examines the covariances / correlations between variables Singular value decomposition which examines the covariances / correlations between individuals The function princomp() uses the spectral decomposition approach. The functions prcomp() and PCA()[FactoMineR] use the singular value decomposition (SVD). 3.2 prcomp() and princomp() functions The simplified format of these 2 functions are : prcomp(x, scale = FALSE) princomp(x, cor = FALSE, scores = TRUE) Arguments for prcomp(): x: a numeric matrix or data frame scale: a logical value indicating whether the variables should be scaled to have unit variance before the analysis takes place Arguments for princomp(): x: a numeric matrix or data frame cor: a logical value. If TRUE, the data will be centered and scaled before the analysis scores: a logical value. If TRUE, the coordinates on each principal component are calculated 3.3 factoextra # install.packages(&quot;factoextra&quot;) library(factoextra) #&gt; Loading required package: ggplot2 #&gt; Registered S3 methods overwritten by &#39;ggplot2&#39;: #&gt; method from #&gt; [.quosures rlang #&gt; c.quosures rlang #&gt; print.quosures rlang #&gt; Welcome! Related Books: `Practical Guide To Cluster Analysis in R` at https://goo.gl/13EFCZ 3.4 demo dataset We’ll use the data sets decathlon2 [in factoextra], which has been already described at: PCA - Data format. Briefly, it contains: Active individuals (rows 1 to 23) and active variables (columns 1 to 10), which are used to perform the principal component analysis Supplementary individuals (rows 24 to 27) and supplementary variables (columns 11 to 13), which coordinates will be predicted using the PCA information and parameters obtained with active individuals/variables. library(&quot;factoextra&quot;) data(decathlon2) decathlon2.active &lt;- decathlon2[1:23, 1:10] head(decathlon2.active[, 1:6]) #&gt; X100m Long.jump Shot.put High.jump X400m X110m.hurdle #&gt; SEBRLE 11.0 7.58 14.8 2.07 49.8 14.7 #&gt; CLAY 10.8 7.40 14.3 1.86 49.4 14.1 #&gt; BERNARD 11.0 7.23 14.2 1.92 48.9 15.0 #&gt; YURKOV 11.3 7.09 15.2 2.10 50.4 15.3 #&gt; ZSIVOCZKY 11.1 7.30 13.5 2.01 48.6 14.2 #&gt; McMULLEN 10.8 7.31 13.8 2.13 49.9 14.4 decathlon2.supplementary &lt;- decathlon2[24:27, 1:10] head(decathlon2.supplementary[, 1:6]) #&gt; X100m Long.jump Shot.put High.jump X400m X110m.hurdle #&gt; KARPOV 11.0 7.30 14.8 2.04 48.4 14.1 #&gt; WARNERS 11.1 7.60 14.3 1.98 48.7 14.2 #&gt; Nool 10.8 7.53 14.3 1.88 48.8 14.8 #&gt; Drews 10.9 7.38 13.1 1.88 48.5 14.0 3.5 Compute PCA in R using prcomp() In this section we’ll provide an easy-to-use R code to compute and visualize PCA in R using the prcomp() function and the factoextra package. `. Load factoextra for visualization library(factoextra) compute PCA # compute PCA res.pca &lt;- prcomp(decathlon2.active, scale = TRUE) Visualize eigenvalues (scree plot). Show the percentage of variances explained by each principal component. # Visualize eigenvalues (scree plot). fviz_eig(res.pca, addlabels = TRUE, ylim = c(0, 50)) From the plot above, we might want to stop at the fifth principal component. 87% of the information (variances) contained in the data are retained by the first five principal components. 3.6 Plots: quality and contribution Graph of individuals. Individuals with a similar profile are grouped together. # Graph of individuals. fviz_pca_ind(res.pca, col.ind = &quot;cos2&quot;, # Color by the quality of representation gradient.cols = c(&quot;#00AFBB&quot;, &quot;#E7B800&quot;, &quot;#FC4E07&quot;), repel = TRUE # Avoid text overlapping ) Graph of variables. Positive correlated variables point to the same side of the plot. Negative correlated variables point to opposite sides of the graph. # Graph of variables. fviz_pca_var(res.pca, col.var = &quot;contrib&quot;, # Color by contributions to the PC gradient.cols = c(&quot;#00AFBB&quot;, &quot;#E7B800&quot;, &quot;#FC4E07&quot;), repel = TRUE # Avoid text overlapping ) Biplot of individuals and variables # Biplot of individuals and variables fviz_pca_biplot(res.pca, repel = TRUE, col.var = &quot;#2E9FDF&quot;, # Variables color col.ind = &quot;#696969&quot; # Individuals color ) 3.7 Access to the PCA results library(factoextra) # Eigenvalues eig.val &lt;- get_eigenvalue(res.pca) eig.val #&gt; eigenvalue variance.percent cumulative.variance.percent #&gt; Dim.1 4.124 41.24 41.2 #&gt; Dim.2 1.839 18.39 59.6 #&gt; Dim.3 1.239 12.39 72.0 #&gt; Dim.4 0.819 8.19 80.2 #&gt; Dim.5 0.702 7.02 87.2 #&gt; Dim.6 0.423 4.23 91.5 #&gt; Dim.7 0.303 3.03 94.5 #&gt; Dim.8 0.274 2.74 97.2 #&gt; Dim.9 0.155 1.55 98.8 #&gt; Dim.10 0.122 1.22 100.0 # Results for Variables res.var &lt;- get_pca_var(res.pca) res.var$coord # Coordinates #&gt; Dim.1 Dim.2 Dim.3 Dim.4 Dim.5 Dim.6 Dim.7 #&gt; X100m -0.85063 0.1794 -0.3016 0.0336 -0.194 0.03537 -0.09134 #&gt; Long.jump 0.79418 -0.2809 0.1905 -0.1154 0.233 -0.03373 -0.15433 #&gt; Shot.put 0.73391 -0.0854 -0.5176 0.1285 -0.249 -0.23979 -0.00989 #&gt; High.jump 0.61008 0.4652 -0.3301 0.1446 0.403 -0.28464 0.02816 #&gt; X400m -0.70160 -0.2902 -0.2835 0.4308 0.104 -0.04929 0.28611 #&gt; X110m.hurdle -0.76413 0.0247 -0.4489 -0.0169 0.224 0.00263 -0.37007 #&gt; Discus 0.74321 -0.0497 -0.1765 0.3950 -0.408 0.19854 -0.14273 #&gt; Pole.vault -0.21727 -0.8075 -0.0941 -0.3390 -0.222 -0.32746 -0.01039 #&gt; Javeline 0.42823 -0.3861 -0.6041 -0.3317 0.198 0.36210 0.13356 #&gt; X1500m 0.00428 -0.7845 0.2195 0.4480 0.263 0.04205 -0.11137 #&gt; Dim.8 Dim.9 Dim.10 #&gt; X100m -0.10472 -0.3031 0.04442 #&gt; Long.jump -0.39738 -0.0516 0.02972 #&gt; Shot.put 0.02436 0.0478 0.21745 #&gt; High.jump 0.08441 -0.1121 -0.13357 #&gt; X400m -0.23355 0.0822 -0.03417 #&gt; X110m.hurdle -0.00834 0.1618 -0.01563 #&gt; Discus -0.03956 0.0134 -0.17259 #&gt; Pole.vault 0.03291 -0.0258 -0.13721 #&gt; Javeline 0.05284 -0.0405 -0.00385 #&gt; X1500m 0.19447 -0.1022 0.06283 res.var$contrib # Contributions to the PCs #&gt; Dim.1 Dim.2 Dim.3 Dim.4 Dim.5 Dim.6 Dim.7 #&gt; X100m 1.75e+01 1.7505 7.339 0.1376 5.39 0.29592 2.7571 #&gt; Long.jump 1.53e+01 4.2904 2.930 1.6249 7.75 0.26900 7.8716 #&gt; Shot.put 1.31e+01 0.3967 21.620 2.0141 8.82 13.59686 0.0323 #&gt; High.jump 9.02e+00 11.7716 8.793 2.5499 23.12 19.15961 0.2620 #&gt; X400m 1.19e+01 4.5799 6.488 22.6509 1.54 0.57451 27.0527 #&gt; X110m.hurdle 1.42e+01 0.0333 16.261 0.0348 7.17 0.00164 45.2616 #&gt; Discus 1.34e+01 0.1341 2.515 19.0413 23.76 9.32175 6.7323 #&gt; Pole.vault 1.14e+00 35.4619 0.714 14.0231 7.01 25.35762 0.0357 #&gt; Javeline 4.45e+00 8.1087 29.453 13.4296 5.58 31.00496 5.8957 #&gt; X1500m 4.44e-04 33.4729 3.887 24.4939 9.88 0.41813 4.0989 #&gt; Dim.8 Dim.9 Dim.10 #&gt; X100m 3.9952 59.174 1.6176 #&gt; Long.jump 57.5332 1.715 0.7241 #&gt; Shot.put 0.2162 1.471 38.7677 #&gt; High.jump 2.5957 8.102 14.6265 #&gt; X400m 19.8734 4.349 0.9573 #&gt; X110m.hurdle 0.0254 16.858 0.2003 #&gt; Discus 0.5702 0.115 24.4217 #&gt; Pole.vault 0.3947 0.428 15.4356 #&gt; Javeline 1.0173 1.054 0.0122 #&gt; X1500m 13.7787 6.734 3.2370 res.var$cos2 # Quality of representation #&gt; Dim.1 Dim.2 Dim.3 Dim.4 Dim.5 Dim.6 Dim.7 #&gt; X100m 7.24e-01 0.032184 0.09094 0.001127 0.0378 1.25e-03 8.34e-03 #&gt; Long.jump 6.31e-01 0.078881 0.03631 0.013315 0.0544 1.14e-03 2.38e-02 #&gt; Shot.put 5.39e-01 0.007294 0.26791 0.016504 0.0619 5.75e-02 9.77e-05 #&gt; High.jump 3.72e-01 0.216424 0.10896 0.020895 0.1622 8.10e-02 7.93e-04 #&gt; X400m 4.92e-01 0.084203 0.08039 0.185611 0.0108 2.43e-03 8.19e-02 #&gt; X110m.hurdle 5.84e-01 0.000612 0.20150 0.000285 0.0503 6.93e-06 1.37e-01 #&gt; Discus 5.52e-01 0.002466 0.03116 0.156032 0.1667 3.94e-02 2.04e-02 #&gt; Pole.vault 4.72e-02 0.651977 0.00885 0.114911 0.0491 1.07e-01 1.08e-04 #&gt; Javeline 1.83e-01 0.149080 0.36497 0.110048 0.0391 1.31e-01 1.78e-02 #&gt; X1500m 1.83e-05 0.615409 0.04817 0.200713 0.0693 1.77e-03 1.24e-02 #&gt; Dim.8 Dim.9 Dim.10 #&gt; X100m 1.10e-02 0.091848 1.97e-03 #&gt; Long.jump 1.58e-01 0.002661 8.83e-04 #&gt; Shot.put 5.93e-04 0.002284 4.73e-02 #&gt; High.jump 7.12e-03 0.012575 1.78e-02 #&gt; X400m 5.45e-02 0.006750 1.17e-03 #&gt; X110m.hurdle 6.96e-05 0.026166 2.44e-04 #&gt; Discus 1.56e-03 0.000179 2.98e-02 #&gt; Pole.vault 1.08e-03 0.000664 1.88e-02 #&gt; Javeline 2.79e-03 0.001637 1.49e-05 #&gt; X1500m 3.78e-02 0.010453 3.95e-03 # Results for individuals res.ind &lt;- get_pca_ind(res.pca) res.ind$coord # Coordinates #&gt; Dim.1 Dim.2 Dim.3 Dim.4 Dim.5 Dim.6 Dim.7 #&gt; SEBRLE 0.191 -1.554 -0.628 0.0821 1.142614 -0.4639 -0.2080 #&gt; CLAY 0.790 -2.420 1.357 1.2698 -0.806848 1.3042 -0.2129 #&gt; BERNARD -1.329 -1.612 -0.196 -1.9209 0.082343 -0.4006 -0.4064 #&gt; YURKOV -0.869 0.433 -2.474 0.6972 0.398858 0.1029 -0.3249 #&gt; ZSIVOCZKY -0.106 2.023 1.305 -0.0993 -0.197024 0.8955 0.0883 #&gt; McMULLEN 0.119 0.992 0.844 1.3122 1.585871 0.1866 0.4783 #&gt; MARTINEAU -2.392 1.285 -0.898 0.3731 -2.243352 -0.4567 -0.2998 #&gt; HERNU -1.891 -1.178 -0.156 0.8913 -0.126741 0.4362 -0.5661 #&gt; BARRAS -1.774 0.413 0.658 0.2287 -0.233837 0.0903 0.2159 #&gt; NOOL -2.777 1.573 0.607 -1.5555 1.424184 0.4972 -0.5321 #&gt; BOURGUIGNON -4.414 -1.264 -0.010 0.6668 0.419152 -0.0820 -0.5983 #&gt; Sebrle 3.451 -1.217 -1.678 -0.8087 -0.025053 -0.0828 0.0102 #&gt; Clay 3.316 -1.623 -0.618 -0.3168 0.569165 0.7772 0.2575 #&gt; Karpov 4.070 0.798 1.015 0.3134 -0.797426 -0.3296 -1.3637 #&gt; Macey 1.848 2.064 -0.979 0.5847 -0.000216 -0.1973 -0.2693 #&gt; Warners 1.387 -0.282 2.000 -1.0196 -0.040540 -0.5567 -0.2674 #&gt; Zsivoczky 0.472 0.927 -1.728 -0.1848 0.407303 -0.1138 0.0399 #&gt; Hernu 0.276 1.166 0.171 -0.8487 -0.689480 -0.3317 0.4431 #&gt; Bernard 1.367 1.478 0.831 0.7453 0.859802 -0.3281 0.3636 #&gt; Schwarzl -0.710 -0.658 1.041 -0.9272 -0.288757 -0.6889 0.5657 #&gt; Pogorelov -0.214 -0.861 0.298 1.3556 -0.015053 -1.5938 0.7837 #&gt; Schoenbeck -0.495 -1.300 0.103 -0.2493 -0.645226 0.1617 0.8575 #&gt; Barras -0.316 0.819 -0.862 -0.5894 -0.779739 1.1742 0.9451 #&gt; Dim.8 Dim.9 Dim.10 #&gt; SEBRLE 0.04346 -0.65934 0.0327 #&gt; CLAY 0.61724 -0.06013 -0.3172 #&gt; BERNARD 0.70386 0.17008 -0.0991 #&gt; YURKOV 0.11500 -0.10952 -0.1197 #&gt; ZSIVOCZKY -0.20234 -0.52310 -0.3484 #&gt; McMULLEN 0.29309 -0.10562 -0.3932 #&gt; MARTINEAU -0.29163 -0.22342 -0.6164 #&gt; HERNU -1.52940 0.00618 0.5537 #&gt; BARRAS 0.68258 -0.66928 0.5309 #&gt; NOOL -0.43339 -0.11578 -0.0962 #&gt; BOURGUIGNON 0.56362 0.52581 0.0586 #&gt; Sebrle -0.03059 -0.84721 0.2197 #&gt; Clay -0.58064 0.40978 -0.6160 #&gt; Karpov 0.34531 0.19306 0.2172 #&gt; Macey -0.36322 0.36826 0.2125 #&gt; Warners -0.10947 0.18028 0.2421 #&gt; Zsivoczky 0.53804 0.58597 -0.1427 #&gt; Hernu 0.24729 0.06691 -0.2087 #&gt; Bernard 0.00617 0.27949 0.3207 #&gt; Schwarzl -0.68705 -0.00836 -0.3021 #&gt; Pogorelov -0.03762 -0.13053 -0.0370 #&gt; Schoenbeck -0.25585 0.56422 0.2968 #&gt; Barras 0.36555 0.10226 0.6119 res.ind$contrib # Contributions to the PCs #&gt; Dim.1 Dim.2 Dim.3 Dim.4 Dim.5 Dim.6 Dim.7 #&gt; SEBRLE 0.0385 5.712 1.39e+00 0.0357 8.09e+00 2.2126 0.62143 #&gt; CLAY 0.6581 13.854 6.46e+00 8.5557 4.03e+00 17.4880 0.65141 #&gt; BERNARD 1.8627 6.144 1.35e-01 19.5783 4.20e-02 1.6502 2.37365 #&gt; YURKOV 0.7969 0.443 2.15e+01 2.5794 9.86e-01 0.1088 1.51656 #&gt; ZSIVOCZKY 0.0118 9.682 5.97e+00 0.0523 2.41e-01 8.2456 0.11192 #&gt; McMULLEN 0.0148 2.325 2.50e+00 9.1353 1.56e+01 0.3579 3.28702 #&gt; MARTINEAU 6.0337 3.904 2.83e+00 0.7386 3.12e+01 2.1441 1.29111 #&gt; HERNU 3.7700 3.284 8.58e-02 4.2151 9.96e-02 1.9566 4.60485 #&gt; BARRAS 3.3194 0.402 1.52e+00 0.2776 3.39e-01 0.0838 0.67004 #&gt; NOOL 8.1299 5.849 1.29e+00 12.8376 1.26e+01 2.5413 4.06767 #&gt; BOURGUIGNON 20.5373 3.776 3.53e-04 2.3588 1.09e+00 0.0691 5.14425 #&gt; Sebrle 12.5584 3.502 9.88e+00 3.4701 3.89e-03 0.0705 0.00148 #&gt; Clay 11.5936 6.232 1.34e+00 0.5325 2.01e+00 6.2097 0.95282 #&gt; Karpov 17.4661 1.507 3.61e+00 0.5210 3.94e+00 1.1168 26.72016 #&gt; Macey 3.6021 10.073 3.36e+00 1.8139 2.89e-07 0.4001 1.04191 #&gt; Warners 2.0291 0.188 1.40e+01 5.5159 1.02e-02 3.1867 1.02738 #&gt; Zsivoczky 0.2344 2.031 1.05e+01 0.1813 1.03e+00 0.1332 0.02289 #&gt; Hernu 0.0805 3.214 1.02e-01 3.8217 2.95e+00 1.1311 2.82103 #&gt; Bernard 1.9708 5.166 2.43e+00 2.9474 4.58e+00 1.1066 1.89945 #&gt; Schwarzl 0.5318 1.025 3.80e+00 4.5612 5.17e-01 4.8796 4.59812 #&gt; Pogorelov 0.0484 1.753 3.11e-01 9.7503 1.40e-03 26.1167 8.82532 #&gt; Schoenbeck 0.2586 3.997 3.72e-02 0.3297 2.58e+00 0.2689 10.56627 #&gt; Barras 0.1052 1.588 2.61e+00 1.8430 3.77e+00 14.1743 12.83542 #&gt; Dim.8 Dim.9 Dim.10 #&gt; SEBRLE 2.99e-02 12.17748 0.0382 #&gt; CLAY 6.04e+00 0.10126 3.5857 #&gt; BERNARD 7.85e+00 0.81032 0.3499 #&gt; YURKOV 2.09e-01 0.33601 0.5107 #&gt; ZSIVOCZKY 6.49e-01 7.66492 4.3274 #&gt; McMULLEN 1.36e+00 0.31250 5.5105 #&gt; MARTINEAU 1.35e+00 1.39820 13.5440 #&gt; HERNU 3.71e+01 0.00107 10.9278 #&gt; BARRAS 7.38e+00 12.54733 10.0454 #&gt; NOOL 2.98e+00 0.37548 0.3300 #&gt; BOURGUIGNON 5.03e+00 7.74457 0.1222 #&gt; Sebrle 1.48e-02 20.10555 1.7206 #&gt; Clay 5.34e+00 4.70357 13.5271 #&gt; Karpov 1.89e+00 1.04399 1.6819 #&gt; Macey 2.09e+00 3.79877 1.6096 #&gt; Warners 1.90e-01 0.91042 2.0890 #&gt; Zsivoczky 4.59e+00 9.61785 0.7261 #&gt; Hernu 9.69e-01 0.12540 1.5523 #&gt; Bernard 6.02e-04 2.18807 3.6657 #&gt; Schwarzl 7.48e+00 0.00196 3.2536 #&gt; Pogorelov 2.24e-02 0.47727 0.0487 #&gt; Schoenbeck 1.04e+00 8.91730 3.1402 #&gt; Barras 2.12e+00 0.29289 13.3453 res.ind$cos2 # Quality of representation #&gt; Dim.1 Dim.2 Dim.3 Dim.4 Dim.5 Dim.6 Dim.7 #&gt; SEBRLE 0.00753 0.4975 8.13e-02 0.00139 2.69e-01 0.044324 8.91e-03 #&gt; CLAY 0.04870 0.4570 1.44e-01 0.12579 5.08e-02 0.132691 3.54e-03 #&gt; BERNARD 0.19720 0.2900 4.29e-03 0.41182 7.57e-04 0.017913 1.84e-02 #&gt; YURKOV 0.09611 0.0238 7.78e-01 0.06181 2.02e-02 0.001345 1.34e-02 #&gt; ZSIVOCZKY 0.00157 0.5764 2.40e-01 0.00139 5.47e-03 0.112918 1.10e-03 #&gt; McMULLEN 0.00218 0.1522 1.10e-01 0.26649 3.89e-01 0.005388 3.54e-02 #&gt; MARTINEAU 0.40401 0.1165 5.69e-02 0.00983 3.55e-01 0.014721 6.34e-03 #&gt; HERNU 0.39928 0.1551 2.73e-03 0.08870 1.79e-03 0.021248 3.58e-02 #&gt; BARRAS 0.61624 0.0333 8.48e-02 0.01024 1.07e-02 0.001594 9.13e-03 #&gt; NOOL 0.48987 0.1571 2.34e-02 0.15369 1.29e-01 0.015701 1.80e-02 #&gt; BOURGUIGNON 0.85970 0.0705 4.45e-06 0.01962 7.75e-03 0.000297 1.58e-02 #&gt; Sebrle 0.67538 0.0840 1.60e-01 0.03708 3.56e-05 0.000389 5.85e-06 #&gt; Clay 0.68759 0.1648 2.39e-02 0.00627 2.03e-02 0.037763 4.15e-03 #&gt; Karpov 0.78367 0.0301 4.87e-02 0.00464 3.01e-02 0.005138 8.80e-02 #&gt; Macey 0.36344 0.4531 1.02e-01 0.03636 4.95e-09 0.004140 7.71e-03 #&gt; Warners 0.25565 0.0106 5.31e-01 0.13808 2.18e-04 0.041169 9.50e-03 #&gt; Zsivoczky 0.04505 0.1740 6.05e-01 0.00692 3.36e-02 0.002625 3.23e-04 #&gt; Hernu 0.02482 0.4418 9.46e-03 0.23420 1.55e-01 0.035771 6.38e-02 #&gt; Bernard 0.28935 0.3381 1.07e-01 0.08598 1.14e-01 0.016659 2.05e-02 #&gt; Schwarzl 0.11672 0.1003 2.51e-01 0.19889 1.93e-02 0.109806 7.40e-02 #&gt; Pogorelov 0.00780 0.1259 1.50e-02 0.31210 3.85e-05 0.431416 1.04e-01 #&gt; Schoenbeck 0.06707 0.4620 2.90e-03 0.01699 1.14e-01 0.007150 2.01e-01 #&gt; Barras 0.01897 0.1277 1.41e-01 0.06604 1.16e-01 0.262130 1.70e-01 #&gt; Dim.8 Dim.9 Dim.10 #&gt; SEBRLE 3.89e-04 8.95e-02 0.000221 #&gt; CLAY 2.97e-02 2.82e-04 0.007847 #&gt; BERNARD 5.53e-02 3.23e-03 0.001096 #&gt; YURKOV 1.68e-03 1.53e-03 0.001822 #&gt; ZSIVOCZKY 5.76e-03 3.85e-02 0.017092 #&gt; McMULLEN 1.33e-02 1.73e-03 0.023927 #&gt; MARTINEAU 6.00e-03 3.52e-03 0.026821 #&gt; HERNU 2.61e-01 4.27e-06 0.034229 #&gt; BARRAS 9.12e-02 8.77e-02 0.055153 #&gt; NOOL 1.19e-02 8.51e-04 0.000588 #&gt; BOURGUIGNON 1.40e-02 1.22e-02 0.000151 #&gt; Sebrle 5.30e-05 4.07e-02 0.002737 #&gt; Clay 2.11e-02 1.05e-02 0.023726 #&gt; Karpov 5.64e-03 1.76e-03 0.002232 #&gt; Macey 1.40e-02 1.44e-02 0.004803 #&gt; Warners 1.59e-03 4.32e-03 0.007784 #&gt; Zsivoczky 5.87e-02 6.96e-02 0.004127 #&gt; Hernu 1.99e-02 1.46e-03 0.014160 #&gt; Bernard 5.88e-06 1.21e-02 0.015917 #&gt; Schwarzl 1.09e-01 1.62e-05 0.021117 #&gt; Pogorelov 2.40e-04 2.89e-03 0.000232 #&gt; Schoenbeck 1.79e-02 8.70e-02 0.024083 #&gt; Barras 2.54e-02 1.99e-03 0.071184 "],
["predict-using-pca.html", "Chapter 4 Predict using PCA 4.1 Supplementary variables 4.2 Theory behind PCA results", " Chapter 4 Predict using PCA In this section, we’ll show how to predict the coordinates of supplementary individuals and variables using only the information provided by the previously performed PCA. Data: rows 24 to 27 and columns 1 to to 10 [in decathlon2 data sets]. The new data must contain columns (variables) with the same names and in the same order as the active data used to compute PCA. # Data for the supplementary individuals ind.sup &lt;- decathlon2[24:27, 1:10] ind.sup[, 1:6] #&gt; X100m Long.jump Shot.put High.jump X400m X110m.hurdle #&gt; KARPOV 11.0 7.30 14.8 2.04 48.4 14.1 #&gt; WARNERS 11.1 7.60 14.3 1.98 48.7 14.2 #&gt; Nool 10.8 7.53 14.3 1.88 48.8 14.8 #&gt; Drews 10.9 7.38 13.1 1.88 48.5 14.0 Predict the coordinates of new individuals data. Use the R base function predict(): ind.sup.coord &lt;- predict(res.pca, newdata = ind.sup) ind.sup.coord[, 1:4] #&gt; PC1 PC2 PC3 PC4 #&gt; KARPOV 0.777 -0.762 1.597 1.686 #&gt; WARNERS -0.378 0.119 1.701 -0.691 #&gt; Nool -0.547 -1.934 0.472 -2.228 #&gt; Drews -1.085 -0.017 2.982 -1.501 Graph of individuals including the supplementary individuals: # Plot of active individuals p &lt;- fviz_pca_ind(res.pca, repel = TRUE) # Add supplementary individuals fviz_add(p, ind.sup.coord, color =&quot;blue&quot;) The predicted coordinates of individuals can be manually calculated as follow: Center and scale the new individuals data using the center and the scale of the PCA Calculate the predicted coordinates by multiplying the scaled values with the eigenvectors (loadings) of the principal components. The R code below can be used : # Centering and scaling the supplementary individuals ind.scaled &lt;- scale(ind.sup, center = res.pca$center, scale = res.pca$scale) # Coordinates of the individividuals coord_func &lt;- function(ind, loadings){ r &lt;- loadings*ind apply(r, 2, sum) } pca.loadings &lt;- res.pca$rotation ind.sup.coord &lt;- t(apply(ind.scaled, 1, coord_func, pca.loadings )) ind.sup.coord[, 1:4] #&gt; PC1 PC2 PC3 PC4 #&gt; KARPOV 0.777 -0.762 1.597 1.686 #&gt; WARNERS -0.378 0.119 1.701 -0.691 #&gt; Nool -0.547 -1.934 0.472 -2.228 #&gt; Drews -1.085 -0.017 2.982 -1.501 4.1 Supplementary variables 4.1.1 Qualitative / categorical variables The data sets decathlon2 contain a supplementary qualitative variable at columns 13 corresponding to the type of competitions. Qualitative / categorical variables can be used to color individuals by groups. The grouping variable should be of same length as the number of active individuals (here 23). groups &lt;- as.factor(decathlon2$Competition[1:23]) fviz_pca_ind(res.pca, col.ind = groups, # color by groups palette = c(&quot;#00AFBB&quot;, &quot;#FC4E07&quot;), addEllipses = TRUE, # Concentration ellipses ellipse.type = &quot;confidence&quot;, legend.title = &quot;Groups&quot;, repel = TRUE ) Calculate the coordinates for the levels of grouping variables. The coordinates for a given group is calculated as the mean coordinates of the individuals in the group. library(magrittr) # for pipe %&gt;% library(dplyr) # everything else #&gt; #&gt; Attaching package: &#39;dplyr&#39; #&gt; The following objects are masked from &#39;package:stats&#39;: #&gt; #&gt; filter, lag #&gt; The following objects are masked from &#39;package:base&#39;: #&gt; #&gt; intersect, setdiff, setequal, union # 1. Individual coordinates res.ind &lt;- get_pca_ind(res.pca) # 2. Coordinate of groups coord.groups &lt;- res.ind$coord %&gt;% as_data_frame() %&gt;% select(Dim.1, Dim.2) %&gt;% mutate(competition = groups) %&gt;% group_by(competition) %&gt;% summarise( Dim.1 = mean(Dim.1), Dim.2 = mean(Dim.2) ) #&gt; Warning: `as_data_frame()` is deprecated, use `as_tibble()` (but mind the new semantics). #&gt; This warning is displayed once per session. coord.groups #&gt; # A tibble: 2 x 3 #&gt; competition Dim.1 Dim.2 #&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Decastar -1.31 -0.119 #&gt; 2 OlympicG 1.20 0.109 4.1.2 Quantitative variables Data: columns 11:12. Should be of same length as the number of active individuals (here 23) quanti.sup &lt;- decathlon2[1:23, 11:12, drop = FALSE] head(quanti.sup) #&gt; Rank Points #&gt; SEBRLE 1 8217 #&gt; CLAY 2 8122 #&gt; BERNARD 4 8067 #&gt; YURKOV 5 8036 #&gt; ZSIVOCZKY 7 8004 #&gt; McMULLEN 8 7995 The coordinates of a given quantitative variable are calculated as the correlation between the quantitative variables and the principal components. # Predict coordinates and compute cos2 quanti.coord &lt;- cor(quanti.sup, res.pca$x) quanti.cos2 &lt;- quanti.coord^2 # Graph of variables including supplementary variables p &lt;- fviz_pca_var(res.pca) fviz_add(p, quanti.coord, color =&quot;blue&quot;, geom=&quot;arrow&quot;) 4.2 Theory behind PCA results 4.2.1 PCA results for variables Here we’ll show how to calculate the PCA results for variables: coordinates, cos2 and contributions: var.coord = loadings * the component standard deviations var.cos2 = var.coord^2 var.contrib. The contribution of a variable to a given principal component is (in percentage) : (var.cos2 * 100) / (total cos2 of the component) # Helper function #:::::::::::::::::::::::::::::::::::::::: var_coord_func &lt;- function(loadings, comp.sdev){ loadings*comp.sdev } # Compute Coordinates #:::::::::::::::::::::::::::::::::::::::: loadings &lt;- res.pca$rotation sdev &lt;- res.pca$sdev var.coord &lt;- t(apply(loadings, 1, var_coord_func, sdev)) head(var.coord[, 1:4]) #&gt; PC1 PC2 PC3 PC4 #&gt; X100m -0.851 0.1794 -0.302 0.0336 #&gt; Long.jump 0.794 -0.2809 0.191 -0.1154 #&gt; Shot.put 0.734 -0.0854 -0.518 0.1285 #&gt; High.jump 0.610 0.4652 -0.330 0.1446 #&gt; X400m -0.702 -0.2902 -0.284 0.4308 #&gt; X110m.hurdle -0.764 0.0247 -0.449 -0.0169 # Compute Cos2 #:::::::::::::::::::::::::::::::::::::::: var.cos2 &lt;- var.coord^2 head(var.cos2[, 1:4]) #&gt; PC1 PC2 PC3 PC4 #&gt; X100m 0.724 0.032184 0.0909 0.001127 #&gt; Long.jump 0.631 0.078881 0.0363 0.013315 #&gt; Shot.put 0.539 0.007294 0.2679 0.016504 #&gt; High.jump 0.372 0.216424 0.1090 0.020895 #&gt; X400m 0.492 0.084203 0.0804 0.185611 #&gt; X110m.hurdle 0.584 0.000612 0.2015 0.000285 # Compute contributions #:::::::::::::::::::::::::::::::::::::::: comp.cos2 &lt;- apply(var.cos2, 2, sum) contrib &lt;- function(var.cos2, comp.cos2){var.cos2*100/comp.cos2} var.contrib &lt;- t(apply(var.cos2,1, contrib, comp.cos2)) head(var.contrib[, 1:4]) #&gt; PC1 PC2 PC3 PC4 #&gt; X100m 17.54 1.7505 7.34 0.1376 #&gt; Long.jump 15.29 4.2904 2.93 1.6249 #&gt; Shot.put 13.06 0.3967 21.62 2.0141 #&gt; High.jump 9.02 11.7716 8.79 2.5499 #&gt; X400m 11.94 4.5799 6.49 22.6509 #&gt; X110m.hurdle 14.16 0.0333 16.26 0.0348 4.2.2 PCA results for individuals ind.coord = res.pca$x Cos2 of individuals. Two steps: Calculate the square distance between each individual and the PCA center of gravity: d2 = [(var1_ind_i - mean_var1)/sd_var1]^2 + …+ [(var10_ind_i - mean_var10)/sd_var10]^2 + …+.. Calculate the cos2 as ind.coord^2/d2 Contributions of individuals to the principal components: 100 * (1 / number_of_individuals)*(ind.coord^2 / comp_sdev^2). Note that the sum of all the contributions per column is 100 # Coordinates of individuals #:::::::::::::::::::::::::::::::::: ind.coord &lt;- res.pca$x head(ind.coord[, 1:4]) #&gt; PC1 PC2 PC3 PC4 #&gt; SEBRLE 0.191 -1.554 -0.628 0.0821 #&gt; CLAY 0.790 -2.420 1.357 1.2698 #&gt; BERNARD -1.329 -1.612 -0.196 -1.9209 #&gt; YURKOV -0.869 0.433 -2.474 0.6972 #&gt; ZSIVOCZKY -0.106 2.023 1.305 -0.0993 #&gt; McMULLEN 0.119 0.992 0.844 1.3122 # Cos2 of individuals #::::::::::::::::::::::::::::::::: # 1. square of the distance between an individual and the # PCA center of gravity center &lt;- res.pca$center scale&lt;- res.pca$scale getdistance &lt;- function(ind_row, center, scale){ return(sum(((ind_row-center)/scale)^2)) } d2 &lt;- apply(decathlon2.active,1, getdistance, center, scale) # 2. Compute the cos2. The sum of each row is 1 cos2 &lt;- function(ind.coord, d2){return(ind.coord^2/d2)} ind.cos2 &lt;- apply(ind.coord, 2, cos2, d2) head(ind.cos2[, 1:4]) #&gt; PC1 PC2 PC3 PC4 #&gt; SEBRLE 0.00753 0.4975 0.08133 0.00139 #&gt; CLAY 0.04870 0.4570 0.14363 0.12579 #&gt; BERNARD 0.19720 0.2900 0.00429 0.41182 #&gt; YURKOV 0.09611 0.0238 0.77823 0.06181 #&gt; ZSIVOCZKY 0.00157 0.5764 0.23975 0.00139 #&gt; McMULLEN 0.00218 0.1522 0.11014 0.26649 # Contributions of individuals #::::::::::::::::::::::::::::::: contrib &lt;- function(ind.coord, comp.sdev, n.ind){ 100*(1/n.ind)*ind.coord^2/comp.sdev^2 } ind.contrib &lt;- t(apply(ind.coord, 1, contrib, res.pca$sdev, nrow(ind.coord))) head(ind.contrib[, 1:4]) #&gt; PC1 PC2 PC3 PC4 #&gt; SEBRLE 0.0385 5.712 1.385 0.0357 #&gt; CLAY 0.6581 13.854 6.460 8.5557 #&gt; BERNARD 1.8627 6.144 0.135 19.5783 #&gt; YURKOV 0.7969 0.443 21.476 2.5794 #&gt; ZSIVOCZKY 0.0118 9.682 5.975 0.0523 #&gt; McMULLEN 0.0148 2.325 2.497 9.1353 "],
["principal-components-methods.html", "Chapter 5 Principal Components Methods 5.1 Data standardization 5.2 Eigenvalues / Variances 5.3 Graph of variables 5.4 Correlation circle 5.5 Quality of representation 5.6 Contributions of variables to PCs 5.7 Color by a custom continuous variable 5.8 Color by groups 5.9 Dimension description 5.10 Graph of individuals 5.11 Plots: quality and contribution 5.12 Color by a custom continuous variable 5.13 Color by groups 5.14 Graph customization 5.15 Size and shape of plot elements 5.16 Ellipses 5.17 Group mean points 5.18 Axis lines 5.19 Graphical parameters 5.20 Biplot 5.21 Supplementary elements 5.22 Quantitative variables 5.23 Individuals 5.24 Qualitative variables 5.25 Filtering results 5.26 Exporting results 5.27 Export results to txt/csv files 5.28 Summary", " Chapter 5 Principal Components Methods http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/112-pca-principal-component-analysis-essentials/ Principal component analysis (PCA) allows us to summarize and to visualize the information in a data set containing individuals/observations described by multiple inter-correlated quantitative variables. Each variable could be considered as a different dimension. If you have more than 3 variables in your data sets, it could be very difficult to visualize a multi-dimensional hyperspace. Principal component analysis is used to extract the important information from a multivariate data table and to express this information as a set of few new variables called principal components. These new variables correspond to a linear combination of the originals. The number of principal components is less than or equal to the number of original variables. The information in a given data set corresponds to the total variation it contains. The goal of PCA is to identify directions (or principal components) along which the variation in the data is maximal. In other words, PCA reduces the dimensionality of a multivariate data to two or three principal components, that can be visualized graphically, with minimal loss of information. # install.packages(c(&quot;FactoMineR&quot;, &quot;factoextra&quot;)) library(FactoMineR) library(factoextra) #&gt; Loading required package: ggplot2 #&gt; Registered S3 methods overwritten by &#39;ggplot2&#39;: #&gt; method from #&gt; [.quosures rlang #&gt; c.quosures rlang #&gt; print.quosures rlang #&gt; Welcome! Related Books: `Practical Guide To Cluster Analysis in R` at https://goo.gl/13EFCZ data(decathlon2) # head(decathlon2) In PCA terminology, our data contains : Active individuals (in light blue, rows 1:23) : Individuals that are used during the principal component analysis. Supplementary individuals (in dark blue, rows 24:27) : The coordinates of these individuals will be predicted using the PCA information and parameters obtained with active individuals/variables Active variables (in pink, columns 1:10) : Variables that are used for the principal component analysis. Supplementary variables: As supplementary individuals, the coordinates of these variables will be predicted also. These can be: Supplementary continuous variables (red): Columns 11 and 12 corresponding respectively to the rank and the points of athletes. Supplementary qualitative variables (green): Column 13 corresponding to the two athlete-tic meetings (2004 Olympic Game or 2004 Decastar). This is a categorical (or factor) variable factor. It can be used to color individuals by groups. We start by subsetting active individuals and active variables for the principal component analysis: decathlon2.active &lt;- decathlon2[1:23, 1:10] head(decathlon2.active[, 1:6], 4) #&gt; X100m Long.jump Shot.put High.jump X400m X110m.hurdle #&gt; SEBRLE 11.0 7.58 14.8 2.07 49.8 14.7 #&gt; CLAY 10.8 7.40 14.3 1.86 49.4 14.1 #&gt; BERNARD 11.0 7.23 14.2 1.92 48.9 15.0 #&gt; YURKOV 11.3 7.09 15.2 2.10 50.4 15.3 5.1 Data standardization In principal component analysis, variables are often scaled (i.e. standardized). This is particularly recommended when variables are measured in different scales (e.g: kilograms, kilometers, centimeters, …); otherwise, the PCA outputs obtained will be severely affected. The goal is to make the variables comparable. Generally variables are scaled to have i) standard deviation one and ii) mean zero. The function PCA() [FactoMineR package] can be used. A simplified format is: library(FactoMineR) res.pca &lt;- PCA(decathlon2.active, graph = FALSE) print(res.pca) #&gt; **Results for the Principal Component Analysis (PCA)** #&gt; The analysis was performed on 23 individuals, described by 10 variables #&gt; *The results are available in the following objects: #&gt; #&gt; name description #&gt; 1 &quot;$eig&quot; &quot;eigenvalues&quot; #&gt; 2 &quot;$var&quot; &quot;results for the variables&quot; #&gt; 3 &quot;$var$coord&quot; &quot;coord. for the variables&quot; #&gt; 4 &quot;$var$cor&quot; &quot;correlations variables - dimensions&quot; #&gt; 5 &quot;$var$cos2&quot; &quot;cos2 for the variables&quot; #&gt; 6 &quot;$var$contrib&quot; &quot;contributions of the variables&quot; #&gt; 7 &quot;$ind&quot; &quot;results for the individuals&quot; #&gt; 8 &quot;$ind$coord&quot; &quot;coord. for the individuals&quot; #&gt; 9 &quot;$ind$cos2&quot; &quot;cos2 for the individuals&quot; #&gt; 10 &quot;$ind$contrib&quot; &quot;contributions of the individuals&quot; #&gt; 11 &quot;$call&quot; &quot;summary statistics&quot; #&gt; 12 &quot;$call$centre&quot; &quot;mean of the variables&quot; #&gt; 13 &quot;$call$ecart.type&quot; &quot;standard error of the variables&quot; #&gt; 14 &quot;$call$row.w&quot; &quot;weights for the individuals&quot; #&gt; 15 &quot;$call$col.w&quot; &quot;weights for the variables&quot; The object that is created using the function PCA() contains many information found in many different lists and matrices. These values are described in the next section. 5.2 Eigenvalues / Variances As described in previous sections, the eigenvalues measure the amount of variation retained by each principal component. Eigenvalues are large for the first PCs and small for the subsequent PCs. That is, the first PCs corresponds to the directions with the maximum amount of variation in the data set. We examine the eigenvalues to determine the number of principal components to be considered. The eigenvalues and the proportion of variances (i.e., information) retained by the principal components (PCs) can be extracted using the function get_eigenvalue() [factoextra package]. library(factoextra) eig.val &lt;- get_eigenvalue(res.pca) eig.val #&gt; eigenvalue variance.percent cumulative.variance.percent #&gt; Dim.1 4.124 41.24 41.2 #&gt; Dim.2 1.839 18.39 59.6 #&gt; Dim.3 1.239 12.39 72.0 #&gt; Dim.4 0.819 8.19 80.2 #&gt; Dim.5 0.702 7.02 87.2 #&gt; Dim.6 0.423 4.23 91.5 #&gt; Dim.7 0.303 3.03 94.5 #&gt; Dim.8 0.274 2.74 97.2 #&gt; Dim.9 0.155 1.55 98.8 #&gt; Dim.10 0.122 1.22 100.0 The sum of all the eigenvalues give a total variance of 10. The proportion of variation explained by each eigenvalue is given in the second column. For example, 4.124 divided by 10 equals 0.4124, or, about 41.24% of the variation is explained by this first eigenvalue. The cumulative percentage explained is obtained by adding the successive proportions of variation explained to obtain the running total. For instance, 41.242% plus 18.385% equals 59.627%, and so forth. Therefore, about 59.627% of the variation is explained by the first two eigenvalues together. Unfortunately, there is no well-accepted objective way to decide how many principal components are enough. This will depend on the specific field of application and the specific data set. In practice, we tend to look at the first few principal components in order to find interesting patterns in the data. In our analysis, the first three principal components explain 72% of the variation. This is an acceptably large percentage. An alternative method to determine the number of principal components is to look at a Scree Plot, which is the plot of eigenvalues ordered from largest to the smallest. The number of component is determined at the point, beyond which the remaining eigenvalues are all relatively small and of comparable size (Jollife 2002, Peres-Neto, Jackson, and Somers (2005)). The scree plot can be produced using the function fviz_eig() or fviz_screeplot() [factoextra package]. fviz_eig(res.pca, addlabels = TRUE, ylim = c(0, 50)) From the plot above, we might want to stop at the fifth principal component. 87% of the information (variances) contained in the data are retained by the first five principal components. 5.3 Graph of variables Results A simple method to extract the results, for variables, from a PCA output is to use the function get_pca_var() [factoextra package]. This function provides a list of matrices containing all the results for the active variables (coordinates, correlation between variables and axes, squared cosine and contributions) var &lt;- get_pca_var(res.pca) var #&gt; Principal Component Analysis Results for variables #&gt; =================================================== #&gt; Name Description #&gt; 1 &quot;$coord&quot; &quot;Coordinates for the variables&quot; #&gt; 2 &quot;$cor&quot; &quot;Correlations between variables and dimensions&quot; #&gt; 3 &quot;$cos2&quot; &quot;Cos2 for the variables&quot; #&gt; 4 &quot;$contrib&quot; &quot;contributions of the variables&quot; The components of the get_pca_var() can be used in the plot of variables as follow: var$coord: coordinates of variables to create a scatter plot var$cos2: represents the quality of representation for variables on the factor map. It’s calculated as the squared coordinates: var.cos2 = var.coord * var.coord. var$contrib: contains the contributions (in percentage) of the variables to the principal components. The contribution of a variable (var) to a given principal component is (in percentage) : (var.cos2 * 100) / (total cos2 of the component). Note that, it’s possible to plot variables and to color them according to either i) their quality on the factor map (cos2) or ii) their contribution values to the principal components (contrib). The different components can be accessed as follow: # Coordinates head(var$coord) #&gt; Dim.1 Dim.2 Dim.3 Dim.4 Dim.5 #&gt; X100m -0.851 -0.1794 0.302 0.0336 -0.194 #&gt; Long.jump 0.794 0.2809 -0.191 -0.1154 0.233 #&gt; Shot.put 0.734 0.0854 0.518 0.1285 -0.249 #&gt; High.jump 0.610 -0.4652 0.330 0.1446 0.403 #&gt; X400m -0.702 0.2902 0.284 0.4308 0.104 #&gt; X110m.hurdle -0.764 -0.0247 0.449 -0.0169 0.224 # Cos2: quality on the factore map head(var$cos2) #&gt; Dim.1 Dim.2 Dim.3 Dim.4 Dim.5 #&gt; X100m 0.724 0.032184 0.0909 0.001127 0.0378 #&gt; Long.jump 0.631 0.078881 0.0363 0.013315 0.0544 #&gt; Shot.put 0.539 0.007294 0.2679 0.016504 0.0619 #&gt; High.jump 0.372 0.216424 0.1090 0.020895 0.1622 #&gt; X400m 0.492 0.084203 0.0804 0.185611 0.0108 #&gt; X110m.hurdle 0.584 0.000612 0.2015 0.000285 0.0503 # Contributions to the principal components head(var$contrib) #&gt; Dim.1 Dim.2 Dim.3 Dim.4 Dim.5 #&gt; X100m 17.54 1.7505 7.34 0.1376 5.39 #&gt; Long.jump 15.29 4.2904 2.93 1.6249 7.75 #&gt; Shot.put 13.06 0.3967 21.62 2.0141 8.82 #&gt; High.jump 9.02 11.7716 8.79 2.5499 23.12 #&gt; X400m 11.94 4.5799 6.49 22.6509 1.54 #&gt; X110m.hurdle 14.16 0.0333 16.26 0.0348 7.17 In this section, we describe how to visualize variables and draw conclusions about their correlations. Next, we highlight variables according to either i) their quality of representation on the factor map or ii) their contributions to the principal components. 5.4 Correlation circle The correlation between a variable and a principal component (PC) is used as the coordinates of the variable on the PC. The representation of variables differs from the plot of the observations: The observations are represented by their projections, but the variables are represented by their correlations (Abdi and Williams 2010). # Coordinates of variables head(var$coord, 4) #&gt; Dim.1 Dim.2 Dim.3 Dim.4 Dim.5 #&gt; X100m -0.851 -0.1794 0.302 0.0336 -0.194 #&gt; Long.jump 0.794 0.2809 -0.191 -0.1154 0.233 #&gt; Shot.put 0.734 0.0854 0.518 0.1285 -0.249 #&gt; High.jump 0.610 -0.4652 0.330 0.1446 0.403 To plot variables, type this: fviz_pca_var(res.pca, col.var = &quot;black&quot;) The plot above is also known as variable correlation plots. It shows the relationships between all variables. It can be interpreted as follow: Positively correlated variables are grouped together. Negatively correlated variables are positioned on opposite sides of the plot origin (opposed quadrants). The distance between variables and the origin measures the quality of the variables on the factor map. Variables that are away from the origin are well represented on the factor map. 5.5 Quality of representation The quality of representation of the variables on factor map is called cos2 (square cosine, squared coordinates) . You can access to the cos2 as follow: head(var$cos2, 4) #&gt; Dim.1 Dim.2 Dim.3 Dim.4 Dim.5 #&gt; X100m 0.724 0.03218 0.0909 0.00113 0.0378 #&gt; Long.jump 0.631 0.07888 0.0363 0.01331 0.0544 #&gt; Shot.put 0.539 0.00729 0.2679 0.01650 0.0619 #&gt; High.jump 0.372 0.21642 0.1090 0.02089 0.1622 You can visualize the cos2 of variables on all the dimensions using the corrplot package: library(corrplot) #&gt; corrplot 0.84 loaded corrplot(var$cos2, is.corr=FALSE) It’s also possible to create a bar plot of variables cos2 using the function fviz_cos2() [in factoextra]: # Total cos2 of variables on Dim.1 and Dim.2 fviz_cos2(res.pca, choice = &quot;var&quot;, axes = 1:2) Note that, A high cos2 indicates a good representation of the variable on the principal component. In this case the variable is positioned close to the circumference of the correlation circle. A low cos2 indicates that the variable is not perfectly represented by the PCs. In this case the variable is close to the center of the circle. For a given variable, the sum of the cos2 on all the principal components is equal to one. If a variable is perfectly represented by only two principal components (Dim.1 &amp; Dim.2), the sum of the cos2 on these two PCs is equal to one. In this case the variables will be positioned on the circle of correlations. For some of the variables, more than 2 components might be required to perfectly represent the data. In this case the variables are positioned inside the circle of correlations. In summary: The cos2 values are used to estimate the quality of the representation The closer a variable is to the circle of correlations, the better its representation on the factor map (and the more important it is to interpret these components) Variables that are closed to the center of the plot are less important for the first components. It’s possible to color variables by their cos2 values using the argument col.var = “cos2”. This produces a gradient colors. In this case, the argument gradient.cols can be used to provide a custom color. For instance, gradient.cols = c(“white”, “blue”, “red”) means that: variables with low cos2 values will be colored in “white” variables with mid cos2 values will be colored in “blue” variables with high cos2 values will be colored in red # Color by cos2 values: quality on the factor map fviz_pca_var(res.pca, col.var = &quot;cos2&quot;, gradient.cols = c(&quot;#00AFBB&quot;, &quot;#E7B800&quot;, &quot;#FC4E07&quot;), repel = TRUE # Avoid text overlapping ) Note that, it’s also possible to change the transparency of the variables according to their cos2 values using the option alpha.var = “cos2”. For example, type this: # Change the transparency by cos2 values fviz_pca_var(res.pca, alpha.var = &quot;cos2&quot;) 5.6 Contributions of variables to PCs The contributions of variables in accounting for the variability in a given principal component are expressed in percentage. Variables that are correlated with PC1 (i.e., Dim.1) and PC2 (i.e., Dim.2) are the most important in explaining the variability in the data set. Variables that do not correlated with any PC or correlated with the last dimensions are variables with low contribution and might be removed to simplify the overall analysis. The contribution of variables can be extracted as follow : head(var$contrib, 4) #&gt; Dim.1 Dim.2 Dim.3 Dim.4 Dim.5 #&gt; X100m 17.54 1.751 7.34 0.138 5.39 #&gt; Long.jump 15.29 4.290 2.93 1.625 7.75 #&gt; Shot.put 13.06 0.397 21.62 2.014 8.82 #&gt; High.jump 9.02 11.772 8.79 2.550 23.12 The larger the value of the contribution, the more the variable contributes to the component. It’s possible to use the function corrplot() [corrplot package] to highlight the most contributing variables for each dimension: library(&quot;corrplot&quot;) corrplot(var$contrib, is.corr=FALSE) The function fviz_contrib() [factoextra package] can be used to draw a bar plot of variable contributions. If your data contains many variables, you can decide to show only the top contributing variables. The R code below shows the top 10 variables contributing to the principal components: # Contributions of variables to PC1 fviz_contrib(res.pca, choice = &quot;var&quot;, axes = 1, top = 10) # Contributions of variables to PC2 fviz_contrib(res.pca, choice = &quot;var&quot;, axes = 2, top = 10) The total contribution to PC1 and PC2 is obtained with the following R code: fviz_contrib(res.pca, choice = &quot;var&quot;, axes = 1:2, top = 10) The red dashed line on the graph above indicates the expected average contribution. If the contribution of the variables were uniform, the expected value would be 1/length(variables) = 1/10 = 10%. For a given component, a variable with a contribution larger than this cutoff could be considered as important in contributing to the component. Note that, the total contribution of a given variable, on explaining the variations retained by two principal components, say PC1 and PC2, is calculated as contrib = [(C1 * Eig1) + (C2 * Eig2)]/(Eig1 + Eig2), where C1 and C2 are the contributions of the variable on PC1 and PC2, respectively Eig1 and Eig2 are the eigenvalues of PC1 and PC2, respectively. Recall that eigenvalues measure the amount of variation retained by each PC. In this case, the expected average contribution (cutoff) is calculated as follow: As mentioned above, if the contributions of the 10 variables were uniform, the expected average contribution on a given PC would be 1/10 = 10%. The expected average contribution of a variable for PC1 and PC2 is : [(10* Eig1) + (10 * Eig2)]/(Eig1 + Eig2) It can be seen that the variables - X100m, Long.jump and Pole.vault - contribute the most to the dimensions 1 and 2. The most important (or, contributing) variables can be highlighted on the correlation plot as follow: fviz_pca_var(res.pca, col.var = &quot;contrib&quot;, gradient.cols = c(&quot;#00AFBB&quot;, &quot;#E7B800&quot;, &quot;#FC4E07&quot;) ) Note that, it’s also possible to change the transparency of variables according to their contrib values using the option alpha.var = “contrib”. For example, type this: # Change the transparency by contrib values fviz_pca_var(res.pca, alpha.var = &quot;contrib&quot;) 5.7 Color by a custom continuous variable In the previous sections, we showed how to color variables by their contributions and their cos2. Note that, it’s possible to color variables by any custom continuous variable. The coloring variable should have the same length as the number of active variables in the PCA (here n = 10). For example, type this: # Create a random continuous variable of length 10 set.seed(123) my.cont.var &lt;- rnorm(10) # Color variables by the continuous variable fviz_pca_var(res.pca, col.var = my.cont.var, gradient.cols = c(&quot;blue&quot;, &quot;yellow&quot;, &quot;red&quot;), legend.title = &quot;Cont.Var&quot;) 5.8 Color by groups It’s also possible to change the color of variables by groups defined by a qualitative/categorical variable, also called factor in R terminology. As we don’t have any grouping variable in our data sets for classifying variables, we’ll create it. In the following demo example, we start by classifying the variables into 3 groups using the kmeans clustering algorithm. Next, we use the clusters returned by the kmeans algorithm to color variables. # Create a grouping variable using kmeans # Create 3 groups of variables (centers = 3) set.seed(123) res.km &lt;- kmeans(var$coord, centers = 3, nstart = 25) grp &lt;- as.factor(res.km$cluster) # Color variables by groups fviz_pca_var(res.pca, col.var = grp, palette = c(&quot;#0073C2FF&quot;, &quot;#EFC000FF&quot;, &quot;#868686FF&quot;), legend.title = &quot;Cluster&quot;) 5.9 Dimension description In the section (???)(pca-variable-contributions), we described how to highlight variables according to their contributions to the principal components. Note also that, the function dimdesc() [in FactoMineR], for dimension description, can be used to identify the most significantly associated variables with a given principal component . It can be used as follow: res.desc &lt;- dimdesc(res.pca, axes = c(1,2), proba = 0.05) # Description of dimension 1 res.desc$Dim.1 #&gt; $quanti #&gt; correlation p.value #&gt; Long.jump 0.794 6.06e-06 #&gt; Discus 0.743 4.84e-05 #&gt; Shot.put 0.734 6.72e-05 #&gt; High.jump 0.610 1.99e-03 #&gt; Javeline 0.428 4.15e-02 #&gt; X400m -0.702 1.91e-04 #&gt; X110m.hurdle -0.764 2.20e-05 #&gt; X100m -0.851 2.73e-07 # Description of dimension 2 res.desc$Dim.2 #&gt; $quanti #&gt; correlation p.value #&gt; Pole.vault 0.807 3.21e-06 #&gt; X1500m 0.784 9.38e-06 #&gt; High.jump -0.465 2.53e-02 5.10 Graph of individuals Results The results, for individuals can be extracted using the function get_pca_ind() [factoextra package]. Similarly to the get_pca_var(), the function get_pca_ind() provides a list of matrices containing all the results for the individuals (coordinates, correlation between individuals and axes, squared cosine and contributions) ind &lt;- get_pca_ind(res.pca) ind #&gt; Principal Component Analysis Results for individuals #&gt; =================================================== #&gt; Name Description #&gt; 1 &quot;$coord&quot; &quot;Coordinates for the individuals&quot; #&gt; 2 &quot;$cos2&quot; &quot;Cos2 for the individuals&quot; #&gt; 3 &quot;$contrib&quot; &quot;contributions of the individuals&quot; To get access to the different components, use this: # Coordinates of individuals head(ind$coord) #&gt; Dim.1 Dim.2 Dim.3 Dim.4 Dim.5 #&gt; SEBRLE 0.196 1.589 0.642 0.0839 1.1683 #&gt; CLAY 0.808 2.475 -1.387 1.2984 -0.8250 #&gt; BERNARD -1.359 1.648 0.201 -1.9641 0.0842 #&gt; YURKOV -0.889 -0.443 2.530 0.7129 0.4078 #&gt; ZSIVOCZKY -0.108 -2.069 -1.334 -0.1015 -0.2015 #&gt; McMULLEN 0.121 -1.014 -0.863 1.3416 1.6215 # Quality of individuals head(ind$cos2) #&gt; Dim.1 Dim.2 Dim.3 Dim.4 Dim.5 #&gt; SEBRLE 0.00753 0.4975 0.08133 0.00139 0.268903 #&gt; CLAY 0.04870 0.4570 0.14363 0.12579 0.050785 #&gt; BERNARD 0.19720 0.2900 0.00429 0.41182 0.000757 #&gt; YURKOV 0.09611 0.0238 0.77823 0.06181 0.020228 #&gt; ZSIVOCZKY 0.00157 0.5764 0.23975 0.00139 0.005465 #&gt; McMULLEN 0.00218 0.1522 0.11014 0.26649 0.389262 # Contributions of individuals head(ind$contrib) #&gt; Dim.1 Dim.2 Dim.3 Dim.4 Dim.5 #&gt; SEBRLE 0.0403 5.971 1.448 0.0373 8.4589 #&gt; CLAY 0.6881 14.484 6.754 8.9446 4.2179 #&gt; BERNARD 1.9474 6.423 0.141 20.4682 0.0439 #&gt; YURKOV 0.8331 0.463 22.452 2.6966 1.0308 #&gt; ZSIVOCZKY 0.0123 10.122 6.246 0.0547 0.2515 #&gt; McMULLEN 0.0155 2.431 2.610 9.5506 16.2949 5.11 Plots: quality and contribution The fviz_pca_ind() is used to produce the graph of individuals. To create a simple plot, type this: fviz_pca_ind(res.pca) Like variables, it’s also possible to color individuals by their cos2 values: fviz_pca_ind(res.pca, col.ind = &quot;cos2&quot;, gradient.cols = c(&quot;#00AFBB&quot;, &quot;#E7B800&quot;, &quot;#FC4E07&quot;), repel = TRUE # Avoid text overlapping (slow if many points) ) Note that, individuals that are similar are grouped together on the plot. You can also change the point size according the cos2 of the corresponding individuals: fviz_pca_ind(res.pca, pointsize = &quot;cos2&quot;, pointshape = 21, fill = &quot;#E7B800&quot;, repel = TRUE # Avoid text overlapping (slow if many points) ) To change both point size and color by cos2, try this: fviz_pca_ind(res.pca, col.ind = &quot;cos2&quot;, pointsize = &quot;cos2&quot;, gradient.cols = c(&quot;#00AFBB&quot;, &quot;#E7B800&quot;, &quot;#FC4E07&quot;), repel = TRUE # Avoid text overlapping (slow if many points) ) To create a bar plot of the quality of representation (cos2) of individuals on the factor map, you can use the function fviz_cos2() as previously described for variables: fviz_cos2(res.pca, choice = &quot;ind&quot;) To visualize the contribution of individuals to the first two principal components, type this: # Total contribution on PC1 and PC2 fviz_contrib(res.pca, choice = &quot;ind&quot;, axes = 1:2) 5.12 Color by a custom continuous variable As for variables, individuals can be colored by any custom continuous variable by specifying the argument col.ind. For example, type this: # Create a random continuous variable of length 23, # Same length as the number of active individuals in the PCA set.seed(123) my.cont.var &lt;- rnorm(23) # Color individuals by the continuous variable fviz_pca_ind(res.pca, col.ind = my.cont.var, gradient.cols = c(&quot;blue&quot;, &quot;yellow&quot;, &quot;red&quot;), legend.title = &quot;Cont.Var&quot;) 5.13 Color by groups Here, we describe how to color individuals by group. Additionally, we show how to add concentration ellipses and confidence ellipses by groups. For this, we’ll use the iris data as demo data sets. Iris data sets look like this: head(iris, 3) #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width Species #&gt; 1 5.1 3.5 1.4 0.2 setosa #&gt; 2 4.9 3.0 1.4 0.2 setosa #&gt; 3 4.7 3.2 1.3 0.2 setosa The column “Species” will be used as grouping variable. We start by computing principal component analysis as follow: # The variable Species (index = 5) is removed # before PCA analysis iris.pca &lt;- PCA(iris[,-5], graph = FALSE) In the R code below: the argument habillage or col.ind can be used to specify the factor variable for coloring the individuals by groups. To add a concentration ellipse around each group, specify the argument addEllipses = TRUE. The argument palette can be used to change group colors. fviz_pca_ind(iris.pca, geom.ind = &quot;point&quot;, # show points only (nbut not &quot;text&quot;) col.ind = iris$Species, # color by groups palette = c(&quot;#00AFBB&quot;, &quot;#E7B800&quot;, &quot;#FC4E07&quot;), addEllipses = TRUE, # Concentration ellipses legend.title = &quot;Groups&quot; ) To remove the group mean point, specify the argument mean.point = FALSE. If you want confidence ellipses instead of concentration ellipses, use ellipse.type = “confidence”. # Add confidence ellipses fviz_pca_ind(iris.pca, geom.ind = &quot;point&quot;, col.ind = iris$Species, palette = c(&quot;#00AFBB&quot;, &quot;#E7B800&quot;, &quot;#FC4E07&quot;), addEllipses = TRUE, ellipse.type = &quot;confidence&quot;, legend.title = &quot;Groups&quot; ) Note that, allowed values for palette include: “grey” for grey color palettes; brewer palettes e.g. “RdBu”, “Blues”, …; To view all, type this in R: RColorBrewer::display.brewer.all(). custom color palette e.g. c(“blue”, “red”); and scientific journal palettes from ggsci R package, e.g.: “npg”, “aaas”, * “lancet”, “jco”, “ucscgb”, “uchicago”, “simpsons” and “rickandmorty”. For example, to use the jco (journal of clinical oncology) color palette, type this: fviz_pca_ind(iris.pca, label = &quot;none&quot;, # hide individual labels habillage = iris$Species, # color by groups addEllipses = TRUE, # Concentration ellipses palette = &quot;jco&quot; ) 5.14 Graph customization Note that, fviz_pca_ind() and fviz_pca_var() and related functions are wrapper around the core function fviz() [in factoextra]. fviz() is a wrapper around the function ggscatter() [in ggpubr]. Therefore, further arguments, to be passed to the function fviz() and ggscatter(), can be specified in fviz_pca_ind() and fviz_pca_var(). Here, we present some of these additional arguments to customize the PCA graph of variables and individuals. 5.14.1 Dimensions By default, variables/individuals are represented on dimensions 1 and 2. If you want to visualize them on dimensions 2 and 3, for example, you should specify the argument axes = c(2, 3). # Variables on dimensions 2 and 3 fviz_pca_var(res.pca, axes = c(2, 3)) # Individuals on dimensions 2 and 3 fviz_pca_ind(res.pca, axes = c(2, 3)) Plot elements: point, text, arrow The argument geom (for geometry) and derivatives are used to specify the geometry elements or graphical elements to be used for plotting. geom.var: a text specifying the geometry to be used for plotting variables. Allowed values are the combination of c(“point”, “arrow”, “text”). Use geom.var = “point”, to show only points; Use geom.var = “text” to show only text labels; Use geom.var = c(“point”, “text”) to show both points and text labels Use geom.var = c(“arrow”, “text”) to show arrows and labels (default). For example, type this: # Show variable points and text labels fviz_pca_var(res.pca, geom.var = c(&quot;point&quot;, &quot;text&quot;)) # Show individuals text labels only fviz_pca_ind(res.pca, geom.ind = &quot;text&quot;) 5.15 Size and shape of plot elements # Change the size of arrows an labels fviz_pca_var(res.pca, arrowsize = 1, labelsize = 5, repel = TRUE) # Change points size, shape and fill color # Change labelsize fviz_pca_ind(res.pca, pointsize = 3, pointshape = 21, fill = &quot;lightblue&quot;, labelsize = 5, repel = TRUE) 5.16 Ellipses # Add confidence ellipses fviz_pca_ind(iris.pca, geom.ind = &quot;point&quot;, col.ind = iris$Species, # color by groups palette = c(&quot;#00AFBB&quot;, &quot;#E7B800&quot;, &quot;#FC4E07&quot;), addEllipses = TRUE, ellipse.type = &quot;confidence&quot;, legend.title = &quot;Groups&quot; ) # Convex hull fviz_pca_ind(iris.pca, geom.ind = &quot;point&quot;, col.ind = iris$Species, # color by groups palette = c(&quot;#00AFBB&quot;, &quot;#E7B800&quot;, &quot;#FC4E07&quot;), addEllipses = TRUE, ellipse.type = &quot;convex&quot;, legend.title = &quot;Groups&quot; ) 5.17 Group mean points When coloring individuals by groups (section (???)(color-ind-by-groups)), the mean points of groups (barycenters) are also displayed by default. To remove the mean points, use the argument mean.point = FALSE. fviz_pca_ind(iris.pca, geom.ind = &quot;point&quot;, # show points only (but not &quot;text&quot;) group.ind = iris$Species, # color by groups legend.title = &quot;Groups&quot;, mean.point = FALSE) 5.18 Axis lines fviz_pca_var(res.pca, axes.linetype = &quot;blank&quot;) 5.19 Graphical parameters To change easily the graphical of any ggplots, you can use the function ggpar() [ggpubr package] ind.p &lt;- fviz_pca_ind(iris.pca, geom = &quot;point&quot;, col.ind = iris$Species) ggpubr::ggpar(ind.p, title = &quot;Principal Component Analysis&quot;, subtitle = &quot;Iris data set&quot;, caption = &quot;Source: factoextra&quot;, xlab = &quot;PC1&quot;, ylab = &quot;PC2&quot;, legend.title = &quot;Species&quot;, legend.position = &quot;top&quot;, ggtheme = theme_gray(), palette = &quot;jco&quot; ) 5.20 Biplot To make a simple biplot of individuals and variables, type this: fviz_pca_biplot(res.pca, repel = TRUE, col.var = &quot;#2E9FDF&quot;, # Variables color col.ind = &quot;#696969&quot; # Individuals color ) Note that, the biplot might be only useful when there is a low number of variables and individuals in the data set; otherwise the final plot would be unreadable. Note also that, the coordinate of individuals and variables are not constructed on the same space. Therefore, in the biplot, you should mainly focus on the direction of variables but not on their absolute positions on the plot. Roughly speaking a biplot can be interpreted as follow: * an individual that is on the same side of a given variable has a high value for this variable; * an individual that is on the opposite side of a given variable has a low value for this variable. fviz_pca_biplot(iris.pca, col.ind = iris$Species, palette = &quot;jco&quot;, addEllipses = TRUE, label = &quot;var&quot;, col.var = &quot;black&quot;, repel = TRUE, legend.title = &quot;Species&quot;) In the following example, we want to color both individuals and variables by groups. The trick is to use pointshape = 21 for individual points. This particular point shape can be filled by a color using the argument fill.ind. The border line color of individual points is set to “black” using col.ind. To color variable by groups, the argument col.var will be used. To customize individuals and variable colors, we use the helper functions fill_palette() and color_palette() [in ggpubr package]. fviz_pca_biplot(iris.pca, # Fill individuals by groups geom.ind = &quot;point&quot;, pointshape = 21, pointsize = 2.5, fill.ind = iris$Species, col.ind = &quot;black&quot;, # Color variable by groups col.var = factor(c(&quot;sepal&quot;, &quot;sepal&quot;, &quot;petal&quot;, &quot;petal&quot;)), legend.title = list(fill = &quot;Species&quot;, color = &quot;Clusters&quot;), repel = TRUE # Avoid label overplotting )+ ggpubr::fill_palette(&quot;jco&quot;)+ # Indiviual fill color ggpubr::color_palette(&quot;npg&quot;) # Variable colors Another complex example is to color individuals by groups (discrete color) and variables by their contributions to the principal components (gradient colors). Additionally, we’ll change the transparency of variables by their contributions using the argument alpha.var. fviz_pca_biplot(iris.pca, # Individuals geom.ind = &quot;point&quot;, fill.ind = iris$Species, col.ind = &quot;black&quot;, pointshape = 21, pointsize = 2, palette = &quot;jco&quot;, addEllipses = TRUE, # Variables alpha.var =&quot;contrib&quot;, col.var = &quot;contrib&quot;, gradient.cols = &quot;RdYlBu&quot;, legend.title = list(fill = &quot;Species&quot;, color = &quot;Contrib&quot;, alpha = &quot;Contrib&quot;) ) 5.21 Supplementary elements Definition and types As described above (section (???)(pca-data-format)), the decathlon2 data sets contain supplementary continuous variables (quanti.sup, columns 11:12), supplementary qualitative variables (quali.sup, column 13) and supplementary individuals (ind.sup, rows 24:27). Supplementary variables and individuals are not used for the determination of the principal components. Their coordinates are predicted using only the information provided by the performed principal component analysis on active variables/individuals. Specification in PCA To specify supplementary individuals and variables, the function PCA() can be used as follow: res.pca &lt;- PCA(decathlon2, ind.sup = 24:27, quanti.sup = 11:12, quali.sup = 13, graph=FALSE) 5.22 Quantitative variables Predicted results (coordinates, correlation and cos2) for the supplementary quantitative variables: res.pca$quanti.sup #&gt; $coord #&gt; Dim.1 Dim.2 Dim.3 Dim.4 Dim.5 #&gt; Rank -0.701 -0.2452 -0.183 0.0558 -0.0738 #&gt; Points 0.964 0.0777 0.158 -0.1662 -0.0311 #&gt; #&gt; $cor #&gt; Dim.1 Dim.2 Dim.3 Dim.4 Dim.5 #&gt; Rank -0.701 -0.2452 -0.183 0.0558 -0.0738 #&gt; Points 0.964 0.0777 0.158 -0.1662 -0.0311 #&gt; #&gt; $cos2 #&gt; Dim.1 Dim.2 Dim.3 Dim.4 Dim.5 #&gt; Rank 0.492 0.06012 0.0336 0.00311 0.00545 #&gt; Points 0.929 0.00603 0.0250 0.02763 0.00097 Visualize all variables (active and supplementary ones): fviz_pca_var(res.pca) Note that, by default, supplementary quantitative variables are shown in blue color and dashed lines. Further arguments to customize the plot: # Change color of variables fviz_pca_var(res.pca, col.var = &quot;black&quot;, # Active variables col.quanti.sup = &quot;red&quot; # Suppl. quantitative variables ) # Hide active variables on the plot, # show only supplementary variables fviz_pca_var(res.pca, invisible = &quot;var&quot;) # Hide supplementary variables fviz_pca_var(res.pca, invisible = &quot;quanti.sup&quot;) Using the fviz_pca_var(), the quantitative supplementary variables are displayed automatically on the correlation circle plot. Note that, you can add the quanti.sup variables manually, using the fviz_add() function, for further customization. An example is shown below. # Plot of active variables p &lt;- fviz_pca_var(res.pca, invisible = &quot;quanti.sup&quot;) # Add supplementary active variables fviz_add(p, res.pca$quanti.sup$coord, geom = c(&quot;arrow&quot;, &quot;text&quot;), color = &quot;red&quot;) 5.23 Individuals Predicted results for the supplementary individuals (ind.sup): res.pca$ind.sup #&gt; $coord #&gt; Dim.1 Dim.2 Dim.3 Dim.4 Dim.5 #&gt; KARPOV 0.795 0.7795 -1.633 1.724 -0.7507 #&gt; WARNERS -0.386 -0.1216 -1.739 -0.706 -0.0323 #&gt; Nool -0.559 1.9775 -0.483 -2.278 -0.2546 #&gt; Drews -1.109 0.0174 -3.049 -1.534 -0.3264 #&gt; #&gt; $cos2 #&gt; Dim.1 Dim.2 Dim.3 Dim.4 Dim.5 #&gt; KARPOV 0.0510 4.91e-02 0.2155 0.2403 0.045549 #&gt; WARNERS 0.0242 2.40e-03 0.4904 0.0809 0.000169 #&gt; Nool 0.0290 3.62e-01 0.0216 0.4811 0.006008 #&gt; Drews 0.0921 2.27e-05 0.6956 0.1762 0.007974 #&gt; #&gt; $dist #&gt; KARPOV WARNERS Nool Drews #&gt; 3.52 2.48 3.28 3.66 Visualize all individuals (active and supplementary ones). On the graph, you can add also the supplementary qualitative variables (quali.sup), which coordinates is accessible using res.pca\\(quali.supp\\)coord. p &lt;- fviz_pca_ind(res.pca, col.ind.sup = &quot;blue&quot;, repel = TRUE) p &lt;- fviz_add(p, res.pca$quali.sup$coord, color = &quot;red&quot;) p Supplementary individuals are shown in blue. The levels of the supplementary qualitative variable are shown in red color. 5.24 Qualitative variables In the previous section, we showed that you can add the supplementary qualitative variables on individuals plot using fviz_add(). Note that, the supplementary qualitative variables can be also used for coloring individuals by groups. This can help to interpret the data. The data sets decathlon2 contain a supplementary qualitative variable at columns 13 corresponding to the type of competitions. The results concerning the supplementary qualitative variable are: res.pca$quali #&gt; $coord #&gt; Dim.1 Dim.2 Dim.3 Dim.4 Dim.5 #&gt; Decastar -1.34 0.122 -0.0379 0.181 0.134 #&gt; OlympicG 1.23 -0.112 0.0347 -0.166 -0.123 #&gt; #&gt; $cos2 #&gt; Dim.1 Dim.2 Dim.3 Dim.4 Dim.5 #&gt; Decastar 0.905 0.00744 0.00072 0.0164 0.00905 #&gt; OlympicG 0.905 0.00744 0.00072 0.0164 0.00905 #&gt; #&gt; $v.test #&gt; Dim.1 Dim.2 Dim.3 Dim.4 Dim.5 #&gt; Decastar -2.97 0.403 -0.153 0.897 0.72 #&gt; OlympicG 2.97 -0.403 0.153 -0.897 -0.72 #&gt; #&gt; $dist #&gt; Decastar OlympicG #&gt; 1.41 1.29 #&gt; #&gt; $eta2 #&gt; Dim.1 Dim.2 Dim.3 Dim.4 Dim.5 #&gt; Competition 0.401 0.0074 0.00106 0.0366 0.0236 To color individuals by a supplementary qualitative variable, the argument habillage is used to specify the index of the supplementary qualitative variable. Historically, this argument name comes from the FactoMineR package. It’s a french word meaning “dressing” in english. To keep consistency between FactoMineR and factoextra, we decided to keep the same argument name fviz_pca_ind(res.pca, habillage = 13, addEllipses =TRUE, ellipse.type = &quot;confidence&quot;, palette = &quot;jco&quot;, repel = TRUE) Recall that, to remove the mean points of groups, specify the argument mean.point = FALSE. 5.25 Filtering results If you have many individuals/variable, it’s possible to visualize only some of them using the arguments select.ind and select.var. # Visualize variable with cos2 &gt;= 0.6 fviz_pca_var(res.pca, select.var = list(cos2 = 0.6)) # Top 5 active variables with the highest cos2 fviz_pca_var(res.pca, select.var= list(cos2 = 5)) # Select by names name &lt;- list(name = c(&quot;Long.jump&quot;, &quot;High.jump&quot;, &quot;X100m&quot;)) fviz_pca_var(res.pca, select.var = name) # top 5 contributing individuals and variable fviz_pca_biplot(res.pca, select.ind = list(contrib = 5), select.var = list(contrib = 5), ggtheme = theme_minimal()) When the selection is done according to the contribution values, supplementary individuals/variables are not shown because they don’t contribute to the construction of the axes. 5.26 Exporting results Export plots to PDF/PNG files The factoextra package produces a ggplot2-based graphs. To save any ggplots, the standard R code is as follow: # Print the plot to a pdf file pdf(&quot;myplot.pdf&quot;) print(myplot) dev.off() In the following examples, we’ll show you how to save the different graphs into pdf or png files. The first step is to create the plots you want as an R object: # Scree plot scree.plot &lt;- fviz_eig(res.pca) # Plot of individuals ind.plot &lt;- fviz_pca_ind(res.pca) # Plot of variables var.plot &lt;- fviz_pca_var(res.pca) pdf(file.path(data_out_dir, &quot;PCA.pdf&quot;)) # Create a new pdf device print(scree.plot) print(ind.plot) print(var.plot) dev.off() # Close the pdf device #&gt; png #&gt; 2 Note that, using the above R code will create the PDF file into your current working directory. To see the path of your current working directory, type getwd() in the R console. To print each plot to specific png file, the R code looks like this: # Print scree plot to a png file png(file.path(data_out_dir, &quot;pca-scree-plot.png&quot;)) print(scree.plot) dev.off() #&gt; png #&gt; 2 # Print individuals plot to a png file png(file.path(data_out_dir, &quot;pca-variables.png&quot;)) print(var.plot) dev.off() #&gt; png #&gt; 2 # Print variables plot to a png file png(file.path(data_out_dir, &quot;pca-individuals.png&quot;)) print(ind.plot) dev.off() #&gt; png #&gt; 2 Another alternative, to export ggplots, is to use the function ggexport() [in ggpubr package]. We like ggexport(), because it’s very simple. With one line R code, it allows us to export individual plots to a file (pdf, eps or png) (one plot per page). It can also arrange the plots (2 plot per page, for example) before exporting them. The examples below demonstrates how to export ggplots using ggexport(). Export individual plots to a pdf file (one plot per page): library(ggpubr) #&gt; Loading required package: magrittr ggexport(plotlist = list(scree.plot, ind.plot, var.plot), filename = file.path(data_out_dir, &quot;PCA.pdf&quot;)) #&gt; file saved to /home/datascience/repos/machine-learning-rsuite/export/PCA.pdf Arrange and export. Specify nrow and ncol to display multiple plots on the same page: ggexport(plotlist = list(scree.plot, ind.plot, var.plot), nrow = 2, ncol = 2, filename = file.path(data_out_dir, &quot;PCA.pdf&quot;)) #&gt; file saved to /home/datascience/repos/machine-learning-rsuite/export/PCA.pdf Export plots to png files. If you specify a list of plots, then multiple png files will be automatically created to hold each plot. ggexport(plotlist = list(scree.plot, ind.plot, var.plot), filename = file.path(data_out_dir, &quot;PCA.png&quot;)) #&gt; [1] &quot;/home/datascience/repos/machine-learning-rsuite/export/PCA%03d.png&quot; #&gt; file saved to /home/datascience/repos/machine-learning-rsuite/export/PCA%03d.png 5.27 Export results to txt/csv files All the outputs of the PCA (individuals/variables coordinates, contributions, etc) can be exported at once, into a TXT/CSV file, using the function write.infile() [in FactoMineR] package: # Export into a TXT file write.infile(res.pca, file.path(data_out_dir, &quot;pca.txt&quot;), sep = &quot;\\t&quot;) # Export into a CSV file write.infile(res.pca, file.path(data_out_dir, &quot;pca.csv&quot;), sep = &quot;;&quot;) 5.28 Summary In conclusion, we described how to perform and interpret principal component analysis (PCA). We computed PCA using the PCA() function [FactoMineR]. Next, we used the factoextra R package to produce ggplot2-based visualization of the PCA results. There are other functions [packages] to compute PCA in R: Using prcomp() [stats] res.pca &lt;- prcomp(iris[, -5], scale. = TRUE) res.pca &lt;- princomp(iris[, -5], cor = TRUE) Using dudi.pca() [ade4] library(ade4) #&gt; #&gt; Attaching package: &#39;ade4&#39; #&gt; The following object is masked from &#39;package:FactoMineR&#39;: #&gt; #&gt; reconst res.pca &lt;- dudi.pca(iris[, -5], scannf = FALSE, nf = 5) Using epPCA() [ExPosition] library(ExPosition) #&gt; Loading required package: prettyGraphs res.pca &lt;- epPCA(iris[, -5], graph = FALSE) No matter what functions you decide to use, in the list above, the factoextra package can handle the output for creating beautiful plots similar to what we described in the previous sections for FactoMineR: fviz_eig(res.pca) # Scree plot fviz_pca_ind(res.pca) # Graph of individuals fviz_pca_var(res.pca) # Graph of variables "],
["biplot-of-the-iris-data-set.html", "Chapter 6 Biplot of the Iris data set", " Chapter 6 Biplot of the Iris data set # devtools::install_github(&quot;vqv/ggbiplot&quot;) library(ggbiplot) #&gt; Loading required package: ggplot2 #&gt; Registered S3 methods overwritten by &#39;ggplot2&#39;: #&gt; method from #&gt; [.quosures rlang #&gt; c.quosures rlang #&gt; print.quosures rlang #&gt; Loading required package: plyr #&gt; Loading required package: scales #&gt; Loading required package: grid iris.pca &lt;- prcomp(iris[, 1:4], center = TRUE, scale = TRUE) print(iris.pca) #&gt; Standard deviations (1, .., p=4): #&gt; [1] 1.708 0.956 0.383 0.144 #&gt; #&gt; Rotation (n x k) = (4 x 4): #&gt; PC1 PC2 PC3 PC4 #&gt; Sepal.Length 0.521 -0.3774 0.720 0.261 #&gt; Sepal.Width -0.269 -0.9233 -0.244 -0.124 #&gt; Petal.Length 0.580 -0.0245 -0.142 -0.801 #&gt; Petal.Width 0.565 -0.0669 -0.634 0.524 summary(iris.pca) #&gt; Importance of components: #&gt; PC1 PC2 PC3 PC4 #&gt; Standard deviation 1.71 0.956 0.3831 0.14393 #&gt; Proportion of Variance 0.73 0.229 0.0367 0.00518 #&gt; Cumulative Proportion 0.73 0.958 0.9948 1.00000 g &lt;- ggbiplot(iris.pca, obs.scale = 1, var.scale = 1, groups = iris$Species, ellipse = TRUE, circle = TRUE) + scale_color_discrete(name = &quot;&quot;) + theme(legend.direction = &quot;horizontal&quot;, legend.position = &quot;top&quot;) print(g) The PC1 axis explains 0.730 of the variance, while the PC2 axis explains 0.229 of the variance. "],
["iris-underlying-principal-components.html", "Chapter 7 Iris: underlying principal components", " Chapter 7 Iris: underlying principal components # Run PCA here with prcomp () iris.pca &lt;- prcomp(iris[, 1:4], center = TRUE, scale = TRUE) print(iris.pca) #&gt; Standard deviations (1, .., p=4): #&gt; [1] 1.708 0.956 0.383 0.144 #&gt; #&gt; Rotation (n x k) = (4 x 4): #&gt; PC1 PC2 PC3 PC4 #&gt; Sepal.Length 0.521 -0.3774 0.720 0.261 #&gt; Sepal.Width -0.269 -0.9233 -0.244 -0.124 #&gt; Petal.Length 0.580 -0.0245 -0.142 -0.801 #&gt; Petal.Width 0.565 -0.0669 -0.634 0.524 # Now, compute the new dataset aligned to the PCs by # using the predict() function . df.new &lt;- predict(iris.pca, iris[, 1:4]) head(df.new) #&gt; PC1 PC2 PC3 PC4 #&gt; [1,] -2.26 -0.478 0.1273 0.02409 #&gt; [2,] -2.07 0.672 0.2338 0.10266 #&gt; [3,] -2.36 0.341 -0.0441 0.02828 #&gt; [4,] -2.29 0.595 -0.0910 -0.06574 #&gt; [5,] -2.38 -0.645 -0.0157 -0.03580 #&gt; [6,] -2.07 -1.484 -0.0269 0.00659 # Show the PCA model’s sdev values are the square root # of the projected variances, which are along the diagonal # of the covariance matrix of the projected data. iris.pca$sdev^2 #&gt; [1] 2.9185 0.9140 0.1468 0.0207 # # Compute covariance matrix for new dataset. # Recall that the standard deviation is the square root of the variance. round(cov(df.new), 5) #&gt; PC1 PC2 PC3 PC4 #&gt; PC1 2.92 0.000 0.000 0.0000 #&gt; PC2 0.00 0.914 0.000 0.0000 #&gt; PC3 0.00 0.000 0.147 0.0000 #&gt; PC4 0.00 0.000 0.000 0.0207 "],
["iris-compute-the-eigenvectors-and-eigenvalues.html", "Chapter 8 Iris. Compute the eigenvectors and eigenvalues", " Chapter 8 Iris. Compute the eigenvectors and eigenvalues # Scale and center the data. df.scaled &lt;- scale(iris[, 1:4], center = TRUE, scale = TRUE) # Compute the covariance matrix. cov.df.scaled &lt;- cov(df.scaled) # Compute the eigenvectors and eigen values. # Each eigenvector (column) is a principal component. # Each eigenvalue is the variance explained by the # associated eigenvector. eigenInformation &lt;- eigen(cov.df.scaled) print(eigenInformation) #&gt; eigen() decomposition #&gt; $values #&gt; [1] 2.9185 0.9140 0.1468 0.0207 #&gt; #&gt; $vectors #&gt; [,1] [,2] [,3] [,4] #&gt; [1,] 0.521 -0.3774 0.720 0.261 #&gt; [2,] -0.269 -0.9233 -0.244 -0.124 #&gt; [3,] 0.580 -0.0245 -0.142 -0.801 #&gt; [4,] 0.565 -0.0669 -0.634 0.524 # Now, compute the new dataset aligned to the PCs by # multiplying the eigenvector and data matrices. # Create transposes in preparation for matrix multiplication eigenvectors.t &lt;- t(eigenInformation$vectors) # 4x4 df.scaled.t &lt;- t(df.scaled) # 4x150 # Perform matrix multiplication. df.new &lt;- eigenvectors.t %*% df.scaled.t # 4x150 # Create new data frame. First take transpose and # then add column names. df.new.t &lt;- t(df.new) # 150x4 colnames(df.new.t) &lt;- c(&quot;PC1&quot;, &quot;PC2&quot;, &quot;PC3&quot;, &quot;PC4&quot;) head(df.new.t) #&gt; PC1 PC2 PC3 PC4 #&gt; [1,] -2.26 -0.478 0.1273 0.02409 #&gt; [2,] -2.07 0.672 0.2338 0.10266 #&gt; [3,] -2.36 0.341 -0.0441 0.02828 #&gt; [4,] -2.29 0.595 -0.0910 -0.06574 #&gt; [5,] -2.38 -0.645 -0.0157 -0.03580 #&gt; [6,] -2.07 -1.484 -0.0269 0.00659 # Compute covariance matrix for new dataset round(cov(df.new.t), 5) #&gt; PC1 PC2 PC3 PC4 #&gt; PC1 2.92 0.000 0.000 0.0000 #&gt; PC2 0.00 0.914 0.000 0.0000 #&gt; PC3 0.00 0.000 0.147 0.0000 #&gt; PC4 0.00 0.000 0.000 0.0207 "],
["dealing-with-unbalanced-data.html", "Chapter 9 Dealing with unbalanced data 9.1 Introduction 9.2 Read and process the data 9.3 Under-sampling 9.4 Oversampling 9.5 Predictions 9.6 Final notes", " Chapter 9 Dealing with unbalanced data 9.1 Introduction Source: https://shiring.github.io/machine_learning/2017/04/02/unbalanced library(caret) #&gt; Loading required package: lattice #&gt; Loading required package: ggplot2 #&gt; Registered S3 methods overwritten by &#39;ggplot2&#39;: #&gt; method from #&gt; [.quosures rlang #&gt; c.quosures rlang #&gt; print.quosures rlang library(mice) #&gt; #&gt; Attaching package: &#39;mice&#39; #&gt; The following objects are masked from &#39;package:base&#39;: #&gt; #&gt; cbind, rbind library(ggplot2) In my last post, where I shared the code that I used to produce an example analysis to go along with my webinar on building meaningful models for disease prediction, I mentioned that it is advised to consider over- or under-sampling when you have unbalanced data sets. Because my focus in this webinar was on evaluating model performance, I did not want to add an additional layer of complexity and therefore did not further discuss how to specifically deal with unbalanced data. But because I had gotten a few questions regarding this, I thought it would be worthwhile to explain over- and under-sampling techniques in more detail and show how you can very easily implement them with caret. 9.2 Read and process the data bc_data &lt;- read.table(file.path(data_raw_dir, &quot;breast-cancer-wisconsin.data&quot;), header = FALSE, sep = &quot;,&quot;) colnames(bc_data) &lt;- c(&quot;sample_code_number&quot;, &quot;clump_thickness&quot;, &quot;uniformity_of_cell_size&quot;, &quot;uniformity_of_cell_shape&quot;, &quot;marginal_adhesion&quot;, &quot;single_epithelial_cell_size&quot;, &quot;bare_nuclei&quot;, &quot;bland_chromatin&quot;, &quot;normal_nucleoli&quot;, &quot;mitosis&quot;, &quot;classes&quot;) bc_data$classes &lt;- ifelse(bc_data$classes == &quot;2&quot;, &quot;benign&quot;, ifelse(bc_data$classes == &quot;4&quot;, &quot;malignant&quot;, NA)) bc_data[bc_data == &quot;?&quot;] &lt;- NA # how many NAs are in the data length(which(is.na(bc_data))) #&gt; [1] 16 # impute missing data # skip columns: sample_code_number and classes bc_data[,2:10] &lt;- apply(bc_data[, 2:10], 2, function(x) as.numeric(as.character(x))) # impute but stay mute dataset_impute &lt;- mice(bc_data[, 2:10], print = FALSE) # bind &quot;classes&quot; with the rest. skip &quot;sample_code_number&quot; bc_data &lt;- cbind(bc_data[, 11, drop = FALSE], mice::complete(dataset_impute, action = 1)) bc_data$classes &lt;- as.factor(bc_data$classes) 9.2.1 Unbalanced data In this context, unbalanced data refers to classification problems where we have unequal instances for different classes. Having unbalanced data is actually very common in general, but it is especially prevalent when working with disease data where we usually have more healthy control samples than disease cases. Even more extreme unbalance is seen with fraud detection, where e.g. most credit card uses are okay and only very few will be fraudulent. In the example I used for my webinar, a breast cancer dataset, we had about twice as many benign than malignant samples. # how many benign and malignant cases are there? summary(bc_data$classes) #&gt; benign malignant #&gt; 458 241 9.2.1.1 Why is unbalanced data a problem in machine learning? Most machine learning classification algorithms are sensitive to unbalance in the predictor classes. Let’s consider an even more extreme example than our breast cancer dataset: assume we had 10 malignant vs 90 benign samples. A machine learning model that has been trained and tested on such a dataset could now predict “benign” for all samples and still gain a very high accuracy. An unbalanced dataset will bias the prediction model towards the more common class! 9.2.1.2 How to balance data for modeling The basic theoretical concepts behind over- and under-sampling are very simple: With under-sampling, we randomly select a subset of samples from the class with more instances to match the number of samples coming from each class. In our example, we would randomly pick 241 out of the 458 benign cases. The main disadvantage of under-sampling is that we lose potentially relevant information from the left-out samples. With oversampling, we randomly duplicate samples from the class with fewer instances or we generate additional instances based on the data that we have, so as to match the number of samples in each class. While we avoid losing information with this approach, we also run the risk of overfitting our model as we are more likely to get the same samples in the training and in the test data, i.e. the test data is no longer independent from training data. This would lead to an overestimation of our model’s performance and generalizability. In reality though, we should not simply perform over- or under-sampling on our training data and then run the model. We need to account for cross-validation and perform over- or under-sampling on each fold independently to get an honest estimate of model performance! 9.2.1.3 Modeling the original unbalanced data Here is the same model I used in my webinar example: I randomly divide the data into training and test sets (stratified by class) and perform Random Forest modeling with 10 x 10 repeated cross-validation. Final model performance is then measured on the test set. set.seed(42) index &lt;- createDataPartition(bc_data$classes, p = 0.7, list = FALSE) train_data &lt;- bc_data[index, ] test_data &lt;- bc_data[-index, ] set.seed(42) model_rf &lt;- caret::train(classes ~ ., data = train_data, method = &quot;rf&quot;, preProcess = c(&quot;scale&quot;, &quot;center&quot;), trControl = trainControl(method = &quot;repeatedcv&quot;, number = 10, repeats = 10, verboseIter = FALSE)) final &lt;- data.frame(actual = test_data$classes, predict(model_rf, newdata = test_data, type = &quot;prob&quot;)) final$predict &lt;- ifelse(final$benign &gt; 0.5, &quot;benign&quot;, &quot;malignant&quot;) final_predict &lt;- as.factor(final$predict) test_data_classes &lt;- as.factor(test_data$classes) cm_original &lt;- confusionMatrix(final_predict, test_data_classes) cm_original$byClass[&#39;Sensitivity&#39;] #&gt; Sensitivity #&gt; 0.978 9.3 Under-sampling Luckily, caret makes it very easy to incorporate over- and under-sampling techniques with cross-validation resampling. We can simply add the sampling option to our trainControl and choose down for under- (also called down-) sampling. The rest stays the same as with our original model. set.seed(42) ctrl &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 10, repeats = 10, verboseIter = FALSE, sampling = &quot;down&quot;) model_rf_under &lt;- caret::train(classes ~ ., data = train_data, method = &quot;rf&quot;, preProcess = c(&quot;scale&quot;, &quot;center&quot;), trControl = ctrl) final_under &lt;- data.frame(actual = test_data$classes, predict(model_rf_under, newdata = test_data, type = &quot;prob&quot;)) final_under$predict &lt;- ifelse(final_under$benign &gt; 0.5, &quot;benign&quot;, &quot;malignant&quot;) final_under_predict &lt;- as.factor(final_under$predict) test_data_classes &lt;- test_data$classes cm_under &lt;- confusionMatrix(final_under_predict, test_data_classes) cm_under$byClass[&#39;Sensitivity&#39;] #&gt; Sensitivity #&gt; 0.978 9.4 Oversampling For over- (also called up-) sampling we simply specify sampling = “up”. set.seed(42) ctrl &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 10, repeats = 10, verboseIter = FALSE, sampling = &quot;up&quot;) model_rf_over &lt;- caret::train(classes ~ ., data = train_data, method = &quot;rf&quot;, preProcess = c(&quot;scale&quot;, &quot;center&quot;), trControl = ctrl) final_over &lt;- data.frame(actual = test_data$classes, predict(model_rf_over, newdata = test_data, type = &quot;prob&quot;)) final_over$predict &lt;- ifelse(final_over$benign &gt; 0.5, &quot;benign&quot;, &quot;malignant&quot;) final_over_predict &lt;- as.factor(final_over$predict) test_data_classes &lt;- test_data$classes cm_over &lt;- confusionMatrix(final_over_predict, test_data_classes) cm_over$byClass[&#39;Sensitivity&#39;] #&gt; Sensitivity #&gt; 0.978 9.4.1 ROSE Besides over- and under-sampling, there are hybrid methods that combine under-sampling with the generation of additional data. Two of the most popular are ROSE and SMOTE. From Nicola Lunardon, Giovanna Menardi and Nicola Torelli’s “ROSE: A Package for Binary Imbalanced Learning” (R Journal, 2014, Vol. 6 Issue 1, p. 79): “The ROSE package provides functions to deal with binary classification problems in the presence of imbalanced classes. Artificial balanced samples are generated according to a smoothed bootstrap approach and allow for aiding both the phases of estimation and accuracy evaluation of a binary classifier in the presence of a rare class. Functions that implement more traditional remedies for the class imbalance and different metrics to evaluate accuracy are also provided. These are estimated by holdout, bootstrap, or cross-validation methods.” You implement them the same way as before, this time choosing sampling = “rose”… set.seed(42) ctrl &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 10, repeats = 10, verboseIter = FALSE, sampling = &quot;rose&quot;) model_rf_rose &lt;- caret::train(classes ~ ., data = train_data, method = &quot;rf&quot;, preProcess = c(&quot;scale&quot;, &quot;center&quot;), trControl = ctrl) #&gt; Loaded ROSE 0.0-3 final_rose &lt;- data.frame(actual = test_data$classes, predict(model_rf_rose, newdata = test_data, type = &quot;prob&quot;)) final_rose$predict &lt;- ifelse(final_rose$benign &gt; 0.5, &quot;benign&quot;, &quot;malignant&quot;) cm_rose &lt;- confusionMatrix(as.factor(final_rose$predict), as.factor(test_data$classes)) cm_rose$byClass[&#39;Sensitivity&#39;] #&gt; Sensitivity #&gt; 0.985 9.4.2 SMOTE … or by choosing sampling = “smote” in the trainControl settings. From Nitesh V. Chawla, Kevin W. Bowyer, Lawrence O. Hall and W. Philip Kegelmeyer’s “SMOTE: Synthetic Minority Over-sampling Technique” (Journal of Artificial Intelligence Research, 2002, Vol. 16, pp. 321–357): “This paper shows that a combination of our method of over-sampling the minority (abnormal) class and under-sampling the majority (normal) class can achieve better classifier performance (in ROC space) than only under-sampling the majority class. This paper also shows that a combination of our method of over-sampling the minority class and under-sampling the majority class can achieve better classifier performance (in ROC space) than varying the loss ratios in Ripper or class priors in Naive Bayes. Our method of over-sampling the minority class involves creating synthetic minority class examples.” set.seed(42) ctrl &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 10, repeats = 10, verboseIter = FALSE, sampling = &quot;smote&quot;) model_rf_smote &lt;- caret::train(classes ~ ., data = train_data, method = &quot;rf&quot;, preProcess = c(&quot;scale&quot;, &quot;center&quot;), trControl = ctrl) #&gt; Loading required package: grid #&gt; Registered S3 method overwritten by &#39;xts&#39;: #&gt; method from #&gt; as.zoo.xts zoo #&gt; Registered S3 method overwritten by &#39;quantmod&#39;: #&gt; method from #&gt; as.zoo.data.frame zoo final_smote &lt;- data.frame(actual = test_data$classes, predict(model_rf_smote, newdata = test_data, type = &quot;prob&quot;)) final_smote$predict &lt;- ifelse(final_smote$benign &gt; 0.5, &quot;benign&quot;, &quot;malignant&quot;) cm_smote &lt;- confusionMatrix(as.factor(final_smote$predict), as.factor(test_data$classes)) cm_smote$byClass[&#39;Sensitivity&#39;] #&gt; Sensitivity #&gt; 0.978 9.5 Predictions Now let’s compare the predictions of all these models: models &lt;- list( original = model_rf, under = model_rf_under, over = model_rf_over, smote = model_rf_smote, rose = model_rf_rose) resampling &lt;- resamples(models) bwplot(resampling) library(dplyr) #&gt; #&gt; Attaching package: &#39;dplyr&#39; #&gt; The following objects are masked from &#39;package:stats&#39;: #&gt; #&gt; filter, lag #&gt; The following objects are masked from &#39;package:base&#39;: #&gt; #&gt; intersect, setdiff, setequal, union comparison &lt;- data.frame(model = names(models), Sensitivity = rep(NA, length(models)), Specificity = rep(NA, length(models)), Precision = rep(NA, length(models)), Recall = rep(NA, length(models)), F1 = rep(NA, length(models))) for (name in names(models)) { cm_model &lt;- get(paste0(&quot;cm_&quot;, name)) comparison[comparison$model==name, ] &lt;- filter(comparison, model==name) %&gt;% mutate(Sensitivity = cm_model$byClass[&quot;Sensitivity&quot;], Specificity = cm_model$byClass[&quot;Specificity&quot;], Precision = cm_model$byClass[&quot;Precision&quot;], Recall = cm_model$byClass[&quot;Recall&quot;], F1 = cm_model$byClass[&quot;F1&quot;] ) } print(comparison) #&gt; model Sensitivity Specificity Precision Recall F1 #&gt; 1 original 0.978 0.986 0.993 0.978 0.985 #&gt; 2 under 0.978 1.000 1.000 0.978 0.989 #&gt; 3 over 0.978 0.986 0.993 0.978 0.985 #&gt; 4 smote 0.978 0.986 0.993 0.978 0.985 #&gt; 5 rose 0.985 0.986 0.993 0.985 0.989 library(tidyr) #&gt; #&gt; Attaching package: &#39;tidyr&#39; #&gt; The following object is masked from &#39;package:mice&#39;: #&gt; #&gt; complete comparison %&gt;% gather(x, y, Sensitivity:F1) %&gt;% ggplot(aes(x = x, y = y, color = model)) + geom_jitter(width = 0.2, alpha = 0.5, size = 3) With this small dataset, we can already see how the different techniques can influence model performance. Sensitivity (or recall) describes the proportion of benign cases that have been predicted correctly, while specificity describes the proportion of malignant cases that have been predicted correctly. Precision describes the true positives, i.e. the proportion of benign predictions that were actual from benign samples. F1 is the weighted average of precision and sensitivity/ recall. 9.6 Final notes Here, all four methods improved specificity and precision compared to the original model. Under-sampling, over-sampling and ROSE additionally improved precision and the F1 score. This post shows a simple example of how to correct for unbalance in datasets for machine learning. For more advanced instructions and potential caveats with these techniques, check out the excellent caret documentation. If you are interested in more machine learning posts, check out the category listing for machine_learning on my blog. "],
["feature-selection-variable-importance.html", "Chapter 10 Feature selection, Variable Importance 10.1 Introduction 10.2 1. Boruta 10.3 2. Variable Importance from Machine Learning Algorithms 10.4 3. Lasso Regression 10.5 4. Step wise Forward and Backward Selection 10.6 5. Relative Importance from Linear Regression 10.7 6. Recursive Feature Elimination (RFE) 10.8 7. Genetic Algorithm 10.9 8. Simulated Annealing 10.10 9. Information Value and Weights of Evidence 10.11 10. DALEX Package 10.12 Conclusion", " Chapter 10 Feature selection, Variable Importance Source: https://www.machinelearningplus.com/machine-learning/feature-selection/ #&gt; Error in readChar(con, 5L, useBytes = TRUE): cannot open the connection 10.1 Introduction In real-world datasets, it is fairly common to have columns that are nothing but noise. You are better off getting rid of such variables because of the memory space they occupy, the time and the computational esources it is going to cost, especially in large datasets. Sometimes, you have a variable that makes business sense, but you are not sure if it actually helps in predicting the Y. You also need to consider the fact that, a feature that could be useful in one ML algorithm (say a decision tree) may go underrepresented or unused by another (like a regression model). Having said that, it is still possible that a variable that shows poor signs of helping to explain the response variable (Y), can turn out to be significantly useful in the presence of (or combination with) other predictors. What I mean by that is, a variable might have a low correlation value of (~0.2) with Y. But in the presence of other variables, it can help to explain certain patterns/phenomenon that other variables can’t explain. In such cases, it can be hard to make a call whether to include or exclude such variables. The strategies we are about to discuss can help fix such problems. Not only that, it will also help understand if a particular variable is important or not and how much it is contributing to the model An important caveat. It is always best to have variables that have sound business logic backing the inclusion of a variable and rely solely on variable importance metrics. Alright. Let’s load up the ‘Glaucoma’ dataset where the goal is to predict if a patient has Glaucoma or not based on 63 different physiological measurements. You can directly run the codes or download the dataset here. A lot of interesting examples ahead. Let’s get started. # Load Packages and prepare dataset library(TH.data) library(caret) library(tictoc) data(&quot;GlaucomaM&quot;, package = &quot;TH.data&quot;) trainData &lt;- GlaucomaM head(trainData) #&gt; ag at as an ai eag eat eas ean eai abrg abrt #&gt; 2 2.22 0.354 0.580 0.686 0.601 1.267 0.336 0.346 0.255 0.331 0.479 0.260 #&gt; 43 2.68 0.475 0.672 0.868 0.667 2.053 0.440 0.520 0.639 0.454 1.090 0.377 #&gt; 25 1.98 0.343 0.508 0.624 0.504 1.200 0.299 0.396 0.259 0.246 0.465 0.209 #&gt; 65 1.75 0.269 0.476 0.525 0.476 0.612 0.147 0.017 0.044 0.405 0.170 0.062 #&gt; 70 2.99 0.599 0.686 1.039 0.667 2.513 0.543 0.607 0.871 0.492 1.800 0.431 #&gt; 16 2.92 0.483 0.763 0.901 0.770 2.200 0.462 0.637 0.504 0.597 1.311 0.394 #&gt; abrs abrn abri hic mhcg mhct mhcs mhcn mhci phcg #&gt; 2 0.107 0.014 0.098 0.214 0.111 0.412 0.036 0.105 -0.022 -0.139 #&gt; 43 0.257 0.212 0.245 0.382 0.140 0.338 0.104 0.080 0.109 -0.015 #&gt; 25 0.112 0.041 0.103 0.195 0.062 0.356 0.045 -0.009 -0.048 -0.149 #&gt; 65 0.000 0.000 0.108 -0.030 -0.015 0.074 -0.084 -0.050 0.035 -0.182 #&gt; 70 0.494 0.601 0.274 0.383 0.089 0.233 0.145 0.023 0.007 -0.131 #&gt; 16 0.365 0.251 0.301 0.442 0.128 0.375 0.049 0.111 0.052 -0.088 #&gt; phct phcs phcn phci hvc vbsg vbst vbss vbsn vbsi vasg #&gt; 2 0.242 -0.053 0.010 -0.139 0.613 0.303 0.103 0.088 0.022 0.090 0.062 #&gt; 43 0.296 -0.015 -0.015 0.036 0.382 0.676 0.181 0.186 0.141 0.169 0.029 #&gt; 25 0.206 -0.092 -0.081 -0.149 0.557 0.300 0.084 0.088 0.046 0.082 0.036 #&gt; 65 -0.097 -0.125 -0.138 -0.182 0.373 0.048 0.011 0.000 0.000 0.036 0.070 #&gt; 70 0.163 0.055 -0.131 -0.115 0.405 0.889 0.151 0.253 0.330 0.155 0.020 #&gt; 16 0.281 -0.067 -0.062 -0.088 0.507 0.972 0.213 0.316 0.197 0.246 0.043 #&gt; vast vass vasn vasi vbrg vbrt vbrs vbrn vbri varg vart vars #&gt; 2 0.000 0.011 0.032 0.018 0.075 0.039 0.021 0.002 0.014 0.756 0.009 0.209 #&gt; 43 0.001 0.007 0.011 0.010 0.370 0.127 0.099 0.050 0.093 0.410 0.006 0.105 #&gt; 25 0.002 0.004 0.016 0.013 0.081 0.034 0.019 0.007 0.021 0.565 0.014 0.132 #&gt; 65 0.005 0.030 0.033 0.002 0.005 0.001 0.000 0.000 0.004 0.380 0.032 0.147 #&gt; 70 0.001 0.004 0.008 0.007 0.532 0.103 0.173 0.181 0.075 0.228 0.011 0.026 #&gt; 16 0.001 0.005 0.028 0.009 0.467 0.136 0.148 0.078 0.104 0.540 0.008 0.133 #&gt; varn vari mdg mdt mds mdn mdi tmg tmt tms tmn #&gt; 2 0.298 0.240 0.705 0.637 0.738 0.596 0.691 -0.236 -0.018 -0.230 -0.510 #&gt; 43 0.181 0.117 0.898 0.850 0.907 0.771 0.940 -0.211 -0.014 -0.165 -0.317 #&gt; 25 0.243 0.177 0.687 0.643 0.689 0.684 0.700 -0.185 -0.097 -0.235 -0.337 #&gt; 65 0.151 0.050 0.207 0.171 0.022 0.046 0.221 -0.148 -0.035 -0.449 -0.217 #&gt; 70 0.105 0.087 0.721 0.638 0.730 0.730 0.640 -0.052 -0.105 0.084 -0.012 #&gt; 16 0.232 0.167 0.927 0.842 0.953 0.906 0.898 -0.040 0.087 0.018 -0.094 #&gt; tmi mr rnf mdic emd mv Class #&gt; 2 -0.158 0.841 0.410 0.137 0.239 0.035 normal #&gt; 43 -0.192 0.924 0.256 0.252 0.329 0.022 normal #&gt; 25 -0.020 0.795 0.378 0.152 0.250 0.029 normal #&gt; 65 -0.091 0.746 0.200 0.027 0.078 0.023 normal #&gt; 70 -0.054 0.977 0.193 0.297 0.354 0.034 normal #&gt; 16 -0.051 0.965 0.339 0.333 0.442 0.028 normal 10.2 1. Boruta Boruta is a feature ranking and selection algorithm based on random forests algorithm. The advantage with Boruta is that it clearly decides if a variable is important or not and helps to select variables that are statistically significant. Besides, you can adjust the strictness of the algorithm by adjusting the \\(p\\) values that defaults to 0.01 and the maxRuns. maxRuns is the number of times the algorithm is run. The higher the maxRuns the more selective you get in picking the variables. The default value is 100. In the process of deciding if a feature is important or not, some features may be marked by Boruta as ‘Tentative’. Sometimes increasing the maxRuns can help resolve the ‘Tentativeness’ of the feature. Lets see an example based on the Glaucoma dataset from TH.data package that I created earlier. # install.packages(&#39;Boruta&#39;) library(Boruta) The boruta function uses a formula interface just like most predictive modeling functions. So the first argument to boruta() is the formula with the response variable on the left and all the predictors on the right. By placing a dot, all the variables in trainData other than Class will be included in the model. The doTrace argument controls the amount of output printed to the console. Higher the value, more the log details you get. So save space I have set it to 0, but try setting it to 1 and 2 if you are running the code. Finally the output is stored in boruta_output. # Perform Boruta search boruta_output &lt;- Boruta(Class ~ ., data=na.omit(trainData), doTrace=0) Let’s see what the boruta_output contains. names(boruta_output) #&gt; [1] &quot;finalDecision&quot; &quot;ImpHistory&quot; &quot;pValue&quot; &quot;maxRuns&quot; #&gt; [5] &quot;light&quot; &quot;mcAdj&quot; &quot;timeTaken&quot; &quot;roughfixed&quot; #&gt; [9] &quot;call&quot; &quot;impSource&quot; # Get significant variables including tentatives boruta_signif &lt;- getSelectedAttributes(boruta_output, withTentative = TRUE) print(boruta_signif) #&gt; [1] &quot;as&quot; &quot;ai&quot; &quot;eas&quot; &quot;ean&quot; &quot;abrg&quot; &quot;abrs&quot; &quot;abrn&quot; &quot;abri&quot; &quot;hic&quot; &quot;mhcg&quot; #&gt; [11] &quot;mhcs&quot; &quot;mhcn&quot; &quot;mhci&quot; &quot;phcg&quot; &quot;phcn&quot; &quot;phci&quot; &quot;hvc&quot; &quot;vbsg&quot; &quot;vbss&quot; &quot;vbsn&quot; #&gt; [21] &quot;vbsi&quot; &quot;vasg&quot; &quot;vass&quot; &quot;vasi&quot; &quot;vbrg&quot; &quot;vbrs&quot; &quot;vbrn&quot; &quot;vbri&quot; &quot;varg&quot; &quot;vart&quot; #&gt; [31] &quot;vars&quot; &quot;varn&quot; &quot;vari&quot; &quot;mdn&quot; &quot;tmg&quot; &quot;tmt&quot; &quot;tms&quot; &quot;tmi&quot; &quot;mr&quot; &quot;rnf&quot; #&gt; [41] &quot;mdic&quot; &quot;emd&quot; If you are not sure about the tentative variables being selected for granted, you can choose a TentativeRoughFix on boruta_output. # Do a tentative rough fix roughFixMod &lt;- TentativeRoughFix(boruta_output) boruta_signif &lt;- getSelectedAttributes(roughFixMod) print(boruta_signif) #&gt; [1] &quot;as&quot; &quot;ai&quot; &quot;ean&quot; &quot;abrg&quot; &quot;abrs&quot; &quot;abrn&quot; &quot;abri&quot; &quot;hic&quot; &quot;mhcg&quot; &quot;mhcn&quot; #&gt; [11] &quot;mhci&quot; &quot;phcg&quot; &quot;phcn&quot; &quot;phci&quot; &quot;hvc&quot; &quot;vbsn&quot; &quot;vbsi&quot; &quot;vasg&quot; &quot;vass&quot; &quot;vasi&quot; #&gt; [21] &quot;vbrg&quot; &quot;vbrs&quot; &quot;vbrn&quot; &quot;vbri&quot; &quot;varg&quot; &quot;vart&quot; &quot;vars&quot; &quot;varn&quot; &quot;vari&quot; &quot;mdn&quot; #&gt; [31] &quot;tmg&quot; &quot;tms&quot; &quot;tmi&quot; &quot;mr&quot; &quot;rnf&quot; &quot;mdic&quot; There you go. Boruta has decided on the ‘Tentative’ variables on our behalf. Let’s find out the importance scores of these variables. # Variable Importance Scores imps &lt;- attStats(roughFixMod) imps2 = imps[imps$decision != &#39;Rejected&#39;, c(&#39;meanImp&#39;, &#39;decision&#39;)] head(imps2[order(-imps2$meanImp), ]) # descending sort #&gt; meanImp decision #&gt; vari 12.37 Confirmed #&gt; varg 11.74 Confirmed #&gt; vars 10.74 Confirmed #&gt; phci 8.34 Confirmed #&gt; hic 8.21 Confirmed #&gt; varn 7.88 Confirmed Let’s plot it to see the importances of these variables. # Plot variable importance plot(boruta_output, cex.axis=.7, las=2, xlab=&quot;&quot;, main=&quot;Variable Importance&quot;) This plot reveals the importance of each of the features. The columns in green are ‘confirmed’ and the ones in red are not. There are couple of blue bars representing ShadowMax and ShadowMin. They are not actual features, but are used by the boruta algorithm to decide if a variable is important or not. 10.3 2. Variable Importance from Machine Learning Algorithms Another way to look at feature selection is to consider variables most used by various ML algorithms the most to be important. Depending on how the machine learning algorithm learns the relationship between X’s and Y, different machine learning algorithms may possibly end up using different variables (but mostly common vars) to various degrees. What I mean by that is, the variables that proved useful in a tree-based algorithm like rpart, can turn out to be less useful in a regression-based model. So all variables need not be equally useful to all algorithms. So how do we find the variable importance for a given ML algo? train() the desired model using the caret package. Then, use varImp() to determine the feature importances. You may want to try out multiple algorithms, to get a feel of the usefulness of the features across algos. 10.3.1 rpart # Train an rpart model and compute variable importance. library(caret) set.seed(100) rPartMod &lt;- train(Class ~ ., data=trainData, method=&quot;rpart&quot;) rpartImp &lt;- varImp(rPartMod) print(rpartImp) #&gt; rpart variable importance #&gt; #&gt; only 20 most important variables shown (out of 62) #&gt; #&gt; Overall #&gt; varg 100.0 #&gt; vari 93.2 #&gt; vars 85.2 #&gt; varn 76.9 #&gt; tmi 72.3 #&gt; mhcn 0.0 #&gt; as 0.0 #&gt; phcs 0.0 #&gt; vbst 0.0 #&gt; abrt 0.0 #&gt; vbsg 0.0 #&gt; eai 0.0 #&gt; vbrs 0.0 #&gt; vbsi 0.0 #&gt; eag 0.0 #&gt; tmt 0.0 #&gt; phcn 0.0 #&gt; vart 0.0 #&gt; mds 0.0 #&gt; an 0.0 Only 5 of the 63 features was used by rpart and if you look closely, the 5 variables used here are in the top 6 that boruta selected. Let’s do one more: the variable importances from Regularized Random Forest (RRF) algorithm. 10.3.2 Regularized Random Forest (RRF) tic() # Train an RRF model and compute variable importance. set.seed(100) rrfMod &lt;- train(Class ~ ., data = trainData, method = &quot;RRF&quot;) rrfImp &lt;- varImp(rrfMod, scale=F) toc() #&gt; 424.816 sec elapsed rrfImp #&gt; RRF variable importance #&gt; #&gt; only 20 most important variables shown (out of 62) #&gt; #&gt; Overall #&gt; varg 25.07 #&gt; vari 18.78 #&gt; vars 5.29 #&gt; tmi 4.09 #&gt; mhcg 3.25 #&gt; mhci 2.81 #&gt; hic 2.69 #&gt; hvc 2.50 #&gt; mv 2.00 #&gt; vasg 1.99 #&gt; phci 1.77 #&gt; phcn 1.53 #&gt; phct 1.43 #&gt; vass 1.37 #&gt; phcg 1.37 #&gt; tms 1.32 #&gt; tmg 1.16 #&gt; abrs 1.16 #&gt; tmt 1.13 #&gt; mdic 1.13 plot(rrfImp, top = 20, main=&#39;Variable Importance&#39;) The topmost important variables are pretty much from the top tier of Boruta’s selections. Some of the other algorithms available in train() that you can use to compute varImp are the following: ada, AdaBag, AdaBoost.M1, adaboost, bagEarth, bagEarthGCV, bagFDA, bagFDAGCV, bartMachine, blasso, BstLm, bstSm, C5.0, C5.0Cost, C5.0Rules, C5.0Tree, cforest, chaid, ctree, ctree2, cubist, deepboost, earth, enet, evtree, extraTrees, fda, gamboost, gbm_h2o, gbm, gcvEarth, glmnet_h2o, glmnet, glmStepAIC, J48, JRip, lars, lars2, lasso, LMT, LogitBoost, M5, M5Rules, msaenet, nodeHarvest, OneR, ordinalNet, ORFlog, ORFpls, ORFridge, ORFsvm, pam, parRF, PART, penalized, PenalizedLDA, qrf, ranger, Rborist, relaxo, rf, rFerns, rfRules, rotationForest, rotationForestCp, rpart, rpart1SE, rpart2, rpartCost, rpartScore, rqlasso, rqnc, RRF, RRFglobal, sdwd, smda, sparseLDA, spikeslab, wsrf, xgbLinear, xgbTree. 10.4 3. Lasso Regression Least Absolute Shrinkage and Selection Operator (LASSO) regression is a type of regularization method that penalizes with L1-norm. It basically imposes a cost to having large weights (value of coefficients). And its called L1 regularization, because the cost added, is proportional to the absolute value of weight coefficients. As a result, in the process of shrinking the coefficients, it eventually reduces the coefficients of certain unwanted features all the to zero. That is, it removes the unneeded variables altogether. So effectively, LASSO regression can be considered as a variable selection technique as well. library(glmnet) # online data # trainData &lt;- read.csv(&#39;https://raw.githubusercontent.com/selva86/datasets/master/GlaucomaM.csv&#39;) trainData &lt;- read.csv(file.path(data_raw_dir, &quot;glaucoma.csv&quot;)) x &lt;- as.matrix(trainData[,-63]) # all X vars y &lt;- as.double(as.matrix(ifelse(trainData[, 63]==&#39;normal&#39;, 0, 1))) # Only Class # Fit the LASSO model (Lasso: Alpha = 1) set.seed(100) cv.lasso &lt;- cv.glmnet(x, y, family=&#39;binomial&#39;, alpha=1, parallel=TRUE, standardize=TRUE, type.measure=&#39;auc&#39;) # Results plot(cv.lasso) Let’s see how to interpret this plot. The X axis of the plot is the log of lambda. That means when it is 2 here, the lambda value is actually 100. The numbers at the top of the plot show how many predictors were included in the model. The position of red dots along the Y-axis tells what AUC we got when you include as many variables shown on the top x-axis. You can also see two dashed vertical lines. The first one on the left points to the lambda with the lowest mean squared error. The one on the right point to the number of variables with the highest deviance within 1 standard deviation. The best lambda value is stored inside ‘cv.lasso$lambda.min’. # plot(cv.lasso$glmnet.fit, xvar=&quot;lambda&quot;, label=TRUE) cat(&#39;Min Lambda: &#39;, cv.lasso$lambda.min, &#39;\\n 1Sd Lambda: &#39;, cv.lasso$lambda.1se) #&gt; Min Lambda: 0.0224 #&gt; 1Sd Lambda: 0.144 df_coef &lt;- round(as.matrix(coef(cv.lasso, s=cv.lasso$lambda.min)), 2) # See all contributing variables df_coef[df_coef[, 1] != 0, ] #&gt; (Intercept) as mhci phci hvc vast #&gt; 2.68 -1.59 3.85 5.60 -2.41 -13.90 #&gt; vars vari mdn mdi tmg tms #&gt; -20.18 -1.58 0.50 0.99 0.06 2.56 #&gt; tmi #&gt; 2.23 The above output shows what variables LASSO considered important. A high positive or low negative implies more important is that variable. 10.5 4. Step wise Forward and Backward Selection Stepwise regression can be used to select features if the Y variable is a numeric variable. It is particularly used in selecting best linear regression models. It searches for the best possible regression model by iteratively selecting and dropping variables to arrive at a model with the lowest possible AIC. It can be implemented using the step() function and you need to provide it with a lower model, which is the base model from which it won’t remove any features and an upper model, which is a full model that has all possible features you want to have. Our case is not so complicated (&lt; 20 vars), so lets just do a simple stepwise in ‘both’ directions. I will use the ozone dataset for this where the objective is to predict the ozone_reading based on other weather related observations. # Load data # online # trainData &lt;- read.csv(&quot;http://rstatistics.net/wp-content/uploads/2015/09/ozone1.csv&quot;, # stringsAsFactors=F) trainData &lt;- read.csv(file.path(data_raw_dir, &quot;ozone1.csv&quot;)) print(head(trainData)) #&gt; Month Day_of_month Day_of_week ozone_reading pressure_height Wind_speed #&gt; 1 1 1 4 3 5480 8 #&gt; 2 1 2 5 3 5660 6 #&gt; 3 1 3 6 3 5710 4 #&gt; 4 1 4 7 5 5700 3 #&gt; 5 1 5 1 5 5760 3 #&gt; 6 1 6 2 6 5720 4 #&gt; Humidity Temperature_Sandburg Temperature_ElMonte Inversion_base_height #&gt; 1 20 40.5 39.8 5000 #&gt; 2 41 38.0 46.7 4109 #&gt; 3 28 40.0 49.5 2693 #&gt; 4 37 45.0 52.3 590 #&gt; 5 51 54.0 45.3 1450 #&gt; 6 69 35.0 49.6 1568 #&gt; Pressure_gradient Inversion_temperature Visibility #&gt; 1 -15 30.6 200 #&gt; 2 -14 48.0 300 #&gt; 3 -25 47.7 250 #&gt; 4 -24 55.0 100 #&gt; 5 25 57.0 60 #&gt; 6 15 53.8 60 The data is ready. Let’s perform the stepwise. # Step 1: Define base intercept only model base.mod &lt;- lm(ozone_reading ~ 1 , data=trainData) # Step 2: Full model with all predictors all.mod &lt;- lm(ozone_reading ~ . , data= trainData) # Step 3: Perform step-wise algorithm. direction=&#39;both&#39; implies both forward and backward stepwise stepMod &lt;- step(base.mod, scope = list(lower = base.mod, upper = all.mod), direction = &quot;both&quot;, trace = 0, steps = 1000) # Step 4: Get the shortlisted variable. shortlistedVars &lt;- names(unlist(stepMod[[1]])) #&gt; Error in stepMod[[1]]: class name too long in &#39;[[&#39; shortlistedVars &lt;- shortlistedVars[!shortlistedVars %in% &quot;(Intercept)&quot;] # remove intercept #&gt; Error in eval(expr, envir, enclos): object &#39;shortlistedVars&#39; not found # Show print(shortlistedVars) #&gt; Error in print(shortlistedVars): object &#39;shortlistedVars&#39; not found The selected model has the above 6 features in it. But if you have too many features (&gt; 100) in training data, then it might be a good idea to split the dataset into chunks of 10 variables each with Y as mandatory in each dataset. Loop through all the chunks and collect the best features. We are doing it this way because some variables that came as important in a training data with fewer features may not show up in a linear reg model built on lots of features. Finally, from a pool of shortlisted features (from small chunk models), run a full stepwise model to get the final set of selected features. You can take this as a learning assignment to be solved within 20 minutes. 10.6 5. Relative Importance from Linear Regression This technique is specific to linear regression models. Relative importance can be used to assess which variables contributed how much in explaining the linear model’s R-squared value. So, if you sum up the produced importances, it will add up to the model’s R-sq value. In essence, it is not directly a feature selection method, because you have already provided the features that go in the model. But after building the model, the relaimpo can provide a sense of how important each feature is in contributing to the R-sq, or in other words, in ‘explaining the Y variable’. So, how to calculate relative importance? It is implemented in the relaimpo package. Basically, you build a linear regression model and pass that as the main argument to calc.relimp(). relaimpo has multiple options to compute the relative importance, but the recommended method is to use type='lmg', as I have done below. # install.packages(&#39;relaimpo&#39;) library(relaimpo) # Build linear regression model model_formula = ozone_reading ~ Temperature_Sandburg + Humidity + Temperature_ElMonte + Month + pressure_height + Inversion_base_height lmMod &lt;- lm(model_formula, data=trainData) # calculate relative importance relImportance &lt;- calc.relimp(lmMod, type = &quot;lmg&quot;, rela = F) # Sort cat(&#39;Relative Importances: \\n&#39;) #&gt; Relative Importances: sort(round(relImportance$lmg, 3), decreasing=TRUE) #&gt; Temperature_ElMonte Temperature_Sandburg pressure_height #&gt; 0.214 0.203 0.104 #&gt; Inversion_base_height Humidity Month #&gt; 0.096 0.086 0.012 Additionally, you can use bootstrapping (using boot.relimp) to compute the confidence intervals of the produced relative importances. bootsub &lt;- boot.relimp(ozone_reading ~ Temperature_Sandburg + Humidity + Temperature_ElMonte + Month + pressure_height + Inversion_base_height, data=trainData, b = 1000, type = &#39;lmg&#39;, rank = TRUE, diff = TRUE) plot(booteval.relimp(bootsub, level=.95)) 10.7 6. Recursive Feature Elimination (RFE) Recursive feature elimnation (rfe) offers a rigorous way to determine the important variables before you even feed them into a ML algo. It can be implemented using the rfe() from caret package. The rfe() also takes two important parameters. sizes rfeControl So, what does sizes and rfeControl represent? The sizes determines the number of most important features the rfe should iterate. Below, I have set the size as 1 to 5, 10, 15 and 18. Secondly, the rfeControl parameter receives the output of the rfeControl(). You can set what type of variable evaluation algorithm must be used. Here, I have used random forests based rfFuncs. The method='repeatedCV' means it will do a repeated k-Fold cross validation with repeats=5. Once complete, you get the accuracy and kappa for each model size you provided. The final selected model subset size is marked with a * in the rightmost selected column. str(trainData) #&gt; &#39;data.frame&#39;: 366 obs. of 13 variables: #&gt; $ Month : int 1 1 1 1 1 1 1 1 1 1 ... #&gt; $ Day_of_month : int 1 2 3 4 5 6 7 8 9 10 ... #&gt; $ Day_of_week : int 4 5 6 7 1 2 3 4 5 6 ... #&gt; $ ozone_reading : num 3 3 3 5 5 6 4 4 6 7 ... #&gt; $ pressure_height : num 5480 5660 5710 5700 5760 5720 5790 5790 5700 5700 ... #&gt; $ Wind_speed : int 8 6 4 3 3 4 6 3 3 3 ... #&gt; $ Humidity : num 20 41 28 37 51 ... #&gt; $ Temperature_Sandburg : num 40.5 38 40 45 54 ... #&gt; $ Temperature_ElMonte : num 39.8 46.7 49.5 52.3 45.3 ... #&gt; $ Inversion_base_height: num 5000 4109 2693 590 1450 ... #&gt; $ Pressure_gradient : num -15 -14 -25 -24 25 15 -33 -28 23 -2 ... #&gt; $ Inversion_temperature: num 30.6 48 47.7 55 57 ... #&gt; $ Visibility : int 200 300 250 100 60 60 100 250 120 120 ... tic() set.seed(100) options(warn=-1) subsets &lt;- c(1:5, 10, 15, 18) ctrl &lt;- rfeControl(functions = rfFuncs, method = &quot;repeatedcv&quot;, repeats = 5, verbose = FALSE) lmProfile &lt;- rfe(x=trainData[, c(1:3, 5:13)], y=trainData$ozone_reading, sizes = subsets, rfeControl = ctrl) toc() #&gt; 273.428 sec elapsed lmProfile #&gt; #&gt; Recursive feature selection #&gt; #&gt; Outer resampling method: Cross-Validated (10 fold, repeated 5 times) #&gt; #&gt; Resampling performance over subset size: #&gt; #&gt; Variables RMSE Rsquared MAE RMSESD RsquaredSD MAESD Selected #&gt; 1 5.13 0.595 3.92 0.826 0.1275 0.586 #&gt; 2 4.03 0.746 3.11 0.542 0.0743 0.416 #&gt; 3 3.95 0.756 3.06 0.472 0.0670 0.380 #&gt; 4 3.93 0.759 3.01 0.468 0.0683 0.361 #&gt; 5 3.90 0.763 2.98 0.467 0.0659 0.350 #&gt; 10 3.77 0.782 2.85 0.496 0.0734 0.393 * #&gt; 12 3.77 0.781 2.86 0.508 0.0756 0.401 #&gt; #&gt; The top 5 variables (out of 10): #&gt; Temperature_ElMonte, Pressure_gradient, Temperature_Sandburg, Inversion_temperature, Humidity So, it says, Temperature_ElMonte, Pressure_gradient, Temperature_Sandburg, Inversion_temperature, Humidity are the top 5 variables in that order. And the best model size out of the provided models sizes (in subsets) is 10. You can see all of the top 10 variables from ‘lmProfile$optVariables’ that was created using rfe function above. 10.8 7. Genetic Algorithm You can perform a supervised feature selection with genetic algorithms using the gafs(). This is quite resource expensive so consider that before choosing the number of iterations (iters) and the number of repeats in gafsControl(). tic() # Define control function ga_ctrl &lt;- gafsControl(functions = rfGA, # another option is `caretGA`. method = &quot;cv&quot;, repeats = 3) # Genetic Algorithm feature selection set.seed(100) ga_obj &lt;- gafs(x=trainData[, c(1:3, 5:13)], y=trainData[, 4], iters = 3, # normally much higher (100+) gafsControl = ga_ctrl) toc() #&gt; 1191.582 sec elapsed ga_obj #&gt; #&gt; Genetic Algorithm Feature Selection #&gt; #&gt; 366 samples #&gt; 12 predictors #&gt; #&gt; Maximum generations: 3 #&gt; Population per generation: 50 #&gt; Crossover probability: 0.8 #&gt; Mutation probability: 0.1 #&gt; Elitism: 0 #&gt; #&gt; Internal performance values: RMSE, Rsquared #&gt; Subset selection driven to minimize internal RMSE #&gt; #&gt; External performance values: RMSE, Rsquared, MAE #&gt; Best iteration chose by minimizing external RMSE #&gt; External resampling method: Cross-Validated (10 fold) #&gt; #&gt; During resampling: #&gt; * the top 5 selected variables (out of a possible 12): #&gt; Month (100%), Pressure_gradient (100%), Temperature_ElMonte (100%), Humidity (80%), Visibility (80%) #&gt; * on average, 6.8 variables were selected (min = 5, max = 9) #&gt; #&gt; In the final search using the entire training set: #&gt; * 9 features selected at iteration 2 including: #&gt; Month, Day_of_month, pressure_height, Wind_speed, Humidity ... #&gt; * external performance at this iteration is #&gt; #&gt; RMSE Rsquared MAE #&gt; 3.721 0.788 2.800 # Optimal variables ga_obj$optVariables #&gt; [1] &quot;Month&quot; &quot;Day_of_month&quot; &quot;pressure_height&quot; #&gt; [4] &quot;Wind_speed&quot; &quot;Humidity&quot; &quot;Temperature_ElMonte&quot; #&gt; [7] &quot;Inversion_base_height&quot; &quot;Pressure_gradient&quot; &quot;Inversion_temperature&quot; ‘Month’ ‘Day_of_month’ ‘Wind_speed’ ‘Temperature_ElMonte’ ‘Pressure_gradient’ ‘Visibility’ So the optimal variables according to the genetic algorithms are listed above. But, I wouldn’t use it just yet because, the above variant was tuned for only 3 iterations, which is quite low. I had to set it so low to save computing time. 10.9 8. Simulated Annealing Simulated annealing is a global search algorithm that allows a suboptimal solution to be accepted in hope that a better solution will show up eventually. It works by making small random changes to an initial solution and sees if the performance improved. The change is accepted if it improves, else it can still be accepted if the difference of performances meet an acceptance criteria. In caret it has been implemented in the safs() which accepts a control parameter that can be set using the safsControl() function. safsControl is similar to other control functions in caret (like you saw in rfe and ga), and additionally it accepts an improve parameter which is the number of iterations it should wait without improvement until the values are reset to previous iteration. tic() # Define control function sa_ctrl &lt;- safsControl(functions = rfSA, method = &quot;repeatedcv&quot;, repeats = 3, improve = 5) # n iterations without improvement before a reset # Genetic Algorithm feature selection set.seed(100) sa_obj &lt;- safs(x=trainData[, c(1:3, 5:13)], y=trainData[, 4], safsControl = sa_ctrl) toc() #&gt; 124.285 sec elapsed sa_obj #&gt; #&gt; Simulated Annealing Feature Selection #&gt; #&gt; 366 samples #&gt; 12 predictors #&gt; #&gt; Maximum search iterations: 10 #&gt; Restart after 5 iterations without improvement (0.3 restarts on average) #&gt; #&gt; Internal performance values: RMSE, Rsquared #&gt; Subset selection driven to minimize internal RMSE #&gt; #&gt; External performance values: RMSE, Rsquared, MAE #&gt; Best iteration chose by minimizing external RMSE #&gt; External resampling method: Cross-Validated (10 fold, repeated 3 times) #&gt; #&gt; During resampling: #&gt; * the top 5 selected variables (out of a possible 12): #&gt; Temperature_Sandburg (80%), Month (66.7%), Pressure_gradient (66.7%), Temperature_ElMonte (63.3%), Visibility (60%) #&gt; * on average, 6.5 variables were selected (min = 3, max = 11) #&gt; #&gt; In the final search using the entire training set: #&gt; * 6 features selected at iteration 9 including: #&gt; Day_of_week, pressure_height, Wind_speed, Humidity, Inversion_base_height ... #&gt; * external performance at this iteration is #&gt; #&gt; RMSE Rsquared MAE #&gt; 4.108 0.743 3.111 # Optimal variables print(sa_obj$optVariables) #&gt; [1] &quot;Day_of_week&quot; &quot;pressure_height&quot; &quot;Wind_speed&quot; #&gt; [4] &quot;Humidity&quot; &quot;Inversion_base_height&quot; &quot;Pressure_gradient&quot; 10.10 9. Information Value and Weights of Evidence The Information Value can be used to judge how important a given categorical variable is in explaining the binary Y variable. It goes well with logistic regression and other classification models that can model binary variables. Let’s try to find out how important the categorical variables are in predicting if an individual will earn &gt; 50k from the adult.csv dataset. Just run the code below to import the dataset. library(InformationValue) # online data # inputData &lt;- read.csv(&quot;http://rstatistics.net/wp-content/uploads/2015/09/adult.csv&quot;) inputData &lt;- read.csv(file.path(data_raw_dir, &quot;adult.csv&quot;)) print(head(inputData)) #&gt; AGE WORKCLASS FNLWGT EDUCATION EDUCATIONNUM MARITALSTATUS #&gt; 1 39 State-gov 77516 Bachelors 13 Never-married #&gt; 2 50 Self-emp-not-inc 83311 Bachelors 13 Married-civ-spouse #&gt; 3 38 Private 215646 HS-grad 9 Divorced #&gt; 4 53 Private 234721 11th 7 Married-civ-spouse #&gt; 5 28 Private 338409 Bachelors 13 Married-civ-spouse #&gt; 6 37 Private 284582 Masters 14 Married-civ-spouse #&gt; OCCUPATION RELATIONSHIP RACE SEX CAPITALGAIN CAPITALLOSS #&gt; 1 Adm-clerical Not-in-family White Male 2174 0 #&gt; 2 Exec-managerial Husband White Male 0 0 #&gt; 3 Handlers-cleaners Not-in-family White Male 0 0 #&gt; 4 Handlers-cleaners Husband Black Male 0 0 #&gt; 5 Prof-specialty Wife Black Female 0 0 #&gt; 6 Exec-managerial Wife White Female 0 0 #&gt; HOURSPERWEEK NATIVECOUNTRY ABOVE50K #&gt; 1 40 United-States 0 #&gt; 2 13 United-States 0 #&gt; 3 40 United-States 0 #&gt; 4 40 United-States 0 #&gt; 5 40 Cuba 0 #&gt; 6 40 United-States 0 # Choose Categorical Variables to compute Info Value. cat_vars &lt;- c (&quot;WORKCLASS&quot;, &quot;EDUCATION&quot;, &quot;MARITALSTATUS&quot;, &quot;OCCUPATION&quot;, &quot;RELATIONSHIP&quot;, &quot;RACE&quot;, &quot;SEX&quot;, &quot;NATIVECOUNTRY&quot;) # get all categorical variables factor_vars &lt;- cat_vars # Init Output df_iv &lt;- data.frame(VARS=cat_vars, IV=numeric(length(cat_vars)), STRENGTH=character(length(cat_vars)), stringsAsFactors = F) # init output dataframe # Get Information Value for each variable for (factor_var in factor_vars){ df_iv[df_iv$VARS == factor_var, &quot;IV&quot;] &lt;- InformationValue::IV(X=inputData[, factor_var], Y=inputData$ABOVE50K) df_iv[df_iv$VARS == factor_var, &quot;STRENGTH&quot;] &lt;- attr(InformationValue::IV(X=inputData[, factor_var], Y=inputData$ABOVE50K), &quot;howgood&quot;) } # Sort df_iv &lt;- df_iv[order(-df_iv$IV), ] df_iv #&gt; VARS IV STRENGTH #&gt; 5 RELATIONSHIP 1.5356 Highly Predictive #&gt; 3 MARITALSTATUS 1.3388 Highly Predictive #&gt; 4 OCCUPATION 0.7762 Highly Predictive #&gt; 2 EDUCATION 0.7411 Highly Predictive #&gt; 7 SEX 0.3033 Highly Predictive #&gt; 1 WORKCLASS 0.1634 Highly Predictive #&gt; 8 NATIVECOUNTRY 0.0794 Somewhat Predictive #&gt; 6 RACE 0.0693 Somewhat Predictive Here is what the quantum of Information Value means: Less than 0.02, then the predictor is not useful for modeling (separating the Goods from the Bads) 0.02 to 0.1, then the predictor has only a weak relationship. 0.1 to 0.3, then the predictor has a medium strength relationship. 0.3 or higher, then the predictor has a strong relationship. That was about IV. Then what is Weight of Evidence? Weights of evidence can be useful to find out how important a given categorical variable is in explaining the ‘events’ (called ‘Goods’ in below table.) The ‘Information Value’ of the categorical variable can then be derived from the respective WOE values. IV=(perc good of all goods−perc bad of all bads) *WOE The ‘WOETable’ below given the computation in more detail. WOETable(X=inputData[, &#39;WORKCLASS&#39;], Y=inputData$ABOVE50K) #&gt; CAT GOODS BADS TOTAL PCT_G PCT_B WOE IV #&gt; 1 ? 191 1645 1836 0.02429 0.066545 -1.008 0.042574 #&gt; 2 Federal-gov 371 589 960 0.04719 0.023827 0.683 0.015964 #&gt; 3 Local-gov 617 1476 2093 0.07848 0.059709 0.273 0.005131 #&gt; 4 Never-worked 7 7 7 0.00089 0.000283 1.146 0.000696 #&gt; 5 Private 4963 17733 22696 0.63126 0.717354 -0.128 0.011006 #&gt; 6 Self-emp-inc 622 494 1116 0.07911 0.019984 1.376 0.081363 #&gt; 7 Self-emp-not-inc 724 1817 2541 0.09209 0.073503 0.225 0.004190 #&gt; 8 State-gov 353 945 1298 0.04490 0.038228 0.161 0.001073 #&gt; 9 Without-pay 14 14 14 0.00178 0.000566 1.146 0.001391 The total IV of a variable is the sum of IV’s of its categories. 10.11 10. DALEX Package The DALEX is a powerful package that explains various things about the variables used in an ML model. For example, using the variable_dropout() function you can find out how important a variable is based on a dropout loss, that is how much loss is incurred by removing a variable from the model. Apart from this, it also has the single_variable() function that gives you an idea of how the model’s output will change by changing the values of one of the X’s in the model. It also has the single_prediction() that can decompose a single model prediction so as to understand which variable caused what effect in predicting the value of Y. library(randomForest) library(DALEX) # Load data # inputData &lt;- read.csv(&quot;http://rstatistics.net/wp-content/uploads/2015/09/adult.csv&quot;) inputData &lt;- read.csv(file.path(data_raw_dir, &quot;adult.csv&quot;)) # Train random forest model rf_mod &lt;- randomForest(factor(ABOVE50K) ~ ., data=inputData, ntree=100) rf_mod #&gt; #&gt; Call: #&gt; randomForest(formula = factor(ABOVE50K) ~ ., data = inputData, ntree = 100) #&gt; Type of random forest: classification #&gt; Number of trees: 100 #&gt; No. of variables tried at each split: 3 #&gt; #&gt; OOB estimate of error rate: 13.6% #&gt; Confusion matrix: #&gt; 0 1 class.error #&gt; 0 23051 1669 0.0675 #&gt; 1 2754 5087 0.3512 # Variable importance with DALEX explained_rf &lt;- explain(rf_mod, data=inputData, y=inputData$ABOVE50K) # Get the variable importances varimps = variable_dropout(explained_rf, type=&#39;raw&#39;) print(varimps) #&gt; variable dropout_loss label #&gt; 1 _full_model_ 31.6 randomForest #&gt; 2 ABOVE50K 31.6 randomForest #&gt; 3 RACE 36.6 randomForest #&gt; 4 SEX 39.4 randomForest #&gt; 5 CAPITALLOSS 39.9 randomForest #&gt; 6 NATIVECOUNTRY 40.3 randomForest #&gt; 7 WORKCLASS 51.0 randomForest #&gt; 8 CAPITALGAIN 53.8 randomForest #&gt; 9 FNLWGT 56.2 randomForest #&gt; 10 HOURSPERWEEK 56.7 randomForest #&gt; 11 EDUCATION 58.0 randomForest #&gt; 12 RELATIONSHIP 58.5 randomForest #&gt; 13 EDUCATIONNUM 59.2 randomForest #&gt; 14 MARITALSTATUS 71.0 randomForest #&gt; 15 OCCUPATION 83.1 randomForest #&gt; 16 AGE 86.8 randomForest #&gt; 17 _baseline_ 304.4 randomForest plot(varimps) 10.12 Conclusion Hope you find these methods useful. As it turns out different methods showed different variables as important, or at least the degree of importance changed. This need not be a conflict, because each method gives a different perspective of how the variable can be useful depending on how the algorithms learn Y ~ x. So its cool. If you find any code breaks or bugs, report the issue here or just write it below. "],
["logistic-regression-diabetes.html", "Chapter 11 Logistic Regression. Diabetes 11.1 Introduction 11.2 Exploring the data 11.3 Logistic regression with R 11.4 A second model 11.5 Classification rate and confusion matrix 11.6 Plots and decision boundaries", " Chapter 11 Logistic Regression. Diabetes 11.1 Introduction Source: https://github.com/AntoineGuillot2/Logistic-Regression-R/blob/master/script.R Source: http://enhancedatascience.com/2017/04/26/r-basics-logistic-regression-with-r/ Data: https://www.kaggle.com/uciml/pima-indians-diabetes-database The goal of logistic regression is to predict whether an outcome will be positive (aka 1) or negative (i.e: 0). Some real life example could be: Will Emmanuel Macron win the French Presidential election or will he lose? Does Mr.X has this illness or not? Will this visitor click on my link or not? So, logistic regression can be used in a lot of binary classification cases and will often be run before more advanced methods. For this tutorial, we will use the diabetes detection dataset from Kaggle. This dataset contains data from Pima Indians Women such as the number of pregnancies, the blood pressure, the skin thickness, … the goal of the tutorial is to be able to detect diabetes using only these measures. 11.2 Exploring the data As usual, first, let’s take a look at our data. You can download the data here then please put the file diabetes.csv in your working directory. With the summary function, we can easily summarise the different variables: library(ggplot2) #&gt; Registered S3 methods overwritten by &#39;ggplot2&#39;: #&gt; method from #&gt; [.quosures rlang #&gt; c.quosures rlang #&gt; print.quosures rlang library(data.table) DiabetesData &lt;- data.table(read.csv(file.path(data_raw_dir, &#39;diabetes.csv&#39;))) # Quick data summary summary(DiabetesData) #&gt; Pregnancies Glucose BloodPressure SkinThickness #&gt; Min. : 0.00 Min. : 0 Min. : 0.0 Min. : 0.0 #&gt; 1st Qu.: 1.00 1st Qu.: 99 1st Qu.: 62.0 1st Qu.: 0.0 #&gt; Median : 3.00 Median :117 Median : 72.0 Median :23.0 #&gt; Mean : 3.85 Mean :121 Mean : 69.1 Mean :20.5 #&gt; 3rd Qu.: 6.00 3rd Qu.:140 3rd Qu.: 80.0 3rd Qu.:32.0 #&gt; Max. :17.00 Max. :199 Max. :122.0 Max. :99.0 #&gt; Insulin BMI DiabetesPedigreeFunction Age #&gt; Min. : 0 Min. : 0.0 Min. :0.078 Min. :21.0 #&gt; 1st Qu.: 0 1st Qu.:27.3 1st Qu.:0.244 1st Qu.:24.0 #&gt; Median : 30 Median :32.0 Median :0.372 Median :29.0 #&gt; Mean : 80 Mean :32.0 Mean :0.472 Mean :33.2 #&gt; 3rd Qu.:127 3rd Qu.:36.6 3rd Qu.:0.626 3rd Qu.:41.0 #&gt; Max. :846 Max. :67.1 Max. :2.420 Max. :81.0 #&gt; Outcome #&gt; Min. :0.000 #&gt; 1st Qu.:0.000 #&gt; Median :0.000 #&gt; Mean :0.349 #&gt; 3rd Qu.:1.000 #&gt; Max. :1.000 The mean of the outcome is 0.35 which shows an imbalance between the classes. However, the imbalance should not be too strong to be a problem. To understand the relationship between variables, a Scatter Plot Matrix will be used. To plot it, the GGally package was used. # Scatter plot matrix library(GGally) #&gt; Registered S3 method overwritten by &#39;GGally&#39;: #&gt; method from #&gt; +.gg ggplot2 ggpairs(DiabetesData, lower = list(continuous=&#39;smooth&#39;)) The correlations between explanatory variables do not seem too strong. Hence the model is not likely to suffer from multicollinearity. All explanatory variable are correlated with the Outcome. At first sight, glucose rate is the most important factor to detect the outcome. 11.3 Logistic regression with R After variable exploration, a first model can be fitted using the glm function. With stargazer, it is easy to get nice output in ASCII or even Latex. # first model: all features glm1 = glm(Outcome~., DiabetesData, family = binomial(link=&quot;logit&quot;)) summary(glm1) #&gt; #&gt; Call: #&gt; glm(formula = Outcome ~ ., family = binomial(link = &quot;logit&quot;), #&gt; data = DiabetesData) #&gt; #&gt; Deviance Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -2.557 -0.727 -0.416 0.727 2.930 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) -8.404696 0.716636 -11.73 &lt; 2e-16 *** #&gt; Pregnancies 0.123182 0.032078 3.84 0.00012 *** #&gt; Glucose 0.035164 0.003709 9.48 &lt; 2e-16 *** #&gt; BloodPressure -0.013296 0.005234 -2.54 0.01107 * #&gt; SkinThickness 0.000619 0.006899 0.09 0.92852 #&gt; Insulin -0.001192 0.000901 -1.32 0.18607 #&gt; BMI 0.089701 0.015088 5.95 2.8e-09 *** #&gt; DiabetesPedigreeFunction 0.945180 0.299147 3.16 0.00158 ** #&gt; Age 0.014869 0.009335 1.59 0.11119 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for binomial family taken to be 1) #&gt; #&gt; Null deviance: 993.48 on 767 degrees of freedom #&gt; Residual deviance: 723.45 on 759 degrees of freedom #&gt; AIC: 741.4 #&gt; #&gt; Number of Fisher Scoring iterations: 5 require(stargazer) #&gt; Loading required package: stargazer #&gt; #&gt; Please cite as: #&gt; Hlavac, Marek (2018). stargazer: Well-Formatted Regression and Summary Statistics Tables. #&gt; R package version 5.2.2. https://CRAN.R-project.org/package=stargazer stargazer(glm1,type=&#39;text&#39;) #&gt; #&gt; ==================================================== #&gt; Dependent variable: #&gt; --------------------------- #&gt; Outcome #&gt; ---------------------------------------------------- #&gt; Pregnancies 0.123*** #&gt; (0.032) #&gt; #&gt; Glucose 0.035*** #&gt; (0.004) #&gt; #&gt; BloodPressure -0.013** #&gt; (0.005) #&gt; #&gt; SkinThickness 0.001 #&gt; (0.007) #&gt; #&gt; Insulin -0.001 #&gt; (0.001) #&gt; #&gt; BMI 0.090*** #&gt; (0.015) #&gt; #&gt; DiabetesPedigreeFunction 0.945*** #&gt; (0.299) #&gt; #&gt; Age 0.015 #&gt; (0.009) #&gt; #&gt; Constant -8.400*** #&gt; (0.717) #&gt; #&gt; ---------------------------------------------------- #&gt; Observations 768 #&gt; Log Likelihood -362.000 #&gt; Akaike Inf. Crit. 741.000 #&gt; ==================================================== #&gt; Note: *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01 The overall model is significant. As expected the glucose rate has the lowest p-value of all the variables. However, Age, Insulin and Skin Thickness are not good predictors of Diabetes. 11.4 A second model Since some variables are not significant, removing them is a good way to improve model robustness. In the second model, SkinThickness, Insulin, and Age are removed. # second model: selected features glm2 = glm(Outcome~., data = DiabetesData[,c(1:3,6:7,9), with=F], family = binomial(link=&quot;logit&quot;)) summary(glm2) #&gt; #&gt; Call: #&gt; glm(formula = Outcome ~ ., family = binomial(link = &quot;logit&quot;), #&gt; data = DiabetesData[, c(1:3, 6:7, 9), with = F]) #&gt; #&gt; Deviance Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -2.793 -0.736 -0.419 0.725 2.955 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) -7.95495 0.67582 -11.77 &lt; 2e-16 *** #&gt; Pregnancies 0.15349 0.02784 5.51 3.5e-08 *** #&gt; Glucose 0.03466 0.00339 10.21 &lt; 2e-16 *** #&gt; BloodPressure -0.01201 0.00503 -2.39 0.017 * #&gt; BMI 0.08483 0.01412 6.01 1.9e-09 *** #&gt; DiabetesPedigreeFunction 0.91063 0.29403 3.10 0.002 ** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for binomial family taken to be 1) #&gt; #&gt; Null deviance: 993.48 on 767 degrees of freedom #&gt; Residual deviance: 728.56 on 762 degrees of freedom #&gt; AIC: 740.6 #&gt; #&gt; Number of Fisher Scoring iterations: 5 11.5 Classification rate and confusion matrix Now that we have our model, let’s access its performance. # Correctly classified observations mean((glm2$fitted.values&gt;0.5)==DiabetesData$Outcome) #&gt; [1] 0.775 Around 77.4% of all observations are correctly classified. Due to class imbalance, we need to go further with a confusion matrix. ###Confusion matrix count RP=sum((glm2$fitted.values&gt;=0.5)==DiabetesData$Outcome &amp; DiabetesData$Outcome==1) FP=sum((glm2$fitted.values&gt;=0.5)!=DiabetesData$Outcome &amp; DiabetesData$Outcome==0) RN=sum((glm2$fitted.values&gt;=0.5)==DiabetesData$Outcome &amp; DiabetesData$Outcome==0) FN=sum((glm2$fitted.values&gt;=0.5)!=DiabetesData$Outcome &amp; DiabetesData$Outcome==1) confMat&lt;-matrix(c(RP,FP,FN,RN),ncol = 2) colnames(confMat)&lt;-c(&quot;Pred Diabetes&quot;,&#39;Pred no diabetes&#39;) rownames(confMat)&lt;-c(&quot;Real Diabetes&quot;,&#39;Real no diabetes&#39;) confMat #&gt; Pred Diabetes Pred no diabetes #&gt; Real Diabetes 154 114 #&gt; Real no diabetes 59 441 The model is good to detect people who do not have diabetes. However, its performance on ill people is not great (only 154 out of 268 have been correctly classified). You can also get the percentage of Real/False Positive/Negative: # Confusion matrix proportion RPR=RP/sum(DiabetesData$Outcome==1)*100 FNR=FN/sum(DiabetesData$Outcome==1)*100 FPR=FP/sum(DiabetesData$Outcome==0)*100 RNR=RN/sum(DiabetesData$Outcome==0)*100 confMat&lt;-matrix(c(RPR,FPR,FNR,RNR),ncol = 2) colnames(confMat)&lt;-c(&quot;Pred Diabetes&quot;,&#39;Pred no diabetes&#39;) rownames(confMat)&lt;-c(&quot;Real Diabetes&quot;,&#39;Real no diabetes&#39;) confMat #&gt; Pred Diabetes Pred no diabetes #&gt; Real Diabetes 57.5 42.5 #&gt; Real no diabetes 11.8 88.2 And here is the matrix, 57.5% of people with diabetes are correctly classified. A way to improve the false negative rate would lower the detection threshold. However, as a consequence, the false positive rate would increase. 11.6 Plots and decision boundaries The two strongest predictors of the outcome are Glucose rate and BMI. High glucose rate and high BMI are strong indicators of Diabetes. # Plot and decision boundaries DiabetesData$Predicted &lt;- glm2$fitted.values ggplot(DiabetesData, aes(x = BMI, y = Glucose, color = Predicted &gt; 0.5)) + geom_point(size=2, alpha=0.5) ggplot(DiabetesData, aes(x=BMI, y = Glucose, color=Outcome == (Predicted &gt; 0.5))) + geom_point(size=2, alpha=0.5) We can also plot both BMI and glucose against the outcomes, the other variables are taken at their mean level. range(DiabetesData$BMI) #&gt; [1] 0.0 67.1 # BMI vs predicted BMI_plot = data.frame(BMI = ((min(DiabetesData$BMI-2)*100): (max(DiabetesData$BMI+2)*100))/100, Glucose = mean(DiabetesData$Glucose), Pregnancies = mean(DiabetesData$Pregnancies), BloodPressure = mean(DiabetesData$BloodPressure), DiabetesPedigreeFunction = mean(DiabetesData$DiabetesPedigreeFunction)) BMI_plot$Predicted = predict(glm2, newdata = BMI_plot, type = &#39;response&#39;) ggplot(BMI_plot, aes(x = BMI, y = Predicted)) + geom_line() range(BMI_plot$BMI) #&gt; [1] -2.0 69.1 range(DiabetesData$Glucose) #&gt; [1] 0 199 # Glucose vs predicted Glucose_plot=data.frame(Glucose = ((min(DiabetesData$Glucose-2)*100): (max(DiabetesData$Glucose+2)*100))/100, BMI=mean(DiabetesData$BMI), Pregnancies=mean(DiabetesData$Pregnancies), BloodPressure=mean(DiabetesData$BloodPressure), DiabetesPedigreeFunction=mean(DiabetesData$DiabetesPedigreeFunction)) Glucose_plot$Predicted = predict(glm2, newdata = Glucose_plot, type = &#39;response&#39;) ggplot(Glucose_plot, aes(x = Glucose, y = Predicted)) + geom_line() range(Glucose_plot$Glucose) #&gt; [1] -2 201 "],
["flu-prediction-with-mice.html", "Chapter 12 Flu Prediction with mice 12.1 The data 12.2 Features 12.3 Imputing missing values 12.4 Test, train and validation data sets 12.5 Machine Learning algorithms 12.6 Comparing accuracy of models", " Chapter 12 Flu Prediction with mice Source: https://shirinsplayground.netlify.com/2018/04/flu_prediction/ library(outbreaks) library(tidyverse) #&gt; Registered S3 methods overwritten by &#39;ggplot2&#39;: #&gt; method from #&gt; [.quosures rlang #&gt; c.quosures rlang #&gt; print.quosures rlang #&gt; Registered S3 method overwritten by &#39;rvest&#39;: #&gt; method from #&gt; read_xml.response xml2 #&gt; ── Attaching packages ────────────────────────────────────────────────────── tidyverse 1.2.1 ── #&gt; ✔ ggplot2 3.1.1 ✔ purrr 0.3.2 #&gt; ✔ tibble 2.1.1 ✔ dplyr 0.8.0.1 #&gt; ✔ tidyr 0.8.3 ✔ stringr 1.4.0 #&gt; ✔ readr 1.3.1 ✔ forcats 0.4.0 #&gt; ── Conflicts ───────────────────────────────────────────────────────── tidyverse_conflicts() ── #&gt; ✖ dplyr::filter() masks stats::filter() #&gt; ✖ dplyr::lag() masks stats::lag() library(plyr) #&gt; ------------------------------------------------------------------------- #&gt; You have loaded plyr after dplyr - this is likely to cause problems. #&gt; If you need functions from both plyr and dplyr, please load plyr first, then dplyr: #&gt; library(plyr); library(dplyr) #&gt; ------------------------------------------------------------------------- #&gt; #&gt; Attaching package: &#39;plyr&#39; #&gt; The following objects are masked from &#39;package:dplyr&#39;: #&gt; #&gt; arrange, count, desc, failwith, id, mutate, rename, summarise, #&gt; summarize #&gt; The following object is masked from &#39;package:purrr&#39;: #&gt; #&gt; compact library(mice) #&gt; Loading required package: lattice #&gt; #&gt; Attaching package: &#39;mice&#39; #&gt; The following object is masked from &#39;package:tidyr&#39;: #&gt; #&gt; complete #&gt; The following objects are masked from &#39;package:base&#39;: #&gt; #&gt; cbind, rbind library(caret) #&gt; #&gt; Attaching package: &#39;caret&#39; #&gt; The following object is masked from &#39;package:purrr&#39;: #&gt; #&gt; lift library(purrr) library(&quot;tibble&quot;) library(&quot;dplyr&quot;) library(&quot;tidyr&quot;) Since I migrated my blog from Github Pages to blogdown and Netlify, I wanted to start migrating (most of) my old posts too - and use that opportunity to update them and make sure the code still works. Here I am updating my very first machine learning post from 27 Nov 2016: Can we predict flu deaths with Machine Learning and R?. Changes are marked as bold comments. The main changes I made are: using the tidyverse more consistently throughout the analysis focusing on comparing multiple imputations from the mice package, rather than comparing different algorithms using purrr, map(), nest() and unnest() to model and predict the machine learning algorithm over the different imputed datasets Among the many nice R packages containing data collections is the outbreaks package. It contains a dataset on epidemics and among them is data from the 2013 outbreak of influenza A H7N9 in China as analysed by Kucharski et al. (2014): A. Kucharski, H. Mills, A. Pinsent, C. Fraser, M. Van Kerkhove, C. A. Donnelly, and S. Riley. 2014. Distinguishing between reservoir exposure and human-to-human transmission for emerging pathogens using case onset data. PLOS Currents Outbreaks. Mar 7, edition 1. doi: 10.1371/currents.outbreaks.e1473d9bfc99d080ca242139a06c455f. A. Kucharski, H. Mills, A. Pinsent, C. Fraser, M. Van Kerkhove, C. A. Donnelly, and S. Riley. 2014. Data from: Distinguishing between reservoir exposure and human-to-human transmission for emerging pathogens using case onset data. Dryad Digital Repository. http://dx.doi.org/10.5061/dryad.2g43n. I will be using their data as an example to show how to use Machine Learning algorithms for predicting disease outcome. 12.1 The data The dataset contains case ID, date of onset, date of hospitalization, date of outcome, gender, age, province and of course outcome: Death or Recovery. 12.1.1 Pre-processing Change: variable names (i.e. column names) have been renamed, dots have been replaced with underscores, letters are all lower case now. Change: I am using the tidyverse notation more consistently. First, I’m doing some preprocessing, including: renaming missing data as NA adding an ID column setting column types gathering date columns changing factor names of dates (to make them look nicer in plots) and of province (to combine provinces with few cases) from1 &lt;- c(&quot;date_of_onset&quot;, &quot;date_of_hospitalisation&quot;, &quot;date_of_outcome&quot;) to1 &lt;- c(&quot;date of onset&quot;, &quot;date of hospitalisation&quot;, &quot;date of outcome&quot;) from2 &lt;- c(&quot;Anhui&quot;, &quot;Beijing&quot;, &quot;Fujian&quot;, &quot;Guangdong&quot;, &quot;Hebei&quot;, &quot;Henan&quot;, &quot;Hunan&quot;, &quot;Jiangxi&quot;, &quot;Shandong&quot;, &quot;Taiwan&quot;) to2 &lt;- rep(&quot;Other&quot;, 10) fluH7N9_china_2013$age[which(fluH7N9_china_2013$age == &quot;?&quot;)] &lt;- NA fluH7N9_china_2013_gather &lt;- fluH7N9_china_2013 %&gt;% mutate(case_id = paste(&quot;case&quot;, case_id, sep = &quot;_&quot;), age = as.numeric(age)) %&gt;% gather(Group, Date, date_of_onset:date_of_outcome) %&gt;% mutate(Group = as.factor(mapvalues(Group, from = from1, to = to1)), province = mapvalues(province, from = from2, to = to2)) fluH7N9_china_2013 &lt;- as.tibble(fluH7N9_china_2013) #&gt; Warning: `as.tibble()` is deprecated, use `as_tibble()` (but mind the new semantics). #&gt; This warning is displayed once per session. fluH7N9_china_2013_gather &lt;- as.tibble(fluH7N9_china_2013_gather) print(fluH7N9_china_2013) #&gt; # A tibble: 136 x 8 #&gt; case_id date_of_onset date_of_hospita… date_of_outcome outcome gender #&gt; &lt;fct&gt; &lt;date&gt; &lt;date&gt; &lt;date&gt; &lt;fct&gt; &lt;fct&gt; #&gt; 1 1 2013-02-19 NA 2013-03-04 Death m #&gt; 2 2 2013-02-27 2013-03-03 2013-03-10 Death m #&gt; 3 3 2013-03-09 2013-03-19 2013-04-09 Death f #&gt; 4 4 2013-03-19 2013-03-27 NA &lt;NA&gt; f #&gt; 5 5 2013-03-19 2013-03-30 2013-05-15 Recover f #&gt; 6 6 2013-03-21 2013-03-28 2013-04-26 Death f #&gt; # … with 130 more rows, and 2 more variables: age &lt;fct&gt;, province &lt;fct&gt; I’m also adding a third gender level for unknown gender levels(fluH7N9_china_2013_gather$gender) &lt;- c(levels(fluH7N9_china_2013_gather$gender), &quot;unknown&quot;) fluH7N9_china_2013_gather$gender[is.na(fluH7N9_china_2013_gather$gender)] &lt;- &quot;unknown&quot; print(fluH7N9_china_2013_gather) #&gt; # A tibble: 408 x 7 #&gt; case_id outcome gender age province Group Date #&gt; &lt;chr&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;date&gt; #&gt; 1 case_1 Death m 58 Shanghai date of onset 2013-02-19 #&gt; 2 case_2 Death m 7 Shanghai date of onset 2013-02-27 #&gt; 3 case_3 Death f 11 Other date of onset 2013-03-09 #&gt; 4 case_4 &lt;NA&gt; f 18 Jiangsu date of onset 2013-03-19 #&gt; 5 case_5 Recover f 20 Jiangsu date of onset 2013-03-19 #&gt; 6 case_6 Death f 9 Jiangsu date of onset 2013-03-21 #&gt; # … with 402 more rows For plotting, I am defining a custom ggplot2 theme: my_theme &lt;- function(base_size = 12, base_family = &quot;sans&quot;){ theme_minimal(base_size = base_size, base_family = base_family) + theme( axis.text = element_text(size = 12), axis.text.x = element_text(angle = 45, vjust = 0.5, hjust = 0.5), axis.title = element_text(size = 14), panel.grid.major = element_line(color = &quot;grey&quot;), panel.grid.minor = element_blank(), panel.background = element_rect(fill = &quot;aliceblue&quot;), strip.background = element_rect(fill = &quot;lightgrey&quot;, color = &quot;grey&quot;, size = 1), strip.text = element_text(face = &quot;bold&quot;, size = 12, color = &quot;black&quot;), legend.position = &quot;bottom&quot;, legend.justification = &quot;top&quot;, legend.box = &quot;horizontal&quot;, legend.box.background = element_rect(colour = &quot;grey50&quot;), legend.background = element_blank(), panel.border = element_rect(color = &quot;grey&quot;, fill = NA, size = 0.5) ) } And use that theme to visualize the data: ggplot(data = fluH7N9_china_2013_gather, aes(x = Date, y = age, fill = outcome)) + stat_density2d(aes(alpha = ..level..), geom = &quot;polygon&quot;) + geom_jitter(aes(color = outcome, shape = gender), size = 1.5) + geom_rug(aes(color = outcome)) + scale_y_continuous(limits = c(0, 90)) + labs( fill = &quot;Outcome&quot;, color = &quot;Outcome&quot;, alpha = &quot;Level&quot;, shape = &quot;Gender&quot;, x = &quot;Date in 2013&quot;, y = &quot;Age&quot;, title = &quot;2013 Influenza A H7N9 cases in China&quot;, subtitle = &quot;Dataset from &#39;outbreaks&#39; package (Kucharski et al. 2014)&quot;, caption = &quot;&quot; ) + facet_grid(Group ~ province) + my_theme() + scale_shape_manual(values = c(15, 16, 17)) + scale_color_brewer(palette=&quot;Set1&quot;, na.value = &quot;grey50&quot;) + scale_fill_brewer(palette=&quot;Set1&quot;) #&gt; Warning: Removed 149 rows containing non-finite values (stat_density2d). #&gt; Warning: Removed 149 rows containing missing values (geom_point). ggplot(data = fluH7N9_china_2013_gather, aes(x = Date, y = age, color = outcome)) + geom_point(aes(color = outcome, shape = gender), size = 1.5, alpha = 0.6) + geom_path(aes(group = case_id)) + facet_wrap( ~ province, ncol = 2) + my_theme() + scale_shape_manual(values = c(15, 16, 17)) + scale_color_brewer(palette=&quot;Set1&quot;, na.value = &quot;grey50&quot;) + scale_fill_brewer(palette=&quot;Set1&quot;) + labs( color = &quot;Outcome&quot;, shape = &quot;Gender&quot;, x = &quot;Date in 2013&quot;, y = &quot;Age&quot;, title = &quot;2013 Influenza A H7N9 cases in China&quot;, subtitle = &quot;Dataset from &#39;outbreaks&#39; package (Kucharski et al. 2014)&quot;, caption = &quot;\\nTime from onset of flu to outcome.&quot; ) #&gt; Warning: Removed 149 rows containing missing values (geom_point). #&gt; Warning: Removed 122 rows containing missing values (geom_path). 12.2 Features In machine learning-speak features are what we call the variables used for model training. Using the right features dramatically influences the accuracy and success of your model. For this example, I am keeping age, but I am also generating new features from the date information and converting gender and province into numerical values. delta_dates &lt;- function(onset, ref) { d2 = as.Date(as.character(onset), format = &quot;%Y-%m-%d&quot;) d1 = as.Date(as.character(ref), format = &quot;%Y-%m-%d&quot;) as.numeric(as.character(gsub(&quot; days&quot;, &quot;&quot;, d1 - d2))) } dataset &lt;- fluH7N9_china_2013 %&gt;% mutate( hospital = as.factor(ifelse(is.na(date_of_hospitalisation), 0, 1)), gender_f = as.factor(ifelse(gender == &quot;f&quot;, 1, 0)), province_Jiangsu = as.factor(ifelse(province == &quot;Jiangsu&quot;, 1, 0)), province_Shanghai = as.factor(ifelse(province == &quot;Shanghai&quot;, 1, 0)), province_Zhejiang = as.factor(ifelse(province == &quot;Zhejiang&quot;, 1, 0)), province_other = as.factor(ifelse(province == &quot;Zhejiang&quot; | province == &quot;Jiangsu&quot; | province == &quot;Shanghai&quot;, 0, 1)), days_onset_to_outcome = delta_dates(date_of_onset, date_of_outcome), days_onset_to_hospital = delta_dates(date_of_onset, date_of_hospitalisation), age = age, early_onset = as.factor(ifelse(date_of_onset &lt; summary(date_of_onset)[[3]], 1, 0)), early_outcome = as.factor(ifelse(date_of_outcome &lt; summary(date_of_outcome)[[3]], 1, 0)) ) %&gt;% subset(select = -c(2:4, 6, 8)) rownames(dataset) &lt;- dataset$case_id #&gt; Warning: Setting row names on a tibble is deprecated. dataset[, -2] &lt;- as.numeric(as.matrix(dataset[, -2])) print(dataset) #&gt; # A tibble: 136 x 13 #&gt; case_id outcome age hospital gender_f province_Jiangsu province_Shangh… #&gt; * &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 Death 87 0 0 0 1 #&gt; 2 2 Death 27 1 0 0 1 #&gt; 3 3 Death 35 1 1 0 0 #&gt; 4 4 &lt;NA&gt; 45 1 1 1 0 #&gt; 5 5 Recover 48 1 1 1 0 #&gt; 6 6 Death 32 1 1 1 0 #&gt; # … with 130 more rows, and 6 more variables: province_Zhejiang &lt;dbl&gt;, #&gt; # province_other &lt;dbl&gt;, days_onset_to_outcome &lt;dbl&gt;, #&gt; # days_onset_to_hospital &lt;dbl&gt;, early_onset &lt;dbl&gt;, early_outcome &lt;dbl&gt; summary(dataset$outcome) #&gt; Death Recover NA&#39;s #&gt; 32 47 57 12.3 Imputing missing values I am using the mice package for imputing missing values Note: Since publishing this blogpost I learned that the idea behind using mice is to compare different imputations to see how stable they are, instead of picking one imputed set as fixed for the remainder of the analysis. Therefore, I changed the focus of this post a little bit: in the old post I compared many different algorithms and their outcome; in this updated version I am only showing the Random Forest algorithm and focus on comparing the different imputed datasets. I am ignoring feature importance and feature plots because nothing changed compared to the old post. # plot the missing data in a matrix by variables md_pattern &lt;- md.pattern(dataset, rotate.names = TRUE) dataset_impute &lt;- mice(data = dataset[, -2], print = FALSE) #&gt; Warning: Number of logged events: 150 12.3.1 Generate a dataframe of five imputting strategies by default, mice() calculates five (m = 5) imputed data sets we can combine them all in one output with the complete(“long”) function I did not want to impute missing values in the outcome column, so I have to merge it back in with the imputed data # c(1,2): case_id, outcome datasets_complete &lt;- right_join(dataset[, c(1, 2)], complete(dataset_impute, &quot;long&quot;), by = &quot;case_id&quot;) %&gt;% mutate(.imp = as.factor(.imp)) %&gt;% select(-.id) %&gt;% print() #&gt; # A tibble: 680 x 14 #&gt; case_id outcome .imp age hospital gender_f province_Jiangsu #&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 Death 1 87 0 0 0 #&gt; 2 2 Death 1 27 1 0 0 #&gt; 3 3 Death 1 35 1 1 0 #&gt; 4 4 &lt;NA&gt; 1 45 1 1 1 #&gt; 5 5 Recover 1 48 1 1 1 #&gt; 6 6 Death 1 32 1 1 1 #&gt; # … with 674 more rows, and 7 more variables: province_Shanghai &lt;dbl&gt;, #&gt; # province_Zhejiang &lt;dbl&gt;, province_other &lt;dbl&gt;, #&gt; # days_onset_to_outcome &lt;dbl&gt;, days_onset_to_hospital &lt;dbl&gt;, #&gt; # early_onset &lt;dbl&gt;, early_outcome &lt;dbl&gt; Let’s compare the distributions of the five different imputed datasets: 12.3.2 plot effect of imputting on features datasets_complete %&gt;% gather(x, y, age:early_outcome) %&gt;% ggplot(aes(x = y, fill = .imp, color = .imp)) + geom_density(alpha = 0.20) + facet_wrap(~ x, ncol = 3, scales = &quot;free&quot;) + scale_fill_brewer(palette=&quot;Set1&quot;, na.value = &quot;grey50&quot;) + scale_color_brewer(palette=&quot;Set1&quot;, na.value = &quot;grey50&quot;) + my_theme() 12.4 Test, train and validation data sets Now, we can go ahead with machine learning! The dataset contains a few missing values in the outcome column; those will be the test set used for final predictions (see the old blog post for this). length(which(is.na(datasets_complete$outcome))) length(which(!is.na(datasets_complete$outcome))) #&gt; [1] 285 #&gt; [1] 395 train_index &lt;- which(is.na(datasets_complete$outcome)) train_data &lt;- datasets_complete[-train_index, ] test_data &lt;- datasets_complete[train_index, -2] # remove variable outcome print(train_data) #&gt; # A tibble: 395 x 14 #&gt; case_id outcome .imp age hospital gender_f province_Jiangsu #&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 Death 1 87 0 0 0 #&gt; 2 2 Death 1 27 1 0 0 #&gt; 3 3 Death 1 35 1 1 0 #&gt; 4 5 Recover 1 48 1 1 1 #&gt; 5 6 Death 1 32 1 1 1 #&gt; 6 7 Death 1 83 1 0 1 #&gt; # … with 389 more rows, and 7 more variables: province_Shanghai &lt;dbl&gt;, #&gt; # province_Zhejiang &lt;dbl&gt;, province_other &lt;dbl&gt;, #&gt; # days_onset_to_outcome &lt;dbl&gt;, days_onset_to_hospital &lt;dbl&gt;, #&gt; # early_onset &lt;dbl&gt;, early_outcome &lt;dbl&gt; # outcome variable removed print(test_data) #&gt; # A tibble: 285 x 13 #&gt; case_id .imp age hospital gender_f province_Jiangsu province_Shangh… #&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 4 1 45 1 1 1 0 #&gt; 2 9 1 67 1 0 0 0 #&gt; 3 15 1 61 0 1 1 0 #&gt; 4 16 1 79 0 0 1 0 #&gt; 5 22 1 85 1 0 1 0 #&gt; 6 28 1 79 0 0 0 0 #&gt; # … with 279 more rows, and 6 more variables: province_Zhejiang &lt;dbl&gt;, #&gt; # province_other &lt;dbl&gt;, days_onset_to_outcome &lt;dbl&gt;, #&gt; # days_onset_to_hospital &lt;dbl&gt;, early_onset &lt;dbl&gt;, early_outcome &lt;dbl&gt; The remainder of the data will be used for modeling. Here, I am splitting the data into 70% training and 30% test data. Because I want to model each imputed dataset separately, I am using the nest() and map() functions. train_data_nest &lt;- train_data %&gt;% group_by(.imp) %&gt;% nest() %&gt;% print() #&gt; # A tibble: 5 x 2 #&gt; .imp data #&gt; &lt;fct&gt; &lt;list&gt; #&gt; 1 1 &lt;tibble [79 × 13]&gt; #&gt; 2 2 &lt;tibble [79 × 13]&gt; #&gt; 3 3 &lt;tibble [79 × 13]&gt; #&gt; 4 4 &lt;tibble [79 × 13]&gt; #&gt; 5 5 &lt;tibble [79 × 13]&gt; # split the training data in validation training and validation test set.seed(42) val_data &lt;- train_data_nest %&gt;% mutate(val_index = map(data, ~ createDataPartition(.$outcome, p = 0.7, list = FALSE)), val_train_data = map2(data, val_index, ~ .x[.y, ]), val_test_data = map2(data, val_index, ~ .x[-.y, ])) %&gt;% print() #&gt; # A tibble: 5 x 5 #&gt; .imp data val_index val_train_data val_test_data #&gt; &lt;fct&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; #&gt; 1 1 &lt;tibble [79 × … &lt;int[,1] [56 × … &lt;tibble [56 × 13… &lt;tibble [23 × 1… #&gt; 2 2 &lt;tibble [79 × … &lt;int[,1] [56 × … &lt;tibble [56 × 13… &lt;tibble [23 × 1… #&gt; 3 3 &lt;tibble [79 × … &lt;int[,1] [56 × … &lt;tibble [56 × 13… &lt;tibble [23 × 1… #&gt; 4 4 &lt;tibble [79 × … &lt;int[,1] [56 × … &lt;tibble [56 × 13… &lt;tibble [23 × 1… #&gt; 5 5 &lt;tibble [79 × … &lt;int[,1] [56 × … &lt;tibble [56 × 13… &lt;tibble [23 × 1… 12.5 Machine Learning algorithms 12.5.1 Random Forest To make the code tidier, I am first defining the modeling function with the parameters I want. model_function &lt;- function(df) { caret::train(outcome ~ ., data = df, method = &quot;rf&quot;, preProcess = c(&quot;scale&quot;, &quot;center&quot;), trControl = trainControl(method = &quot;repeatedcv&quot;, number = 5, repeats = 3, verboseIter = FALSE)) } 12.5.2 Add model and prediction to nested dataframe and calculate Next, I am using the nested tibble from before to map() the model function, predict the outcome and calculate confusion matrices. 12.5.2.1 add model list-column val_data_model &lt;- val_data %&gt;% mutate(model = map(val_train_data, ~ model_function(.x))) %&gt;% select(-val_index) %&gt;% print() #&gt; # A tibble: 5 x 5 #&gt; .imp data val_train_data val_test_data model #&gt; &lt;fct&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; #&gt; 1 1 &lt;tibble [79 × 13]&gt; &lt;tibble [56 × 13]&gt; &lt;tibble [23 × 13]&gt; &lt;train&gt; #&gt; 2 2 &lt;tibble [79 × 13]&gt; &lt;tibble [56 × 13]&gt; &lt;tibble [23 × 13]&gt; &lt;train&gt; #&gt; 3 3 &lt;tibble [79 × 13]&gt; &lt;tibble [56 × 13]&gt; &lt;tibble [23 × 13]&gt; &lt;train&gt; #&gt; 4 4 &lt;tibble [79 × 13]&gt; &lt;tibble [56 × 13]&gt; &lt;tibble [23 × 13]&gt; &lt;train&gt; #&gt; 5 5 &lt;tibble [79 × 13]&gt; &lt;tibble [56 × 13]&gt; &lt;tibble [23 × 13]&gt; &lt;train&gt; 12.5.2.2 add prediction and confusion matrix list-columns set.seed(42) val_data_model &lt;- val_data_model %&gt;% mutate( predict = map2(model, val_test_data, ~ data.frame(prediction = predict(.x, .y[, -2]))), predict_prob = map2(model, val_test_data, ~ data.frame(outcome = .y[, 2], prediction = predict(.x, .y[, -2], type = &quot;prob&quot;))), confusion_matrix = map2(val_test_data, predict, ~ confusionMatrix(.x$outcome, .y$prediction)), confusion_matrix_tbl = map(confusion_matrix, ~ as.tibble(.x$table))) %&gt;% print() #&gt; # A tibble: 5 x 9 #&gt; .imp data val_train_data val_test_data model predict predict_prob #&gt; &lt;fct&gt; &lt;lis&gt; &lt;list&gt; &lt;list&gt; &lt;lis&gt; &lt;list&gt; &lt;list&gt; #&gt; 1 1 &lt;tib… &lt;tibble [56 ×… &lt;tibble [23 … &lt;tra… &lt;df[,1… &lt;df[,3] [23… #&gt; 2 2 &lt;tib… &lt;tibble [56 ×… &lt;tibble [23 … &lt;tra… &lt;df[,1… &lt;df[,3] [23… #&gt; 3 3 &lt;tib… &lt;tibble [56 ×… &lt;tibble [23 … &lt;tra… &lt;df[,1… &lt;df[,3] [23… #&gt; 4 4 &lt;tib… &lt;tibble [56 ×… &lt;tibble [23 … &lt;tra… &lt;df[,1… &lt;df[,3] [23… #&gt; 5 5 &lt;tib… &lt;tibble [56 ×… &lt;tibble [23 … &lt;tra… &lt;df[,1… &lt;df[,3] [23… #&gt; # … with 2 more variables: confusion_matrix &lt;list&gt;, #&gt; # confusion_matrix_tbl &lt;list&gt; Finally, we have a nested dataframe of 5 rows or cases, one per imputting strategy with its corresponding models and prediction results. 12.6 Comparing accuracy of models To compare how the different imputations did, I am plotting the confusion matrices: val_data_model_unnest &lt;- val_data_model %&gt;% unnest(confusion_matrix_tbl) %&gt;% print() #&gt; # A tibble: 20 x 4 #&gt; .imp Prediction Reference n #&gt; &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 1 Death Death 5 #&gt; 2 1 Recover Death 3 #&gt; 3 1 Death Recover 4 #&gt; 4 1 Recover Recover 11 #&gt; 5 2 Death Death 3 #&gt; 6 2 Recover Death 3 #&gt; # … with 14 more rows val_data_model_unnest %&gt;% ggplot(aes(x = Prediction, y = Reference, fill = n)) + facet_wrap(~ .imp, ncol = 5, scales = &quot;free&quot;) + geom_tile() + my_theme() and the prediction probabilities for correct and wrong predictions: val_data_model_gather &lt;- val_data_model %&gt;% unnest(predict_prob) %&gt;% gather(x, y, prediction.Death:prediction.Recover) %&gt;% print() #&gt; # A tibble: 230 x 4 #&gt; .imp outcome x y #&gt; &lt;fct&gt; &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 1 Death prediction.Death 0.758 #&gt; 2 1 Recover prediction.Death 0.864 #&gt; 3 1 Death prediction.Death 0.828 #&gt; 4 1 Death prediction.Death 0.828 #&gt; 5 1 Recover prediction.Death 0.342 #&gt; 6 1 Recover prediction.Death 0.552 #&gt; # … with 224 more rows val_data_model_gather %&gt;% ggplot(aes(x = x, y = y, fill = outcome)) + facet_wrap(~ .imp, ncol = 5) + geom_boxplot() + scale_fill_brewer(palette=&quot;Set1&quot;, na.value = &quot;grey50&quot;) + my_theme() Hope, you found that example interesting and helpful! sessionInfo() #&gt; R version 3.6.0 (2019-04-26) #&gt; Platform: x86_64-pc-linux-gnu (64-bit) #&gt; Running under: Ubuntu 18.04.3 LTS #&gt; #&gt; Matrix products: default #&gt; BLAS/LAPACK: /usr/lib/x86_64-linux-gnu/libopenblasp-r0.2.20.so #&gt; #&gt; locale: #&gt; [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C #&gt; [3] LC_TIME=en_US.UTF-8 LC_COLLATE=en_US.UTF-8 #&gt; [5] LC_MONETARY=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8 #&gt; [7] LC_PAPER=en_US.UTF-8 LC_NAME=C #&gt; [9] LC_ADDRESS=C LC_TELEPHONE=C #&gt; [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C #&gt; #&gt; attached base packages: #&gt; [1] stats graphics grDevices utils datasets methods base #&gt; #&gt; other attached packages: #&gt; [1] caret_6.0-84 mice_3.4.0 lattice_0.20-38 plyr_1.8.4 #&gt; [5] forcats_0.4.0 stringr_1.4.0 dplyr_0.8.0.1 purrr_0.3.2 #&gt; [9] readr_1.3.1 tidyr_0.8.3 tibble_2.1.1 ggplot2_3.1.1 #&gt; [13] tidyverse_1.2.1 outbreaks_1.5.0 logging_0.9-107 #&gt; #&gt; loaded via a namespace (and not attached): #&gt; [1] nlme_3.1-139 lubridate_1.7.4 RColorBrewer_1.1-2 #&gt; [4] httr_1.4.0 rprojroot_1.3-2 tools_3.6.0 #&gt; [7] backports_1.1.4 utf8_1.1.4 R6_2.4.0 #&gt; [10] rpart_4.1-15 lazyeval_0.2.2 colorspace_1.4-1 #&gt; [13] jomo_2.6-7 nnet_7.3-12 withr_2.1.2 #&gt; [16] tidyselect_0.2.5 compiler_3.6.0 cli_1.1.0 #&gt; [19] rvest_0.3.3 xml2_1.2.0 labeling_0.3 #&gt; [22] bookdown_0.10 scales_1.0.0 randomForest_4.6-14 #&gt; [25] digest_0.6.18 minqa_1.2.4 rmarkdown_1.12 #&gt; [28] pkgconfig_2.0.2 htmltools_0.3.6 lme4_1.1-21 #&gt; [31] rlang_0.3.4 readxl_1.3.1 rstudioapi_0.10 #&gt; [34] generics_0.0.2 jsonlite_1.6 ModelMetrics_1.2.2 #&gt; [37] magrittr_1.5 Matrix_1.2-17 Rcpp_1.0.1 #&gt; [40] munsell_0.5.0 fansi_0.4.0 stringi_1.4.3 #&gt; [43] yaml_2.2.0 MASS_7.3-51.4 recipes_0.1.5 #&gt; [46] grid_3.6.0 parallel_3.6.0 mitml_0.3-7 #&gt; [49] crayon_1.3.4 haven_2.1.0 splines_3.6.0 #&gt; [52] hms_0.4.2 zeallot_0.1.0 knitr_1.22 #&gt; [55] pillar_1.4.0 boot_1.3-22 reshape2_1.4.3 #&gt; [58] codetools_0.2-16 stats4_3.6.0 pan_1.6 #&gt; [61] glue_1.3.1 evaluate_0.13 data.table_1.12.2 #&gt; [64] modelr_0.1.4 vctrs_0.1.0 nloptr_1.2.1 #&gt; [67] foreach_1.4.4 cellranger_1.1.0 gtable_0.3.0 #&gt; [70] assertthat_0.2.1 xfun_0.6 gower_0.2.0 #&gt; [73] prodlim_2018.04.18 broom_0.5.2 e1071_1.7-1 #&gt; [76] class_7.3-15 survival_2.44-1.1 timeDate_3043.102 #&gt; [79] iterators_1.0.10 lava_1.6.5 ipred_0.9-9 "],
["what-is-hat-in-regression-output.html", "Chapter 13 What is .hat in regression output", " Chapter 13 What is .hat in regression output https://stats.stackexchange.com/a/256364/154908 Q. The augment() function in the broom package for R creates a dataframe of predicted values from a regression model. Columns created include the fitted values, the standard error of the fit and Cook’s distance. They also include something with which I’m not familar and that is the column .hat. library(broom) data(mtcars) m1 &lt;- lm(mpg ~ wt, data = mtcars) head(augment(m1)) #&gt; # A tibble: 6 x 10 #&gt; .rownames mpg wt .fitted .se.fit .resid .hat .sigma .cooksd #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Mazda RX4 21 2.62 23.3 0.634 -2.28 0.0433 3.07 1.33e-2 #&gt; 2 Mazda RX… 21 2.88 21.9 0.571 -0.920 0.0352 3.09 1.72e-3 #&gt; 3 Datsun 7… 22.8 2.32 24.9 0.736 -2.09 0.0584 3.07 1.54e-2 #&gt; 4 Hornet 4… 21.4 3.22 20.1 0.538 1.30 0.0313 3.09 3.02e-3 #&gt; 5 Hornet S… 18.7 3.44 18.9 0.553 -0.200 0.0329 3.10 7.60e-5 #&gt; 6 Valiant 18.1 3.46 18.8 0.555 -0.693 0.0332 3.10 9.21e-4 #&gt; # … with 1 more variable: .std.resid &lt;dbl&gt; # .hat vector augment(m1)$.hat #&gt; [1] 0.0433 0.0352 0.0584 0.0313 0.0329 0.0332 0.0354 0.0313 0.0314 0.0329 #&gt; [11] 0.0329 0.0558 0.0401 0.0419 0.1705 0.1953 0.1838 0.0661 0.1177 0.0956 #&gt; [21] 0.0503 0.0343 0.0328 0.0443 0.0445 0.0866 0.0704 0.1291 0.0313 0.0380 #&gt; [31] 0.0354 0.0377 Can anyone explain what this value is, and is it different between linear regression and logistic regression? A. Those would be the diagonal elements of the hat-matrix which describe the leverage each point has on its fitted values. If one fits: \\[\\vec{Y} = \\mathbf{X} \\vec {\\beta} + \\vec {\\epsilon}\\] then: \\[\\mathbf{H} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\] In this example: \\[ \\begin{pmatrix}Y_1\\\\ \\vdots\\\\ Y_{32}\\end{pmatrix} = \\begin{pmatrix} 1 &amp; 2.620\\\\ \\vdots\\\\ 1 &amp; 2.780 \\end{pmatrix} \\cdot \\begin{pmatrix} \\beta_0\\\\ \\beta_1 \\end{pmatrix} + \\begin{pmatrix}\\epsilon_1\\\\ \\vdots\\\\ \\epsilon_{32}\\end{pmatrix} \\] Then calculating this \\(\\mathbf{H}\\) matrix results in: library(MASS) wt &lt;- mtcars[, 6] X &lt;- matrix(cbind(rep(1, length(wt)), wt), ncol=2) H &lt;- X %*% ginv(t(X) %*% X) %*% t(X) Where this last matrix is a \\(32 \\times 32\\) matrix and contains these hat values on the diagonal. X 32x2 t(X) 2x32 X %*% t(X) 32x32 t(X) %*% X 2x2 ginv(t(X) %*% X) 2x2 ginv(t(X) %*% X) %*% t(X) 2x32 X %*% ginv(t(X) %*% X) 32x2 dim(ginv(t(X) %*% X) %*% t(X)) #&gt; [1] 2 32 x1 &lt;- X %*% ginv(t(X) %*% X) dim(x1) #&gt; [1] 32 2 dim(x1 %*% t(X)) #&gt; [1] 32 32 x2 &lt;- ginv(t(X) %*% X) %*% t(X) dim(x2) #&gt; [1] 2 32 dim(X %*% x2) #&gt; [1] 32 32 # this last matrix is a 32×32 matrix and contains these hat values on the diagonal. diag(H) #&gt; [1] 0.0433 0.0352 0.0584 0.0313 0.0329 0.0332 0.0354 0.0313 0.0314 0.0329 #&gt; [11] 0.0329 0.0558 0.0401 0.0419 0.1705 0.1953 0.1838 0.0661 0.1177 0.0956 #&gt; [21] 0.0503 0.0343 0.0328 0.0443 0.0445 0.0866 0.0704 0.1291 0.0313 0.0380 #&gt; [31] 0.0354 0.0377 "],
["q-q-normal-to-compare-data-to-distributions.html", "Chapter 14 Q-Q normal to compare data to distributions 14.1 Introduction 14.2 Why we want to compare emprirical vs theoretical distributions 14.3 The normal q-q plot 14.4 Using R’s built-in functions 14.5 Using the ggplot2 plotting environment", " Chapter 14 Q-Q normal to compare data to distributions 14.1 Introduction https://mgimond.github.io/ES218/Week06a.html Thus far, we have used the quantile-quantile plots to compare the distributions between two empirical (i.e. observational) datasets. This is sometimes referred to as an empirical Q-Q plot. We can also use the q-q plot to compare an empirical observation to a theoretical observation (i.e. one defined mathematically). Such a plot is usually referred to as a theoretical Q-Q plot. Examples of popular theoretical observations are the normal distribution (aka the Gaussian distribution), the chi-square distribution, and the exponential distribution just to name a few. #&gt; Registered S3 methods overwritten by &#39;ggplot2&#39;: #&gt; method from #&gt; [.quosures rlang #&gt; c.quosures rlang #&gt; print.quosures rlang 14.2 Why we want to compare emprirical vs theoretical distributions There are many reasons we might want to compare empirical data to theoretical distributions: A theoretical distribution is easy to parameterize. For example, if the shape of the distribution of a batch of numbers can be approximated by a normal distribution we can reduce the complexity of our data to just two values: the mean and the standard deviation. If data can be approximated by certain theoretical distributions, then many mainstream statistical procedures can be applied to the data. In inferential statistics, knowing that a sample was derived from a population whose distribution follows a theoretical distribution allows us to derive certain properties of the population from the sample. For example, if we know that a sample comes from a normally distributed population, we can define confidence intervals for the sample mean using a t-distribution. Modeling the distribution of the observed data can provide insight into the underlying process that generated the data. But very few empirical datasets follow any theoretical distributions exactly. So the questions usually ends up being “how well does theoretical distribution X fit my data?” The theoretical quantile-quantile plot is a tool to explore how a batch of numbers deviates from a theoretical distribution and to visually assess whether the difference is significant for the purpose of the analysis. In the following examples, we will compare empirical data to the normal distribution using the normal quantile-quantile plot. 14.3 The normal q-q plot The normal q-q plot is just a special case of the empirical q-q plot we’ve explored so far; the difference being that we assign the normal distribution quantiles to the x-axis. 14.3.1 Drawing a normal q-q plot from scratch In the following example, we’ll compare the Alto 1 group to a normal distribution. First, we’ll extract the Alto 1 height values and save them as an atomic vector object using dplyr’s piping operations. However, dplyr’s operations will return a dataframe–even if a single column is selected. To force the output to an atomic vector, we’ll pipe the subset to pull(height) which will extract the height column into a plain vector element. library(dplyr) #&gt; #&gt; Attaching package: &#39;dplyr&#39; #&gt; The following object is masked from &#39;package:gridExtra&#39;: #&gt; #&gt; combine #&gt; The following objects are masked from &#39;package:stats&#39;: #&gt; #&gt; filter, lag #&gt; The following objects are masked from &#39;package:base&#39;: #&gt; #&gt; intersect, setdiff, setequal, union df &lt;- lattice::singer alto &lt;- df %&gt;% filter(voice.part == &quot;Alto 1&quot;) %&gt;% arrange(height) %&gt;% pull(height) %&gt;% print #&gt; [1] 60 61 61 61 61 62 62 62 63 63 63 63 64 64 64 65 65 65 65 66 66 66 66 #&gt; [24] 66 66 66 67 67 67 67 68 68 69 70 72 Next, we need to find the matching normal distribution quantiles. We first find the f-values for alto, then use qnorm to find the matching normal distribution values from those same f-values i &lt;- 1:length(alto) fi &lt;- (i - 0.5) / length(alto) fi #&gt; [1] 0.0143 0.0429 0.0714 0.1000 0.1286 0.1571 0.1857 0.2143 0.2429 0.2714 #&gt; [11] 0.3000 0.3286 0.3571 0.3857 0.4143 0.4429 0.4714 0.5000 0.5286 0.5571 #&gt; [21] 0.5857 0.6143 0.6429 0.6714 0.7000 0.7286 0.7571 0.7857 0.8143 0.8429 #&gt; [31] 0.8714 0.9000 0.9286 0.9571 0.9857 x.norm &lt;- qnorm(fi) x.norm #&gt; [1] -2.1893 -1.7185 -1.4652 -1.2816 -1.1332 -1.0063 -0.8938 -0.7916 #&gt; [9] -0.6971 -0.6085 -0.5244 -0.4439 -0.3661 -0.2905 -0.2165 -0.1437 #&gt; [17] -0.0717 0.0000 0.0717 0.1437 0.2165 0.2905 0.3661 0.4439 #&gt; [25] 0.5244 0.6085 0.6971 0.7916 0.8938 1.0063 1.1332 1.2816 #&gt; [33] 1.4652 1.7185 2.1893 plot(x.norm) Now we can plot the sorted alto values against the normal values. plot( alto ~ x.norm, type=&quot;p&quot;, xlab=&quot;Normal quantiles&quot;, pch=20) When comparing a batch of numbers to a theoretical distribution on a q-q plot, we are looking for significant deviation from a straight line. To make it easier to judge straightness, we can fit a line to the points. Note that we are not creating a 45° (or x=y) slope; the range of values between both sets of numbers do not match. Here, we are only seeking the straightness of the points. There are many ways one can fit a line to the data, Cleveland opts to fit a line to the first and third quartile of the q-q plot. The following chunk of code identifies the quantiles for both the alto dataset and the theoretical normal distribution. It then computes the slope and intercept from these coordinates. # Find 1st and 3rd quartile for the Alto 1 data y &lt;- quantile(alto, c(0.25, 0.75), type=5) y #&gt; 25% 75% #&gt; 63.0 66.8 # Find the 1st and 3rd quartile of the normal distribution x &lt;- qnorm( c(0.25, 0.75)) x #&gt; [1] -0.674 0.674 # Now we can compute the intercept and slope of the line that passes # through these points slope &lt;- diff(y) / diff(x) int &lt;- y[1] - slope * x[1] Next, we add the line to the plot. plot( alto ~ x.norm, type=&quot;p&quot;, xlab=&quot;Normal quantiles&quot;, pch=20) abline(a=int, b=slope ) 14.4 Using R’s built-in functions R has two built-in functions that facilitate the plot building task when comparing a batch to a normal distribution: qqnorm and qqline. Note that the function qqline allows the user to define the quantile method via the qtype= parameter. Here, we set it to 5 to match our choice of f-value calculation. qqnorm(alto) # plot the points qqline(alto, qtype=5) # plot the line That’s it. Just two lines of code! 14.5 Using the ggplot2 plotting environment We can take advantage of the stat_qq() function to plot the points, but the equation for the line must be computed manually (as was done earlier). Those steps will be repeated here. # normal distribution library(ggplot2) # Find the slope and intercept of the line that passes through the 1st and 3rd # quartile of the normal q-q plot y &lt;- quantile(alto, c(0.25, 0.75), type=5) # Find the 1st and 3rd quartiles x &lt;- qnorm( c(0.25, 0.75)) # Find the matching normal values on the x-axis slope &lt;- diff(y) / diff(x) # Compute the line slope int &lt;- y[1] - slope * x[1] # Compute the line intercept # Generate normal q-q plot ggplot() + aes(sample=alto) + stat_qq(distribution=qnorm) + geom_abline(intercept=int, slope=slope) + ylab(&quot;Height&quot;) qq_any &lt;- function(var, f) { # Find the slope and intercept of the line that passes through the 1st and 3rd # quartile of the normal q-q plot y &lt;- quantile(var, c(0.25, 0.75), type=5) # Find the 1st and 3rd quartiles x &lt;- f( c(0.25, 0.75)) # Find the matching normal values x-axis slope &lt;- diff(y) / diff(x) # Compute the line slope int &lt;- y[1] - slope * x[1] # Compute the line intercept ggplot() + aes(sample = var) + stat_qq(distribution = f) + geom_abline(intercept=int, slope=slope) } # two function only, for the moment qq_any(alto, qexp) qq_any(alto, qnorm) We can, of course, make use of ggplot’s faceting function to generate trellised plots. For example, the following plot replicates Cleveland’s figure 2.11 (except for the layout which we’ll setup as a single row of plots instead). But first, we will need to compute the slopes for each singer group. We’ll use dplyr’s piping operations to create a new dataframe with singer group name, slope and intercept. library(dplyr) intsl &lt;- df %&gt;% group_by(voice.part) %&gt;% summarize(q25 = quantile(height,0.25, type=5), q75 = quantile(height,0.75, type=5), norm25 = qnorm( 0.25), norm75 = qnorm( 0.75), slope = (q25 - q75) / (norm25 - norm75), int = q25 - slope * norm25) %&gt;% select(voice.part, slope, int) %&gt;% print #&gt; # A tibble: 8 x 3 #&gt; voice.part slope int #&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Bass 2 2.97 72 #&gt; 2 Bass 1 2.22 70.5 #&gt; 3 Tenor 2 1.48 70 #&gt; 4 Tenor 1 3.89 68.6 #&gt; 5 Alto 2 2.22 65.5 #&gt; 6 Alto 1 2.78 64.9 #&gt; # … with 2 more rows It’s important that the voice.part names match those in df letter-for-letter so that when ggplot is called, it will know which facet to assign the slope and intercept values to via geom_abline. ggplot(df, aes(sample = height)) + stat_qq(distribution = qnorm) + geom_abline(data=intsl, aes(intercept=int, slope=slope), col=&quot;blue&quot;) + facet_wrap(~voice.part, nrow=1) + ylab(&quot;Height&quot;) "],
["qq-and-pp-plots.html", "Chapter 15 QQ and PP Plots 15.1 QQ Plot 15.2 Some Examples 15.3 Calibrating the Variability 15.4 Scalability 15.5 Comparing Two Distributions", " Chapter 15 QQ and PP Plots https://homepage.divms.uiowa.edu/~luke/classes/STAT4580/qqpp.html 15.1 QQ Plot One way to assess how well a particular theoretical model describes a data distribution is to plot data quantiles against theoretical quantiles. Base graphics provides qqnorm, lattice has qqmath, and ggplot2 has geom_qq. The default theoretical distribution used in these is a standard normal, but, except for qqnorm, these allow you to specify an alternative. For a large sample from the theoretical distribution the plot should be a straight line through the origin with slope 1: library(ggplot2) #&gt; Registered S3 methods overwritten by &#39;ggplot2&#39;: #&gt; method from #&gt; [.quosures rlang #&gt; c.quosures rlang #&gt; print.quosures rlang n &lt;- 10000 ggplot() + geom_qq(aes(sample = rnorm(n))) If the plot is a straight line with a different slope or intercept, then the data distribution corresponds to a location-scale transformation of the theoretical distribution. The slope is the scale and the intercept is the location: ggplot() + geom_qq(aes(sample = rnorm(n, 10, 4))) + geom_abline(intercept = 10, slope = 4, color = &quot;red&quot;, size = 1.5, alpha = 0.8) The QQ plot can be constructed directly as a scatterplot of the sorted sample \\(i = 1, \\dots, n\\) against quantiles for \\[p_i = \\frac{i}{n} - \\frac{1}{2n}\\] p &lt;- (1 : n) / n - 0.5 / n y &lt;- rnorm(n, 10, 4) ggplot() + geom_point(aes(x = qnorm(p), y = sort(y))) 15.2 Some Examples The histograms and density estimates for the duration variable in the geyser data set showed that the distribution is far from a normal distribution, and the normal QQ plot shows this as well: library(MASS) ggplot(geyser) + geom_qq(aes(sample = duration)) Except for rounding the parent heights in the Galton data seemed not too fat from normally distributed: library(psych) ggplot(galton) + geom_qq(aes(sample = parent)) Rounding interferes more with this visualization than with a histogram or a density plot. Rounding is more visible with this visualization than with a histogram or a density plot. Another Gatlton dataset available in the UsingR package with less rounding is father.son: library(UsingR) ggplot(father.son) + geom_qq(aes(sample = fheight)) The middle seems to be fairly straight, but the ends are somewhat wiggly. How can you calibrate your judgment? 15.3 Calibrating the Variability One approach is to use simulation, sometimes called a graphical bootstrap. The nboot function will simulate R samples from a normal distribution that match a variable x on sample size, sample mean, and sample SD. The result is returned in a dataframe suitable for plotting: nsim &lt;- function(n, m = 0, s = 1) { z &lt;- rnorm(n) m + s * ((z - mean(z)) / sd(z)) } nboot &lt;- function(x, R) { n &lt;- length(x) m &lt;- mean(x) s &lt;- sd(x) do.call(rbind, lapply(1 : R, function(i) { xx &lt;- sort(nsim(n, m, s)) p &lt;- seq_along(x) / n - 0.5 / n data.frame(x = xx, p = p, sim = i) })) } Plotting these as lines shows the variability in shapes we can expect when sampling from the theoretical normal distribution: gb &lt;- nboot(father.son$fheight, 50) tibble::as_tibble(gb) #&gt; # A tibble: 53,900 x 3 #&gt; x p sim #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 59.8 0.000464 1 #&gt; 2 59.9 0.00139 1 #&gt; 3 59.9 0.00232 1 #&gt; 4 60.8 0.00325 1 #&gt; 5 60.8 0.00417 1 #&gt; 6 60.9 0.00510 1 #&gt; # … with 5.389e+04 more rows ggplot() + geom_line(aes(x = qnorm(p), y = x, group = sim), color = &quot;gray&quot;, data = gb) We can then insert this simulation behind our data to help calibrate the visualization: ggplot(father.son) + geom_line(aes(x = qnorm(p), y = x, group = sim), color = &quot;gray&quot;, data = gb) + geom_qq(aes(sample = fheight)) 15.4 Scalability For large sample sizes overplotting will occur: ggplot(diamonds) + geom_qq(aes(sample = price)) This can be alleviated by using a grid of quantiles: nq &lt;- 100 p &lt;- (1 : nq) / nq - 0.5 / nq ggplot() + geom_point(aes(x = qnorm(p), y = quantile(diamonds$price, p))) A more reasonable model might be an exponential distribution: ggplot() + geom_point(aes(x = qexp(p), y = quantile(diamonds$price, p))) 15.5 Comparing Two Distributions The QQ plot can also be used to compare two distributions based on a sample from each. If the samples are the same size then this is just a plot of the ordered sample values against each other. Choosing a fixed set of quantiles allows samples of unequal size to be compared. Using a small set of quantiles we can compare the distributions of waiting times between eruptions of Old Faithful from the two different data sets we have looked at: nq &lt;- 31 # user defined nq &lt;- min(length(geyser$waiting), length(faithful$waiting)) # or take the minimum p &lt;- (1 : nq) / nq - 0.5 / nq wg &lt;- geyser$waiting wf &lt;- faithful$waiting ggplot() + geom_point(aes(x = quantile(wg, p), y = quantile(wf, p))) "],
["pp-plots.html", "Chapter 16 PP Plots", " Chapter 16 PP Plots The PP plot for comparing a sample to a theoretical model plots the theoretical proportion less than or equal to each observed value against the actual proportion. For a theoretical cumulative distribution function F this means plotting \\[F(x(i))∼pi\\] For the fheight variable in the father.son data: m &lt;- mean(father.son$fheight) s &lt;- sd(father.son$fheight) n &lt;- nrow(father.son) p &lt;- (1 : n) / n - 0.5 / n ggplot(father.son) + geom_point(aes(x = p, y = sort(pnorm(fheight, m, s)))) The values on the vertical axis are the probability integral transform of the data for the theoretical distribution. If the data are a sample from the theoretical distribution then these transforms would be uniformly distributed on [0,1]. The PP plot is a QQ plot of these transformed values against a uniform distribution. The PP plot goes through the points (0,0) and (1,1) and so is much less variable in the tails: pp &lt;- ggplot() + geom_line(aes(x = p, y = pnorm(x, m, s), group = sim), color = &quot;gray&quot;, data = gb) pp Adding the data: pp + geom_point(aes(x = p, y = sort(pnorm(fheight, m, s))), data = (father.son)) The PP plot is also less sensitive to deviations in the tails. A compromise between the QQ and PP plots uses the arcsine square root variance-stabilizing transformation, which makes the variability approximately constant across the range of the plot: vpp &lt;- ggplot() + geom_line(aes(x = asin(sqrt(p)), y = asin(sqrt(pnorm(x, m, s))), group = sim), color = &quot;gray&quot;, data = gb) vpp Adding the data: vpp + geom_point(aes(x = asin(sqrt(p)), y = sort(asin(sqrt(pnorm(fheight, m, s))))), data = (father.son)) "],
["plots-for-assessing-model-fit.html", "Chapter 17 Plots For Assessing Model Fit", " Chapter 17 Plots For Assessing Model Fit Both QQ and PP plots can be used to asses how well a theoretical family of models fits your data, or your residuals. To use a PP plot you have to estimate the parameters first. For a location-scale family, like the normal distribution family, you can use a QQ plot with a standard member of the family. Some other families can use other transformations that lead to straight lines for family members: The Weibull family is widely used in reliability modeling; its CDF is \\[F(t) = 1 - \\exp\\left\\{-\\left(\\frac{t}{b}\\right)^a\\right\\}\\] The logarithms of Weibull random variables form a location-scale family. Special paper used to be available for Weibull probability plots. A Weibull QQ plot for price in the diamonds data: n &lt;- nrow(diamonds) p &lt;- (1 : n) / n - 0.5 / n ggplot(diamonds) + geom_point(aes(x = log10(qweibull(p, 1, 1)), y = log10(sort(price)))) The lower tail does not match a Weibull distribution. Is this important? In engineering applications it often is. In selecting a reasonable model to capture the shape of this distribution it may not be. QQ plots are helpful for understanding departures from a theoretical model. No data will fit a theoretical model perfectly. Case-specific judgment is needed to decide whether departures are important. George Box: All models are wrong but some are useful. "],
["data-visualization-working-with-models.html", "Chapter 18 Data Visualization: Working with models 18.1 Introduction 18.2 Show several fits at once, with a legend 18.3 Look inside model objects 18.4 Get model-based graphics right 18.5 Generate predictions to graph 18.6 Tidy model objects with broom 18.7 Grouped analysis and list-columns 18.8 Plot marginal effects 18.9 Plots from complex surveys 18.10 Where to go next", " Chapter 18 Data Visualization: Working with models 18.1 Introduction Source: https://socviz.co/modeling.html Data visualization is about more than generating figures that display the raw numbers from a table of data. Right from the beginning, it involves summarizing or transforming parts of the data, and then plotting the results. Statistical models are a central part of that process. In this Chapter, we will begin by looking briefly at how ggplot can use various modeling techniques directly within geoms. Then we will see how to use the broom and margins libraries to tidily extract and plot estimates from models that we fit ourselves. # load libraries library(ggplot2) #&gt; Registered S3 methods overwritten by &#39;ggplot2&#39;: #&gt; method from #&gt; [.quosures rlang #&gt; c.quosures rlang #&gt; print.quosures rlang library(dplyr) #&gt; #&gt; Attaching package: &#39;dplyr&#39; #&gt; The following objects are masked from &#39;package:stats&#39;: #&gt; #&gt; filter, lag #&gt; The following objects are masked from &#39;package:base&#39;: #&gt; #&gt; intersect, setdiff, setequal, union library(tidyr) library(purrr) library(socviz) # devtools::install_github(&quot;kjhealy/socviz&quot;) library(gapminder) # plot two lines p &lt;- ggplot(data = gapminder, mapping = aes(x = log(gdpPercap), y = lifeExp)) p + geom_point(alpha=0.1) + geom_smooth(color = &quot;tomato&quot;, fill=&quot;tomato&quot;, method = MASS::rlm) + geom_smooth(color = &quot;steelblue&quot;, fill=&quot;steelblue&quot;, method = &quot;lm&quot;) # plot spline p + geom_point(alpha=0.1) + geom_smooth(color = &quot;tomato&quot;, method = &quot;lm&quot;, size = 1.2, formula = y ~ splines::bs(x, 3), se = FALSE) p + geom_point(alpha=0.1) + geom_quantile(color = &quot;tomato&quot;, size = 1.2, method = &quot;rqss&quot;, lambda = 1, quantiles = c(0.20, 0.5, 0.85)) #&gt; Loading required package: SparseM #&gt; #&gt; Attaching package: &#39;SparseM&#39; #&gt; The following object is masked from &#39;package:base&#39;: #&gt; #&gt; backsolve #&gt; Smoothing formula not specified. Using: y ~ qss(x, lambda = 1) #&gt; Warning in rq.fit.sfn(x, y, tau = tau, rhs = rhs, control = control, ...): tiny diagonals replaced with Inf when calling blkfct Histograms, density plots, boxplots, and other geoms compute either single numbers or new variables before plotting them. As we saw in Section 4.4, these calculations are done by stat_ functions, each of which works hand-in-hand with its default geom_ function, and vice versa. Moreover, from the smoothing lines we drew from almost the very first plots we made, we have seen that stat_ functions can do a fair amount of calculation and even model estimation on the fly. The geom_smooth() function can take a range of method arguments to fit LOESS, OLS, and robust regression lines, amongst others. Both the geom_smooth() and geom_quantile() functions can also be instructed to use different formulas to produce their fits. In the top panel of Figure 6.1, we access the MASS library’s rlm function to fit a robust regression line. In the second panel, the bs function is invoked directly from the splines library in the same way, to fit a polynominal curve to the data. This is the same approach to directly accessing functions without loading a whole library that we have already used several times when using functions from the scales library. The geom_quantile() function, meanwhile, is like a specialized version of geom_smooth() that can fit quantile regression lines using a variety of methods. The quantiles argument takes a vector specifying the quantiles at which to fit the lines. 18.2 Show several fits at once, with a legend As we just saw in the first panel of Figure 6.1, where we plotted both an OLS and a robust regression line, we can look at several fits at once on the same plot by layering on new smoothers with geom_smooth(). As long as we set the color and fill aesthetics to different values for each fit, we can easily distinguish them visually. However, ggplot will not draw a legend that guides us about which fit is which. This is because the smoothers are not logically connected to one another. They exist as separate layers. What if we are comparing several different fits and want a legend describing them? As it turns out, geom_smooth() can do this via the slightly unusual route of mapping the color and fill aesthetics to a string describing the model we are fitting, and then using scale_color_manual() and scale_fill_manual() to create the legend. First we use brewer.pal() from the RColorBrewer library to extract three qualitatively different colors from a larger palette. The colors are represented as hex values. As before use the :: convention to use the function without loading the whole library: model_colors &lt;- RColorBrewer::brewer.pal(3, &quot;Set1&quot;) model_colors #&gt; [1] &quot;#E41A1C&quot; &quot;#377EB8&quot; &quot;#4DAF4A&quot; Then we create a plot with three different smoothers, mapping the color and fill within the aes() function as the name of the smoother: p0 &lt;- ggplot(data = gapminder, mapping = aes(x = log(gdpPercap), y = lifeExp)) p1 &lt;- p0 + geom_point(alpha = 0.2) + geom_smooth(method = &quot;lm&quot;, aes(color = &quot;OLS&quot;, fill = &quot;OLS&quot;)) + geom_smooth(method = &quot;lm&quot;, formula = y ~ splines::bs(x, df = 3), aes(color = &quot;Cubic Spline&quot;, fill = &quot;Cubic Spline&quot;)) + geom_smooth(method = &quot;loess&quot;, aes(color = &quot;LOESS&quot;, fill = &quot;LOESS&quot;)) p1 + scale_color_manual(name = &quot;Models&quot;, values = model_colors) + scale_fill_manual(name = &quot;Models&quot;, values = model_colors) + theme(legend.position = &quot;top&quot;) In a way we have cheated a little here to make the plot work. Until now, we have always mapped aesthetics to the names of variables, not to strings like “OLS” or “Cubic Splines”. In Chapter 3, when we discussed mapping versus setting aesthetics, we saw what happened when we tried to change the color of the points in a scatterplot by setting them to “purple” inside the aes()function. The result was that the points turned red instead, as ggplot in effect created a new variable and labeled it with the word “purple”. We learned there that the aes() function was for mapping variables to aesthetics. Here we take advantage of that behavior, creating a new single-value variable for the name of each of our models. Ggplot will properly construct the relevant guide if we call scale_color_manual() and scale_fill_manual(). Remember that we have to call two scale functions because we have two mappings. The result is a single plot containing not just our three smoothers, but also an appropriate legend to guide the reader. These model-fitting features make ggplot very useful for exploratory work, and make it straightforward to generate and compare model-based trends and other summaries as part of the process of descriptive data visualization. The various stat_ functions are a flexible way to add summary estimates of various kinds to plots. But we will also want more than this, including presenting results from models we fit ourselves. 18.3 Look inside model objects Covering the details of fitting statistical models in R is beyond the scope of this book. For a comprehensive, modern introduction to that topic you should work your way through (Gelman &amp; Hill, 2018). (Harrell, 2016) is also very good on the many practical connections between modeling and graphing data. Similarly, (Gelman, 2004) provides a detailed discussion of the use of graphics as a tool in model-checking and validation. Here we will discuss some ways to take the models that you fit and extract information that is easy to work with in ggplot. Our goal, as always, is to get from however the object is stored to a tidy table of numbers that we can plot. Most classes of statistical model in R will contain the information we need, or will have a special set of functions, or methods, designed to extract it. We can start by learning a little more about how the output of models is stored in R. Remember, we are always working with objects, and objects have an internal structure consisting of named pieces. Sometimes these are single numbers, sometimes vectors, and sometimes lists of things like vectors, matrices, or formulas. We have been working extensively with tibbles and data frames. These store tables of data with named columns, perhaps consisting of different classes of variable, such as integers, characters, dates, or factors. Model objects are a little more complicated again. gapminder #&gt; # A tibble: 1,704 x 6 #&gt; country continent year lifeExp pop gdpPercap #&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 Afghanistan Asia 1952 28.8 8425333 779. #&gt; 2 Afghanistan Asia 1957 30.3 9240934 821. #&gt; 3 Afghanistan Asia 1962 32.0 10267083 853. #&gt; 4 Afghanistan Asia 1967 34.0 11537966 836. #&gt; 5 Afghanistan Asia 1972 36.1 13079460 740. #&gt; 6 Afghanistan Asia 1977 38.4 14880372 786. #&gt; # … with 1,698 more rows Remember, we can use the str() function to learn more about the internal structure of any object. For example, we can get some information on what class (or classes) of object gapminder is, how large it is, and what components it has. The output from str(gapminder) is somewhat dense: str(gapminder) #&gt; Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 1704 obs. of 6 variables: #&gt; $ country : Factor w/ 142 levels &quot;Afghanistan&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... #&gt; $ continent: Factor w/ 5 levels &quot;Africa&quot;,&quot;Americas&quot;,..: 3 3 3 3 3 3 3 3 3 3 ... #&gt; $ year : int 1952 1957 1962 1967 1972 1977 1982 1987 1992 1997 ... #&gt; $ lifeExp : num 28.8 30.3 32 34 36.1 ... #&gt; $ pop : int 8425333 9240934 10267083 11537966 13079460 14880372 12881816 13867957 16317921 22227415 ... #&gt; $ gdpPercap: num 779 821 853 836 740 ... There is a lot of information here about the object as a whole and each variable in it. In the same way, statistical models in R have an internal structure. But because models are more complex entities than data tables, their structure is correspondingly more complicated. There are more pieces of information, and more kinds of information, that we might want to use. All of this information is generally stored in or is computable from parts of a model object. We can create a linear model, an ordinary OLS regression, using the gapminder data. This dataset has a country-year structure that makes an OLS specification like this the wrong one to use. But never mind that for now. We use the lm() function to run the model, and store it in an object called out: out &lt;- lm(formula = lifeExp ~ gdpPercap + pop + continent, data = gapminder) The first argument is the formula for the model. lifeExp is the dependent variable and the tilde ~ operator is used to designate the left- and right-hand sides of a model (including in cases, as we saw with facet_wrap() where the model just has a right-hand side.) Let’s look at the results by asking R to print a summary of the model. summary(out) #&gt; #&gt; Call: #&gt; lm(formula = lifeExp ~ gdpPercap + pop + continent, data = gapminder) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -49.16 -4.49 0.30 5.11 25.17 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 4.78e+01 3.40e-01 140.82 &lt;2e-16 *** #&gt; gdpPercap 4.50e-04 2.35e-05 19.16 &lt;2e-16 *** #&gt; pop 6.57e-09 1.98e-09 3.33 9e-04 *** #&gt; continentAmericas 1.35e+01 6.00e-01 22.46 &lt;2e-16 *** #&gt; continentAsia 8.19e+00 5.71e-01 14.34 &lt;2e-16 *** #&gt; continentEurope 1.75e+01 6.25e-01 27.97 &lt;2e-16 *** #&gt; continentOceania 1.81e+01 1.78e+00 10.15 &lt;2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 8.37 on 1697 degrees of freedom #&gt; Multiple R-squared: 0.582, Adjusted R-squared: 0.581 #&gt; F-statistic: 394 on 6 and 1697 DF, p-value: &lt;2e-16 When we use the summary() function on out, we are not getting a simple feed of what’s in the model object. Instead, like any function, summary() takes its input, performs some actions, and produces output. In this case, what is printed to the console is partly information that is stored inside the model object, and partly information that the summary() function has calculated and formated for display on the screen. Behind the scenes, summary() gets help from other functions. Objects of different classes have default methods associated with them, so that when the generic summary() function is applied to a linear model object, the function knows to pass the work on to a more specialized function that does a bunch of calculations and formatting appropriate to a linear model object. We use the same generic summary() function on data frames, as in summary(gapminder), but in that case a different default method is applied. Schematic view of a linear model object.Figure 6.3: Schematic view of a linear model object. The output from summary() gives a precis of the model, but we can’t really do any further analysis with it directly. For example, what if we want to plot something from the model? The information necessary to make plots is inside the out object, but it is not obvious how to use it. If we take a look at the structure of the model object with str(out) we will find that there is a lot of information in there. Like most complex objects in R, out is organized as a list of components or elements. Several of these elements are themselves lists. Figure 6.3 gives you a schematic view of the contents of a linear model object. In this list of items, elements are single values, some are data frames, and some are additional lists of simpler items. Again, remember our earlier discussion where we said objects could be thought of as being organized like a filing system: cabinets contain drawers, and drawer may contain which may contain pages of information, whole documents, or groups of folders with more documents inside. As an alternative analogy, and sticking with the image of a list, you can think of a master to-do list for a project, where the top-level headings lead to contain additional lists of tasks of different kinds. The out object created by lm contains several different named elements. Some, like the residual degrees of freedom in the model, are just a single number. Try out$df.residual at the console. Others are much larger entities, such as the data frame used to fit the model, which is retained by default. Try out$model, but be prepared for a lot of stuff to be printed at the console. Other elements have been computed by R and then stored, such as the coefficients of the model and other quantities. You can try out$coefficients, out$residuals, and out$fitted.values, for instance. Others are lists themselves (like qr). So you can see that the summary() function is selecting and printing only a small amount of core information, in comparison to what is stored in the model object. Just like the tables of data we saw earlier in Section A.1.3, the output of summary() is presented in a way that is compact and efficient in terms of getting information across, but also untidy when considered from the point of view of further manipulation. There is a table of coefficients, but the variable names are in the rows. The column names are awkward, and some information (e.g. at the bottom of the output) has been calculated and printed out, but is not stored in the model object. 18.4 Get model-based graphics right Figures based on statistical models face all the ordinary challenges of effective data visualization, and then some. This is because model results usually carry a considerable extra burden of interpretation and necessary background knowledge. The more complex the model, the trickier it becomes to convey this information effectively, and the easier it becomes to lead one’s audience or oneself into error. Within the social sciences, our ability to clearly and honestly present model-based graphics has greatly improved over the past ten or fifteen years. Over the same period, it has become clearer that some kinds of models are quite tricky to understand, even ones that had previously been seen as straightforward elements of the modeling toolkit (Ai &amp; Norton, 2003; Brambor, Clark, &amp; Golder, 2006). Plotting model estimates is closely connected to properly estimating models in the first place. This means there is no substitute for learning the statistics. You should not use graphical methods as a substitute for understanding the model used to produce them. While this book cannot teach you that material, we can make a few general points about what good model-based graphics look like, and work through some examples of how ggplot and some additional libraries can make it easier to get good results. 18.4.1 Present your findings in substantive terms Useful model-based plots show results in ways that are substantively meaningful and directly interpretable with respect to the questions the analysis is trying to answer. This means showing results in a context where other variables in the analysis are held at sensible values, such as their means or medians. With continuous variables, it can often be useful to generate predicted values that cover some substantively meaningful move across the distribution, such as from the 25th to the 75th percentile, rather than a single-unit increment in the variable of interest. For unordered categorical variables, predicted values might be presented with respect to the modal category in the data, or for a particular category of theoretical interest. Presenting substantively interpretable findings often also means using (and sometimes converting to) a scale that readers can easily understand. If your model reports results in log-odds, for example, converting the estimates to predicted probabilities will make it easier to interpret. All of this advice is quite general. Each of these points applies equally well to the presentation of summary results in a table rather than a graph. There is nothing distinctively graphical about putting the focus on the substantive meaning of your findings. 18.4.2 Show your degree of confidence Much the same applies to presenting the degree of uncertainty or confidence you have in your results. Model estimates come with various measures of precision, confidence, credence, or significance. Presenting and interpreting these measures is notoriously prone to misinterpretation, or over-interpretation, as researchers and audiences both demand more from things like confidence intervals and p-values than these statistics can deliver. At a minimum, having decided on an appropriate measure of model fit or the right assessment of confidence, you should show their range when you present your results. A family of related ggplot geoms allow you to show a range or interval defined by position on the x-axis and then a ymin and ymax range on the y-axis. These geoms include geom_pointrange() and geom_errorbar(), which we will see in action shortly. A related geom, geom_ribbon() uses the same arguments to draw filled areas, and is useful for plotting ranges of y-axis values along some continuously varying x-axis. 18.4.3 Show your data when you can Plotting the results from a multivariate model generally means one of two things. First, we can show what is in effect a table of coefficients with associated measures of confidence, perhaps organizing the coefficients into meaningful groups, or by the size of the predicted association, or both. Second, we can show the predicted values of some variables (rather than just a model’s coefficients) across some range of interest. The latter approach lets us show the original data points if we wish. The way ggplot builds graphics layer by layer allows us to easily combine model estimates (e.g. a regression line and an associated range) and the underlying data. In effect these are manually-constructed versions of the automatically-generated plots that we have been producing with geom_smooth() since the beginning of this book. 18.5 Generate predictions to graph Having fitted a model, then, we might want to get a picture of the estimates it produces over the range of some particular variable, holding other covariates constant at some sensible values. The predict() function is a generic way of using model objects to produce this kind of prediction. In R, “generic” functions take their inputs and pass them along to more specific functions behind the scenes, ones that are suited to working with the particular kind of model object we have. The details of getting predicted values from a OLS model, for instance, will be somewhat different from getting predictions out of a logistic regression. But in each case we can use the same predict() function, taking care to check the documentation to see what form the results are returned in for the kind of model we are working with. Many of the most commonly-used functions in R are generic in this way. The summary() function, for example, works on objects of many different classes, from vectors to data frames and statistical models, producing appropriate output in each case by way of a class-specific function in the background. For predict() to calculate the new values for us, it needs some new data to fit the model to. We will generate a new data frame whose columns have the same names as the variables in the model’s original data, but where the rows have new values. A very useful function called expand.grid() will help us do this. We will give it a list of variables, specifying the range of values we want each variable to take. Then expand.grid() will generate the will multiply out the full range of values for all combinations of the values we give it, thus creating a new data frame with the new data we need. In the following bit of code, we use min() and max() to get the minimum and maximum values for per capita GDP, and then create a vector with one hundred evenly-spaced elements between the minimum and the maximum. We hold population constant at its median, and we let continent take all of its five available values. min_gdp &lt;- min(gapminder$gdpPercap) max_gdp &lt;- max(gapminder$gdpPercap) med_pop &lt;- median(gapminder$pop) pred_df &lt;- expand.grid(gdpPercap = (seq(from = min_gdp, to = max_gdp, length.out = 100)), pop = med_pop, continent = c(&quot;Africa&quot;, &quot;Americas&quot;, &quot;Asia&quot;, &quot;Europe&quot;, &quot;Oceania&quot;)) dim(pred_df) #&gt; [1] 500 3 head(pred_df) #&gt; gdpPercap pop continent #&gt; 1 241 7023596 Africa #&gt; 2 1385 7023596 Africa #&gt; 3 2530 7023596 Africa #&gt; 4 3674 7023596 Africa #&gt; 5 4818 7023596 Africa #&gt; 6 5962 7023596 Africa Now we can use predict(). If we give the function our new data and model, without any further argument, it will calculate the fitted values for every row in the data frame. If we specify interval = 'predict' as an argument, it will calculate 95% prediction intervals in addition to the point estimate. pred_out &lt;- predict(object = out, newdata = pred_df, interval = &quot;predict&quot;) head(pred_out) #&gt; fit lwr upr #&gt; 1 48.0 31.5 64.4 #&gt; 2 48.5 32.1 64.9 #&gt; 3 49.0 32.6 65.4 #&gt; 4 49.5 33.1 65.9 #&gt; 5 50.0 33.6 66.4 #&gt; 6 50.5 34.1 67.0 Because we know that, by construction, the cases in pred_df and pred_out correspond row for row, we can bind the two data frames together by column. This method of joining or merging tables is definitely not recommended when you are dealing with data. pred_df &lt;- cbind(pred_df, pred_out) head(pred_df) #&gt; gdpPercap pop continent fit lwr upr #&gt; 1 241 7023596 Africa 48.0 31.5 64.4 #&gt; 2 1385 7023596 Africa 48.5 32.1 64.9 #&gt; 3 2530 7023596 Africa 49.0 32.6 65.4 #&gt; 4 3674 7023596 Africa 49.5 33.1 65.9 #&gt; 5 4818 7023596 Africa 50.0 33.6 66.4 #&gt; 6 5962 7023596 Africa 50.5 34.1 67.0 The end result is a tidy data frame, containing the predicted values from the model for the range of values we specified. Now we can plot the results. Because we produced a full range of predicted values, we can decide whether or not to use all of them. Here we further subset the predictions to just those for Europe and Africa. p &lt;- ggplot(data = subset(pred_df, continent %in% c(&quot;Europe&quot;, &quot;Africa&quot;)), aes(x = gdpPercap, y = fit, ymin = lwr, ymax = upr, color = continent, fill = continent, group = continent)) p + geom_point(data = subset(gapminder, continent %in% c(&quot;Europe&quot;, &quot;Africa&quot;)), aes(x = gdpPercap, y = lifeExp, color = continent), alpha = 0.5, inherit.aes = FALSE) + geom_line() + geom_ribbon(alpha = 0.2, color = FALSE) + scale_x_log10(labels = scales::dollar) We use a new geom here to draw the area covered by the prediction intervals: geom_ribbon(). It takes an x argument like a line, but a ymin and ymax argument as specified in the ggplot() aesthetic mapping. This defines the lower and upper limits of the prediction interval. In practice, you may not use predict() directly all that often. Instead, you might write code using additional libraries that encapsulate the process of producing predictions and plots from models. These are especially useful when your model is a little more complex and the interpretation of coefficients becomes trickier. This happens, for instance, when you have a binary outcome variable and need to convert the results of a logistic regression into predicted probabilities, or when you have interaction terms amongst your predictions. We will discuss some of these helper libraries in the next few sections. However, bear in mind that predict() and its ability to work safely with different classes of model underpins many of those libraries. So it’s useful to see it in action first hand in order to understand what it is doing. 18.6 Tidy model objects with broom The predict method is very useful, but there are a lot of other things we might want to do with our model output. We will use David Robinson’s broom package to help us out. It is a library of functions that help us get from the model results that R generates to numbers that we can plot. It will take model objects and turn pieces of them into data frames that you can use easily with ggplot. library(broom) Broom takes ggplot’s approach to tidy data and extends it to the model objects that R produces. Its methods can tidily extract three kinds of information. First, we can see component-level information about aspects of the model itself, such as coefficients and t-statistics. Second, we can obtain observation-level information about the model’s connection to the underlying data. This includes the fitted values and residuals for each observation in the data. And finally we can get model-level information that summarizes the fit as a whole, such as an F-statistic, the model deviance, or the r-squared. There is a broom function for each of these tasks. 18.6.1 Get component-level statistics with tidy() The tidy() function takes a model object and returns a data frame of component-level information. We can work with this to make plots in a familiar way, and much more easily than fishing inside the model object to extract the various terms. Here is an example, using the default results as just returned. For a more convenient display of the results, we will pipe the object we create with tidy() through a function that rounds the numeric columns of the data frame to two decimal places. This doesn’t change anything about the object itself, of course. out_comp &lt;- tidy(out) out_comp %&gt;% round_df() #&gt; # A tibble: 7 x 5 #&gt; term estimate std.error statistic p.value #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 (Intercept) 47.8 0.34 141. 0 #&gt; 2 gdpPercap 0 0 19.2 0 #&gt; 3 pop 0 0 3.33 0 #&gt; 4 continentAmericas 13.5 0.6 22.5 0 #&gt; 5 continentAsia 8.19 0.570 14.3 0 #&gt; 6 continentEurope 17.5 0.62 28.0 0 #&gt; # … with 1 more row We are now able to treat this dataframe just like all the other data that we have seen so far. p &lt;- ggplot(out_comp, mapping = aes(x = term, y = estimate)) p + geom_point() + coord_flip() We can extend and clean up this plot in a variety of ways. For example, we can tell tidy() to calculate confidence intervals for the estimates, using R’s confint() function. out_conf &lt;- tidy(out, conf.int = TRUE) out_conf %&gt;% round_df() #&gt; # A tibble: 7 x 7 #&gt; term estimate std.error statistic p.value conf.low conf.high #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 (Intercept) 47.8 0.34 141. 0 47.2 48.5 #&gt; 2 gdpPercap 0 0 19.2 0 0 0 #&gt; 3 pop 0 0 3.33 0 0 0 #&gt; 4 continentAmericas 13.5 0.6 22.5 0 12.3 14.6 #&gt; 5 continentAsia 8.19 0.570 14.3 0 7.07 9.31 #&gt; 6 continentEurope 17.5 0.62 28.0 0 16.2 18.7 #&gt; # … with 1 more row The convenience “not in” operator %nin% is available via the socviz library. It does the opposite of %in% and selects only the items in a first vector of characters that are not in the second. We’ll use it to drop the intercept term from the table. We also want to something about the labels. When fitting a model with categorical variables, R will create coefficient names based on the variable name and the category name, like continentAmericas. Normally we like to clean these up before plotting. Most commonly, we just want to strip away the variable name at the beginning of the coefficient label. For this we can use prefix_strip(), a convenience function in the socviz library. We tell it which prefixes to drop, using it to create a new column variable in out_conf that corresponds to the terms column, but that has nicer labels. out_conf &lt;- subset(out_conf, term %nin% &quot;(Intercept)&quot;) out_conf$nicelabs &lt;- prefix_strip(out_conf$term, &quot;continent&quot;) Now we can use geom_pointrange()to make a figure that displays some information about our confidence in the variable estimates, as opposed to just the coefficients. As with the boxplots earlier, we use reorder() to sort the names of the model’s terms by the estimate variable, thus arranging our plot of effects from largest to smallest in magnitude. p &lt;- ggplot(out_conf, mapping = aes(x = reorder(nicelabs, estimate), y = estimate, ymin = conf.low, ymax = conf.high)) p + geom_pointrange() + coord_flip() + labs(x=&quot;&quot;, y=&quot;OLS Estimate&quot;) Dotplots of this kind can be very compact. The vertical axis can often be compressed quite a bit, with no loss in comprehension. In fact, they are often easier to read with much less room between the rows than given by a default square shape. 18.6.2 Get observation-level statistics with augment() The values returned by augment() are all statistics calculated at the level of the original observations. As such, they can be added on to the data frame that the model is based on. Working from a call to augment() will return a data frame with all the original observations used in the estimation of the model, together with columns like the following: .fitted — The fitted values of the model. .se.fit — The standard errors of the fitted values. .resid — The residuals. .hat — The diagonal of the hat matrix. .sigma — An estimate of residual standard deviation when the corresponding observation is dropped from the model. .cooksd — Cook’s distance, a common regression diagnostic; and .std.resid — The standardized residuals. Each of these variables is named with a leading dot, for example .hat rather than hat, and so on. This is to guard against accidentally confusing it with (or accidentally overwriting) an existing variable in your data with this name. The columns of values return will differ slightly depending on the class of model being fitted. out_aug &lt;- augment(out) head(out_aug) %&gt;% round_df() #&gt; # A tibble: 6 x 11 #&gt; lifeExp gdpPercap pop continent .fitted .se.fit .resid .hat .sigma #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 28.8 779. 8.43e6 Asia 56.4 0.47 -27.6 0 8.34 #&gt; 2 30.3 821. 9.24e6 Asia 56.4 0.47 -26.1 0 8.34 #&gt; 3 32 853. 1.03e7 Asia 56.5 0.47 -24.5 0 8.35 #&gt; 4 34.0 836. 1.15e7 Asia 56.5 0.47 -22.4 0 8.35 #&gt; 5 36.1 740. 1.31e7 Asia 56.4 0.47 -20.3 0 8.35 #&gt; 6 38.4 786. 1.49e7 Asia 56.5 0.47 -18.0 0 8.36 #&gt; # … with 2 more variables: .cooksd &lt;dbl&gt;, .std.resid &lt;dbl&gt; By default, augment() will extract the available data from the model object. This will usually include the variables used in the model itself, but not any additional ones contained in the original data frame. Sometimes it is useful to have these. We can add them by specifying the data argument: out_aug &lt;- augment(out, data = gapminder) head(out_aug) %&gt;% round_df() #&gt; # A tibble: 6 x 13 #&gt; country continent year lifeExp pop gdpPercap .fitted .se.fit .resid #&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Afghan… Asia 1952 28.8 8.43e6 779. 56.4 0.47 -27.6 #&gt; 2 Afghan… Asia 1957 30.3 9.24e6 821. 56.4 0.47 -26.1 #&gt; 3 Afghan… Asia 1962 32 1.03e7 853. 56.5 0.47 -24.5 #&gt; 4 Afghan… Asia 1967 34.0 1.15e7 836. 56.5 0.47 -22.4 #&gt; 5 Afghan… Asia 1972 36.1 1.31e7 740. 56.4 0.47 -20.3 #&gt; 6 Afghan… Asia 1977 38.4 1.49e7 786. 56.5 0.47 -18.0 #&gt; # … with 4 more variables: .hat &lt;dbl&gt;, .sigma &lt;dbl&gt;, .cooksd &lt;dbl&gt;, #&gt; # .std.resid &lt;dbl&gt; If some rows containing missing data were dropped to fit the model, then these will not be carried over to the augmented dataframe. The new columns created by augment() can be used to create some standard regression plots. For example, we can plot the residuals versus the fitted values. Figure 6.7 suggests, unsurprisingly, that our country-year data has rather more structure than is captured by our OLS model. p &lt;- ggplot(data = out_aug, mapping = aes(x = .fitted, y = .resid)) p + geom_point() 18.6.3 Get model-level statistics with glance() This function organizes the information typically presented at the bottom of a model’s summary() output. By itself, it usually just returns a table with a single row in it. But as we shall see in a moment, the real power of broom’s approach is the way that it can scale up to cases where we are grouping or subsampling our data. glance(out) %&gt;% round_df() #&gt; # A tibble: 1 x 11 #&gt; r.squared adj.r.squared sigma statistic p.value df logLik AIC #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.580 0.580 8.37 394. 0 7 -6034. 12084. #&gt; # … with 3 more variables: BIC &lt;dbl&gt;, deviance &lt;dbl&gt;, df.residual &lt;dbl&gt; Broom is able to tidy (and augment, and glance at) a wide range of model types. Not all functions are available for all classes of model. Consult broom’s documentation for more details on what is available. For example, here is a plot created from the tidied output of an event-history analysis. First we generate a Cox proportional hazards model of some survival data. library(survival) #&gt; #&gt; Attaching package: &#39;survival&#39; #&gt; The following object is masked from &#39;package:quantreg&#39;: #&gt; #&gt; untangle.specials out_cph &lt;- coxph(Surv(time, status) ~ age + sex, data = lung) out_surv &lt;- survfit(out_cph) The details of the fit are not important here, but in the first step the Surv() function creates the response or outcome variable for the proportional hazards model that is then fitted by the coxph()function. Then the survfit() function creates the survival curve from the model, much like we used predict() to generate predicted values earlier. Try summary(out_cph) to see the model, and summary(out_surv) to see the table of predicted values that will form the basis for our plot. Next we tidy out_surv to get a data frame, and plot it. # Figure 6.8: A Kaplan-Meier plot. out_tidy &lt;- tidy(out_surv) p &lt;- ggplot(data = out_tidy, mapping = aes(time, estimate)) p + geom_line() + geom_ribbon(mapping = aes(ymin = conf.low, ymax = conf.high), alpha = .2) 18.7 Grouped analysis and list-columns Broom makes it possible to quickly fit models to different subsets of your data and get consistent and usable tables of results out the other end. For example, let’s say we wanted to look at the gapminder data by examining the relationship between life expectancy and GDP by continent, for each year in the data. The gapminder data is at bottom organized by country-years. That is the unit of observation in the rows. If we wanted, we could take a slice of the data manually, such as “all countries observed in Asia, in 1962” or “all in Africa, 2002”. Here is “Europe, 1977”: eu77 &lt;- gapminder %&gt;% filter(continent == &quot;Europe&quot;, year == 1977) We could then see what the relationship between life expectancy and GDP looked like for that continent-year group: fit &lt;- lm(lifeExp ~ log(gdpPercap), data = eu77) summary(fit) #&gt; #&gt; Call: #&gt; lm(formula = lifeExp ~ log(gdpPercap), data = eu77) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -7.496 -1.031 0.093 1.176 3.712 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 29.489 7.161 4.12 0.00031 *** #&gt; log(gdpPercap) 4.488 0.756 5.94 2.2e-06 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 2.11 on 28 degrees of freedom #&gt; Multiple R-squared: 0.557, Adjusted R-squared: 0.541 #&gt; F-statistic: 35.2 on 1 and 28 DF, p-value: 2.17e-06 With dplyr and broom we can do this for every continent-year slice of the data in a compact and tidy way. We start with our table of data, and then (%&gt;%) group the countries by continent and year using the group_by() function. We introduced this grouping operation in Chapter 4. Our data is reorganized first by continent, and within continent by year. Here we will take one further step and nest the data that make up each group: out_le &lt;- gapminder %&gt;% group_by(continent, year) %&gt;% nest() out_le #&gt; # A tibble: 60 x 3 #&gt; continent year data #&gt; &lt;fct&gt; &lt;int&gt; &lt;list&gt; #&gt; 1 Asia 1952 &lt;tibble [33 × 4]&gt; #&gt; 2 Asia 1957 &lt;tibble [33 × 4]&gt; #&gt; 3 Asia 1962 &lt;tibble [33 × 4]&gt; #&gt; 4 Asia 1967 &lt;tibble [33 × 4]&gt; #&gt; 5 Asia 1972 &lt;tibble [33 × 4]&gt; #&gt; 6 Asia 1977 &lt;tibble [33 × 4]&gt; #&gt; # … with 54 more rows Think of what nest() does as a more intensive version what group_by() does. The resulting object is has the tabular form we expect (it is a tibble) but it looks a little unusual. The first two columns are the familiar continent and year. But we now also have a new column, data, that contains a small table of data corresponding to each continent-year group. This is a list-column, something we have not seen before. It turns out to be very useful for bundling together complex objects (structured, in this case, as a list of tibbles, each being a 33x4 table of data) within the rows of our data (which remains tabular). Our “Europe 1977” fit is in there. We can look at it, if we like, by filtering the data and then unnesting the list column. out_le %&gt;% filter(continent == &quot;Europe&quot; &amp; year == 1977) %&gt;% unnest() #&gt; # A tibble: 30 x 6 #&gt; continent year country lifeExp pop gdpPercap #&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 Europe 1977 Albania 68.9 2509048 3533. #&gt; 2 Europe 1977 Austria 72.2 7568430 19749. #&gt; 3 Europe 1977 Belgium 72.8 9821800 19118. #&gt; 4 Europe 1977 Bosnia and Herzegovina 69.9 4086000 3528. #&gt; 5 Europe 1977 Bulgaria 70.8 8797022 7612. #&gt; 6 Europe 1977 Croatia 70.6 4318673 11305. #&gt; # … with 24 more rows List-columns are useful because we can act on them in a compact and tidy way. In particular, we can pass functions along to each row of the list-column and make something happen. For example, a moment ago we ran a regression of life expectancy and logged GDP for European countries in 1977. We can do that for every continent-year combination in the data. We first create a convenience function called fit_ols() that takes a single argument, df (for data frame) and that fits the linear model we are interested in. Then we map that function to each of our list-column rows in turn. Recall from Chapter 4 that mutate creates new variables or columns on the fly within a pipeline. The map action is an important idea in functional programming. If you have written code in other, more imperative languages you can think of it as a compact alternative to writing for … next loops. You can of course write loops like this in R. Computationally they are often not any less efficient than their functional alternatives. But mapping functions to arrays is more easily integrated into a sequence of data transformations. fit_ols &lt;- function(df) { lm(lifeExp ~ log(gdpPercap), data = df) } out_le &lt;- gapminder %&gt;% group_by(continent, year) %&gt;% nest() %&gt;% mutate(model = map(data, fit_ols)) out_le #&gt; # A tibble: 60 x 4 #&gt; continent year data model #&gt; &lt;fct&gt; &lt;int&gt; &lt;list&gt; &lt;list&gt; #&gt; 1 Asia 1952 &lt;tibble [33 × 4]&gt; &lt;lm&gt; #&gt; 2 Asia 1957 &lt;tibble [33 × 4]&gt; &lt;lm&gt; #&gt; 3 Asia 1962 &lt;tibble [33 × 4]&gt; &lt;lm&gt; #&gt; 4 Asia 1967 &lt;tibble [33 × 4]&gt; &lt;lm&gt; #&gt; 5 Asia 1972 &lt;tibble [33 × 4]&gt; &lt;lm&gt; #&gt; 6 Asia 1977 &lt;tibble [33 × 4]&gt; &lt;lm&gt; #&gt; # … with 54 more rows Before starting the pipeline we create a new function: It is a convenience function whose only job is to estimate a particular OLS model on some data. Like almost everything in R, functions are a kind of object. To make a new one, we use the slightly special function() function. (Nerds love that sort of thing.) There is a little more detail on creating functions in the Appendix. To see what fit_ols() looks like once it is created, type fit_ols without parentheses at the Console. To see what it does, try fit_ols(df = gapminder), or summary(fit_ols(gapminder)). Now we have two list-columns: data, and model. The latter was created by mapping the fit_ols() function to each row of data. Inside each element of model is a linear model for that continent-year. So we now have sixty OLS fits, one for every continent-year grouping. Having the models inside the list column is not much use to us in and of itself. But we can extract the information we want while keeping things in a tidy tabular form. For clarity we will run the pipeline from the beginning again, this time adding a few new steps. First we extract summary statistics from each model by mapping the tidy() function from broom to the model list column. Then we unnest the result, dropping the other columns in the process. Finally, we filter out all the Intercept terms, and also drop all observations from Oceania. In the case of the Intercepts we do this just out of convenience. Oceania we drop just because there are so few observations. We put the results in an object called out_tidy. fit_ols &lt;- function(df) { lm(lifeExp ~ log(gdpPercap), data = df) } out_tidy &lt;- gapminder %&gt;% group_by(continent, year) %&gt;% nest() %&gt;% mutate(model = map(data, fit_ols), tidied = map(model, tidy)) %&gt;% unnest(tidied, .drop = TRUE) %&gt;% filter(term %nin% &quot;(Intercept)&quot; &amp; continent %nin% &quot;Oceania&quot;) out_tidy %&gt;% sample_n(5) #&gt; # A tibble: 5 x 7 #&gt; continent year term estimate std.error statistic p.value #&gt; &lt;fct&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Americas 1992 log(gdpPercap) 6.06 0.895 6.77 0.000000664 #&gt; 2 Europe 2002 log(gdpPercap) 3.74 0.445 8.40 0.00000000391 #&gt; 3 Asia 2007 log(gdpPercap) 5.16 0.694 7.43 0.0000000226 #&gt; 4 Americas 1952 log(gdpPercap) 10.4 2.72 3.84 0.000827 #&gt; 5 Americas 1957 log(gdpPercap) 10.3 2.40 4.31 0.000261 We now have tidy regression output with an estimate of the association between log GDP per capita and life expectancy for each year, within continents. We can plot these estimates in a way that takes advantage of their groupiness. # Figure 6.9: Yearly estimates of the association between GDP and Life Expectancy, pooled by continent. p &lt;- ggplot(data = out_tidy, mapping = aes(x = year, y = estimate, ymin = estimate - 2*std.error, ymax = estimate + 2*std.error, group = continent, color = continent)) p + geom_pointrange(position = position_dodge(width = 1)) + scale_x_continuous(breaks = unique(gapminder$year)) + theme(legend.position = &quot;top&quot;) + labs(x = &quot;Year&quot;, y = &quot;Estimate&quot;, color = &quot;Continent&quot;) The call to position_dodge() within geom_pointrange() allows the point ranges for each continent to be near each other within years, instead of being plotted right on top of one another. We could have faceted the results by continent, but doing it this way lets us see differences in the yearly estimates much more easily. This technique is very useful not just for cases like this, but also when you want to compare the coefficients given by different kinds of statistical model. This sometimes happens when we’re interested in seeing how, say, OLS performs against some other model specification. 18.8 Plot marginal effects Our earlier discussion of predict() was about obtaining estimates of the average effect of some coefficient, net of the other terms in the model. Over the past decade, estimating and plotting partial or marginal effects from a model has become an increasingly common way of presenting accurate and interpretively useful predictions. Interest in marginal effects plots was stimulated by the realization that the interpretation of terms in logistic regression models, in particular, was trickier than it seemed—especially when there were interaction terms in the model (Ai &amp; Norton, 2003). Thomas Leeper’s margins package can make these plots for us. library(margins) To see it in action, we’ll take another look at the General Social Survey data in gss_sm, this time focusing on the binary variable, obama.As is common with retrospective questions on elections, rather more people claim to have voted for Obama than is consistent with the vote share he received in the election. It is coded 1 if the respondent said they voted for Barack Obama in the 2012 presidential election, and 0 otherwise. In this case, mostly for convenience here, the zero code includes all other answers to the question, including those who said they voted for Mitt Romney, those who said they did not vote, those who refused to answer, and those who said they didn’t know who they voted for. We will fit a logistic regression on obama, with age, polviews, race, and sex as the predictors. The age variable is the respondent’s age in years. The sex variable is coded as “Male” or “Female” with “Male” as the reference category. The race variable is coded as “White”, “Black”, or “Other” with “White” as the reference category. The polviews measure is a self-reported scale of the respondent’s political orientation from “Extremely Conservative” through “Extremely Liberal”, with “Moderate” in the middle. We take polviews and create a new variable, polviews_m, using the relevel() function to recode “Moderate” to be the reference category. We fit the model with the glm() function, and specify an interaction between race and sex. gss_sm$polviews_m &lt;- relevel(gss_sm$polviews, ref = &quot;Moderate&quot;) out_bo &lt;- glm(obama ~ polviews_m + sex*race, family = &quot;binomial&quot;, data = gss_sm) summary(out_bo) #&gt; #&gt; Call: #&gt; glm(formula = obama ~ polviews_m + sex * race, family = &quot;binomial&quot;, #&gt; data = gss_sm) #&gt; #&gt; Deviance Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -2.905 -0.554 0.177 0.542 2.244 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) 0.29649 0.13409 2.21 0.0270 * #&gt; polviews_mExtremely Liberal 2.37295 0.52504 4.52 6.2e-06 *** #&gt; polviews_mLiberal 2.60003 0.35667 7.29 3.1e-13 *** #&gt; polviews_mSlightly Liberal 1.29317 0.24843 5.21 1.9e-07 *** #&gt; polviews_mSlightly Conservative -1.35528 0.18129 -7.48 7.7e-14 *** #&gt; polviews_mConservative -2.34746 0.20038 -11.71 &lt; 2e-16 *** #&gt; polviews_mExtremely Conservative -2.72738 0.38721 -7.04 1.9e-12 *** #&gt; sexFemale 0.25487 0.14537 1.75 0.0796 . #&gt; raceBlack 3.84953 0.50132 7.68 1.6e-14 *** #&gt; raceOther -0.00214 0.43576 0.00 0.9961 #&gt; sexFemale:raceBlack -0.19751 0.66007 -0.30 0.7648 #&gt; sexFemale:raceOther 1.57483 0.58766 2.68 0.0074 ** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for binomial family taken to be 1) #&gt; #&gt; Null deviance: 2247.9 on 1697 degrees of freedom #&gt; Residual deviance: 1345.9 on 1686 degrees of freedom #&gt; (1169 observations deleted due to missingness) #&gt; AIC: 1370 #&gt; #&gt; Number of Fisher Scoring iterations: 6 The summary reports the coefficients and other information. We can now graph the data in any one of several ways. Using margins() we calculate the marginal effects for each variable: bo_m &lt;- margins(out_bo) summary(bo_m) #&gt; factor AME SE z p lower #&gt; polviews_mConservative -0.4119 0.0283 -14.5394 0.0000 -0.4674 #&gt; polviews_mExtremely Conservative -0.4538 0.0420 -10.7971 0.0000 -0.5361 #&gt; polviews_mExtremely Liberal 0.2681 0.0295 9.0996 0.0000 0.2103 #&gt; polviews_mLiberal 0.2768 0.0229 12.0736 0.0000 0.2319 #&gt; polviews_mSlightly Conservative -0.2658 0.0330 -8.0596 0.0000 -0.3304 #&gt; polviews_mSlightly Liberal 0.1933 0.0303 6.3896 0.0000 0.1340 #&gt; raceBlack 0.4032 0.0173 23.3568 0.0000 0.3694 #&gt; raceOther 0.1247 0.0386 3.2297 0.0012 0.0490 #&gt; sexFemale 0.0443 0.0177 2.5073 0.0122 0.0097 #&gt; upper #&gt; -0.3564 #&gt; -0.3714 #&gt; 0.3258 #&gt; 0.3218 #&gt; -0.2011 #&gt; 0.2526 #&gt; 0.4371 #&gt; 0.2005 #&gt; 0.0789 The margins library comes with several plot methods of its own. If you wish, at this point you can just try plot(bo_m) to see a plot of the average marginal effects, produced with the general look of a Stata graphic. Other plot methods in the margins library include cplot(), which visualizes marginal effects conditional on a second variable, and image(), which shows predictions or marginal effects as a filled heatmap or contour plot. Alternatively, we can take results from margins() and plot them ourselves. To clean up the summary a little a little, we convert it to a tibble, then use prefix_strip() and prefix_replace() to tidy the labels. We want to strip the polviews_m and sex prefixes, and (to avoid ambiguity about “Other”), adjust the race prefix. bo_gg &lt;- as_tibble(summary(bo_m)) prefixes &lt;- c(&quot;polviews_m&quot;, &quot;sex&quot;) bo_gg$factor &lt;- prefix_strip(bo_gg$factor, prefixes) bo_gg$factor &lt;- prefix_replace(bo_gg$factor, &quot;race&quot;, &quot;Race: &quot;) bo_gg %&gt;% select(factor, AME, lower, upper) #&gt; # A tibble: 9 x 4 #&gt; factor AME lower upper #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Conservative -0.412 -0.467 -0.356 #&gt; 2 Extremely Conservative -0.454 -0.536 -0.371 #&gt; 3 Extremely Liberal 0.268 0.210 0.326 #&gt; 4 Liberal 0.277 0.232 0.322 #&gt; 5 Slightly Conservative -0.266 -0.330 -0.201 #&gt; 6 Slightly Liberal 0.193 0.134 0.253 #&gt; # … with 3 more rows Now we have a table that we can plot as we have learned: p &lt;- ggplot(data = bo_gg, aes(x = reorder(factor, AME), y = AME, ymin = lower, ymax = upper)) p + geom_hline(yintercept = 0, color = &quot;gray80&quot;) + geom_pointrange() + coord_flip() + labs(x = NULL, y = &quot;Average Marginal Effect&quot;) If we are just interested in getting conditional effects for a particular variable, then conveniently we can ask the plot methods in the margins library to do the work calculating effects for us but without drawing their plot. Instead, they can return the results in a format we can easily use in ggplot, and with less need for clean up, for the clean-up. For example, with cplot(): pv_cp &lt;- cplot(out_bo, x = &quot;sex&quot;, draw = FALSE) #&gt; xvals yvals upper lower #&gt; 1 Male 0.574 0.638 0.509 #&gt; 2 Female 0.634 0.689 0.580 p &lt;- ggplot(data = pv_cp, aes(x = reorder(xvals, yvals), y = yvals, ymin = lower, ymax = upper)) p + geom_hline(yintercept = 0, color = &quot;gray80&quot;) + geom_pointrange() + coord_flip() + labs(x = NULL, y = &quot;Conditional Effect&quot;) The margins package is under active development. It can do much more than described here. The vignettes that come with the package provide more extensive discussion and numerous examples. 18.9 Plots from complex surveys Social scientists often work with data collected using a complex survey design. Survey instruments may be stratified by region or some other characteristic, contain replicate weights to make them comparable to a reference population, have a clustered structure, and so on. In Chapter 4 we learned how calculate and then plot frequency tables of categorical variables, using some data from the General Social Survey (GSS). However, if we want accurate estimates of US households from the GSS, we will need to take the survey’s design into account, and use the survey weights provided in the dataset. Thomas Lumley’s survey library provides a comprehensive set of tools for addressing these issues. The tools and the theory behind them are discussed in detail in Lumley (2010), and an overview of the package is provided in Lumley (2004). While the functions in the survey package are straightforward to use and return results in a generally tidy form, the package predates the tidyverse and its conventions by several years. This means we cannot use survey functions directly with dplyr. However, Greg Freedman Ellis has written a helper package, srvyr, that solves this problem for us, and lets us use the survey library’s functions within a data analysis pipeline in a familiar way. For example, the gss_lon data contains a small subset of measures from every wave of the GSS since its inception in 1972. It also contains several variables that describe the design of the survey and provide replicate weights for observations in various years. These technical details are described in the GSS documentation. Similar information is typically provided by other complex surveys. Here we will use this design information to calculate weighted estimates of the distribution of educational attainment by race, for selected survey years from 1976 to 2016. To begin, we load the survey and srvyr libraries. library(survey) #&gt; Loading required package: grid #&gt; Loading required package: Matrix #&gt; #&gt; Attaching package: &#39;Matrix&#39; #&gt; The following object is masked from &#39;package:tidyr&#39;: #&gt; #&gt; expand #&gt; #&gt; Attaching package: &#39;survey&#39; #&gt; The following object is masked from &#39;package:graphics&#39;: #&gt; #&gt; dotchart library(srvyr) #&gt; #&gt; Attaching package: &#39;srvyr&#39; #&gt; The following object is masked from &#39;package:stats&#39;: #&gt; #&gt; filter Next, we take our gss_lon dataset and use the survey tools to create a new object that contains the data, as before, but with some additional information about the survey’s design: options(survey.lonely.psu = &quot;adjust&quot;) options(na.action=&quot;na.pass&quot;) gss_wt &lt;- subset(gss_lon, year &gt; 1974) %&gt;% mutate(stratvar = interaction(year, vstrat)) %&gt;% as_survey_design(ids = vpsu, strata = stratvar, weights = wtssall, nest = TRUE) The two options set at the beginning provide some information to the survey library about how to behave. You should consult Lumley (2010) and the survey package documentation for details. The subsequent operations create gss_wt, an object with one additional column (stratvar), describing the yearly sampling strata. We use the interaction() function to do this. It multiplies the vstrat variable by the year variable to get a vector of stratum information for each year. We have to do this because of the way the GSS codes its stratum information. In the next step, we use the as_survey_design() function to add the key pieces of information about the survey design. It adds information about the sampling identifiers (ids), the strata (strata), and the replicate weights (weights). With those in place we can take advantage of a large number of specialized functions in the survey library that allow us to calculate properly weighted survey means or estimate models with the correct sampling specification. For example, we can easily calculate the distribution of education by race for a series of years from 1976 to 2016. We use survey_mean() to do this: out_grp &lt;- gss_wt %&gt;% filter(year %in% seq(1976, 2016, by = 4)) %&gt;% group_by(year, race, degree) %&gt;% summarize(prop = survey_mean(na.rm = TRUE)) #&gt; Warning: Factor `degree` contains implicit NA, consider using #&gt; `forcats::fct_explicit_na` out_grp #&gt; # A tibble: 150 x 5 #&gt; year race degree prop prop_se #&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1976 White Lt High School 0.328 0.0160 #&gt; 2 1976 White High School 0.518 0.0162 #&gt; 3 1976 White Junior College 0.0129 0.00298 #&gt; 4 1976 White Bachelor 0.101 0.00960 #&gt; 5 1976 White Graduate 0.0393 0.00644 #&gt; 6 1976 Black Lt High School 0.562 0.0611 #&gt; # … with 144 more rows The results returned in out_grp include standard errors. We can also ask survey_mean() to calculate confidence intervals for us, if we wish. Grouping with group_by() lets us calculate counts or means for the innermost variable, grouped by the next variable “up” or “out”, in this case, degree by race, such that the proportions for degree will sum to one for each group in race, and this will be done separately for each value of year. If we want the marginal frequencies, such that the values for all combinations of race and degree sum to one within each year, we first have to interact the variables we are cross-classifying. Then we group by the new interacted variable and do the calculation as before: out_mrg &lt;- gss_wt %&gt;% filter(year %in% seq(1976, 2016, by = 4)) %&gt;% mutate(racedeg = interaction(race, degree)) %&gt;% group_by(year, racedeg) %&gt;% summarize(prop = survey_mean(na.rm = TRUE)) #&gt; Warning: Factor `racedeg` contains implicit NA, consider using #&gt; `forcats::fct_explicit_na` out_mrg #&gt; # A tibble: 150 x 4 #&gt; year racedeg prop prop_se #&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1976 White.Lt High School 0.298 0.0146 #&gt; 2 1976 Black.Lt High School 0.0471 0.00840 #&gt; 3 1976 Other.Lt High School 0.00195 0.00138 #&gt; 4 1976 White.High School 0.471 0.0160 #&gt; 5 1976 Black.High School 0.0283 0.00594 #&gt; 6 1976 Other.High School 0.00325 0.00166 #&gt; # … with 144 more rows This gives us the numbers that we want and returns them in a tidy data frame. The interaction() function produces variable labels that are a compound of the two variables we interacted, with each combination of categories separated by a period, (such as White.Graduate. However, perhaps we would like to see these categories as two separate columns, one for race and one for education, as before. Because the variable labels are organized in a predictable way, we can use one of the convenient functions in the tidyverse’s tidyr library to separate the single variable into two columns while correctly preserving the row values. Appropriately, this function is called separate(). out_mrg &lt;- gss_wt %&gt;% filter(year %in% seq(1976, 2016, by = 4)) %&gt;% mutate(racedeg = interaction(race, degree)) %&gt;% group_by(year, racedeg) %&gt;% summarize(prop = survey_mean(na.rm = TRUE)) %&gt;% separate(racedeg, sep = &quot;\\\\.&quot;, into = c(&quot;race&quot;, &quot;degree&quot;)) #&gt; Warning: Factor `racedeg` contains implicit NA, consider using #&gt; `forcats::fct_explicit_na` out_mrg #&gt; # A tibble: 150 x 5 #&gt; year race degree prop prop_se #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1976 White Lt High School 0.298 0.0146 #&gt; 2 1976 Black Lt High School 0.0471 0.00840 #&gt; 3 1976 Other Lt High School 0.00195 0.00138 #&gt; 4 1976 White High School 0.471 0.0160 #&gt; 5 1976 Black High School 0.0283 0.00594 #&gt; 6 1976 Other High School 0.00325 0.00166 #&gt; # … with 144 more rows The call to separate() says to take the racedeg column, split each value when it sees a period, and reorganize the results into two columns, race and degree. This gives us a tidy table much like out_grp, but for the marginal frequencies. Reasonable people can disagree over how best to plot a small multiple of a frequency table while faceting by year, especially when there is some measure of uncertainty attached. A barplot is the obvious approach for a single case, but when there are many years it can become difficult to compare bars across panels. This is especially the case when standard errors or confidence intervals are used in conjunction with bars.Sometimes it may be preferable to show that the underlying variable is categorical, as a bar chart makes clear, and not continuous, as a line graph suggests. Here the trade-off is in favor of the line graphs as the bars are very hard to compare across facets. This is sometimes called a “dynamite plot”, not because it looks amazing but because the t-shaped error bars on the tops of the columns make them look like cartoon dynamite plungers. An alternative is to use a line graph to join up the time observations, faceting on educational categories instead of year. Figure 6.12 shows the results for our GSS data in dynamite-plot form, where the error bars are defined as twice the standard error in either direction around the point estimate. p &lt;- ggplot(data = subset(out_grp, race %nin% &quot;Other&quot;), mapping = aes(x = degree, y = prop, ymin = prop - 2*prop_se, ymax = prop + 2*prop_se, fill = race, color = race, group = race)) dodge &lt;- position_dodge(width=0.9) p + geom_col(position = dodge, alpha = 0.2) + geom_errorbar(position = dodge, width = 0.2) + scale_x_discrete(labels = scales::wrap_format(10)) + scale_y_continuous(labels = scales::percent) + scale_color_brewer(type = &quot;qual&quot;, palette = &quot;Dark2&quot;) + scale_fill_brewer(type = &quot;qual&quot;, palette = &quot;Dark2&quot;) + labs(title = &quot;Educational Attainment by Race&quot;, subtitle = &quot;GSS 1976-2016&quot;, fill = &quot;Race&quot;, color = &quot;Race&quot;, x = NULL, y = &quot;Percent&quot;) + facet_wrap(~ year, ncol = 2) + theme(legend.position = &quot;top&quot;) This plot has a few cosmetic details and adjustments that we will learn more about in Chapter 8. As before, I encourage you to peel back the plot from the bottom, one instruction at a time, to see what changes. One useful adjustment to notice is the new call to the scales library to adjust the labels on the x-axis. The adjustment on the y-axis is familiar, scales::percent to convert the proportion to a percentage. On the x-axis, the issue is that several of the labels are rather long. If we do not adjust them they will print over one another. The scales::wrap_format() function will break long labels into lines. It takes a single numerical argument (here 10) that is the maximum length a string can be before it is wrapped onto a new line. Faceting by education instead.Figure 6.13: Faceting by education instead. A graph like this is true to the categorical nature of the data, while showing the breakdown of groups within each year. But you should experiment with some alternatives. For example, we might decide that it is better to facet by degree category instead, and put the year on the x-axis within each panel. If we do that, then we can use geom_line() to show a time trend, which is more natural, and geom_ribbon() to show the error range. This is perhaps a better way to show the data, especially as it brings out the time trends within each degree category, and allows us to see the similarities and differences by racial classification at the same time. p &lt;- ggplot(data = subset(out_grp, race %nin% &quot;Other&quot;), mapping = aes(x = year, y = prop, ymin = prop - 2*prop_se, ymax = prop + 2*prop_se, fill = race, color = race, group = race)) p + geom_ribbon(alpha = 0.3, aes(color = NULL)) + geom_line() + facet_wrap(~ degree, ncol = 1) + scale_y_continuous(labels = scales::percent) + scale_color_brewer(type = &quot;qual&quot;, palette = &quot;Dark2&quot;) + scale_fill_brewer(type = &quot;qual&quot;, palette = &quot;Dark2&quot;) + labs(title = &quot;Educational Attainment\\nby Race&quot;, subtitle = &quot;GSS 1976-2016&quot;, fill = &quot;Race&quot;, color = &quot;Race&quot;, x = NULL, y = &quot;Percent&quot;) + theme(legend.position = &quot;top&quot;) 18.10 Where to go next In general, when you estimate models and want to plot the results, the difficult step is not the plotting but rather calculating and extracting the right numbers. Generating predicted values and measures of confidence or uncertainty from models requires that you understand the model you are fitting, and the function you use to fit it, especially when it involves interactions, cross-level effects, or transformations of the predictor or response scales. The details can vary substantially from model type to model type, and also with the goals of any particular analysis. It is unwise to approach them mechanically. That said, several tools exist to help you work with model objects and produce a default set of plots from them. 18.10.1 Default plots for models Just as model objects in R usually have a default summary() method, printing out an overview tailored to the type of model it is, they will usually have a default plot() method, too. Figures produced by plot() are typically not generated via ggplot, but it is usually worth exploring them. They typically make use of either R’s base graphics or the lattice library (Sarkar, 2008). These are two plotting systems that we do not cover in this book. Default plot methods are easy to examine. Let’s take a look again at our simple OLS model. out &lt;- lm(formula = lifeExp ~ log(gdpPercap) + pop + continent, data = gapminder) To look at some of R’s default plots for this model, use the plot() function. # Plot not shown plot(out, which = c(1,2), ask=FALSE) The which() statement here selects the first two of four default plots for this kind of model. If you want to easily reproduce base R’s default model graphics using ggplot, the ggfortify library is worth examining. It is in some ways similar to broom, in that it tidies the output of model objects, but it focuses on producing a standard plot (or group of plots) for a wide variety of model types. It does this by defining a function called autoplot(). The idea is to be able to use autoplot() with the output of many different kinds of model. A second option worth looking at is the coefplot library. It provides a quick way to produce good-quality plots of point estimates and confidence intervals. It has the advantage of managing the estimation of interaction effects and other occasionally tricky calculations. library(coefplot) out &lt;- lm(formula = lifeExp ~ log(gdpPercap) + log(pop) + continent, data = gapminder) coefplot(out, sort = &quot;magnitude&quot;, intercept = FALSE) 18.10.2 Tools in development Tidyverse tools for modeling and model exploration are being actively developed. The broom and margins libraries continue to get more and more useful. There are also other projects worth paying attention to. The infer packageinfer.netlify.com is in its early stages but can already do useful things in a pipeline-friendly way. You can install it from CRAN with install.packages(&quot;infer&quot;). 18.10.3 Extensions to ggplot The GGally package provides a suite of functions designed to make producing standard but somewhat complex plots a little easier. For instance, it can produce generalized pairs plots, a useful way of quickly examining possible relationships between several different variables at once. This sort of plot is like the visual version of a correlation matrix. It shows a bivariate plot for all pairs of variables in the data. This is relatively straightforward when all the variables are continuous measures. Things get more complex when, as is often the case in the social sciences, some or all variables are categorical or otherwise limited in the range of values they can take. A generalized pairs plot can handle these cases. For example, Figure ?? shows a generalized pairs plot for five variables from the organdata dataset. library(GGally) organdata_sm &lt;- organdata %&gt;% select(donors, pop_dens, pubhealth, roads, consent_law) ggpairs(data = organdata_sm, mapping = aes(color = consent_law), upper = list(continuous = wrap(&quot;density&quot;), combo = &quot;box_no_facet&quot;), lower = list(continuous = wrap(&quot;points&quot;), combo = wrap(&quot;dot_no_facet&quot;))) Multi-panel plots like this are intrinsically very rich in information. When combined with several within-panel types of representation, or any more than a modest number of variables, they can become quite complex. They should be used less for the presentation of finished work, although it is possible. More often they are a useful tool for the working researcher to quickly investigate aspects of a dataset. The goal is not to pithily summarize a single point one already knows, but to open things up for further exploration. "]
]
