[
["index.html", "Feature Engineering Prerequisites", " Feature Engineering Alfonso R. Reyes 2019-09-18 Prerequisites print(assets_dir) #&gt; [1] &quot;/home/datascience/repos/machine-learning-rsuite/import/assets&quot; image_file &lt;- file.path(assets_dir, &quot;linear_regression.jpg&quot;) file.exists(image_file) #&gt; [1] TRUE knitr::include_graphics(image_file) This is a sample book written in Markdown. You can use anything that Pandoc’s Markdown supports, e.g., a math equation \\(a^2 + b^2 = c^2\\). The bookdown package can be installed from CRAN or Github: install.packages(&quot;bookdown&quot;) # or the development version # devtools::install_github(&quot;rstudio/bookdown&quot;) Remember each Rmd file contains one and only one chapter, and a chapter is defined by the first-level heading #. To compile this example to PDF, you need XeLaTeX. You are recommended to install TinyTeX (which includes XeLaTeX): https://yihui.name/tinytex/. "],
["employee-attrition-employee-attrition-dataset-lime-package.html", "Chapter 1 Employee attrition. Employee-Attrition dataset. LIME package 1.1 Introduction 1.2 Modeling Employee attrition 1.3 Model 1.4 Predict 1.5 Performance 1.6 The lime package 1.7 Feature Importance Visualization 1.8 Conclusions", " Chapter 1 Employee attrition. Employee-Attrition dataset. LIME package Article: https://www.business-science.io/business/2017/09/18/hr_employee_attrition.html Data: https://www.ibm.com/communities/analytics/watson-analytics-blog/hr-employee-attrition/ 1.1 Introduction 1.1.1 Employee attrition: a major problem Bill Gates was once quoted as saying, “You take away our top 20 employees and we [Microsoft] become a mediocre company”. His statement cuts to the core of a major problem: employee attrition. An organization is only as good as its employees, and these people are the true source of its competitive advantage. Organizations face huge costs resulting from employee turnover. Some costs are tangible such as training expenses and the time it takes from when an employee starts to when they become a productive member. However, the most important costs are intangible. Consider what’s lost when a productive employee quits: new product ideas, great project management, or customer relationships. With advances in machine learning and data science, its possible to not only predict employee attrition but to understand the key variables that influence turnover. We’ll take a look at two cutting edge techniques: Machine Learning with h2o.automl() from the h2o package: This function takes automated machine learning to the next level by testing a number of advanced algorithms such as random forests, ensemble methods, and deep learning along with more traditional algorithms such as logistic regression. The main takeaway is that we can now easily achieve predictive performance that is in the same ball park (and in some cases even better than) commercial algorithms and ML/AI software. Feature Importance with the lime package: The problem with advanced machine learning algorithms such as deep learning is that it’s near impossible to understand the algorithm because of its complexity. This has all changed with the lime package. The major advancement with lime is that, by recursively analyzing the models locally, it can extract feature importance that repeats globally. What this means to us is that lime has opened the door to understanding the ML models regardless of complexity. Now the best (and typically very complex) models can also be investigated and potentially understood as to what variables or features make the model tick. 1.1.2 Employee attrition: machine learning analysis With these new automated ML tools combined with tools to uncover critical variables, we now have capabilities for both extreme predictive accuracy and understandability, which was previously impossible! We’ll investigate an HR Analytic example of employee attrition that was evaluated by IBM Watson. 1.1.3 Where we got the data The example comes from IBM Watson Analytics website. You can download the data and read the analysis here: Get data used in this post here. Read IBM Watson Analytics article here. To summarize, the article makes a usage case for IBM Watson as an automated ML platform. The article shows that using Watson, the analyst was able to detect features that led to increased probability of attrition. 1.1.4 Automated machine learning (what we did with the data) In this example we’ll show how we can use the combination of H2O for developing a complex model with high predictive accuracy on unseen data and then how we can use LIME to understand important features related to employee attrition. 1.1.5 Load packages Load the following packages. # Load the following packages library(tidyquant) # Loads tidyverse and several other pkgs library(readxl) # Super simple excel reader library(h2o) # Professional grade ML pkg library(lime) # Explain complex black-box ML models 1.1.6 Load data Download the data here. You can load the data using read_excel(), pointing the path to your local file. # Read excel data hr_data_raw &lt;- read_excel(path = file.path(data_raw_dir, &quot;WA_Fn-UseC_-HR-Employee-Attrition.xlsx&quot;)) Let’s check out the raw data. It’s 1470 rows (observations) by 35 columns (features). The “Attrition” column is our target. We’ll use all other columns as features to our model. # View first 10 rows hr_data_raw[1:10,] %&gt;% knitr::kable(caption = &quot;First 10 rows&quot;) Table 1.1: First 10 rows Age Attrition BusinessTravel DailyRate Department DistanceFromHome Education EducationField EmployeeCount EmployeeNumber EnvironmentSatisfaction Gender HourlyRate JobInvolvement JobLevel JobRole JobSatisfaction MaritalStatus MonthlyIncome MonthlyRate NumCompaniesWorked Over18 OverTime PercentSalaryHike PerformanceRating RelationshipSatisfaction StandardHours StockOptionLevel TotalWorkingYears TrainingTimesLastYear WorkLifeBalance YearsAtCompany YearsInCurrentRole YearsSinceLastPromotion YearsWithCurrManager 41 Yes Travel_Rarely 1102 Sales 1 2 Life Sciences 1 1 2 Female 94 3 2 Sales Executive 4 Single 5993 19479 8 Y Yes 11 3 1 80 0 8 0 1 6 4 0 5 49 No Travel_Frequently 279 Research &amp; Development 8 1 Life Sciences 1 2 3 Male 61 2 2 Research Scientist 2 Married 5130 24907 1 Y No 23 4 4 80 1 10 3 3 10 7 1 7 37 Yes Travel_Rarely 1373 Research &amp; Development 2 2 Other 1 4 4 Male 92 2 1 Laboratory Technician 3 Single 2090 2396 6 Y Yes 15 3 2 80 0 7 3 3 0 0 0 0 33 No Travel_Frequently 1392 Research &amp; Development 3 4 Life Sciences 1 5 4 Female 56 3 1 Research Scientist 3 Married 2909 23159 1 Y Yes 11 3 3 80 0 8 3 3 8 7 3 0 27 No Travel_Rarely 591 Research &amp; Development 2 1 Medical 1 7 1 Male 40 3 1 Laboratory Technician 2 Married 3468 16632 9 Y No 12 3 4 80 1 6 3 3 2 2 2 2 32 No Travel_Frequently 1005 Research &amp; Development 2 2 Life Sciences 1 8 4 Male 79 3 1 Laboratory Technician 4 Single 3068 11864 0 Y No 13 3 3 80 0 8 2 2 7 7 3 6 59 No Travel_Rarely 1324 Research &amp; Development 3 3 Medical 1 10 3 Female 81 4 1 Laboratory Technician 1 Married 2670 9964 4 Y Yes 20 4 1 80 3 12 3 2 1 0 0 0 30 No Travel_Rarely 1358 Research &amp; Development 24 1 Life Sciences 1 11 4 Male 67 3 1 Laboratory Technician 3 Divorced 2693 13335 1 Y No 22 4 2 80 1 1 2 3 1 0 0 0 38 No Travel_Frequently 216 Research &amp; Development 23 3 Life Sciences 1 12 4 Male 44 2 3 Manufacturing Director 3 Single 9526 8787 0 Y No 21 4 2 80 0 10 2 3 9 7 1 8 36 No Travel_Rarely 1299 Research &amp; Development 27 3 Medical 1 13 3 Male 94 3 2 Healthcare Representative 3 Married 5237 16577 6 Y No 13 3 2 80 2 17 3 2 7 7 7 7 The only pre-processing we’ll do in this example is change all character data types to factors. This is needed for H2O. We could make a number of other numeric data that is actually categorical factors, but this tends to increase modeling time and can have little improvement on model performance. hr_data &lt;- hr_data_raw %&gt;% mutate_if(is.character, as.factor) %&gt;% select(Attrition, everything()) Let’s take a glimpse at the processed dataset. We can see all of the columns. Note our target (“Attrition”) is the first column. glimpse(hr_data) #&gt; Observations: 1,470 #&gt; Variables: 35 #&gt; $ Attrition &lt;fct&gt; Yes, No, Yes, No, No, No, No, No, No, N… #&gt; $ Age &lt;dbl&gt; 41, 49, 37, 33, 27, 32, 59, 30, 38, 36,… #&gt; $ BusinessTravel &lt;fct&gt; Travel_Rarely, Travel_Frequently, Trave… #&gt; $ DailyRate &lt;dbl&gt; 1102, 279, 1373, 1392, 591, 1005, 1324,… #&gt; $ Department &lt;fct&gt; Sales, Research &amp; Development, Research… #&gt; $ DistanceFromHome &lt;dbl&gt; 1, 8, 2, 3, 2, 2, 3, 24, 23, 27, 16, 15… #&gt; $ Education &lt;dbl&gt; 2, 1, 2, 4, 1, 2, 3, 1, 3, 3, 3, 2, 1, … #&gt; $ EducationField &lt;fct&gt; Life Sciences, Life Sciences, Other, Li… #&gt; $ EmployeeCount &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … #&gt; $ EmployeeNumber &lt;dbl&gt; 1, 2, 4, 5, 7, 8, 10, 11, 12, 13, 14, 1… #&gt; $ EnvironmentSatisfaction &lt;dbl&gt; 2, 3, 4, 4, 1, 4, 3, 4, 4, 3, 1, 4, 1, … #&gt; $ Gender &lt;fct&gt; Female, Male, Male, Female, Male, Male,… #&gt; $ HourlyRate &lt;dbl&gt; 94, 61, 92, 56, 40, 79, 81, 67, 44, 94,… #&gt; $ JobInvolvement &lt;dbl&gt; 3, 2, 2, 3, 3, 3, 4, 3, 2, 3, 4, 2, 3, … #&gt; $ JobLevel &lt;dbl&gt; 2, 2, 1, 1, 1, 1, 1, 1, 3, 2, 1, 2, 1, … #&gt; $ JobRole &lt;fct&gt; Sales Executive, Research Scientist, La… #&gt; $ JobSatisfaction &lt;dbl&gt; 4, 2, 3, 3, 2, 4, 1, 3, 3, 3, 2, 3, 3, … #&gt; $ MaritalStatus &lt;fct&gt; Single, Married, Single, Married, Marri… #&gt; $ MonthlyIncome &lt;dbl&gt; 5993, 5130, 2090, 2909, 3468, 3068, 267… #&gt; $ MonthlyRate &lt;dbl&gt; 19479, 24907, 2396, 23159, 16632, 11864… #&gt; $ NumCompaniesWorked &lt;dbl&gt; 8, 1, 6, 1, 9, 0, 4, 1, 0, 6, 0, 0, 1, … #&gt; $ Over18 &lt;fct&gt; Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, … #&gt; $ OverTime &lt;fct&gt; Yes, No, Yes, Yes, No, No, Yes, No, No,… #&gt; $ PercentSalaryHike &lt;dbl&gt; 11, 23, 15, 11, 12, 13, 20, 22, 21, 13,… #&gt; $ PerformanceRating &lt;dbl&gt; 3, 4, 3, 3, 3, 3, 4, 4, 4, 3, 3, 3, 3, … #&gt; $ RelationshipSatisfaction &lt;dbl&gt; 1, 4, 2, 3, 4, 3, 1, 2, 2, 2, 3, 4, 4, … #&gt; $ StandardHours &lt;dbl&gt; 80, 80, 80, 80, 80, 80, 80, 80, 80, 80,… #&gt; $ StockOptionLevel &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 3, 1, 0, 2, 1, 0, 1, … #&gt; $ TotalWorkingYears &lt;dbl&gt; 8, 10, 7, 8, 6, 8, 12, 1, 10, 17, 6, 10… #&gt; $ TrainingTimesLastYear &lt;dbl&gt; 0, 3, 3, 3, 3, 2, 3, 2, 2, 3, 5, 3, 1, … #&gt; $ WorkLifeBalance &lt;dbl&gt; 1, 3, 3, 3, 3, 2, 2, 3, 3, 2, 3, 3, 2, … #&gt; $ YearsAtCompany &lt;dbl&gt; 6, 10, 0, 8, 2, 7, 1, 1, 9, 7, 5, 9, 5,… #&gt; $ YearsInCurrentRole &lt;dbl&gt; 4, 7, 0, 7, 2, 7, 0, 0, 7, 7, 4, 5, 2, … #&gt; $ YearsSinceLastPromotion &lt;dbl&gt; 0, 1, 0, 3, 2, 3, 0, 0, 1, 7, 0, 0, 4, … #&gt; $ YearsWithCurrManager &lt;dbl&gt; 5, 7, 0, 0, 2, 6, 0, 0, 8, 7, 3, 8, 3, … 1.2 Modeling Employee attrition We are going to use the h2o.automl() function from the H2O platform to model employee attrition. 1.2.1 Machine Learning with h2o First, we need to initialize the Java Virtual Machine (JVM) that H2O uses locally. # Initialize H2O JVM h2o.init() #&gt; #&gt; H2O is not running yet, starting it now... #&gt; #&gt; Note: In case of errors look at the following log files: #&gt; /tmp/RtmpgKFIvi/h2o_datascience_started_from_r.out #&gt; /tmp/RtmpgKFIvi/h2o_datascience_started_from_r.err #&gt; #&gt; #&gt; Starting H2O JVM and connecting: . Connection successful! #&gt; #&gt; R is connected to the H2O cluster: #&gt; H2O cluster uptime: 1 seconds 289 milliseconds #&gt; H2O cluster timezone: America/Chicago #&gt; H2O data parsing timezone: UTC #&gt; H2O cluster version: 3.22.1.1 #&gt; H2O cluster version age: 8 months and 20 days !!! #&gt; H2O cluster name: H2O_started_from_R_datascience_mwl453 #&gt; H2O cluster total nodes: 1 #&gt; H2O cluster total memory: 6.96 GB #&gt; H2O cluster total cores: 8 #&gt; H2O cluster allowed cores: 8 #&gt; H2O cluster healthy: TRUE #&gt; H2O Connection ip: localhost #&gt; H2O Connection port: 54321 #&gt; H2O Connection proxy: NA #&gt; H2O Internal Security: FALSE #&gt; H2O API Extensions: XGBoost, Algos, AutoML, Core V3, Core V4 #&gt; R Version: R version 3.6.0 (2019-04-26) h2o.no_progress() # Turn off output of progress bars Next, we change our data to an h2o object that the package can interpret. We also split the data into training, validation, and test sets. Our preference is to use 70%, 15%, 15%, respectively. # Split data into Train/Validation/Test Sets hr_data_h2o &lt;- as.h2o(hr_data) split_h2o &lt;- h2o.splitFrame(hr_data_h2o, c(0.7, 0.15), seed = 1234 ) train_h2o &lt;- h2o.assign(split_h2o[[1]], &quot;train&quot; ) # 70% valid_h2o &lt;- h2o.assign(split_h2o[[2]], &quot;valid&quot; ) # 15% test_h2o &lt;- h2o.assign(split_h2o[[3]], &quot;test&quot; ) # 15% 1.3 Model Now we are ready to model. We’ll set the target and feature names. The target is what we aim to predict (in our case “Attrition”). The features (every other column) are what we will use to model the prediction. # Set names for h2o y &lt;- &quot;Attrition&quot; x &lt;- setdiff(names(train_h2o), y) Now the fun begins. We run the h2o.automl() setting the arguments it needs to run models against. For more information, see the h2o.automl documentation. x = x: The names of our feature columns. y = y: The name of our target column. training_frame = train_h2o: Our training set consisting of 70% of the data. leaderboard_frame = valid_h2o: Our validation set consisting of 15% of the data. H2O uses this to ensure the model does not overfit the data. max_runtime_secs = 30: We supply this to speed up H2O’s modeling. The algorithm has a large number of complex models so we want to keep things moving at the expense of some accuracy. # Run the automated machine learning automl_models_h2o &lt;- h2o.automl( x = x, y = y, training_frame = train_h2o, leaderboard_frame = valid_h2o, max_runtime_secs = 30 ) All of the models are stored the automl_models_h2o object. However, we are only concerned with the leader, which is the best model in terms of accuracy on the validation set. We’ll extract it from the models object. # Extract leader model automl_leader &lt;- automl_models_h2o@leader 1.4 Predict Now we are ready to predict on our test set, which is unseen from during our modeling process. This is the true test of performance. We use the h2o.predict() function to make predictions. # Predict on hold-out set, test_h2o pred_h2o &lt;- h2o.predict(object = automl_leader, newdata = test_h2o) 1.5 Performance Now we can evaluate our leader model. We’ll reformat the test set an add the predictions as column so we have the actual and prediction columns side-by-side. # Prep for performance assessment test_performance &lt;- test_h2o %&gt;% tibble::as_tibble() %&gt;% select(Attrition) %&gt;% add_column(pred = as.vector(pred_h2o$predict)) %&gt;% mutate_if(is.character, as.factor) test_performance #&gt; # A tibble: 211 x 2 #&gt; Attrition pred #&gt; &lt;fct&gt; &lt;fct&gt; #&gt; 1 No No #&gt; 2 No No #&gt; 3 Yes Yes #&gt; 4 No No #&gt; 5 No No #&gt; 6 No No #&gt; # … with 205 more rows We can use the table() function to quickly get a confusion table of the results. We see that the leader model wasn’t perfect, but it did a decent job identifying employees that are likely to quit. For perspective, a logistic regression would not perform nearly this well. # Confusion table counts confusion_matrix &lt;- test_performance %&gt;% table() confusion_matrix #&gt; pred #&gt; Attrition No Yes #&gt; No 166 16 #&gt; Yes 10 19 We’ll run through a binary classification analysis to understand the model performance. # Performance analysis tn &lt;- confusion_matrix[1] tp &lt;- confusion_matrix[4] fp &lt;- confusion_matrix[3] fn &lt;- confusion_matrix[2] accuracy &lt;- (tp + tn) / (tp + tn + fp + fn) misclassification_rate &lt;- 1 - accuracy recall &lt;- tp / (tp + fn) precision &lt;- tp / (tp + fp) null_error_rate &lt;- tn / (tp + tn + fp + fn) tibble( accuracy, misclassification_rate, recall, precision, null_error_rate ) %&gt;% transpose() #&gt; [[1]] #&gt; [[1]]$accuracy #&gt; [1] 0.877 #&gt; #&gt; [[1]]$misclassification_rate #&gt; [1] 0.123 #&gt; #&gt; [[1]]$recall #&gt; [1] 0.655 #&gt; #&gt; [[1]]$precision #&gt; [1] 0.543 #&gt; #&gt; [[1]]$null_error_rate #&gt; [1] 0.787 It is important to understand is that the accuracy can be misleading: 88% sounds pretty good especially for modeling HR data, but if we just pick Attrition = NO we would get an accuracy of about 79%. Doesn’t sound so great now. Before we make our final judgement, let’s dive a little deeper into precision and recall. Precision is when the model predicts yes, how often is it actually yes. Recall (also true positive rate or specificity) is when the actual value is yes how often is the model correct. Confused yet? Let’s explain in terms of what’s important to HR. Most HR groups would probably prefer to incorrectly classify folks not looking to quit as high potential of quiting rather than classify those that are likely to quit as not at risk. Because it’s important to not miss at risk employees, HR will really care about recall or when the actual value is Attrition = YES how often the model predicts YES. Recall for our model is 62%. In an HR context, this is 62% more employees that could potentially be targeted prior to quiting. From that standpoint, an organization that loses 100 people per year could possibly target 62 implementing measures to retain. 1.6 The lime package We have a very good model that is capable of making very accurate predictions on unseen data, but what can it tell us about what causes attrition? Let’s find out using LIME. 1.6.1 Set up The lime package implements LIME in R. One thing to note is that it’s not setup out-of-the-box to work with h2o. The good news is with a few functions we can get everything working properly. We’ll need to make two custom functions: model_type: Used to tell lime what type of model we are dealing with. It could be classification, regression, survival, etc. predict_model: Used to allow lime to perform predictions that its algorithm can interpret. The first thing we need to do is identify the class of our model leader object. We do this with the class() function. class(automl_leader) #&gt; [1] &quot;H2OBinomialModel&quot; #&gt; attr(,&quot;package&quot;) #&gt; [1] &quot;h2o&quot; Next we create our model_type function. It’s only input is x the h2o model. The function simply returns “classification”, which tells LIME we are classifying. # Setup lime::model_type() function for h2o model_type.H2OBinomialModel &lt;- function(x, ...) { # Function tells lime() what model type we are dealing with # &#39;classification&#39;, &#39;regression&#39;, &#39;survival&#39;, &#39;clustering&#39;, &#39;multilabel&#39;, etc # # x is our h2o model return(&quot;classification&quot;) } Now we can create our predict_model function. The trick here is to realize that it’s inputs must be x a model, newdata a dataframe object (this is important), and type which is not used but can be use to switch the output type. The output is also a little tricky because it must be in the format of probabilities by classification (this is important; shown next). Internally we just call the h2o.predict() function. # Setup lime::predict_model() function for h2o predict_model.H2OBinomialModel &lt;- function(x, newdata, type, ...) { # Function performs prediction and returns dataframe with Response # # x is h2o model # newdata is data frame # type is only setup for data frame pred &lt;- h2o.predict(x, as.h2o(newdata)) # return probs return(as.data.frame(pred[,-1])) } Run this next script to show you what the output looks like and to test our predict_model function. See how it’s the probabilities by classification. It must be in this form for model_type = “classification”. # Test our predict_model() function predict_model(x = automl_leader, newdata = as.data.frame(test_h2o[,-1]), type = &#39;raw&#39;) %&gt;% tibble::as_tibble() #&gt; # A tibble: 211 x 2 #&gt; No Yes #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.849 0.151 #&gt; 2 0.953 0.0469 #&gt; 3 0.0358 0.964 #&gt; 4 0.960 0.0399 #&gt; 5 0.895 0.105 #&gt; 6 0.966 0.0344 #&gt; # … with 205 more rows Now the fun part, we create an explainer using the lime() function. Just pass the training data set without the “Attribution column”. The form must be a data frame, which is OK since our predict_model function will switch it to an h2o object. Setmodel = automl_leader our leader model, and bin_continuous = FALSE. We could tell the algorithm to bin continuous variables, but this may not make sense for categorical numeric data that we didn’t change to factors. # Run lime() on training set explainer &lt;- lime::lime( as.data.frame(train_h2o[,-1]), model = automl_leader, bin_continuous = FALSE) #&gt; Warning: Data contains numeric columns with zero variance Now we run the explain() function, which returns our explanation. This can take a minute to run so we limit it to just the first ten rows of the test data set. We set n_labels = 1 because we care about explaining a single class. Setting n_features = 4 returns the top four features that are critical to each case. Finally, setting kernel_width = 0.5 allows us to increase the “model_r2” value by shrinking the localized evaluation. # Run explain() on explainer explanation &lt;- lime::explain( as.data.frame(test_h2o[1:10,-1]), explainer = explainer, n_labels = 1, n_features = 4, kernel_width = 0.5) 1.7 Feature Importance Visualization The payoff for the work we put in using LIME is this feature importance plot. This allows us to visualize each of the ten cases (observations) from the test data. The top four features for each case are shown. Note that they are not the same for each case. The green bars mean that the feature supports the model conclusion, and the red bars contradict. We’ll focus in on Cases with Label = Yes, which are predicted to have attrition. We can see a common theme with Case 3 and Case 7: Training Time, Job Role, and Over Time are among the top factors influencing attrition. These are only two cases, but they can be used to potentially generalize to the larger population as we will see next. plot_features(explanation) + labs(title = &quot;HR Predictive Analytics: LIME Feature Importance Visualization&quot;, subtitle = &quot;Hold Out (Test) Set, First 10 Cases Shown&quot;) 1.7.1 What features are linked to employee attrition Now we turn to our three critical features from the LIME Feature Importance Plot: Training Time Job Role Over Time We’ll subset this data and visualize to detect trends. # Focus on critical features of attrition attrition_critical_features &lt;- hr_data %&gt;% tibble::as_tibble() %&gt;% select(Attrition, TrainingTimesLastYear, JobRole, OverTime) %&gt;% rowid_to_column(var = &quot;Case&quot;) attrition_critical_features #&gt; # A tibble: 1,470 x 5 #&gt; Case Attrition TrainingTimesLastYear JobRole OverTime #&gt; &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; #&gt; 1 1 Yes 0 Sales Executive Yes #&gt; 2 2 No 3 Research Scientist No #&gt; 3 3 Yes 3 Laboratory Technician Yes #&gt; 4 4 No 3 Research Scientist Yes #&gt; 5 5 No 3 Laboratory Technician No #&gt; 6 6 No 2 Laboratory Technician No #&gt; # … with 1,464 more rows 1.7.2 Training From the violin plot, the employees that stay tend to have a large peaks at two and three trainings per year whereas the employees that leave tend to have a large peak at two trainings per year. This suggests that employees with more trainings may be less likely to leave. ggplot(attrition_critical_features, aes(x = Attrition, y = TrainingTimesLastYear)) + geom_violin() + geom_jitter(alpha = 0.25) attrition_critical_features %&gt;% ggplot(aes(Attrition, TrainingTimesLastYear)) + geom_jitter(alpha = 0.5, fill = palette_light()[[1]]) + geom_violin(alpha = 0.7, fill = palette_light()[[1]]) + theme_tq() + labs( title = &quot;Prevalance of Training is Lower in Attrition = Yes&quot;, subtitle = &quot;Suggests that increased training is related to lower attrition&quot; ) 1.7.3 Overtime The plot below shows a very interesting relationship: a very high proportion of employees that turnover are working over time. The opposite is true for employees that stay. attrition_critical_features %&gt;% mutate(OverTime = case_when( OverTime == &quot;Yes&quot; ~ 1, OverTime == &quot;No&quot; ~ 0 )) %&gt;% ggplot(aes(Attrition, OverTime)) + geom_jitter(alpha = 0.5, fill = palette_light()[[1]]) + geom_violin(alpha = 0.7, fill = palette_light()[[1]]) + theme_tq() + labs( title = &quot;Prevalance of Over Time is Higher in Attrition = Yes&quot;, subtitle = &quot;Suggests that increased overtime is related to higher attrition&quot;) ggplot(attrition_critical_features, aes(x = Attrition, y = OverTime, )) + # geom_violin(aes(y = ..prop.., group = 1)) + geom_jitter(alpha = 0.5) 1.7.4 Job Role Several job roles are experiencing more turnover. Sales reps have the highest turnover at about 40% followed by Lab Technician, Human Resources, Sales Executive, and Research Scientist. It may be worthwhile to investigate what localized issues could be creating the high turnover among these groups within the organization. p &lt;- ggplot(data = subset(attrition_critical_features, Attrition == &quot;Yes&quot;), mapping = aes(x = JobRole)) p + geom_bar(mapping = aes(y = ..prop.., group = 1)) + coord_flip() # geom_bar(mapping = aes(y = ..prop.., group = 1)) p &lt;- ggplot(data = attrition_critical_features, mapping = aes(x = JobRole)) p + geom_bar(mapping = aes(y = ..prop.., group = 1)) + coord_flip() + facet_wrap(Attrition ~ .) attrition_critical_features %&gt;% group_by(JobRole, Attrition) %&gt;% summarize(total = n()) #&gt; # A tibble: 18 x 3 #&gt; # Groups: JobRole [9] #&gt; JobRole Attrition total #&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 Healthcare Representative No 122 #&gt; 2 Healthcare Representative Yes 9 #&gt; 3 Human Resources No 40 #&gt; 4 Human Resources Yes 12 #&gt; 5 Laboratory Technician No 197 #&gt; 6 Laboratory Technician Yes 62 #&gt; # … with 12 more rows attrition_critical_features %&gt;% group_by(JobRole, Attrition) %&gt;% summarize(total = n()) %&gt;% spread(key = Attrition, value = total) %&gt;% mutate(pct_attrition = Yes / (Yes + No)) #&gt; # A tibble: 9 x 4 #&gt; # Groups: JobRole [9] #&gt; JobRole No Yes pct_attrition #&gt; &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 Healthcare Representative 122 9 0.0687 #&gt; 2 Human Resources 40 12 0.231 #&gt; 3 Laboratory Technician 197 62 0.239 #&gt; 4 Manager 97 5 0.0490 #&gt; 5 Manufacturing Director 135 10 0.0690 #&gt; 6 Research Director 78 2 0.025 #&gt; # … with 3 more rows attrition_critical_features %&gt;% group_by(JobRole, Attrition) %&gt;% summarize(total = n()) %&gt;% spread(key = Attrition, value = total) %&gt;% mutate(pct_attrition = Yes / (Yes + No)) %&gt;% ggplot(aes(x = forcats::fct_reorder(JobRole, pct_attrition), y = pct_attrition)) + geom_bar(stat = &quot;identity&quot;, alpha = 1, fill = palette_light()[[1]]) + expand_limits(y = c(0, 1)) + coord_flip() + theme_tq() + labs( title = &quot;Attrition Varies By Job Role&quot;, subtitle = &quot;Sales Rep, Lab Tech, HR, Sales Exec, and Research Scientist have much higher turnover&quot;, y = &quot;Attrition Percentage (Yes / Total)&quot;, x = &quot;JobRole&quot; ) 1.8 Conclusions There’s a lot to take away from this article. We showed how you can use predictive analytics to develop sophisticated models that very accurately detect employees that are at risk of turnover. The autoML algorithm from H2O.ai worked well for classifying attrition with an accuracy around 87% on unseen / unmodeled data. We then used LIME to breakdown the complex ensemble model returned from H2O into critical features that are related to attrition. Overall, this is a really useful example where we can see how machine learning and data science can be used in business applications. "],
["dealing-with-unbalanced-data.html", "Chapter 2 Dealing with unbalanced data 2.1 Breast cancer dataset 2.2 Introduction 2.3 Read and process the data 2.4 Under-sampling 2.5 Oversampling 2.6 Predictions 2.7 Final notes", " Chapter 2 Dealing with unbalanced data 2.1 Breast cancer dataset 2.2 Introduction Source: https://shiring.github.io/machine_learning/2017/04/02/unbalanced library(caret) #&gt; Loading required package: lattice #&gt; Loading required package: ggplot2 #&gt; Registered S3 methods overwritten by &#39;ggplot2&#39;: #&gt; method from #&gt; [.quosures rlang #&gt; c.quosures rlang #&gt; print.quosures rlang library(mice) #&gt; #&gt; Attaching package: &#39;mice&#39; #&gt; The following objects are masked from &#39;package:base&#39;: #&gt; #&gt; cbind, rbind library(ggplot2) In my last post, where I shared the code that I used to produce an example analysis to go along with my webinar on building meaningful models for disease prediction, I mentioned that it is advised to consider over- or under-sampling when you have unbalanced data sets. Because my focus in this webinar was on evaluating model performance, I did not want to add an additional layer of complexity and therefore did not further discuss how to specifically deal with unbalanced data. But because I had gotten a few questions regarding this, I thought it would be worthwhile to explain over- and under-sampling techniques in more detail and show how you can very easily implement them with caret. 2.3 Read and process the data bc_data &lt;- read.table(file.path(data_raw_dir, &quot;breast-cancer-wisconsin.data&quot;), header = FALSE, sep = &quot;,&quot;) colnames(bc_data) &lt;- c(&quot;sample_code_number&quot;, &quot;clump_thickness&quot;, &quot;uniformity_of_cell_size&quot;, &quot;uniformity_of_cell_shape&quot;, &quot;marginal_adhesion&quot;, &quot;single_epithelial_cell_size&quot;, &quot;bare_nuclei&quot;, &quot;bland_chromatin&quot;, &quot;normal_nucleoli&quot;, &quot;mitosis&quot;, &quot;classes&quot;) bc_data$classes &lt;- ifelse(bc_data$classes == &quot;2&quot;, &quot;benign&quot;, ifelse(bc_data$classes == &quot;4&quot;, &quot;malignant&quot;, NA)) bc_data[bc_data == &quot;?&quot;] &lt;- NA # how many NAs are in the data length(which(is.na(bc_data))) #&gt; [1] 16 # impute missing data # skip columns: sample_code_number and classes bc_data[,2:10] &lt;- apply(bc_data[, 2:10], 2, function(x) as.numeric(as.character(x))) # impute but stay mute dataset_impute &lt;- mice(bc_data[, 2:10], print = FALSE) # bind &quot;classes&quot; with the rest. skip &quot;sample_code_number&quot; bc_data &lt;- cbind(bc_data[, 11, drop = FALSE], mice::complete(dataset_impute, action = 1)) bc_data$classes &lt;- as.factor(bc_data$classes) 2.3.1 Unbalanced data In this context, unbalanced data refers to classification problems where we have unequal instances for different classes. Having unbalanced data is actually very common in general, but it is especially prevalent when working with disease data where we usually have more healthy control samples than disease cases. Even more extreme unbalance is seen with fraud detection, where e.g. most credit card uses are okay and only very few will be fraudulent. In the example I used for my webinar, a breast cancer dataset, we had about twice as many benign than malignant samples. # how many benign and malignant cases are there? summary(bc_data$classes) #&gt; benign malignant #&gt; 458 241 2.3.1.1 Why is unbalanced data a problem in machine learning? Most machine learning classification algorithms are sensitive to unbalance in the predictor classes. Let’s consider an even more extreme example than our breast cancer dataset: assume we had 10 malignant vs 90 benign samples. A machine learning model that has been trained and tested on such a dataset could now predict “benign” for all samples and still gain a very high accuracy. An unbalanced dataset will bias the prediction model towards the more common class! 2.3.1.2 How to balance data for modeling The basic theoretical concepts behind over- and under-sampling are very simple: With under-sampling, we randomly select a subset of samples from the class with more instances to match the number of samples coming from each class. In our example, we would randomly pick 241 out of the 458 benign cases. The main disadvantage of under-sampling is that we lose potentially relevant information from the left-out samples. With oversampling, we randomly duplicate samples from the class with fewer instances or we generate additional instances based on the data that we have, so as to match the number of samples in each class. While we avoid losing information with this approach, we also run the risk of overfitting our model as we are more likely to get the same samples in the training and in the test data, i.e. the test data is no longer independent from training data. This would lead to an overestimation of our model’s performance and generalizability. In reality though, we should not simply perform over- or under-sampling on our training data and then run the model. We need to account for cross-validation and perform over- or under-sampling on each fold independently to get an honest estimate of model performance! 2.3.1.3 Modeling the original unbalanced data Here is the same model I used in my webinar example: I randomly divide the data into training and test sets (stratified by class) and perform Random Forest modeling with 10 x 10 repeated cross-validation. Final model performance is then measured on the test set. set.seed(42) index &lt;- createDataPartition(bc_data$classes, p = 0.7, list = FALSE) train_data &lt;- bc_data[index, ] test_data &lt;- bc_data[-index, ] set.seed(42) model_rf &lt;- caret::train(classes ~ ., data = train_data, method = &quot;rf&quot;, preProcess = c(&quot;scale&quot;, &quot;center&quot;), trControl = trainControl(method = &quot;repeatedcv&quot;, number = 10, repeats = 10, verboseIter = FALSE)) final &lt;- data.frame(actual = test_data$classes, predict(model_rf, newdata = test_data, type = &quot;prob&quot;)) final$predict &lt;- ifelse(final$benign &gt; 0.5, &quot;benign&quot;, &quot;malignant&quot;) final_predict &lt;- as.factor(final$predict) test_data_classes &lt;- as.factor(test_data$classes) cm_original &lt;- confusionMatrix(final_predict, test_data_classes) cm_original$byClass[&#39;Sensitivity&#39;] #&gt; Sensitivity #&gt; 0.978 2.4 Under-sampling Luckily, caret makes it very easy to incorporate over- and under-sampling techniques with cross-validation resampling. We can simply add the sampling option to our trainControl and choose down for under- (also called down-) sampling. The rest stays the same as with our original model. set.seed(42) ctrl &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 10, repeats = 10, verboseIter = FALSE, sampling = &quot;down&quot;) model_rf_under &lt;- caret::train(classes ~ ., data = train_data, method = &quot;rf&quot;, preProcess = c(&quot;scale&quot;, &quot;center&quot;), trControl = ctrl) final_under &lt;- data.frame(actual = test_data$classes, predict(model_rf_under, newdata = test_data, type = &quot;prob&quot;)) final_under$predict &lt;- ifelse(final_under$benign &gt; 0.5, &quot;benign&quot;, &quot;malignant&quot;) final_under_predict &lt;- as.factor(final_under$predict) test_data_classes &lt;- test_data$classes cm_under &lt;- confusionMatrix(final_under_predict, test_data_classes) cm_under$byClass[&#39;Sensitivity&#39;] #&gt; Sensitivity #&gt; 0.978 2.5 Oversampling For over- (also called up-) sampling we simply specify sampling = “up”. set.seed(42) ctrl &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 10, repeats = 10, verboseIter = FALSE, sampling = &quot;up&quot;) model_rf_over &lt;- caret::train(classes ~ ., data = train_data, method = &quot;rf&quot;, preProcess = c(&quot;scale&quot;, &quot;center&quot;), trControl = ctrl) final_over &lt;- data.frame(actual = test_data$classes, predict(model_rf_over, newdata = test_data, type = &quot;prob&quot;)) final_over$predict &lt;- ifelse(final_over$benign &gt; 0.5, &quot;benign&quot;, &quot;malignant&quot;) final_over_predict &lt;- as.factor(final_over$predict) test_data_classes &lt;- test_data$classes cm_over &lt;- confusionMatrix(final_over_predict, test_data_classes) cm_over$byClass[&#39;Sensitivity&#39;] #&gt; Sensitivity #&gt; 0.978 2.5.1 ROSE Besides over- and under-sampling, there are hybrid methods that combine under-sampling with the generation of additional data. Two of the most popular are ROSE and SMOTE. From Nicola Lunardon, Giovanna Menardi and Nicola Torelli’s “ROSE: A Package for Binary Imbalanced Learning” (R Journal, 2014, Vol. 6 Issue 1, p. 79): “The ROSE package provides functions to deal with binary classification problems in the presence of imbalanced classes. Artificial balanced samples are generated according to a smoothed bootstrap approach and allow for aiding both the phases of estimation and accuracy evaluation of a binary classifier in the presence of a rare class. Functions that implement more traditional remedies for the class imbalance and different metrics to evaluate accuracy are also provided. These are estimated by holdout, bootstrap, or cross-validation methods.” You implement them the same way as before, this time choosing sampling = “rose”… set.seed(42) ctrl &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 10, repeats = 10, verboseIter = FALSE, sampling = &quot;rose&quot;) model_rf_rose &lt;- caret::train(classes ~ ., data = train_data, method = &quot;rf&quot;, preProcess = c(&quot;scale&quot;, &quot;center&quot;), trControl = ctrl) #&gt; Loaded ROSE 0.0-3 final_rose &lt;- data.frame(actual = test_data$classes, predict(model_rf_rose, newdata = test_data, type = &quot;prob&quot;)) final_rose$predict &lt;- ifelse(final_rose$benign &gt; 0.5, &quot;benign&quot;, &quot;malignant&quot;) cm_rose &lt;- confusionMatrix(as.factor(final_rose$predict), as.factor(test_data$classes)) cm_rose$byClass[&#39;Sensitivity&#39;] #&gt; Sensitivity #&gt; 0.985 2.5.2 SMOTE … or by choosing sampling = “smote” in the trainControl settings. From Nitesh V. Chawla, Kevin W. Bowyer, Lawrence O. Hall and W. Philip Kegelmeyer’s “SMOTE: Synthetic Minority Over-sampling Technique” (Journal of Artificial Intelligence Research, 2002, Vol. 16, pp. 321–357): “This paper shows that a combination of our method of over-sampling the minority (abnormal) class and under-sampling the majority (normal) class can achieve better classifier performance (in ROC space) than only under-sampling the majority class. This paper also shows that a combination of our method of over-sampling the minority class and under-sampling the majority class can achieve better classifier performance (in ROC space) than varying the loss ratios in Ripper or class priors in Naive Bayes. Our method of over-sampling the minority class involves creating synthetic minority class examples.” set.seed(42) ctrl &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 10, repeats = 10, verboseIter = FALSE, sampling = &quot;smote&quot;) model_rf_smote &lt;- caret::train(classes ~ ., data = train_data, method = &quot;rf&quot;, preProcess = c(&quot;scale&quot;, &quot;center&quot;), trControl = ctrl) #&gt; Loading required package: grid #&gt; Registered S3 method overwritten by &#39;xts&#39;: #&gt; method from #&gt; as.zoo.xts zoo #&gt; Registered S3 method overwritten by &#39;quantmod&#39;: #&gt; method from #&gt; as.zoo.data.frame zoo final_smote &lt;- data.frame(actual = test_data$classes, predict(model_rf_smote, newdata = test_data, type = &quot;prob&quot;)) final_smote$predict &lt;- ifelse(final_smote$benign &gt; 0.5, &quot;benign&quot;, &quot;malignant&quot;) cm_smote &lt;- confusionMatrix(as.factor(final_smote$predict), as.factor(test_data$classes)) cm_smote$byClass[&#39;Sensitivity&#39;] #&gt; Sensitivity #&gt; 0.978 2.6 Predictions Now let’s compare the predictions of all these models: models &lt;- list( original = model_rf, under = model_rf_under, over = model_rf_over, smote = model_rf_smote, rose = model_rf_rose) resampling &lt;- resamples(models) bwplot(resampling) library(dplyr) #&gt; #&gt; Attaching package: &#39;dplyr&#39; #&gt; The following objects are masked from &#39;package:stats&#39;: #&gt; #&gt; filter, lag #&gt; The following objects are masked from &#39;package:base&#39;: #&gt; #&gt; intersect, setdiff, setequal, union comparison &lt;- data.frame(model = names(models), Sensitivity = rep(NA, length(models)), Specificity = rep(NA, length(models)), Precision = rep(NA, length(models)), Recall = rep(NA, length(models)), F1 = rep(NA, length(models))) for (name in names(models)) { cm_model &lt;- get(paste0(&quot;cm_&quot;, name)) comparison[comparison$model==name, ] &lt;- filter(comparison, model==name) %&gt;% mutate(Sensitivity = cm_model$byClass[&quot;Sensitivity&quot;], Specificity = cm_model$byClass[&quot;Specificity&quot;], Precision = cm_model$byClass[&quot;Precision&quot;], Recall = cm_model$byClass[&quot;Recall&quot;], F1 = cm_model$byClass[&quot;F1&quot;] ) } print(comparison) #&gt; model Sensitivity Specificity Precision Recall F1 #&gt; 1 original 0.978 0.986 0.993 0.978 0.985 #&gt; 2 under 0.978 1.000 1.000 0.978 0.989 #&gt; 3 over 0.978 0.986 0.993 0.978 0.985 #&gt; 4 smote 0.978 0.986 0.993 0.978 0.985 #&gt; 5 rose 0.985 0.986 0.993 0.985 0.989 library(tidyr) #&gt; #&gt; Attaching package: &#39;tidyr&#39; #&gt; The following object is masked from &#39;package:mice&#39;: #&gt; #&gt; complete comparison %&gt;% gather(x, y, Sensitivity:F1) %&gt;% ggplot(aes(x = x, y = y, color = model)) + geom_jitter(width = 0.2, alpha = 0.5, size = 3) With this small dataset, we can already see how the different techniques can influence model performance. Sensitivity (or recall) describes the proportion of benign cases that have been predicted correctly, while specificity describes the proportion of malignant cases that have been predicted correctly. Precision describes the true positives, i.e. the proportion of benign predictions that were actual from benign samples. F1 is the weighted average of precision and sensitivity/ recall. 2.7 Final notes Here, all four methods improved specificity and precision compared to the original model. Under-sampling, over-sampling and ROSE additionally improved precision and the F1 score. This post shows a simple example of how to correct for unbalance in datasets for machine learning. For more advanced instructions and potential caveats with these techniques, check out the excellent caret documentation. If you are interested in more machine learning posts, check out the category listing for machine_learning on my blog. "],
["ten-different-methods-to-assess-variable-importance.html", "Chapter 3 Ten different methods to assess Variable Importance 3.1 Glaucoma dataset 3.2 Introduction 3.3 1. Boruta 3.4 Variable Importance from Machine Learning Algorithms 3.5 Lasso Regression 3.6 Step wise Forward and Backward Selection 3.7 Relative Importance from Linear Regression 3.8 Recursive Feature Elimination (RFE) 3.9 Genetic Algorithm 3.10 Simulated Annealing 3.11 Information Value and Weights of Evidence 3.12 DALEX Package 3.13 Conclusion", " Chapter 3 Ten different methods to assess Variable Importance 3.1 Glaucoma dataset Source: https://www.machinelearningplus.com/machine-learning/feature-selection/ 3.2 Introduction In real-world datasets, it is fairly common to have columns that are nothing but noise. You are better off getting rid of such variables because of the memory space they occupy, the time and the computational esources it is going to cost, especially in large datasets. Sometimes, you have a variable that makes business sense, but you are not sure if it actually helps in predicting the Y. You also need to consider the fact that, a feature that could be useful in one ML algorithm (say a decision tree) may go underrepresented or unused by another (like a regression model). Having said that, it is still possible that a variable that shows poor signs of helping to explain the response variable (Y), can turn out to be significantly useful in the presence of (or combination with) other predictors. What I mean by that is, a variable might have a low correlation value of (~0.2) with Y. But in the presence of other variables, it can help to explain certain patterns/phenomenon that other variables can’t explain. In such cases, it can be hard to make a call whether to include or exclude such variables. The strategies we are about to discuss can help fix such problems. Not only that, it will also help understand if a particular variable is important or not and how much it is contributing to the model An important caveat. It is always best to have variables that have sound business logic backing the inclusion of a variable and rely solely on variable importance metrics. Alright. Let’s load up the ‘Glaucoma’ dataset where the goal is to predict if a patient has Glaucoma or not based on 63 different physiological measurements. You can directly run the codes or download the dataset here. A lot of interesting examples ahead. Let’s get started. # Load Packages and prepare dataset library(TH.data) #&gt; Loading required package: survival #&gt; Loading required package: MASS #&gt; #&gt; Attaching package: &#39;TH.data&#39; #&gt; The following object is masked from &#39;package:MASS&#39;: #&gt; #&gt; geyser library(caret) #&gt; Loading required package: lattice #&gt; Loading required package: ggplot2 #&gt; Registered S3 methods overwritten by &#39;ggplot2&#39;: #&gt; method from #&gt; [.quosures rlang #&gt; c.quosures rlang #&gt; print.quosures rlang #&gt; #&gt; Attaching package: &#39;caret&#39; #&gt; The following object is masked from &#39;package:survival&#39;: #&gt; #&gt; cluster library(tictoc) data(&quot;GlaucomaM&quot;, package = &quot;TH.data&quot;) trainData &lt;- GlaucomaM head(trainData) #&gt; ag at as an ai eag eat eas ean eai abrg abrt #&gt; 2 2.22 0.354 0.580 0.686 0.601 1.267 0.336 0.346 0.255 0.331 0.479 0.260 #&gt; 43 2.68 0.475 0.672 0.868 0.667 2.053 0.440 0.520 0.639 0.454 1.090 0.377 #&gt; 25 1.98 0.343 0.508 0.624 0.504 1.200 0.299 0.396 0.259 0.246 0.465 0.209 #&gt; 65 1.75 0.269 0.476 0.525 0.476 0.612 0.147 0.017 0.044 0.405 0.170 0.062 #&gt; 70 2.99 0.599 0.686 1.039 0.667 2.513 0.543 0.607 0.871 0.492 1.800 0.431 #&gt; 16 2.92 0.483 0.763 0.901 0.770 2.200 0.462 0.637 0.504 0.597 1.311 0.394 #&gt; abrs abrn abri hic mhcg mhct mhcs mhcn mhci phcg #&gt; 2 0.107 0.014 0.098 0.214 0.111 0.412 0.036 0.105 -0.022 -0.139 #&gt; 43 0.257 0.212 0.245 0.382 0.140 0.338 0.104 0.080 0.109 -0.015 #&gt; 25 0.112 0.041 0.103 0.195 0.062 0.356 0.045 -0.009 -0.048 -0.149 #&gt; 65 0.000 0.000 0.108 -0.030 -0.015 0.074 -0.084 -0.050 0.035 -0.182 #&gt; 70 0.494 0.601 0.274 0.383 0.089 0.233 0.145 0.023 0.007 -0.131 #&gt; 16 0.365 0.251 0.301 0.442 0.128 0.375 0.049 0.111 0.052 -0.088 #&gt; phct phcs phcn phci hvc vbsg vbst vbss vbsn vbsi vasg #&gt; 2 0.242 -0.053 0.010 -0.139 0.613 0.303 0.103 0.088 0.022 0.090 0.062 #&gt; 43 0.296 -0.015 -0.015 0.036 0.382 0.676 0.181 0.186 0.141 0.169 0.029 #&gt; 25 0.206 -0.092 -0.081 -0.149 0.557 0.300 0.084 0.088 0.046 0.082 0.036 #&gt; 65 -0.097 -0.125 -0.138 -0.182 0.373 0.048 0.011 0.000 0.000 0.036 0.070 #&gt; 70 0.163 0.055 -0.131 -0.115 0.405 0.889 0.151 0.253 0.330 0.155 0.020 #&gt; 16 0.281 -0.067 -0.062 -0.088 0.507 0.972 0.213 0.316 0.197 0.246 0.043 #&gt; vast vass vasn vasi vbrg vbrt vbrs vbrn vbri varg vart vars #&gt; 2 0.000 0.011 0.032 0.018 0.075 0.039 0.021 0.002 0.014 0.756 0.009 0.209 #&gt; 43 0.001 0.007 0.011 0.010 0.370 0.127 0.099 0.050 0.093 0.410 0.006 0.105 #&gt; 25 0.002 0.004 0.016 0.013 0.081 0.034 0.019 0.007 0.021 0.565 0.014 0.132 #&gt; 65 0.005 0.030 0.033 0.002 0.005 0.001 0.000 0.000 0.004 0.380 0.032 0.147 #&gt; 70 0.001 0.004 0.008 0.007 0.532 0.103 0.173 0.181 0.075 0.228 0.011 0.026 #&gt; 16 0.001 0.005 0.028 0.009 0.467 0.136 0.148 0.078 0.104 0.540 0.008 0.133 #&gt; varn vari mdg mdt mds mdn mdi tmg tmt tms tmn #&gt; 2 0.298 0.240 0.705 0.637 0.738 0.596 0.691 -0.236 -0.018 -0.230 -0.510 #&gt; 43 0.181 0.117 0.898 0.850 0.907 0.771 0.940 -0.211 -0.014 -0.165 -0.317 #&gt; 25 0.243 0.177 0.687 0.643 0.689 0.684 0.700 -0.185 -0.097 -0.235 -0.337 #&gt; 65 0.151 0.050 0.207 0.171 0.022 0.046 0.221 -0.148 -0.035 -0.449 -0.217 #&gt; 70 0.105 0.087 0.721 0.638 0.730 0.730 0.640 -0.052 -0.105 0.084 -0.012 #&gt; 16 0.232 0.167 0.927 0.842 0.953 0.906 0.898 -0.040 0.087 0.018 -0.094 #&gt; tmi mr rnf mdic emd mv Class #&gt; 2 -0.158 0.841 0.410 0.137 0.239 0.035 normal #&gt; 43 -0.192 0.924 0.256 0.252 0.329 0.022 normal #&gt; 25 -0.020 0.795 0.378 0.152 0.250 0.029 normal #&gt; 65 -0.091 0.746 0.200 0.027 0.078 0.023 normal #&gt; 70 -0.054 0.977 0.193 0.297 0.354 0.034 normal #&gt; 16 -0.051 0.965 0.339 0.333 0.442 0.028 normal 3.3 1. Boruta Boruta is a feature ranking and selection algorithm based on random forests algorithm. The advantage with Boruta is that it clearly decides if a variable is important or not and helps to select variables that are statistically significant. Besides, you can adjust the strictness of the algorithm by adjusting the \\(p\\) values that defaults to 0.01 and the maxRuns. maxRuns is the number of times the algorithm is run. The higher the maxRuns the more selective you get in picking the variables. The default value is 100. In the process of deciding if a feature is important or not, some features may be marked by Boruta as ‘Tentative’. Sometimes increasing the maxRuns can help resolve the ‘Tentativeness’ of the feature. Lets see an example based on the Glaucoma dataset from TH.data package that I created earlier. # install.packages(&#39;Boruta&#39;) library(Boruta) #&gt; Loading required package: ranger The boruta function uses a formula interface just like most predictive modeling functions. So the first argument to boruta() is the formula with the response variable on the left and all the predictors on the right. By placing a dot, all the variables in trainData other than Class will be included in the model. The doTrace argument controls the amount of output printed to the console. Higher the value, more the log details you get. So save space I have set it to 0, but try setting it to 1 and 2 if you are running the code. Finally the output is stored in boruta_output. # Perform Boruta search boruta_output &lt;- Boruta(Class ~ ., data=na.omit(trainData), doTrace=0) Let’s see what the boruta_output contains. names(boruta_output) #&gt; [1] &quot;finalDecision&quot; &quot;ImpHistory&quot; &quot;pValue&quot; &quot;maxRuns&quot; #&gt; [5] &quot;light&quot; &quot;mcAdj&quot; &quot;timeTaken&quot; &quot;roughfixed&quot; #&gt; [9] &quot;call&quot; &quot;impSource&quot; # Get significant variables including tentatives boruta_signif &lt;- getSelectedAttributes(boruta_output, withTentative = TRUE) print(boruta_signif) #&gt; [1] &quot;as&quot; &quot;ai&quot; &quot;eas&quot; &quot;ean&quot; &quot;abrg&quot; &quot;abrs&quot; &quot;abrn&quot; &quot;abri&quot; &quot;hic&quot; &quot;mhcg&quot; #&gt; [11] &quot;mhcs&quot; &quot;mhcn&quot; &quot;mhci&quot; &quot;phcg&quot; &quot;phcn&quot; &quot;phci&quot; &quot;hvc&quot; &quot;vbsg&quot; &quot;vbss&quot; &quot;vbsn&quot; #&gt; [21] &quot;vbsi&quot; &quot;vasg&quot; &quot;vass&quot; &quot;vasi&quot; &quot;vbrg&quot; &quot;vbrs&quot; &quot;vbrn&quot; &quot;vbri&quot; &quot;varg&quot; &quot;vart&quot; #&gt; [31] &quot;vars&quot; &quot;varn&quot; &quot;vari&quot; &quot;mdn&quot; &quot;tmg&quot; &quot;tmt&quot; &quot;tms&quot; &quot;tmi&quot; &quot;mr&quot; &quot;rnf&quot; #&gt; [41] &quot;mdic&quot; &quot;emd&quot; If you are not sure about the tentative variables being selected for granted, you can choose a TentativeRoughFix on boruta_output. # Do a tentative rough fix roughFixMod &lt;- TentativeRoughFix(boruta_output) boruta_signif &lt;- getSelectedAttributes(roughFixMod) print(boruta_signif) #&gt; [1] &quot;as&quot; &quot;ai&quot; &quot;ean&quot; &quot;abrg&quot; &quot;abrs&quot; &quot;abrn&quot; &quot;abri&quot; &quot;hic&quot; &quot;mhcg&quot; &quot;mhcn&quot; #&gt; [11] &quot;mhci&quot; &quot;phcg&quot; &quot;phcn&quot; &quot;phci&quot; &quot;hvc&quot; &quot;vbsn&quot; &quot;vbsi&quot; &quot;vasg&quot; &quot;vass&quot; &quot;vasi&quot; #&gt; [21] &quot;vbrg&quot; &quot;vbrs&quot; &quot;vbrn&quot; &quot;vbri&quot; &quot;varg&quot; &quot;vart&quot; &quot;vars&quot; &quot;varn&quot; &quot;vari&quot; &quot;mdn&quot; #&gt; [31] &quot;tmg&quot; &quot;tms&quot; &quot;tmi&quot; &quot;mr&quot; &quot;rnf&quot; &quot;mdic&quot; There you go. Boruta has decided on the ‘Tentative’ variables on our behalf. Let’s find out the importance scores of these variables. # Variable Importance Scores imps &lt;- attStats(roughFixMod) imps2 = imps[imps$decision != &#39;Rejected&#39;, c(&#39;meanImp&#39;, &#39;decision&#39;)] head(imps2[order(-imps2$meanImp), ]) # descending sort #&gt; meanImp decision #&gt; vari 12.37 Confirmed #&gt; varg 11.74 Confirmed #&gt; vars 10.74 Confirmed #&gt; phci 8.34 Confirmed #&gt; hic 8.21 Confirmed #&gt; varn 7.88 Confirmed Let’s plot it to see the importances of these variables. # Plot variable importance plot(boruta_output, cex.axis=.7, las=2, xlab=&quot;&quot;, main=&quot;Variable Importance&quot;) This plot reveals the importance of each of the features. The columns in green are ‘confirmed’ and the ones in red are not. There are couple of blue bars representing ShadowMax and ShadowMin. They are not actual features, but are used by the boruta algorithm to decide if a variable is important or not. 3.4 Variable Importance from Machine Learning Algorithms Another way to look at feature selection is to consider variables most used by various ML algorithms the most to be important. Depending on how the machine learning algorithm learns the relationship between X’s and Y, different machine learning algorithms may possibly end up using different variables (but mostly common vars) to various degrees. What I mean by that is, the variables that proved useful in a tree-based algorithm like rpart, can turn out to be less useful in a regression-based model. So all variables need not be equally useful to all algorithms. So how do we find the variable importance for a given ML algo? train() the desired model using the caret package. Then, use varImp() to determine the feature importances. You may want to try out multiple algorithms, to get a feel of the usefulness of the features across algos. 3.4.1 rpart # Train an rpart model and compute variable importance. library(caret) set.seed(100) rPartMod &lt;- train(Class ~ ., data=trainData, method=&quot;rpart&quot;) rpartImp &lt;- varImp(rPartMod) print(rpartImp) #&gt; rpart variable importance #&gt; #&gt; only 20 most important variables shown (out of 62) #&gt; #&gt; Overall #&gt; varg 100.0 #&gt; vari 93.2 #&gt; vars 85.2 #&gt; varn 76.9 #&gt; tmi 72.3 #&gt; mhcn 0.0 #&gt; as 0.0 #&gt; phcs 0.0 #&gt; vbst 0.0 #&gt; abrt 0.0 #&gt; vbsg 0.0 #&gt; eai 0.0 #&gt; vbrs 0.0 #&gt; vbsi 0.0 #&gt; eag 0.0 #&gt; tmt 0.0 #&gt; phcn 0.0 #&gt; vart 0.0 #&gt; mds 0.0 #&gt; an 0.0 Only 5 of the 63 features was used by rpart and if you look closely, the 5 variables used here are in the top 6 that boruta selected. Let’s do one more: the variable importances from Regularized Random Forest (RRF) algorithm. 3.4.2 Regularized Random Forest (RRF) tic() # Train an RRF model and compute variable importance. set.seed(100) rrfMod &lt;- train(Class ~ ., data = trainData, method = &quot;RRF&quot;) #&gt; Registered S3 method overwritten by &#39;RRF&#39;: #&gt; method from #&gt; plot.margin randomForest rrfImp &lt;- varImp(rrfMod, scale=F) toc() #&gt; 446.131 sec elapsed rrfImp #&gt; RRF variable importance #&gt; #&gt; only 20 most important variables shown (out of 62) #&gt; #&gt; Overall #&gt; varg 25.07 #&gt; vari 18.78 #&gt; vars 5.29 #&gt; tmi 4.09 #&gt; mhcg 3.25 #&gt; mhci 2.81 #&gt; hic 2.69 #&gt; hvc 2.50 #&gt; mv 2.00 #&gt; vasg 1.99 #&gt; phci 1.77 #&gt; phcn 1.53 #&gt; phct 1.43 #&gt; vass 1.37 #&gt; phcg 1.37 #&gt; tms 1.32 #&gt; tmg 1.16 #&gt; abrs 1.16 #&gt; tmt 1.13 #&gt; mdic 1.13 plot(rrfImp, top = 20, main=&#39;Variable Importance&#39;) The topmost important variables are pretty much from the top tier of Boruta’s selections. Some of the other algorithms available in train() that you can use to compute varImp are the following: ada, AdaBag, AdaBoost.M1, adaboost, bagEarth, bagEarthGCV, bagFDA, bagFDAGCV, bartMachine, blasso, BstLm, bstSm, C5.0, C5.0Cost, C5.0Rules, C5.0Tree, cforest, chaid, ctree, ctree2, cubist, deepboost, earth, enet, evtree, extraTrees, fda, gamboost, gbm_h2o, gbm, gcvEarth, glmnet_h2o, glmnet, glmStepAIC, J48, JRip, lars, lars2, lasso, LMT, LogitBoost, M5, M5Rules, msaenet, nodeHarvest, OneR, ordinalNet, ORFlog, ORFpls, ORFridge, ORFsvm, pam, parRF, PART, penalized, PenalizedLDA, qrf, ranger, Rborist, relaxo, rf, rFerns, rfRules, rotationForest, rotationForestCp, rpart, rpart1SE, rpart2, rpartCost, rpartScore, rqlasso, rqnc, RRF, RRFglobal, sdwd, smda, sparseLDA, spikeslab, wsrf, xgbLinear, xgbTree. 3.5 Lasso Regression Least Absolute Shrinkage and Selection Operator (LASSO) regression is a type of regularization method that penalizes with L1-norm. It basically imposes a cost to having large weights (value of coefficients). And its called L1 regularization, because the cost added, is proportional to the absolute value of weight coefficients. As a result, in the process of shrinking the coefficients, it eventually reduces the coefficients of certain unwanted features all the to zero. That is, it removes the unneeded variables altogether. So effectively, LASSO regression can be considered as a variable selection technique as well. library(glmnet) #&gt; Loading required package: Matrix #&gt; Loading required package: foreach #&gt; Loaded glmnet 2.0-16 # online data # trainData &lt;- read.csv(&#39;https://raw.githubusercontent.com/selva86/datasets/master/GlaucomaM.csv&#39;) trainData &lt;- read.csv(file.path(data_raw_dir, &quot;glaucoma.csv&quot;)) x &lt;- as.matrix(trainData[,-63]) # all X vars y &lt;- as.double(as.matrix(ifelse(trainData[, 63]==&#39;normal&#39;, 0, 1))) # Only Class # Fit the LASSO model (Lasso: Alpha = 1) set.seed(100) cv.lasso &lt;- cv.glmnet(x, y, family=&#39;binomial&#39;, alpha=1, parallel=TRUE, standardize=TRUE, type.measure=&#39;auc&#39;) #&gt; Warning: executing %dopar% sequentially: no parallel backend registered # Results plot(cv.lasso) Let’s see how to interpret this plot. The X axis of the plot is the log of lambda. That means when it is 2 here, the lambda value is actually 100. The numbers at the top of the plot show how many predictors were included in the model. The position of red dots along the Y-axis tells what AUC we got when you include as many variables shown on the top x-axis. You can also see two dashed vertical lines. The first one on the left points to the lambda with the lowest mean squared error. The one on the right point to the number of variables with the highest deviance within 1 standard deviation. The best lambda value is stored inside ‘cv.lasso$lambda.min’. # plot(cv.lasso$glmnet.fit, xvar=&quot;lambda&quot;, label=TRUE) cat(&#39;Min Lambda: &#39;, cv.lasso$lambda.min, &#39;\\n 1Sd Lambda: &#39;, cv.lasso$lambda.1se) #&gt; Min Lambda: 0.0224 #&gt; 1Sd Lambda: 0.144 df_coef &lt;- round(as.matrix(coef(cv.lasso, s=cv.lasso$lambda.min)), 2) # See all contributing variables df_coef[df_coef[, 1] != 0, ] #&gt; (Intercept) as mhci phci hvc vast #&gt; 2.68 -1.59 3.85 5.60 -2.41 -13.90 #&gt; vars vari mdn mdi tmg tms #&gt; -20.18 -1.58 0.50 0.99 0.06 2.56 #&gt; tmi #&gt; 2.23 The above output shows what variables LASSO considered important. A high positive or low negative implies more important is that variable. 3.6 Step wise Forward and Backward Selection Stepwise regression can be used to select features if the Y variable is a numeric variable. It is particularly used in selecting best linear regression models. It searches for the best possible regression model by iteratively selecting and dropping variables to arrive at a model with the lowest possible AIC. It can be implemented using the step() function and you need to provide it with a lower model, which is the base model from which it won’t remove any features and an upper model, which is a full model that has all possible features you want to have. Our case is not so complicated (&lt; 20 vars), so lets just do a simple stepwise in ‘both’ directions. I will use the ozone dataset for this where the objective is to predict the ozone_reading based on other weather related observations. # Load data # online # trainData &lt;- read.csv(&quot;http://rstatistics.net/wp-content/uploads/2015/09/ozone1.csv&quot;, # stringsAsFactors=F) trainData &lt;- read.csv(file.path(data_raw_dir, &quot;ozone1.csv&quot;)) print(head(trainData)) #&gt; Month Day_of_month Day_of_week ozone_reading pressure_height Wind_speed #&gt; 1 1 1 4 3 5480 8 #&gt; 2 1 2 5 3 5660 6 #&gt; 3 1 3 6 3 5710 4 #&gt; 4 1 4 7 5 5700 3 #&gt; 5 1 5 1 5 5760 3 #&gt; 6 1 6 2 6 5720 4 #&gt; Humidity Temperature_Sandburg Temperature_ElMonte Inversion_base_height #&gt; 1 20 40.5 39.8 5000 #&gt; 2 41 38.0 46.7 4109 #&gt; 3 28 40.0 49.5 2693 #&gt; 4 37 45.0 52.3 590 #&gt; 5 51 54.0 45.3 1450 #&gt; 6 69 35.0 49.6 1568 #&gt; Pressure_gradient Inversion_temperature Visibility #&gt; 1 -15 30.6 200 #&gt; 2 -14 48.0 300 #&gt; 3 -25 47.7 250 #&gt; 4 -24 55.0 100 #&gt; 5 25 57.0 60 #&gt; 6 15 53.8 60 The data is ready. Let’s perform the stepwise. # Step 1: Define base intercept only model base.mod &lt;- lm(ozone_reading ~ 1 , data=trainData) # Step 2: Full model with all predictors all.mod &lt;- lm(ozone_reading ~ . , data= trainData) # Step 3: Perform step-wise algorithm. direction=&#39;both&#39; implies both forward and backward stepwise stepMod &lt;- step(base.mod, scope = list(lower = base.mod, upper = all.mod), direction = &quot;both&quot;, trace = 0, steps = 1000) # Step 4: Get the shortlisted variable. shortlistedVars &lt;- names(unlist(stepMod[[1]])) shortlistedVars &lt;- shortlistedVars[!shortlistedVars %in% &quot;(Intercept)&quot;] # remove intercept # Show print(shortlistedVars) #&gt; [1] &quot;Temperature_Sandburg&quot; &quot;Humidity&quot; &quot;Temperature_ElMonte&quot; #&gt; [4] &quot;Month&quot; &quot;pressure_height&quot; &quot;Inversion_base_height&quot; The selected model has the above 6 features in it. But if you have too many features (&gt; 100) in training data, then it might be a good idea to split the dataset into chunks of 10 variables each with Y as mandatory in each dataset. Loop through all the chunks and collect the best features. We are doing it this way because some variables that came as important in a training data with fewer features may not show up in a linear reg model built on lots of features. Finally, from a pool of shortlisted features (from small chunk models), run a full stepwise model to get the final set of selected features. You can take this as a learning assignment to be solved within 20 minutes. 3.7 Relative Importance from Linear Regression This technique is specific to linear regression models. Relative importance can be used to assess which variables contributed how much in explaining the linear model’s R-squared value. So, if you sum up the produced importances, it will add up to the model’s R-sq value. In essence, it is not directly a feature selection method, because you have already provided the features that go in the model. But after building the model, the relaimpo can provide a sense of how important each feature is in contributing to the R-sq, or in other words, in ‘explaining the Y variable’. So, how to calculate relative importance? It is implemented in the relaimpo package. Basically, you build a linear regression model and pass that as the main argument to calc.relimp(). relaimpo has multiple options to compute the relative importance, but the recommended method is to use type='lmg', as I have done below. # install.packages(&#39;relaimpo&#39;) library(relaimpo) #&gt; Loading required package: boot #&gt; #&gt; Attaching package: &#39;boot&#39; #&gt; The following object is masked from &#39;package:lattice&#39;: #&gt; #&gt; melanoma #&gt; The following object is masked from &#39;package:survival&#39;: #&gt; #&gt; aml #&gt; Loading required package: survey #&gt; Loading required package: grid #&gt; #&gt; Attaching package: &#39;survey&#39; #&gt; The following object is masked from &#39;package:graphics&#39;: #&gt; #&gt; dotchart #&gt; Loading required package: mitools #&gt; This is the global version of package relaimpo. #&gt; If you are a non-US user, a version with the interesting additional metric pmvd is available #&gt; from Ulrike Groempings web site at prof.beuth-hochschule.de/groemping. # Build linear regression model model_formula = ozone_reading ~ Temperature_Sandburg + Humidity + Temperature_ElMonte + Month + pressure_height + Inversion_base_height lmMod &lt;- lm(model_formula, data=trainData) # calculate relative importance relImportance &lt;- calc.relimp(lmMod, type = &quot;lmg&quot;, rela = F) # Sort cat(&#39;Relative Importances: \\n&#39;) #&gt; Relative Importances: sort(round(relImportance$lmg, 3), decreasing=TRUE) #&gt; Temperature_ElMonte Temperature_Sandburg pressure_height #&gt; 0.214 0.203 0.104 #&gt; Inversion_base_height Humidity Month #&gt; 0.096 0.086 0.012 Additionally, you can use bootstrapping (using boot.relimp) to compute the confidence intervals of the produced relative importances. bootsub &lt;- boot.relimp(ozone_reading ~ Temperature_Sandburg + Humidity + Temperature_ElMonte + Month + pressure_height + Inversion_base_height, data=trainData, b = 1000, type = &#39;lmg&#39;, rank = TRUE, diff = TRUE) plot(booteval.relimp(bootsub, level=.95)) 3.8 Recursive Feature Elimination (RFE) Recursive feature elimnation (rfe) offers a rigorous way to determine the important variables before you even feed them into a ML algo. It can be implemented using the rfe() from caret package. The rfe() also takes two important parameters. sizes rfeControl So, what does sizes and rfeControl represent? The sizes determines the number of most important features the rfe should iterate. Below, I have set the size as 1 to 5, 10, 15 and 18. Secondly, the rfeControl parameter receives the output of the rfeControl(). You can set what type of variable evaluation algorithm must be used. Here, I have used random forests based rfFuncs. The method='repeatedCV' means it will do a repeated k-Fold cross validation with repeats=5. Once complete, you get the accuracy and kappa for each model size you provided. The final selected model subset size is marked with a * in the rightmost selected column. str(trainData) #&gt; &#39;data.frame&#39;: 366 obs. of 13 variables: #&gt; $ Month : int 1 1 1 1 1 1 1 1 1 1 ... #&gt; $ Day_of_month : int 1 2 3 4 5 6 7 8 9 10 ... #&gt; $ Day_of_week : int 4 5 6 7 1 2 3 4 5 6 ... #&gt; $ ozone_reading : num 3 3 3 5 5 6 4 4 6 7 ... #&gt; $ pressure_height : num 5480 5660 5710 5700 5760 5720 5790 5790 5700 5700 ... #&gt; $ Wind_speed : int 8 6 4 3 3 4 6 3 3 3 ... #&gt; $ Humidity : num 20 41 28 37 51 ... #&gt; $ Temperature_Sandburg : num 40.5 38 40 45 54 ... #&gt; $ Temperature_ElMonte : num 39.8 46.7 49.5 52.3 45.3 ... #&gt; $ Inversion_base_height: num 5000 4109 2693 590 1450 ... #&gt; $ Pressure_gradient : num -15 -14 -25 -24 25 15 -33 -28 23 -2 ... #&gt; $ Inversion_temperature: num 30.6 48 47.7 55 57 ... #&gt; $ Visibility : int 200 300 250 100 60 60 100 250 120 120 ... tic() set.seed(100) options(warn=-1) subsets &lt;- c(1:5, 10, 15, 18) ctrl &lt;- rfeControl(functions = rfFuncs, method = &quot;repeatedcv&quot;, repeats = 5, verbose = FALSE) lmProfile &lt;- rfe(x=trainData[, c(1:3, 5:13)], y=trainData$ozone_reading, sizes = subsets, rfeControl = ctrl) toc() #&gt; 165.249 sec elapsed lmProfile #&gt; #&gt; Recursive feature selection #&gt; #&gt; Outer resampling method: Cross-Validated (10 fold, repeated 5 times) #&gt; #&gt; Resampling performance over subset size: #&gt; #&gt; Variables RMSE Rsquared MAE RMSESD RsquaredSD MAESD Selected #&gt; 1 5.13 0.595 3.92 0.826 0.1275 0.586 #&gt; 2 4.03 0.746 3.11 0.542 0.0743 0.416 #&gt; 3 3.95 0.756 3.06 0.472 0.0670 0.380 #&gt; 4 3.93 0.759 3.01 0.468 0.0683 0.361 #&gt; 5 3.90 0.763 2.98 0.467 0.0659 0.350 #&gt; 10 3.77 0.782 2.85 0.496 0.0734 0.393 * #&gt; 12 3.77 0.781 2.86 0.508 0.0756 0.401 #&gt; #&gt; The top 5 variables (out of 10): #&gt; Temperature_ElMonte, Pressure_gradient, Temperature_Sandburg, Inversion_temperature, Humidity So, it says, Temperature_ElMonte, Pressure_gradient, Temperature_Sandburg, Inversion_temperature, Humidity are the top 5 variables in that order. And the best model size out of the provided models sizes (in subsets) is 10. You can see all of the top 10 variables from ‘lmProfile$optVariables’ that was created using rfe function above. 3.9 Genetic Algorithm You can perform a supervised feature selection with genetic algorithms using the gafs(). This is quite resource expensive so consider that before choosing the number of iterations (iters) and the number of repeats in gafsControl(). tic() # Define control function ga_ctrl &lt;- gafsControl(functions = rfGA, # another option is `caretGA`. method = &quot;cv&quot;, repeats = 3) # Genetic Algorithm feature selection set.seed(100) ga_obj &lt;- gafs(x=trainData[, c(1:3, 5:13)], y=trainData[, 4], iters = 3, # normally much higher (100+) gafsControl = ga_ctrl) toc() #&gt; 1036.57 sec elapsed ga_obj #&gt; #&gt; Genetic Algorithm Feature Selection #&gt; #&gt; 366 samples #&gt; 12 predictors #&gt; #&gt; Maximum generations: 3 #&gt; Population per generation: 50 #&gt; Crossover probability: 0.8 #&gt; Mutation probability: 0.1 #&gt; Elitism: 0 #&gt; #&gt; Internal performance values: RMSE, Rsquared #&gt; Subset selection driven to minimize internal RMSE #&gt; #&gt; External performance values: RMSE, Rsquared, MAE #&gt; Best iteration chose by minimizing external RMSE #&gt; External resampling method: Cross-Validated (10 fold) #&gt; #&gt; During resampling: #&gt; * the top 5 selected variables (out of a possible 12): #&gt; Month (100%), Pressure_gradient (100%), Temperature_ElMonte (100%), Humidity (80%), Visibility (80%) #&gt; * on average, 6.8 variables were selected (min = 5, max = 9) #&gt; #&gt; In the final search using the entire training set: #&gt; * 9 features selected at iteration 2 including: #&gt; Month, Day_of_month, pressure_height, Wind_speed, Humidity ... #&gt; * external performance at this iteration is #&gt; #&gt; RMSE Rsquared MAE #&gt; 3.721 0.788 2.800 # Optimal variables ga_obj$optVariables #&gt; [1] &quot;Month&quot; &quot;Day_of_month&quot; &quot;pressure_height&quot; #&gt; [4] &quot;Wind_speed&quot; &quot;Humidity&quot; &quot;Temperature_ElMonte&quot; #&gt; [7] &quot;Inversion_base_height&quot; &quot;Pressure_gradient&quot; &quot;Inversion_temperature&quot; ‘Month’ ‘Day_of_month’ ‘Wind_speed’ ‘Temperature_ElMonte’ ‘Pressure_gradient’ ‘Visibility’ So the optimal variables according to the genetic algorithms are listed above. But, I wouldn’t use it just yet because, the above variant was tuned for only 3 iterations, which is quite low. I had to set it so low to save computing time. 3.10 Simulated Annealing Simulated annealing is a global search algorithm that allows a suboptimal solution to be accepted in hope that a better solution will show up eventually. It works by making small random changes to an initial solution and sees if the performance improved. The change is accepted if it improves, else it can still be accepted if the difference of performances meet an acceptance criteria. In caret it has been implemented in the safs() which accepts a control parameter that can be set using the safsControl() function. safsControl is similar to other control functions in caret (like you saw in rfe and ga), and additionally it accepts an improve parameter which is the number of iterations it should wait without improvement until the values are reset to previous iteration. tic() # Define control function sa_ctrl &lt;- safsControl(functions = rfSA, method = &quot;repeatedcv&quot;, repeats = 3, improve = 5) # n iterations without improvement before a reset # Genetic Algorithm feature selection set.seed(100) sa_obj &lt;- safs(x=trainData[, c(1:3, 5:13)], y=trainData[, 4], safsControl = sa_ctrl) toc() #&gt; 140.434 sec elapsed sa_obj #&gt; #&gt; Simulated Annealing Feature Selection #&gt; #&gt; 366 samples #&gt; 12 predictors #&gt; #&gt; Maximum search iterations: 10 #&gt; Restart after 5 iterations without improvement (0.3 restarts on average) #&gt; #&gt; Internal performance values: RMSE, Rsquared #&gt; Subset selection driven to minimize internal RMSE #&gt; #&gt; External performance values: RMSE, Rsquared, MAE #&gt; Best iteration chose by minimizing external RMSE #&gt; External resampling method: Cross-Validated (10 fold, repeated 3 times) #&gt; #&gt; During resampling: #&gt; * the top 5 selected variables (out of a possible 12): #&gt; Temperature_Sandburg (80%), Month (66.7%), Pressure_gradient (66.7%), Temperature_ElMonte (63.3%), Visibility (60%) #&gt; * on average, 6.5 variables were selected (min = 3, max = 11) #&gt; #&gt; In the final search using the entire training set: #&gt; * 6 features selected at iteration 9 including: #&gt; Day_of_week, pressure_height, Wind_speed, Humidity, Inversion_base_height ... #&gt; * external performance at this iteration is #&gt; #&gt; RMSE Rsquared MAE #&gt; 4.108 0.743 3.111 # Optimal variables print(sa_obj$optVariables) #&gt; [1] &quot;Day_of_week&quot; &quot;pressure_height&quot; &quot;Wind_speed&quot; #&gt; [4] &quot;Humidity&quot; &quot;Inversion_base_height&quot; &quot;Pressure_gradient&quot; 3.11 Information Value and Weights of Evidence The Information Value can be used to judge how important a given categorical variable is in explaining the binary Y variable. It goes well with logistic regression and other classification models that can model binary variables. Let’s try to find out how important the categorical variables are in predicting if an individual will earn &gt; 50k from the adult.csv dataset. Just run the code below to import the dataset. library(InformationValue) #&gt; #&gt; Attaching package: &#39;InformationValue&#39; #&gt; The following objects are masked from &#39;package:caret&#39;: #&gt; #&gt; confusionMatrix, precision, sensitivity, specificity # online data # inputData &lt;- read.csv(&quot;http://rstatistics.net/wp-content/uploads/2015/09/adult.csv&quot;) inputData &lt;- read.csv(file.path(data_raw_dir, &quot;adult.csv&quot;)) print(head(inputData)) #&gt; AGE WORKCLASS FNLWGT EDUCATION EDUCATIONNUM MARITALSTATUS #&gt; 1 39 State-gov 77516 Bachelors 13 Never-married #&gt; 2 50 Self-emp-not-inc 83311 Bachelors 13 Married-civ-spouse #&gt; 3 38 Private 215646 HS-grad 9 Divorced #&gt; 4 53 Private 234721 11th 7 Married-civ-spouse #&gt; 5 28 Private 338409 Bachelors 13 Married-civ-spouse #&gt; 6 37 Private 284582 Masters 14 Married-civ-spouse #&gt; OCCUPATION RELATIONSHIP RACE SEX CAPITALGAIN CAPITALLOSS #&gt; 1 Adm-clerical Not-in-family White Male 2174 0 #&gt; 2 Exec-managerial Husband White Male 0 0 #&gt; 3 Handlers-cleaners Not-in-family White Male 0 0 #&gt; 4 Handlers-cleaners Husband Black Male 0 0 #&gt; 5 Prof-specialty Wife Black Female 0 0 #&gt; 6 Exec-managerial Wife White Female 0 0 #&gt; HOURSPERWEEK NATIVECOUNTRY ABOVE50K #&gt; 1 40 United-States 0 #&gt; 2 13 United-States 0 #&gt; 3 40 United-States 0 #&gt; 4 40 United-States 0 #&gt; 5 40 Cuba 0 #&gt; 6 40 United-States 0 # Choose Categorical Variables to compute Info Value. cat_vars &lt;- c (&quot;WORKCLASS&quot;, &quot;EDUCATION&quot;, &quot;MARITALSTATUS&quot;, &quot;OCCUPATION&quot;, &quot;RELATIONSHIP&quot;, &quot;RACE&quot;, &quot;SEX&quot;, &quot;NATIVECOUNTRY&quot;) # get all categorical variables factor_vars &lt;- cat_vars # Init Output df_iv &lt;- data.frame(VARS=cat_vars, IV=numeric(length(cat_vars)), STRENGTH=character(length(cat_vars)), stringsAsFactors = F) # init output dataframe # Get Information Value for each variable for (factor_var in factor_vars){ df_iv[df_iv$VARS == factor_var, &quot;IV&quot;] &lt;- InformationValue::IV(X=inputData[, factor_var], Y=inputData$ABOVE50K) df_iv[df_iv$VARS == factor_var, &quot;STRENGTH&quot;] &lt;- attr(InformationValue::IV(X=inputData[, factor_var], Y=inputData$ABOVE50K), &quot;howgood&quot;) } # Sort df_iv &lt;- df_iv[order(-df_iv$IV), ] df_iv #&gt; VARS IV STRENGTH #&gt; 5 RELATIONSHIP 1.5356 Highly Predictive #&gt; 3 MARITALSTATUS 1.3388 Highly Predictive #&gt; 4 OCCUPATION 0.7762 Highly Predictive #&gt; 2 EDUCATION 0.7411 Highly Predictive #&gt; 7 SEX 0.3033 Highly Predictive #&gt; 1 WORKCLASS 0.1634 Highly Predictive #&gt; 8 NATIVECOUNTRY 0.0794 Somewhat Predictive #&gt; 6 RACE 0.0693 Somewhat Predictive Here is what the quantum of Information Value means: Less than 0.02, then the predictor is not useful for modeling (separating the Goods from the Bads) 0.02 to 0.1, then the predictor has only a weak relationship. 0.1 to 0.3, then the predictor has a medium strength relationship. 0.3 or higher, then the predictor has a strong relationship. That was about IV. Then what is Weight of Evidence? Weights of evidence can be useful to find out how important a given categorical variable is in explaining the ‘events’ (called ‘Goods’ in below table.) The ‘Information Value’ of the categorical variable can then be derived from the respective WOE values. IV=(perc good of all goods−perc bad of all bads) *WOE The ‘WOETable’ below given the computation in more detail. WOETable(X=inputData[, &#39;WORKCLASS&#39;], Y=inputData$ABOVE50K) #&gt; CAT GOODS BADS TOTAL PCT_G PCT_B WOE IV #&gt; 1 ? 191 1645 1836 0.02429 0.066545 -1.008 0.042574 #&gt; 2 Federal-gov 371 589 960 0.04719 0.023827 0.683 0.015964 #&gt; 3 Local-gov 617 1476 2093 0.07848 0.059709 0.273 0.005131 #&gt; 4 Never-worked 7 7 7 0.00089 0.000283 1.146 0.000696 #&gt; 5 Private 4963 17733 22696 0.63126 0.717354 -0.128 0.011006 #&gt; 6 Self-emp-inc 622 494 1116 0.07911 0.019984 1.376 0.081363 #&gt; 7 Self-emp-not-inc 724 1817 2541 0.09209 0.073503 0.225 0.004190 #&gt; 8 State-gov 353 945 1298 0.04490 0.038228 0.161 0.001073 #&gt; 9 Without-pay 14 14 14 0.00178 0.000566 1.146 0.001391 The total IV of a variable is the sum of IV’s of its categories. 3.12 DALEX Package The DALEX is a powerful package that explains various things about the variables used in an ML model. For example, using the variable_dropout() function you can find out how important a variable is based on a dropout loss, that is how much loss is incurred by removing a variable from the model. Apart from this, it also has the single_variable() function that gives you an idea of how the model’s output will change by changing the values of one of the X’s in the model. It also has the single_prediction() that can decompose a single model prediction so as to understand which variable caused what effect in predicting the value of Y. library(randomForest) #&gt; randomForest 4.6-14 #&gt; Type rfNews() to see new features/changes/bug fixes. #&gt; #&gt; Attaching package: &#39;randomForest&#39; #&gt; The following object is masked from &#39;package:dplyr&#39;: #&gt; #&gt; combine #&gt; The following object is masked from &#39;package:ranger&#39;: #&gt; #&gt; importance #&gt; The following object is masked from &#39;package:ggplot2&#39;: #&gt; #&gt; margin library(DALEX) #&gt; Welcome to DALEX (version: 0.3.0). #&gt; This is a plain DALEX. Use &#39;install_dependencies()&#39; to get all required packages. #&gt; #&gt; Attaching package: &#39;DALEX&#39; #&gt; The following object is masked from &#39;package:dplyr&#39;: #&gt; #&gt; explain # Load data # inputData &lt;- read.csv(&quot;http://rstatistics.net/wp-content/uploads/2015/09/adult.csv&quot;) inputData &lt;- read.csv(file.path(data_raw_dir, &quot;adult.csv&quot;)) # Train random forest model rf_mod &lt;- randomForest(factor(ABOVE50K) ~ ., data=inputData, ntree=100) rf_mod #&gt; #&gt; Call: #&gt; randomForest(formula = factor(ABOVE50K) ~ ., data = inputData, ntree = 100) #&gt; Type of random forest: classification #&gt; Number of trees: 100 #&gt; No. of variables tried at each split: 3 #&gt; #&gt; OOB estimate of error rate: 13.6% #&gt; Confusion matrix: #&gt; 0 1 class.error #&gt; 0 23051 1669 0.0675 #&gt; 1 2754 5087 0.3512 # Variable importance with DALEX explained_rf &lt;- explain(rf_mod, data=inputData, y=inputData$ABOVE50K) # Get the variable importances varimps = variable_dropout(explained_rf, type=&#39;raw&#39;) print(varimps) #&gt; variable dropout_loss label #&gt; 1 _full_model_ 31.6 randomForest #&gt; 2 ABOVE50K 31.6 randomForest #&gt; 3 RACE 36.6 randomForest #&gt; 4 SEX 39.4 randomForest #&gt; 5 CAPITALLOSS 39.9 randomForest #&gt; 6 NATIVECOUNTRY 40.3 randomForest #&gt; 7 WORKCLASS 51.0 randomForest #&gt; 8 CAPITALGAIN 53.8 randomForest #&gt; 9 FNLWGT 56.2 randomForest #&gt; 10 HOURSPERWEEK 56.7 randomForest #&gt; 11 EDUCATION 58.0 randomForest #&gt; 12 RELATIONSHIP 58.5 randomForest #&gt; 13 EDUCATIONNUM 59.2 randomForest #&gt; 14 MARITALSTATUS 71.0 randomForest #&gt; 15 OCCUPATION 83.1 randomForest #&gt; 16 AGE 86.8 randomForest #&gt; 17 _baseline_ 304.4 randomForest plot(varimps) 3.13 Conclusion Hope you find these methods useful. As it turns out different methods showed different variables as important, or at least the degree of importance changed. This need not be a conflict, because each method gives a different perspective of how the variable can be useful depending on how the algorithms learn Y ~ x. So its cool. If you find any code breaks or bugs, report the issue here or just write it below. "],
["imputting-missing-values-with-random-forest.html", "Chapter 4 Imputting missing values with Random Forest 4.1 Flu Prediction. fluH7N9_china_2013 dataset 4.2 The data 4.3 Features 4.4 Imputing missing values 4.5 Test, train and validation data sets 4.6 Machine Learning algorithms 4.7 Comparing accuracy of models", " Chapter 4 Imputting missing values with Random Forest 4.1 Flu Prediction. fluH7N9_china_2013 dataset Source: https://shirinsplayground.netlify.com/2018/04/flu_prediction/ library(outbreaks) library(tidyverse) library(plyr) library(mice) library(caret) library(purrr) library(&quot;tibble&quot;) library(&quot;dplyr&quot;) library(&quot;tidyr&quot;) Since I migrated my blog from Github Pages to blogdown and Netlify, I wanted to start migrating (most of) my old posts too - and use that opportunity to update them and make sure the code still works. Here I am updating my very first machine learning post from 27 Nov 2016: Can we predict flu deaths with Machine Learning and R?. Changes are marked as bold comments. The main changes I made are: using the tidyverse more consistently throughout the analysis focusing on comparing multiple imputations from the mice package, rather than comparing different algorithms using purrr, map(), nest() and unnest() to model and predict the machine learning algorithm over the different imputed datasets Among the many nice R packages containing data collections is the outbreaks package. It contains a dataset on epidemics and among them is data from the 2013 outbreak of influenza A H7N9 in China as analysed by Kucharski et al. (2014): A. Kucharski, H. Mills, A. Pinsent, C. Fraser, M. Van Kerkhove, C. A. Donnelly, and S. Riley. 2014. Distinguishing between reservoir exposure and human-to-human transmission for emerging pathogens using case onset data. PLOS Currents Outbreaks. Mar 7, edition 1. doi: 10.1371/currents.outbreaks.e1473d9bfc99d080ca242139a06c455f. A. Kucharski, H. Mills, A. Pinsent, C. Fraser, M. Van Kerkhove, C. A. Donnelly, and S. Riley. 2014. Data from: Distinguishing between reservoir exposure and human-to-human transmission for emerging pathogens using case onset data. Dryad Digital Repository. http://dx.doi.org/10.5061/dryad.2g43n. I will be using their data as an example to show how to use Machine Learning algorithms for predicting disease outcome. 4.2 The data The dataset contains case ID, date of onset, date of hospitalization, date of outcome, gender, age, province and of course outcome: Death or Recovery. 4.2.1 Pre-processing Change: variable names (i.e. column names) have been renamed, dots have been replaced with underscores, letters are all lower case now. Change: I am using the tidyverse notation more consistently. First, I’m doing some preprocessing, including: renaming missing data as NA adding an ID column setting column types gathering date columns changing factor names of dates (to make them look nicer in plots) and of province (to combine provinces with few cases) from1 &lt;- c(&quot;date_of_onset&quot;, &quot;date_of_hospitalisation&quot;, &quot;date_of_outcome&quot;) to1 &lt;- c(&quot;date of onset&quot;, &quot;date of hospitalisation&quot;, &quot;date of outcome&quot;) from2 &lt;- c(&quot;Anhui&quot;, &quot;Beijing&quot;, &quot;Fujian&quot;, &quot;Guangdong&quot;, &quot;Hebei&quot;, &quot;Henan&quot;, &quot;Hunan&quot;, &quot;Jiangxi&quot;, &quot;Shandong&quot;, &quot;Taiwan&quot;) to2 &lt;- rep(&quot;Other&quot;, 10) fluH7N9_china_2013$age[which(fluH7N9_china_2013$age == &quot;?&quot;)] &lt;- NA fluH7N9_china_2013_gather &lt;- fluH7N9_china_2013 %&gt;% mutate(case_id = paste(&quot;case&quot;, case_id, sep = &quot;_&quot;), age = as.numeric(age)) %&gt;% gather(Group, Date, date_of_onset:date_of_outcome) %&gt;% mutate(Group = as.factor(mapvalues(Group, from = from1, to = to1)), province = mapvalues(province, from = from2, to = to2)) fluH7N9_china_2013 &lt;- as.tibble(fluH7N9_china_2013) #&gt; Warning: `as.tibble()` is deprecated, use `as_tibble()` (but mind the new semantics). #&gt; This warning is displayed once per session. fluH7N9_china_2013_gather &lt;- as.tibble(fluH7N9_china_2013_gather) print(fluH7N9_china_2013) #&gt; # A tibble: 136 x 8 #&gt; case_id date_of_onset date_of_hospita… date_of_outcome outcome gender #&gt; &lt;fct&gt; &lt;date&gt; &lt;date&gt; &lt;date&gt; &lt;fct&gt; &lt;fct&gt; #&gt; 1 1 2013-02-19 NA 2013-03-04 Death m #&gt; 2 2 2013-02-27 2013-03-03 2013-03-10 Death m #&gt; 3 3 2013-03-09 2013-03-19 2013-04-09 Death f #&gt; 4 4 2013-03-19 2013-03-27 NA &lt;NA&gt; f #&gt; 5 5 2013-03-19 2013-03-30 2013-05-15 Recover f #&gt; 6 6 2013-03-21 2013-03-28 2013-04-26 Death f #&gt; # … with 130 more rows, and 2 more variables: age &lt;fct&gt;, province &lt;fct&gt; I’m also adding a third gender level for unknown gender levels(fluH7N9_china_2013_gather$gender) &lt;- c(levels(fluH7N9_china_2013_gather$gender), &quot;unknown&quot;) fluH7N9_china_2013_gather$gender[is.na(fluH7N9_china_2013_gather$gender)] &lt;- &quot;unknown&quot; print(fluH7N9_china_2013_gather) #&gt; # A tibble: 408 x 7 #&gt; case_id outcome gender age province Group Date #&gt; &lt;chr&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;date&gt; #&gt; 1 case_1 Death m 58 Shanghai date of onset 2013-02-19 #&gt; 2 case_2 Death m 7 Shanghai date of onset 2013-02-27 #&gt; 3 case_3 Death f 11 Other date of onset 2013-03-09 #&gt; 4 case_4 &lt;NA&gt; f 18 Jiangsu date of onset 2013-03-19 #&gt; 5 case_5 Recover f 20 Jiangsu date of onset 2013-03-19 #&gt; 6 case_6 Death f 9 Jiangsu date of onset 2013-03-21 #&gt; # … with 402 more rows For plotting, I am defining a custom ggplot2 theme: my_theme &lt;- function(base_size = 12, base_family = &quot;sans&quot;){ theme_minimal(base_size = base_size, base_family = base_family) + theme( axis.text = element_text(size = 12), axis.text.x = element_text(angle = 45, vjust = 0.5, hjust = 0.5), axis.title = element_text(size = 14), panel.grid.major = element_line(color = &quot;grey&quot;), panel.grid.minor = element_blank(), panel.background = element_rect(fill = &quot;aliceblue&quot;), strip.background = element_rect(fill = &quot;lightgrey&quot;, color = &quot;grey&quot;, size = 1), strip.text = element_text(face = &quot;bold&quot;, size = 12, color = &quot;black&quot;), legend.position = &quot;bottom&quot;, legend.justification = &quot;top&quot;, legend.box = &quot;horizontal&quot;, legend.box.background = element_rect(colour = &quot;grey50&quot;), legend.background = element_blank(), panel.border = element_rect(color = &quot;grey&quot;, fill = NA, size = 0.5) ) } And use that theme to visualize the data: ggplot(data = fluH7N9_china_2013_gather, aes(x = Date, y = age, fill = outcome)) + stat_density2d(aes(alpha = ..level..), geom = &quot;polygon&quot;) + geom_jitter(aes(color = outcome, shape = gender), size = 1.5) + geom_rug(aes(color = outcome)) + scale_y_continuous(limits = c(0, 90)) + labs( fill = &quot;Outcome&quot;, color = &quot;Outcome&quot;, alpha = &quot;Level&quot;, shape = &quot;Gender&quot;, x = &quot;Date in 2013&quot;, y = &quot;Age&quot;, title = &quot;2013 Influenza A H7N9 cases in China&quot;, subtitle = &quot;Dataset from &#39;outbreaks&#39; package (Kucharski et al. 2014)&quot;, caption = &quot;&quot; ) + facet_grid(Group ~ province) + my_theme() + scale_shape_manual(values = c(15, 16, 17)) + scale_color_brewer(palette=&quot;Set1&quot;, na.value = &quot;grey50&quot;) + scale_fill_brewer(palette=&quot;Set1&quot;) #&gt; Warning: Removed 149 rows containing non-finite values (stat_density2d). #&gt; Warning: Removed 149 rows containing missing values (geom_point). ggplot(data = fluH7N9_china_2013_gather, aes(x = Date, y = age, color = outcome)) + geom_point(aes(color = outcome, shape = gender), size = 1.5, alpha = 0.6) + geom_path(aes(group = case_id)) + facet_wrap( ~ province, ncol = 2) + my_theme() + scale_shape_manual(values = c(15, 16, 17)) + scale_color_brewer(palette=&quot;Set1&quot;, na.value = &quot;grey50&quot;) + scale_fill_brewer(palette=&quot;Set1&quot;) + labs( color = &quot;Outcome&quot;, shape = &quot;Gender&quot;, x = &quot;Date in 2013&quot;, y = &quot;Age&quot;, title = &quot;2013 Influenza A H7N9 cases in China&quot;, subtitle = &quot;Dataset from &#39;outbreaks&#39; package (Kucharski et al. 2014)&quot;, caption = &quot;\\nTime from onset of flu to outcome.&quot; ) #&gt; Warning: Removed 149 rows containing missing values (geom_point). #&gt; Warning: Removed 122 rows containing missing values (geom_path). 4.3 Features In machine learning-speak features are what we call the variables used for model training. Using the right features dramatically influences the accuracy and success of your model. For this example, I am keeping age, but I am also generating new features from the date information and converting gender and province into numerical values. delta_dates &lt;- function(onset, ref) { d2 = as.Date(as.character(onset), format = &quot;%Y-%m-%d&quot;) d1 = as.Date(as.character(ref), format = &quot;%Y-%m-%d&quot;) as.numeric(as.character(gsub(&quot; days&quot;, &quot;&quot;, d1 - d2))) } dataset &lt;- fluH7N9_china_2013 %&gt;% mutate( hospital = as.factor(ifelse(is.na(date_of_hospitalisation), 0, 1)), gender_f = as.factor(ifelse(gender == &quot;f&quot;, 1, 0)), province_Jiangsu = as.factor(ifelse(province == &quot;Jiangsu&quot;, 1, 0)), province_Shanghai = as.factor(ifelse(province == &quot;Shanghai&quot;, 1, 0)), province_Zhejiang = as.factor(ifelse(province == &quot;Zhejiang&quot;, 1, 0)), province_other = as.factor(ifelse(province == &quot;Zhejiang&quot; | province == &quot;Jiangsu&quot; | province == &quot;Shanghai&quot;, 0, 1)), days_onset_to_outcome = delta_dates(date_of_onset, date_of_outcome), days_onset_to_hospital = delta_dates(date_of_onset, date_of_hospitalisation), age = age, early_onset = as.factor(ifelse(date_of_onset &lt; summary(date_of_onset)[[3]], 1, 0)), early_outcome = as.factor(ifelse(date_of_outcome &lt; summary(date_of_outcome)[[3]], 1, 0)) ) %&gt;% subset(select = -c(2:4, 6, 8)) rownames(dataset) &lt;- dataset$case_id #&gt; Warning: Setting row names on a tibble is deprecated. dataset[, -2] &lt;- as.numeric(as.matrix(dataset[, -2])) print(dataset) #&gt; # A tibble: 136 x 13 #&gt; case_id outcome age hospital gender_f province_Jiangsu province_Shangh… #&gt; * &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 Death 87 0 0 0 1 #&gt; 2 2 Death 27 1 0 0 1 #&gt; 3 3 Death 35 1 1 0 0 #&gt; 4 4 &lt;NA&gt; 45 1 1 1 0 #&gt; 5 5 Recover 48 1 1 1 0 #&gt; 6 6 Death 32 1 1 1 0 #&gt; # … with 130 more rows, and 6 more variables: province_Zhejiang &lt;dbl&gt;, #&gt; # province_other &lt;dbl&gt;, days_onset_to_outcome &lt;dbl&gt;, #&gt; # days_onset_to_hospital &lt;dbl&gt;, early_onset &lt;dbl&gt;, early_outcome &lt;dbl&gt; summary(dataset$outcome) #&gt; Death Recover NA&#39;s #&gt; 32 47 57 4.4 Imputing missing values I am using the mice package for imputing missing values Note: Since publishing this blogpost I learned that the idea behind using mice is to compare different imputations to see how stable they are, instead of picking one imputed set as fixed for the remainder of the analysis. Therefore, I changed the focus of this post a little bit: in the old post I compared many different algorithms and their outcome; in this updated version I am only showing the Random Forest algorithm and focus on comparing the different imputed datasets. I am ignoring feature importance and feature plots because nothing changed compared to the old post. # plot the missing data in a matrix by variables md_pattern &lt;- md.pattern(dataset, rotate.names = TRUE) dataset_impute &lt;- mice(data = dataset[, -2], print = FALSE) #&gt; Warning: Number of logged events: 150 4.4.1 Generate a dataframe of five imputting strategies by default, mice() calculates five (m = 5) imputed data sets we can combine them all in one output with the complete(“long”) function I did not want to impute missing values in the outcome column, so I have to merge it back in with the imputed data # c(1,2): case_id, outcome datasets_complete &lt;- right_join(dataset[, c(1, 2)], complete(dataset_impute, &quot;long&quot;), by = &quot;case_id&quot;) %&gt;% mutate(.imp = as.factor(.imp)) %&gt;% select(-.id) %&gt;% print() #&gt; # A tibble: 680 x 14 #&gt; case_id outcome .imp age hospital gender_f province_Jiangsu #&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 Death 1 87 0 0 0 #&gt; 2 2 Death 1 27 1 0 0 #&gt; 3 3 Death 1 35 1 1 0 #&gt; 4 4 &lt;NA&gt; 1 45 1 1 1 #&gt; 5 5 Recover 1 48 1 1 1 #&gt; 6 6 Death 1 32 1 1 1 #&gt; # … with 674 more rows, and 7 more variables: province_Shanghai &lt;dbl&gt;, #&gt; # province_Zhejiang &lt;dbl&gt;, province_other &lt;dbl&gt;, #&gt; # days_onset_to_outcome &lt;dbl&gt;, days_onset_to_hospital &lt;dbl&gt;, #&gt; # early_onset &lt;dbl&gt;, early_outcome &lt;dbl&gt; Let’s compare the distributions of the five different imputed datasets: 4.4.2 plot effect of imputting on features datasets_complete %&gt;% gather(x, y, age:early_outcome) %&gt;% ggplot(aes(x = y, fill = .imp, color = .imp)) + geom_density(alpha = 0.20) + facet_wrap(~ x, ncol = 3, scales = &quot;free&quot;) + scale_fill_brewer(palette=&quot;Set1&quot;, na.value = &quot;grey50&quot;) + scale_color_brewer(palette=&quot;Set1&quot;, na.value = &quot;grey50&quot;) + my_theme() 4.5 Test, train and validation data sets Now, we can go ahead with machine learning! The dataset contains a few missing values in the outcome column; those will be the test set used for final predictions (see the old blog post for this). length(which(is.na(datasets_complete$outcome))) length(which(!is.na(datasets_complete$outcome))) #&gt; [1] 285 #&gt; [1] 395 train_index &lt;- which(is.na(datasets_complete$outcome)) train_data &lt;- datasets_complete[-train_index, ] test_data &lt;- datasets_complete[train_index, -2] # remove variable outcome print(train_data) #&gt; # A tibble: 395 x 14 #&gt; case_id outcome .imp age hospital gender_f province_Jiangsu #&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 Death 1 87 0 0 0 #&gt; 2 2 Death 1 27 1 0 0 #&gt; 3 3 Death 1 35 1 1 0 #&gt; 4 5 Recover 1 48 1 1 1 #&gt; 5 6 Death 1 32 1 1 1 #&gt; 6 7 Death 1 83 1 0 1 #&gt; # … with 389 more rows, and 7 more variables: province_Shanghai &lt;dbl&gt;, #&gt; # province_Zhejiang &lt;dbl&gt;, province_other &lt;dbl&gt;, #&gt; # days_onset_to_outcome &lt;dbl&gt;, days_onset_to_hospital &lt;dbl&gt;, #&gt; # early_onset &lt;dbl&gt;, early_outcome &lt;dbl&gt; # outcome variable removed print(test_data) #&gt; # A tibble: 285 x 13 #&gt; case_id .imp age hospital gender_f province_Jiangsu province_Shangh… #&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 4 1 45 1 1 1 0 #&gt; 2 9 1 67 1 0 0 0 #&gt; 3 15 1 61 0 1 1 0 #&gt; 4 16 1 79 0 0 1 0 #&gt; 5 22 1 85 1 0 1 0 #&gt; 6 28 1 79 0 0 0 0 #&gt; # … with 279 more rows, and 6 more variables: province_Zhejiang &lt;dbl&gt;, #&gt; # province_other &lt;dbl&gt;, days_onset_to_outcome &lt;dbl&gt;, #&gt; # days_onset_to_hospital &lt;dbl&gt;, early_onset &lt;dbl&gt;, early_outcome &lt;dbl&gt; The remainder of the data will be used for modeling. Here, I am splitting the data into 70% training and 30% test data. Because I want to model each imputed dataset separately, I am using the nest() and map() functions. train_data_nest &lt;- train_data %&gt;% group_by(.imp) %&gt;% nest() %&gt;% print() #&gt; # A tibble: 5 x 2 #&gt; .imp data #&gt; &lt;fct&gt; &lt;list&gt; #&gt; 1 1 &lt;tibble [79 × 13]&gt; #&gt; 2 2 &lt;tibble [79 × 13]&gt; #&gt; 3 3 &lt;tibble [79 × 13]&gt; #&gt; 4 4 &lt;tibble [79 × 13]&gt; #&gt; 5 5 &lt;tibble [79 × 13]&gt; # split the training data in validation training and validation test set.seed(42) val_data &lt;- train_data_nest %&gt;% mutate(val_index = map(data, ~ createDataPartition(.$outcome, p = 0.7, list = FALSE)), val_train_data = map2(data, val_index, ~ .x[.y, ]), val_test_data = map2(data, val_index, ~ .x[-.y, ])) %&gt;% print() #&gt; # A tibble: 5 x 5 #&gt; .imp data val_index val_train_data val_test_data #&gt; &lt;fct&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; #&gt; 1 1 &lt;tibble [79 × … &lt;int[,1] [56 × … &lt;tibble [56 × 13… &lt;tibble [23 × 1… #&gt; 2 2 &lt;tibble [79 × … &lt;int[,1] [56 × … &lt;tibble [56 × 13… &lt;tibble [23 × 1… #&gt; 3 3 &lt;tibble [79 × … &lt;int[,1] [56 × … &lt;tibble [56 × 13… &lt;tibble [23 × 1… #&gt; 4 4 &lt;tibble [79 × … &lt;int[,1] [56 × … &lt;tibble [56 × 13… &lt;tibble [23 × 1… #&gt; 5 5 &lt;tibble [79 × … &lt;int[,1] [56 × … &lt;tibble [56 × 13… &lt;tibble [23 × 1… 4.6 Machine Learning algorithms 4.6.1 Random Forest To make the code tidier, I am first defining the modeling function with the parameters I want. model_function &lt;- function(df) { caret::train(outcome ~ ., data = df, method = &quot;rf&quot;, preProcess = c(&quot;scale&quot;, &quot;center&quot;), trControl = trainControl(method = &quot;repeatedcv&quot;, number = 5, repeats = 3, verboseIter = FALSE)) } 4.6.2 Add model and prediction to nested dataframe and calculate Next, I am using the nested tibble from before to map() the model function, predict the outcome and calculate confusion matrices. 4.6.2.1 add model list-column val_data_model &lt;- val_data %&gt;% mutate(model = map(val_train_data, ~ model_function(.x))) %&gt;% select(-val_index) %&gt;% print() #&gt; # A tibble: 5 x 5 #&gt; .imp data val_train_data val_test_data model #&gt; &lt;fct&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; #&gt; 1 1 &lt;tibble [79 × 13]&gt; &lt;tibble [56 × 13]&gt; &lt;tibble [23 × 13]&gt; &lt;train&gt; #&gt; 2 2 &lt;tibble [79 × 13]&gt; &lt;tibble [56 × 13]&gt; &lt;tibble [23 × 13]&gt; &lt;train&gt; #&gt; 3 3 &lt;tibble [79 × 13]&gt; &lt;tibble [56 × 13]&gt; &lt;tibble [23 × 13]&gt; &lt;train&gt; #&gt; 4 4 &lt;tibble [79 × 13]&gt; &lt;tibble [56 × 13]&gt; &lt;tibble [23 × 13]&gt; &lt;train&gt; #&gt; 5 5 &lt;tibble [79 × 13]&gt; &lt;tibble [56 × 13]&gt; &lt;tibble [23 × 13]&gt; &lt;train&gt; 4.6.2.2 add prediction and confusion matrix list-columns set.seed(42) val_data_model &lt;- val_data_model %&gt;% mutate( predict = map2(model, val_test_data, ~ data.frame(prediction = predict(.x, .y[, -2]))), predict_prob = map2(model, val_test_data, ~ data.frame(outcome = .y[, 2], prediction = predict(.x, .y[, -2], type = &quot;prob&quot;))), confusion_matrix = map2(val_test_data, predict, ~ confusionMatrix(.x$outcome, .y$prediction)), confusion_matrix_tbl = map(confusion_matrix, ~ as.tibble(.x$table))) %&gt;% print() #&gt; # A tibble: 5 x 9 #&gt; .imp data val_train_data val_test_data model predict predict_prob #&gt; &lt;fct&gt; &lt;lis&gt; &lt;list&gt; &lt;list&gt; &lt;lis&gt; &lt;list&gt; &lt;list&gt; #&gt; 1 1 &lt;tib… &lt;tibble [56 ×… &lt;tibble [23 … &lt;tra… &lt;df[,1… &lt;df[,3] [23… #&gt; 2 2 &lt;tib… &lt;tibble [56 ×… &lt;tibble [23 … &lt;tra… &lt;df[,1… &lt;df[,3] [23… #&gt; 3 3 &lt;tib… &lt;tibble [56 ×… &lt;tibble [23 … &lt;tra… &lt;df[,1… &lt;df[,3] [23… #&gt; 4 4 &lt;tib… &lt;tibble [56 ×… &lt;tibble [23 … &lt;tra… &lt;df[,1… &lt;df[,3] [23… #&gt; 5 5 &lt;tib… &lt;tibble [56 ×… &lt;tibble [23 … &lt;tra… &lt;df[,1… &lt;df[,3] [23… #&gt; # … with 2 more variables: confusion_matrix &lt;list&gt;, #&gt; # confusion_matrix_tbl &lt;list&gt; Finally, we have a nested dataframe of 5 rows or cases, one per imputting strategy with its corresponding models and prediction results. 4.7 Comparing accuracy of models To compare how the different imputations did, I am plotting the confusion matrices: val_data_model_unnest &lt;- val_data_model %&gt;% unnest(confusion_matrix_tbl) %&gt;% print() #&gt; # A tibble: 20 x 4 #&gt; .imp Prediction Reference n #&gt; &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 1 Death Death 5 #&gt; 2 1 Recover Death 3 #&gt; 3 1 Death Recover 4 #&gt; 4 1 Recover Recover 11 #&gt; 5 2 Death Death 3 #&gt; 6 2 Recover Death 3 #&gt; # … with 14 more rows val_data_model_unnest %&gt;% ggplot(aes(x = Prediction, y = Reference, fill = n)) + facet_wrap(~ .imp, ncol = 5, scales = &quot;free&quot;) + geom_tile() + my_theme() and the prediction probabilities for correct and wrong predictions: val_data_model_gather &lt;- val_data_model %&gt;% unnest(predict_prob) %&gt;% gather(x, y, prediction.Death:prediction.Recover) %&gt;% print() #&gt; # A tibble: 230 x 4 #&gt; .imp outcome x y #&gt; &lt;fct&gt; &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 1 Death prediction.Death 0.758 #&gt; 2 1 Recover prediction.Death 0.864 #&gt; 3 1 Death prediction.Death 0.828 #&gt; 4 1 Death prediction.Death 0.828 #&gt; 5 1 Recover prediction.Death 0.342 #&gt; 6 1 Recover prediction.Death 0.552 #&gt; # … with 224 more rows val_data_model_gather %&gt;% ggplot(aes(x = x, y = y, fill = outcome)) + facet_wrap(~ .imp, ncol = 5) + geom_boxplot() + scale_fill_brewer(palette=&quot;Set1&quot;, na.value = &quot;grey50&quot;) + my_theme() Hope, you found that example interesting and helpful! sessionInfo() #&gt; R version 3.6.0 (2019-04-26) #&gt; Platform: x86_64-pc-linux-gnu (64-bit) #&gt; Running under: Ubuntu 18.04.3 LTS #&gt; #&gt; Matrix products: default #&gt; BLAS/LAPACK: /usr/lib/x86_64-linux-gnu/libopenblasp-r0.2.20.so #&gt; #&gt; locale: #&gt; [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C #&gt; [3] LC_TIME=en_US.UTF-8 LC_COLLATE=en_US.UTF-8 #&gt; [5] LC_MONETARY=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8 #&gt; [7] LC_PAPER=en_US.UTF-8 LC_NAME=C #&gt; [9] LC_ADDRESS=C LC_TELEPHONE=C #&gt; [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C #&gt; #&gt; attached base packages: #&gt; [1] stats graphics grDevices utils datasets methods base #&gt; #&gt; other attached packages: #&gt; [1] caret_6.0-84 mice_3.4.0 lattice_0.20-38 plyr_1.8.4 #&gt; [5] forcats_0.4.0 stringr_1.4.0 dplyr_0.8.0.1 purrr_0.3.2 #&gt; [9] readr_1.3.1 tidyr_0.8.3 tibble_2.1.1 ggplot2_3.1.1 #&gt; [13] tidyverse_1.2.1 outbreaks_1.5.0 logging_0.9-107 #&gt; #&gt; loaded via a namespace (and not attached): #&gt; [1] nlme_3.1-139 lubridate_1.7.4 RColorBrewer_1.1-2 #&gt; [4] httr_1.4.0 rprojroot_1.3-2 tools_3.6.0 #&gt; [7] backports_1.1.4 utf8_1.1.4 R6_2.4.0 #&gt; [10] rpart_4.1-15 lazyeval_0.2.2 colorspace_1.4-1 #&gt; [13] jomo_2.6-7 nnet_7.3-12 withr_2.1.2 #&gt; [16] tidyselect_0.2.5 compiler_3.6.0 cli_1.1.0 #&gt; [19] rvest_0.3.3 xml2_1.2.0 labeling_0.3 #&gt; [22] bookdown_0.10 scales_1.0.0 randomForest_4.6-14 #&gt; [25] digest_0.6.18 minqa_1.2.4 rmarkdown_1.12 #&gt; [28] pkgconfig_2.0.2 htmltools_0.3.6 lme4_1.1-21 #&gt; [31] rlang_0.3.4 readxl_1.3.1 rstudioapi_0.10 #&gt; [34] generics_0.0.2 jsonlite_1.6 ModelMetrics_1.2.2 #&gt; [37] magrittr_1.5 Matrix_1.2-17 Rcpp_1.0.1 #&gt; [40] munsell_0.5.0 fansi_0.4.0 stringi_1.4.3 #&gt; [43] yaml_2.2.0 MASS_7.3-51.4 recipes_0.1.5 #&gt; [46] grid_3.6.0 parallel_3.6.0 mitml_0.3-7 #&gt; [49] crayon_1.3.4 haven_2.1.0 splines_3.6.0 #&gt; [52] hms_0.4.2 zeallot_0.1.0 knitr_1.22 #&gt; [55] pillar_1.4.0 boot_1.3-22 reshape2_1.4.3 #&gt; [58] codetools_0.2-16 stats4_3.6.0 pan_1.6 #&gt; [61] glue_1.3.1 evaluate_0.13 data.table_1.12.2 #&gt; [64] modelr_0.1.4 vctrs_0.1.0 nloptr_1.2.1 #&gt; [67] foreach_1.4.4 cellranger_1.1.0 gtable_0.3.0 #&gt; [70] assertthat_0.2.1 xfun_0.6 gower_0.2.0 #&gt; [73] prodlim_2018.04.18 broom_0.5.2 e1071_1.7-1 #&gt; [76] class_7.3-15 survival_2.44-1.1 timeDate_3043.102 #&gt; [79] iterators_1.0.10 lava_1.6.5 ipred_0.9-9 "]
]
